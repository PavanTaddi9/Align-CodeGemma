{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth vllm  \n",
    "!pip install triton==3.1.0  \n",
    "!pip install -U pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "\n",
    "# Patch the FastLanguageModel to integrate GRPO-specific modifications.\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)\n",
    "\n",
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "\n",
    "# Set maximum sequence length and LoRA rank (controls the adaptation complexity).\n",
    "max_seq_length = 1024  # Increase if you need longer reasoning traces.\n",
    "lora_rank = 64         # Larger rank can improve performance but may slow down training.\n",
    "\n",
    "# Load the Qwen model in 4-bit mode for reduced memory usage and enable fast inference with vLLM.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"google/codegemma-7b-it\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,           # Set to False if using LoRA in 16-bit precision.\n",
    "    fast_inference = True,         # Enable vLLM for faster inference.\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.5,  # Adjust GPU memory usage to avoid out-of-memory errors.\n",
    ")\n",
    "\n",
    "# Wrap the model with PEFT (Parameter-Efficient Fine-Tuning) using LoRA.\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,           # Use a rank greater than 0; common choices include 8, 16, 32, 64, or 128.\n",
    "    lora_alpha = lora_rank,  # A higher lora_alpha value means that the LoRA layers have a greater influence on the model's output, \n",
    "                             # while a lower value reduces this influence\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],                                       # Specify target modules; you can remove QKVO if memory is limited.\n",
    "    use_gradient_checkpointing = \"unsloth\",  # Enable gradient checkpointing for long context finetuning.\n",
    "    random_state = 3407,                     # Set a random seed for reproducibility.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from prompt_template import format_instruction\n",
    "import pandas as pd\n",
    "def load_and_format_json(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list = json.load(f)  \n",
    "    data = [\n",
    "        { \n",
    "            \"prompt\": format_instruction(entry[\"instruction\"]) \n",
    "        }\n",
    "        for entry in data_list\n",
    "    ]\n",
    "    df = pd.DataFrame(data)\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    return hf_dataset\n",
    "dataset = load_and_format_json(\"/Users/pavankumartaddi/Desktop/Align-CodeGemma/outputs/test_meta.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "from openai.types import Completion\n",
    "from execserver.code_exec_reqs import run_coverage_batched\n",
    "from utils import JAX_LAX_OPERATIONS,JAX_LIBRARIES,JAX_PRIMITIVES,count_jax_usage\n",
    "def run_tests_and_reward(completions: List[Completion], timeout=60, tests=\"\", timeout_on_client=False) -> List[int]:\n",
    "    server = \"http://localhost:8000\"\n",
    "    codes = []\n",
    "    for completion in completions:\n",
    "        for choice in completion[\"choices\"]:\n",
    "            codes.append(choice[\"text\"])\n",
    "    coverage_results = run_coverage_batched(server, codes, tests, timeout, timeout_on_client)\n",
    "    rewards = [1 if result and result > 0 else 0 for result in coverage_results]\n",
    "    return rewards\n",
    "def format_reward_func(completions, **kwargs):\n",
    "    pattern = r\"^<response>\\s*<think>.*?</think>\\s*<code>.*?</code>\\s*<test>.*?</test>\\s*</response>$\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, content, re.DOTALL) for content in completion_contents]\n",
    "    return [1.0 if match else 0.0 for match in matches]\n",
    "def reward_based_on_jax_usage(completions: List[Completion]) -> List[float]:\n",
    "    codes = []\n",
    "    for completion in completions:\n",
    "        for choice in completion[\"choices\"]:\n",
    "            codes.append(choice[\"text\"])\n",
    "    max_possible_score = len(JAX_LIBRARIES) + len(JAX_PRIMITIVES) + len(JAX_LAX_OPERATIONS)\n",
    "    rewards = [\n",
    "        count_jax_usage(code) / max_possible_score if max_possible_score > 0 else 0.0\n",
    "        for code in codes\n",
    "    ]\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Configure GRPO training parameters.\n",
    "# This configuration sets up the training hyperparameters, optimization settings, and inference acceleration via vLLM.\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = True,                     # Enable vLLM to accelerate inference during training.\n",
    "    learning_rate = 5e-6,                # Set the learning rate for the optimizer.\n",
    "    adam_beta1 = 0.9,                    # First beta parameter for the AdamW optimizer.\n",
    "    adam_beta2 = 0.99,                   # Second beta parameter for the AdamW optimizer.\n",
    "    weight_decay = 0.1,                  # Weight decay to regularize the model and prevent overfitting.\n",
    "    warmup_ratio = 0.1,                  # Fraction of steps used for learning rate warmup.\n",
    "    lr_scheduler_type = \"cosine\",        # Use cosine annealing for the learning rate scheduler.\n",
    "    optim = \"adamw_8bit\",                # Use 8-bit AdamW optimizer for memory efficiency.\n",
    "    logging_steps = 1,                   # Log training information every step.\n",
    "    bf16 = is_bfloat16_supported(),      # Use bfloat16 precision if supported by the GPU.\n",
    "    fp16 = not is_bfloat16_supported(),  # Otherwise, fall back to fp16 precision.\n",
    "    per_device_train_batch_size = 1,     # Batch size per device during training.\n",
    "    gradient_accumulation_steps = 1,     # Accumulate gradients over this many steps (increase for smoother training if needed).\n",
    "    num_generations = 8,                 # Number of generations per prompt (reduce if memory issues occur).\n",
    "    max_prompt_length = 256,             # Maximum length for the input prompt.\n",
    "    max_completion_length = 200,         # Maximum length for the generated completion.\n",
    "    # num_train_epochs = 1,               # Uncomment this line to run training for one epoch.\n",
    "    max_steps = 250,                     # Maximum number of training steps.\n",
    "    save_steps = 250,                    # Save the model checkpoint every specified number of steps.\n",
    "    max_grad_norm = 0.1,                 # Maximum gradient norm for gradient clipping.\n",
    "    report_to = \"none\",                  # Disable reporting to external services like WandB.\n",
    "    output_dir = \"outputs\",              # Directory to save the training outputs and checkpoints.\n",
    ")\n",
    "\n",
    "# Instantiate the GRPO trainer with the model, tokenizer, reward functions, and training dataset.\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,                       # The language model to be trained.\n",
    "    processing_class = tokenizer,        # The tokenizer used to preprocess the data.\n",
    "    reward_funcs = [\n",
    "        xmlcount_reward_func,            # Reward function based on XML tag counts.\n",
    "        soft_format_reward_func,         # Reward function checking for soft adherence to XML formatting.\n",
    "        strict_format_reward_func,       # Reward function checking for strict XML formatting.\n",
    "        int_reward_func,                 # Reward function that provides rewards for integer outputs.\n",
    "        correctness_reward_func,         # Reward function evaluating the correctness of the answer.\n",
    "    ],\n",
    "    args = training_args,                # GRPO training configuration.\n",
    "    train_dataset = dataset,             # The training dataset containing prompts and expected answers.\n",
    ")\n",
    "\n",
    "# Begin training using the GRPO algorithm.\n",
    "trainer.train()\n",
    "\n",
    "# Save the LoRA-adapted model for later use.\n",
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"How many r's are in strawberry?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "\n",
    "# Set the sampling parameters for text generation.\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "\n",
    "# Generate a response from the model without applying any LoRA adapter.\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,  # No LoRA adapter is used here.\n",
    ")[0].outputs[0].text\n",
    "\n",
    "# Print the generated output.\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"How many r's are in strawberry?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "\n",
    "# Set sampling parameters for controlled text generation.\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "\n",
    "# Generate a response using the saved LoRA adapter.\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),  # Load the saved LoRA adapter.\n",
    ")[0].outputs[0].text\n",
    "\n",
    "# Print the generated response.\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
