{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from prompt_template import format_instruction\n",
    "import pandas as pd\n",
    "def load_and_format_json(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list = json.load(f)  \n",
    "    data = [\n",
    "        { \n",
    "            \"prompt\": format_instruction(entry[\"instruction\"]) \n",
    "        }\n",
    "        for entry in data_list\n",
    "    ]\n",
    "    df = pd.DataFrame(data)\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    return hf_dataset\n",
    "dataset = load_and_format_json(\"/Users/pavankumartaddi/Desktop/Align-CodeGemma/outputs/test_meta.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "from openai.types import Completion\n",
    "from execserver.code_exec_reqs import run_coverage_batched\n",
    "from utils import JAX_LAX_OPERATIONS,JAX_LIBRARIES,JAX_PRIMITIVES,count_jax_usage\n",
    "def run_tests_and_reward(completions: List[Completion], timeout=60, tests=\"\", timeout_on_client=False) -> List[int]:\n",
    "    server = \"http://localhost:8000\"\n",
    "    codes = []\n",
    "    for completion in completions:\n",
    "        for choice in completion[\"choices\"]:\n",
    "            codes.append(choice[\"text\"])\n",
    "    coverage_results = run_coverage_batched(server, codes, tests, timeout, timeout_on_client)\n",
    "    rewards = [1 if result and result > 0 else 0 for result in coverage_results]\n",
    "    return rewards\n",
    "def format_reward_func(completions, **kwargs):\n",
    "    pattern = r\"^<response>\\s*<think>.*?</think>\\s*<code>.*?</code>\\s*<test>.*?</test>\\s*</response>$\"\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, content, re.DOTALL) for content in completion_contents]\n",
    "    return [1.0 if match else 0.0 for match in matches]\n",
    "def reward_based_on_jax_usage(completions: List[Completion]) -> List[float]:\n",
    "    codes = []\n",
    "    for completion in completions:\n",
    "        for choice in completion[\"choices\"]:\n",
    "            codes.append(choice[\"text\"])\n",
    "    max_possible_score = len(JAX_LIBRARIES) + len(JAX_PRIMITIVES) + len(JAX_LAX_OPERATIONS)\n",
    "    rewards = [\n",
    "        count_jax_usage(code) / max_possible_score if max_possible_score > 0 else 0.0\n",
    "        for code in codes\n",
    "    ]\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig\n",
    "\n",
    "# our model we are going to use as policy \n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_peft=True,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"qwen-r1-aha-moment\",\n",
    "    learning_rate=5e-7,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    bf16=True,\n",
    "    # GRPO specific parameters\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=1024, # max length of the generated output for our solution\n",
    "    num_generations=2,\n",
    "    beta=0.001,\n",
    "    \n",
    ")\n",
    "trainer = GRPOTrainer(\n",
    "    model=model_config.model_name_or_path,\n",
    "    reward_funcs=[format_reward_func, equation_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=get_peft_config(model_config),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
