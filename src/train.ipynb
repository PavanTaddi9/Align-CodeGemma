{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth vllm  \n",
    "!pip install triton==3.1.0  \n",
    "!pip install -U pynvml\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/workspace/Align-CodeGemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig\n",
    "\n",
    "# our model we are going to use as policy \n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path=\"google/gemma-3-4b-it\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_peft=True,\n",
    "    load_in_4bit=False,\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"google/gemma-3-4b-it\",\n",
    "    learning_rate=5e-7,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    bf16=True,\n",
    "    # GRPO specific parameters\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=1024, # max length of the generated output for our solution\n",
    "    num_generations=2,\n",
    "    beta=0.001,\n",
    "    \n",
    ")\n",
    "trainer = GRPOTrainer(\n",
    "    model=model_config.model_name_or_path,\n",
    "    reward_funcs=[run_tests_and_reward, reward_based_on_jax_usage],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=get_peft_config(model_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from prompt_template import format_instruction\n",
    "import pandas as pd\n",
    "def load_and_format_json(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_list = json.load(f)  \n",
    "    data = [\n",
    "        { \n",
    "            \"prompt\": format_instruction(entry[\"instruction\"]) \n",
    "        }\n",
    "        for entry in data_list\n",
    "    ]\n",
    "    df = pd.DataFrame(data)\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    return hf_dataset\n",
    "train_dataset = load_and_format_json(\"/Users/pavankumartaddi/Desktop/Align-CodeGemma/datas/train_meta.json\")\n",
    "test_dataset = load_and_format_json(\"/Users/pavankumartaddi/Desktop/Align-CodeGemma/datas/test_meta.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"\\nYou are an expert AI assistant specializing in generating highly efficient, well-structured, and optimized code using JAX.  \\nFollow these principles:\\n1. **Prioritize efficiency**: Use the most optimal algorithms, minimize computational overhead, and leverage JAX's just-in-time (JIT) compilation and automatic differentiation capabilities for performance optimization. Use JAX primitives wherever applicable for better performance, such as `jax.jit`, `jax.grad`, `jax.vmap`, and `jax.pmap`.\\n2. **Leverage JAX and standard libraries**: Use JAX's powerful vectorized operations (`jax.numpy`,jax.lax), automatic differentiation (`jax.grad`), and other built-in JAX functions to avoid unnecessary custom implementations. Take full advantage of JAX primitives for parallelism, batching.\\n3. **Verify with test cases**: Always include test cases with assertions, including edge cases, to validate the correctness of the solution and to ensure robustness. When relevant, leverage JAX's primitives for optimized testing and verification, particularly when dealing with differentiable code.\\n4. **Think step by step**: Analyze the problem carefully. Break it down into smaller, manageable pieces. Identify efficient solutions using appropriate data structures and optimized JAX library functions. Build the solution incrementally, testing each step for correctness and efficiency.\\n\\nWhile it's mandatory to follow a strict format, ensures that the tests cover various edge cases to verify correctness and robustness.\\n\\n\\nInstruction:\\nWrite a function `run_comparison_suite` that takes a list of test cases. Each test case is a tuple containing `(test_name: str, original_func: callable, transformed_func: callable, args: tuple)`. For each test case, execute both `original_func(*args)` and `transformed_func(*args)`, compare their numerical results for closeness (using a fixed tolerance, e.g., 1e-5), and store the boolean comparison result. Return a dictionary mapping each `test_name` to its boolean result (True for close, False for not close).\\n\\nResponse:\\n- The code implementation is as follows:\\nCode:\\n```python\\n# Your optimized code implementation goes here.\\n# Include test cases with assert statements to confirm the correctness of the code.\\n# Example test case format:\\n# assert function_name(input_data) == expected_output\\n```\\n\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from openai.types import Completion\n",
    "from execserver.code_exec_reqs import exec_test_batched\n",
    "import utils\n",
    "from typing import List, Dict, Any\n",
    "from openai.types.chat import ChatCompletion  \n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import re\n",
    "\n",
    "def run_tests_and_reward(\n",
    "    prompts: List[str],\n",
    "    completions: List[List[dict]],\n",
    "    timeout: int = 60,\n",
    "    tests: str = \"\",\n",
    "    timeout_on_client: bool = False\n",
    ") -> List[int]:\n",
    "    import utils  # Ensure utils is imported\n",
    "\n",
    "    server = \"http://localhost:8000\"\n",
    "    codes = []\n",
    "\n",
    "    for completion_group in completions:\n",
    "        if not completion_group or \"content\" not in completion_group[0]:\n",
    "            codes.append(0.0)\n",
    "            continue\n",
    "\n",
    "        content = completion_group[0][\"content\"]\n",
    "\n",
    "        # Extract all Python code blocks using the utils function\n",
    "        python_code_blocks = utils.find_code_blocks(content, tag=\"python\")\n",
    "\n",
    "        if not python_code_blocks:\n",
    "            codes.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Join code blocks and clean triple quotes\n",
    "        full_code = \"\\n\\n\".join(python_code_blocks)\n",
    "        full_code = full_code.replace('\\\\\"\"\"', '\"\"\"')\n",
    "\n",
    "        codes.append(full_code)\n",
    "\n",
    "    return exec_test_batched(\n",
    "        server, codes,\n",
    "        timeout=timeout,\n",
    "        timeout_on_client=timeout_on_client\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "def reward_based_on_jax_usage(\n",
    "    prompts: List[str],\n",
    "    completions: List[List[dict]]\n",
    ") -> List[float]:\n",
    "    codes = []\n",
    "    for completion in completions:\n",
    "        content = completion[0][\"content\"] if completion and \"content\" in completion[0] else \"\"\n",
    "        codes.append(utils.find_code_blocks(content, tag=\"python\"))  # Strip leading/trailing whitespace\n",
    "\n",
    "    if not codes:\n",
    "        return []\n",
    "\n",
    "    # Calculate JAX usage scores (assuming count_jax_usage is defined elsewhere)\n",
    "    raw_scores = [utils.count_jax_usage(code) for code in codes]\n",
    "    return raw_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Configure GRPO training parameters.\n",
    "# This configuration sets up the training hyperparameters, optimization settings, and inference acceleration via vLLM.\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = True,                     # Enable vLLM to accelerate inference during training.\n",
    "    learning_rate = 5e-6,                # Set the learning rate for the optimizer.\n",
    "    adam_beta1 = 0.9,                    # First beta parameter for the AdamW optimizer.\n",
    "    adam_beta2 = 0.99,                   # Second beta parameter for the AdamW optimizer.\n",
    "    weight_decay = 0.1,                  # Weight decay to regularize the model and prevent overfitting.\n",
    "    warmup_ratio = 0.1,                  # Fraction of steps used for learning rate warmup.\n",
    "    lr_scheduler_type = \"cosine\",        # Use cosine annealing for the learning rate scheduler.\n",
    "    optim = \"adamw_8bit\",                # Use 8-bit AdamW optimizer for memory efficiency.\n",
    "    logging_steps = 1,                   # Log training information every step.\n",
    "    bf16 = is_bfloat16_supported(),      # Use bfloat16 precision if supported by the GPU.\n",
    "    fp16 = not is_bfloat16_supported(),  # Otherwise, fall back to fp16 precision.\n",
    "    per_device_train_batch_size = 1,     # Batch size per device during training.\n",
    "    gradient_accumulation_steps = 1,     # Accumulate gradients over this many steps (increase for smoother training if needed).\n",
    "    num_generations = 8,                 # Number of generations per prompt (reduce if memory issues occur).\n",
    "    max_prompt_length = 256,             # Maximum length for the input prompt.\n",
    "    max_completion_length = 4096,         # Maximum length for the generated completion.\n",
    "    num_train_epochs = 1,               # Uncomment this line to run training for one epoch.\n",
    "    max_steps = 250,                     # Maximum number of training steps.\n",
    "    save_steps = 250,                    # Save the model checkpoint every specified number of steps.\n",
    "    max_grad_norm = 0.1,                 # Maximum gradient norm for gradient clipping.\n",
    "    report_to = \"wandb\",                  # Disable reporting to external services like WandB.\n",
    "    output_dir = \"/root/workspace/Align-CodeGemma/src/outputs\",              # Directory to save the training outputs and checkpoints.\n",
    ")\n",
    "\n",
    "# Instantiate the GRPO trainer with the model, tokenizer, reward functions, and training dataset.\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,                       # The language model to be trained.\n",
    "    processing_class = tokenizer,        # The tokenizer used to preprocess the data.\n",
    "    reward_funcs = [\n",
    "         run_tests_and_reward,\n",
    "         format_reward_func,\n",
    "        reward_based_on_jax_usage     # Reward function evaluating the correctness of the answer.\n",
    "    ],\n",
    "    args = training_args,                # GRPO training configuration.\n",
    "    train_dataset = train_dataset,  \n",
    "    eval_dataset = test_dataset           # The training dataset containing prompts and expected answers.\n",
    ")\n",
    "\n",
    "# Begin training using the GRPO algorithm.\n",
    "trainer.train()\n",
    "\n",
    "# Save the LoRA-adapted model for later use.\n",
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"How many r's are in strawberry?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "\n",
    "# Set the sampling parameters for text generation.\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "\n",
    "# Generate a response from the model without applying any LoRA adapter.\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,  # No LoRA adapter is used here.\n",
    ")[0].outputs[0].text\n",
    "\n",
    "# Print the generated output.\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"How many r's are in strawberry?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "\n",
    "# Set sampling parameters for controlled text generation.\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "\n",
    "# Generate a response using the saved LoRA adapter.\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),  # Load the saved LoRA adapter.\n",
    ")[0].outputs[0].text\n",
    "\n",
    "# Print the generated response.\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
