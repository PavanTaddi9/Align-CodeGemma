[
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.product(op=(operator.add, operator.mul, operator.sub, (lambda x, y: mgpu.FragmentedArray.min(x, y), np.minimum), (lambda x, y: mgpu.FragmentedArray.max(x, y), np.maximum)), dtype=[jnp.float32, jnp.int32, jnp.uint32], m=(64, 128), n=(8, 16, 32, 64, 80, 128, 256))\n@jtu.ignore_warning(message='(invalid value|divide by zero)', category=RuntimeWarning)\ndef test_binary(self, op, dtype, m=64, n=32):\n    if isinstance(op, tuple):\n        op, np_op = op\n    else:\n        np_op = op\n    for scalar_rhs in [None, 2]:\n\n        def kernel(ctx, dst, _):\n            mlir_dtype = utils.dtype_to_ir_type(dtype)\n            iota = iota_tensor(m, n, dtype)\n            rhs = iota if scalar_rhs is None else c(scalar_rhs, mlir_dtype)\n            op(iota, rhs).store_untiled(dst)\n        out_shape = jax.ShapeDtypeStruct((m, n), dtype)\n        result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n        ref_x = np.arange(m * n, dtype=dtype).reshape(m, n)\n        ref_rhs = scalar_rhs or ref_x\n        np.testing.assert_array_equal(result, np_op(ref_x, ref_rhs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def np_op(start, stop):\n    start = start.astype(np.float32) if dtype == jnp.bfloat16 else start\n    stop = stop.astype(np.float32) if dtype == jnp.bfloat16 else stop\n    return np.geomspace(start, stop, num, endpoint=endpoint, dtype=dtype if dtype != jnp.bfloat16 else np.float32, axis=axis).astype(dtype)"
  },
  {
    "test_code": "@parameterized.product(ops=((lambda x: -x, jax.lax.neg), (lambda x: x + 42, lambda x: x + 42), (lambda x: x.tanh(), jax.lax.tanh)), dtype=[jnp.float32, jnp.int32, jnp.uint32])\ndef test_unary(self, ops, dtype, m=64, n=32):\n    op, np_op = ops\n    if np_op is jax.lax.tanh and jnp.issubdtype(dtype, jnp.integer):\n        raise self.skipTest('Tanh not supported for integer types')\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, dtype)\n        op(iota).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), dtype)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=dtype).reshape(m, n)\n    np.testing.assert_allclose(result, np_op(x), atol=2e-07, rtol=2e-07)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def np_op(start, stop):\n    start = start.astype(np.float32) if dtype == jnp.bfloat16 else start\n    stop = stop.astype(np.float32) if dtype == jnp.bfloat16 else stop\n    return np.geomspace(start, stop, num, endpoint=endpoint, dtype=dtype if dtype != jnp.bfloat16 else np.float32, axis=axis).astype(dtype)"
  },
  {
    "test_code": "@parameterized.product(ops=[(lambda x: mgpu.FragmentedArray.exp(x), np.exp), (lambda x: mgpu.FragmentedArray.sin(x), np.sin), (lambda x: mgpu.FragmentedArray.cos(x), np.cos), (lambda x: mgpu.FragmentedArray.rsqrt(x), jax.lax.rsqrt)], approx=[False, True])\n@jtu.ignore_warning(message='overflow encountered', category=RuntimeWarning)\ndef test_math(self, ops, approx, m=64, n=32):\n    op, np_op = ops\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, jnp.float32)\n        op(iota).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=jnp.float32).reshape(m, n)\n    atol = 0.005 if approx else 2e-07\n    rtol = 4e-06 if approx else 2e-07\n    np.testing.assert_allclose(result, np_op(x), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def np_op(start, stop):\n    start = start.astype(np.float32) if dtype == jnp.bfloat16 else start\n    stop = stop.astype(np.float32) if dtype == jnp.bfloat16 else stop\n    return np.geomspace(start, stop, num, endpoint=endpoint, dtype=dtype if dtype != jnp.bfloat16 else np.float32, axis=axis).astype(dtype)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.product(lhs_transpose=(False, True), rhs_transpose=(False, True), in_mlir_dtype_cls=(ir.F16Type, ir.BF16Type, ir.F32Type), m=(64, 128, 192), n=(64, 128, 192), k_steps=(1, 2), swizzle=(32, 64, 128), jax_out_dtype=(jnp.float16, jnp.float32), rhs_tiling_kind=('large', 'small', 'small+no_transpose'), lhs_tiling_kind=('large', 'small', 'small+no_transpose'))\ndef test_wgmma_basic(self, m, n, k_steps, in_mlir_dtype_cls, lhs_transpose, rhs_transpose, swizzle, jax_out_dtype, rhs_tiling_kind, lhs_tiling_kind):\n    if jax_out_dtype == jnp.float16 and in_mlir_dtype_cls is not ir.F16Type:\n        self.skipTest('Only f16 input is supported for f16 output.')\n    if swizzle != 128 and lhs_transpose and (lhs_tiling_kind == 'large'):\n        self.skipTest('Transpose only supported in 128B swizzled WGMMA')\n    if rhs_tiling_kind == 'small+no_transpose' and (not rhs_transpose):\n        self.skipTest('No transpose happening anyway')\n    if lhs_tiling_kind == 'small+no_transpose' and (not lhs_transpose):\n        self.skipTest('No transpose happening anyway')\n    in_mlir_dtype = in_mlir_dtype_cls.get()\n    out_mlir_dtype = utils.dtype_to_ir_type(jax_out_dtype)\n    if ir.F32Type.isinstance(in_mlir_dtype):\n        in_jax_dtype = jnp.float32\n        if lhs_transpose or not rhs_transpose:\n            self.skipTest('Transpose only supported in 16-bit WGMMA')\n        exponent_bits, mantissa_bits = (8, 10)\n    elif bytewidth(in_mlir_dtype) == 2:\n        if n % 64 != 0:\n            self.skipTest('16-bit WGMMA only supports n % 64 == 0')\n        if ir.F16Type.isinstance(in_mlir_dtype):\n            in_jax_dtype = jnp.float16\n            exponent_bits, mantissa_bits = (5, 10)\n        elif ir.BF16Type.isinstance(in_mlir_dtype):\n            in_jax_dtype = jnp.bfloat16\n            exponent_bits, mantissa_bits = (8, 7)\n        else:\n            raise NotImplementedError(in_mlir_dtype)\n    else:\n        raise NotImplementedError(in_mlir_dtype)\n    nk_tile = swizzle // bytewidth(in_mlir_dtype)\n    k = nk_tile * k_steps\n    assert m % 64 == 0 and n % nk_tile == 0\n    small_rhs_tile = rhs_tiling_kind != 'large'\n    transpose_rhs_tiles = rhs_tiling_kind != 'small+no_transpose'\n    rhs_tiling = (8, nk_tile) if small_rhs_tile else (nk_tile, nk_tile)\n    small_lhs_tile = lhs_tiling_kind != 'large'\n    transpose_lhs_tiles = lhs_tiling_kind != 'small+no_transpose'\n    lhs_tiling = (8, nk_tile) if small_lhs_tile else (64, nk_tile)\n\n    def kernel(ctx, lhs, rhs, out, scratch):\n        lhs_smem, rhs_smem, barriers = scratch\n        lhs_transform = (mgpu.TileTransform(lhs_tiling),)\n        if lhs_transpose and transpose_lhs_tiles:\n            lhs_transform += (mgpu.TransposeTransform((1, 0, 2, 3)),)\n        rhs_transform = (mgpu.TileTransform(rhs_tiling),)\n        if rhs_transpose and transpose_rhs_tiles:\n            rhs_transform += (mgpu.TransposeTransform((1, 0, 2, 3)),)\n        ctx.async_copy(src_ref=lhs, dst_ref=lhs_smem, swizzle=swizzle, gmem_transform=lhs_transform, barrier=barriers[0])\n        ctx.async_copy(src_ref=rhs, dst_ref=rhs_smem, swizzle=swizzle, gmem_transform=rhs_transform, barrier=barriers[1])\n        for i in range(2):\n            barriers[i].wait()\n        init_acc = mgpu.WGMMAAccumulator.zero(m=m, n=n, dtype=out_mlir_dtype)\n        if lhs_transpose:\n            perm = (0, 1, 3, 2) if transpose_lhs_tiles else (1, 0, 3, 2)\n            lhs_smem = memref_transpose(lhs_smem, perm)\n        if rhs_transpose:\n            perm = (0, 1, 3, 2) if transpose_rhs_tiles else (1, 0, 3, 2)\n            rhs_smem = memref_transpose(rhs_smem, perm)\n        acc = mgpu.wgmma(init_acc, lhs_smem, rhs_smem, swizzle=swizzle)\n        nvvm.wgmma_commit_group_sync_aligned()\n        nvvm.wgmma_wait_group_sync_aligned(0)\n        acc.value.store_untiled(out)\n\n    def quantize(x):\n        return jax.lax.reduce_precision(x, exponent_bits, mantissa_bits)\n    x_shape = (k, m) if lhs_transpose else (m, k)\n    x = quantize(self.prng.uniform(-1, 1, x_shape)).astype(in_jax_dtype)\n    y_shape = (n, k) if rhs_transpose else (k, n)\n    y = quantize(self.prng.uniform(-1, 1, y_shape)).astype(in_jax_dtype)\n    out_shape = jax.ShapeDtypeStruct((m, n), jax_out_dtype)\n    if transpose_rhs_tiles:\n        rhs_tiling_t = rhs_tiling[::-1] if rhs_transpose else rhs_tiling\n        rhs_smem_shape = (k // rhs_tiling_t[0], n // rhs_tiling_t[1], *rhs_tiling)\n    else:\n        rhs_smem_shape = tile_shape(y_shape, rhs_tiling)\n    if transpose_lhs_tiles:\n        lhs_tiling_t = lhs_tiling[::-1] if lhs_transpose else lhs_tiling\n        lhs_smem_shape = (m // lhs_tiling_t[0], k // lhs_tiling_t[1], *lhs_tiling)\n    else:\n        lhs_smem_shape = tile_shape(x_shape, lhs_tiling)\n    scratch_shape = [jax.ShapeDtypeStruct(lhs_smem_shape, in_jax_dtype), jax.ShapeDtypeStruct(rhs_smem_shape, in_jax_dtype), mgpu.TMABarrier(2)]\n    z = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (x, y), out_shape, scratch_shape)(x, y)\n    x32, y32 = (x.astype(np.float32), y.astype(np.float32))\n    ref = (x32.T if lhs_transpose else x32) @ (y32.T if rhs_transpose else y32)\n    atol = 0.02 if jax_out_dtype == jnp.float16 else 5e-06\n    np.testing.assert_allclose(z, ref, atol=atol)",
    "assertions": [
      "assert m % 64 == 0 and n % nk_tile == 0"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@staticmethod\ndef zero(dt):\n    neginf = np.array(-np.inf if dtypes.supports_inf(dt.float_dtype) else dtypes.finfo(dt.float_dtype).min, dt.float_dtype)\n    return jax.lax.convert_element_type(neginf, dt)"
  },
  {
    "test_code": "@parameterized.product(m=(64, 128, 192), n=(64, 128, 192), k_steps=(1, 2), rhs_transpose=(False, True), swizzle=(32, 64, 128), dtype=[jnp.float16, jnp.bfloat16], tiled_layout=(False, True))\ndef test_wgmma_reg_lhs(self, m, n, k_steps, rhs_transpose, swizzle, dtype, tiled_layout):\n    index = ir.IndexType.get()\n    bytewidth = 2\n    nk_tile = swizzle // bytewidth\n    k = nk_tile * k_steps\n\n    def kernel(ctx, rhs, out, rhs_smem):\n        del ctx\n        for ki in range(k_steps):\n            for ni in range(n // nk_tile):\n                rhs_slice = (ds(c(ki * nk_tile, index), nk_tile), ds(c(ni * nk_tile, index), nk_tile))\n                if rhs_transpose:\n                    rhs_slice = rhs_slice[::-1]\n                copy(src=memref_slice(rhs, rhs_slice), dst=memref_slice(rhs_smem, (ki, ni)), swizzle=swizzle)\n        init_acc = mgpu.WGMMAAccumulator.zero(m=m, n=n)\n        lhs_regs = iota_tensor(m, k, dtype, tiled_layout)\n        if rhs_transpose:\n            rhs_smem = memref_transpose(rhs_smem, (0, 1, 3, 2))\n        acc = mgpu.wgmma(init_acc, lhs_regs, rhs_smem, swizzle=swizzle)\n        nvvm.wgmma_commit_group_sync_aligned()\n        nvvm.wgmma_wait_group_sync_aligned(0)\n        acc.value.store_untiled(out)\n    y_shape = (n, k) if rhs_transpose else (k, n)\n    y = self.prng.uniform(-1, 1, y_shape).astype(dtype)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    scratch_shape = jax.ShapeDtypeStruct((k_steps, n // nk_tile, nk_tile, nk_tile), dtype)\n    z = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), y, out_shape, scratch_shape)(y)\n    x = np.arange(m * k, dtype=dtype).reshape(m, k)\n    ref = jax.lax.dot(x, y.T if rhs_transpose else y, preferred_element_type=jnp.float32)\n    rtol = 0.0005\n    np.testing.assert_allclose(z, ref, rtol=rtol, atol=0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@staticmethod\ndef zero(dt):\n    neginf = np.array(-np.inf if dtypes.supports_inf(dt.float_dtype) else dtypes.finfo(dt.float_dtype).min, dt.float_dtype)\n    return jax.lax.convert_element_type(neginf, dt)"
  },
  {
    "test_code": "@parameterized.product(rhs_transpose=(False, True), swizzle=(32, 64, 128), n=(8, 16), small_rhs_tile=(False, True))\ndef test_narrow_n(self, rhs_transpose, swizzle, n, small_rhs_tile):\n    m, k_steps = (64, 2)\n    bytewidth = 2\n    nk_tile = swizzle // bytewidth\n    k = nk_tile * k_steps\n    if small_rhs_tile and (not rhs_transpose):\n        self.skipTest('Small tiles only supported for transposed RHS')\n    n_tile = 8 if small_rhs_tile else nk_tile\n\n    def kernel(ctx, rhs, out, smem):\n        rhs_smem, barrier = smem\n        gmem_slice = (ds(0, k), ds(0, max(n_tile, n)))\n        transform = (mgpu.TileTransform((n_tile, nk_tile)),)\n        if rhs_transpose:\n            gmem_slice = gmem_slice[::-1]\n            transform += (mgpu.TransposeTransform((1, 0, 2, 3)),)\n        ctx.async_copy(src_ref=rhs, dst_ref=rhs_smem, swizzle=swizzle, gmem_slice=gmem_slice, gmem_transform=transform, barrier=barrier)\n        barrier.wait()\n        init_acc = mgpu.WGMMAAccumulator.zero(m=m, n=n)\n        lhs_regs = iota_tensor(m, k, jnp.float16)\n        if rhs_transpose:\n            rhs_smem = memref_transpose(rhs_smem, (0, 1, 3, 2))\n        if not small_rhs_tile:\n            smem_slice = (slice(None), slice(None), slice(None), ds(0, n))\n            rhs_smem = memref_slice(rhs_smem, smem_slice)\n        acc = mgpu.wgmma(init_acc, lhs_regs, rhs_smem, swizzle=swizzle)\n        nvvm.wgmma_commit_group_sync_aligned()\n        nvvm.wgmma_wait_group_sync_aligned(0)\n        acc.value.store_untiled(out)\n    jax_dtype = jnp.float16\n    y_shape = (n, k) if rhs_transpose else (k, n)\n    y = self.prng.uniform(-1, 1, y_shape).astype(jax_dtype)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    rhs_scratch_shape = jax.ShapeDtypeStruct((k_steps, (n + n_tile - 1) // n_tile, n_tile, nk_tile), jax_dtype)\n    z = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), y, out_shape, (rhs_scratch_shape, mgpu.TMABarrier()))(y)\n    x = np.arange(m * k, dtype=jax_dtype).reshape(m, k)\n    ref = jax.lax.dot(x, y.T if rhs_transpose else y, preferred_element_type=jnp.float32)\n    np.testing.assert_allclose(z, ref, rtol=0.0005, atol=0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@staticmethod\ndef zero(dt):\n    neginf = np.array(-np.inf if dtypes.supports_inf(dt.float_dtype) else dtypes.finfo(dt.float_dtype).min, dt.float_dtype)\n    return jax.lax.convert_element_type(neginf, dt)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.product(op=(operator.add, operator.mul, operator.sub, (lambda x, y: mgpu.FragmentedArray.min(x, y), np.minimum), (lambda x, y: mgpu.FragmentedArray.max(x, y), np.maximum)), dtype=[jnp.float32, jnp.int32, jnp.uint32], m=(64, 128), n=(8, 16, 32, 64, 80, 128, 256))\n@jtu.ignore_warning(message='(invalid value|divide by zero)', category=RuntimeWarning)\ndef test_binary(self, op, dtype, m=64, n=32):\n    if isinstance(op, tuple):\n        op, np_op = op\n    else:\n        np_op = op\n    for scalar_rhs in [None, 2]:\n\n        def kernel(ctx, dst, _):\n            mlir_dtype = utils.dtype_to_ir_type(dtype)\n            iota = iota_tensor(m, n, dtype)\n            rhs = iota if scalar_rhs is None else c(scalar_rhs, mlir_dtype)\n            op(iota, rhs).store_untiled(dst)\n        out_shape = jax.ShapeDtypeStruct((m, n), dtype)\n        result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n        ref_x = np.arange(m * n, dtype=dtype).reshape(m, n)\n        ref_rhs = scalar_rhs or ref_x\n        np.testing.assert_array_equal(result, np_op(ref_x, ref_rhs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def np_op(start, stop):\n    start = start.astype(np.float32) if dtype == jnp.bfloat16 else start\n    stop = stop.astype(np.float32) if dtype == jnp.bfloat16 else stop\n    return np.geomspace(start, stop, num, endpoint=endpoint, dtype=dtype if dtype != jnp.bfloat16 else np.float32, axis=axis).astype(dtype)"
  },
  {
    "test_code": "@parameterized.product(ops=((lambda x: -x, jax.lax.neg), (lambda x: x + 42, lambda x: x + 42), (lambda x: x.tanh(), jax.lax.tanh)), dtype=[jnp.float32, jnp.int32, jnp.uint32])\ndef test_unary(self, ops, dtype, m=64, n=32):\n    op, np_op = ops\n    if np_op is jax.lax.tanh and jnp.issubdtype(dtype, jnp.integer):\n        raise self.skipTest('Tanh not supported for integer types')\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, dtype)\n        op(iota).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), dtype)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=dtype).reshape(m, n)\n    np.testing.assert_allclose(result, np_op(x), atol=2e-07, rtol=2e-07)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def np_op(start, stop):\n    start = start.astype(np.float32) if dtype == jnp.bfloat16 else start\n    stop = stop.astype(np.float32) if dtype == jnp.bfloat16 else stop\n    return np.geomspace(start, stop, num, endpoint=endpoint, dtype=dtype if dtype != jnp.bfloat16 else np.float32, axis=axis).astype(dtype)"
  },
  {
    "test_code": "@parameterized.product(ops=[(lambda x: mgpu.FragmentedArray.exp(x), np.exp), (lambda x: mgpu.FragmentedArray.sin(x), np.sin), (lambda x: mgpu.FragmentedArray.cos(x), np.cos), (lambda x: mgpu.FragmentedArray.rsqrt(x), jax.lax.rsqrt)], approx=[False, True])\n@jtu.ignore_warning(message='overflow encountered', category=RuntimeWarning)\ndef test_math(self, ops, approx, m=64, n=32):\n    op, np_op = ops\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, jnp.float32)\n        op(iota).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=jnp.float32).reshape(m, n)\n    atol = 0.005 if approx else 2e-07\n    rtol = 4e-06 if approx else 2e-07\n    np.testing.assert_allclose(result, np_op(x), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def np_op(start, stop):\n    start = start.astype(np.float32) if dtype == jnp.bfloat16 else start\n    stop = stop.astype(np.float32) if dtype == jnp.bfloat16 else stop\n    return np.geomspace(start, stop, num, endpoint=endpoint, dtype=dtype if dtype != jnp.bfloat16 else np.float32, axis=axis).astype(dtype)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.product(op=(operator.add, operator.mul, operator.sub, (lambda x, y: mgpu.FragmentedArray.min(x, y), np.minimum), (lambda x, y: mgpu.FragmentedArray.max(x, y), np.maximum)), dtype=[jnp.float32, jnp.int32, jnp.uint32], m=(64, 128), n=(8, 16, 32, 64, 80, 128, 256))\n@jtu.ignore_warning(message='(invalid value|divide by zero)', category=RuntimeWarning)\ndef test_binary(self, op, dtype, m=64, n=32):\n    if isinstance(op, tuple):\n        op, np_op = op\n    else:\n        np_op = op\n    for scalar_rhs in [None, 2]:\n\n        def kernel(ctx, dst, _):\n            mlir_dtype = utils.dtype_to_ir_type(dtype)\n            iota = iota_tensor(m, n, dtype)\n            rhs = iota if scalar_rhs is None else c(scalar_rhs, mlir_dtype)\n            op(iota, rhs).store_untiled(dst)\n        out_shape = jax.ShapeDtypeStruct((m, n), dtype)\n        result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n        ref_x = np.arange(m * n, dtype=dtype).reshape(m, n)\n        ref_rhs = scalar_rhs or ref_x\n        np.testing.assert_array_equal(result, np_op(ref_x, ref_rhs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def np_op(x, axis=None, dtype=None, include_initial=False):\n    axis = axis or 0\n    out = np.cumprod(x, axis=axis, dtype=dtype or x.dtype)\n    if include_initial:\n        ones_shape = list(x.shape)\n        ones_shape[axis] = 1\n        out = jnp.concat([jnp.ones(ones_shape, dtype=out.dtype), out], axis=axis)\n    return out"
  },
  {
    "test_code": "@parameterized.product(ops=((lambda x: -x, jax.lax.neg), (lambda x: x + 42, lambda x: x + 42), (lambda x: x.tanh(), jax.lax.tanh)), dtype=[jnp.float32, jnp.int32, jnp.uint32])\ndef test_unary(self, ops, dtype, m=64, n=32):\n    op, np_op = ops\n    if np_op is jax.lax.tanh and jnp.issubdtype(dtype, jnp.integer):\n        raise self.skipTest('Tanh not supported for integer types')\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, dtype)\n        op(iota).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), dtype)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=dtype).reshape(m, n)\n    np.testing.assert_allclose(result, np_op(x), atol=2e-07, rtol=2e-07)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def np_op(x, axis=None, dtype=None, include_initial=False):\n    axis = axis or 0\n    out = np.cumprod(x, axis=axis, dtype=dtype or x.dtype)\n    if include_initial:\n        ones_shape = list(x.shape)\n        ones_shape[axis] = 1\n        out = jnp.concat([jnp.ones(ones_shape, dtype=out.dtype), out], axis=axis)\n    return out"
  },
  {
    "test_code": "@parameterized.product(ops=[(lambda x: mgpu.FragmentedArray.exp(x), np.exp), (lambda x: mgpu.FragmentedArray.sin(x), np.sin), (lambda x: mgpu.FragmentedArray.cos(x), np.cos), (lambda x: mgpu.FragmentedArray.rsqrt(x), jax.lax.rsqrt)], approx=[False, True])\n@jtu.ignore_warning(message='overflow encountered', category=RuntimeWarning)\ndef test_math(self, ops, approx, m=64, n=32):\n    op, np_op = ops\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, jnp.float32)\n        op(iota).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=jnp.float32).reshape(m, n)\n    atol = 0.005 if approx else 2e-07\n    rtol = 4e-06 if approx else 2e-07\n    np.testing.assert_allclose(result, np_op(x), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def np_op(x, axis=None, dtype=None, include_initial=False):\n    axis = axis or 0\n    out = np.cumprod(x, axis=axis, dtype=dtype or x.dtype)\n    if include_initial:\n        ones_shape = list(x.shape)\n        ones_shape[axis] = 1\n        out = jnp.concat([jnp.ones(ones_shape, dtype=out.dtype), out], axis=axis)\n    return out"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.product(ops=[(lambda x: mgpu.FragmentedArray.exp(x), np.exp), (lambda x: mgpu.FragmentedArray.sin(x), np.sin), (lambda x: mgpu.FragmentedArray.cos(x), np.cos), (lambda x: mgpu.FragmentedArray.rsqrt(x), jax.lax.rsqrt)], approx=[False, True])\n@jtu.ignore_warning(message='overflow encountered', category=RuntimeWarning)\ndef test_math(self, ops, approx, m=64, n=32):\n    op, np_op = ops\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, jnp.float32)\n        op(iota).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=jnp.float32).reshape(m, n)\n    atol = 0.005 if approx else 2e-07\n    rtol = 4e-06 if approx else 2e-07\n    np.testing.assert_allclose(result, np_op(x), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.named_parameters(('bf16_i8', jnp.bfloat16, jnp.int8), ('i8_bf16', jnp.int8, jnp.bfloat16), ('i8_i8', jnp.int8, jnp.int8), ('i4_i4', jnp.int4, jnp.int4), ('i4_bf16', jnp.int4, jnp.bfloat16))\ndef test_convert_tiled(self, jax_dtype_from, jax_dtype_to):\n    mlir_dtype_from = utils.dtype_to_ir_type(jax_dtype_from)\n    mlir_dtype_to = utils.dtype_to_ir_type(jax_dtype_to)\n    m = 128\n    n = 256 * 8 // bitwidth(mlir_dtype_from)\n\n    def kernel(ctx, inp, out, smem):\n        del ctx\n        smem_from, smem_to = smem\n        copy(inp, smem_from, swizzle=128)\n        t = mgpu.FragmentedArray.load_tiled(smem_from, swizzle=128, is_signed=utils.is_signed(jax_dtype_from), layout=fa._tiled_wgmma_layout((m, n)))\n        t = t.astype(mlir_dtype_to, is_signed=utils.is_signed(jax_dtype_to))\n        t.store_tiled(smem_to, swizzle=128)\n        copy(smem_to, out, swizzle=128)\n    from_tiling = (64, 128 * 8 // bitwidth(mlir_dtype_from))\n    to_tiling = (64, 128 * 8 // bitwidth(mlir_dtype_to))\n    int_sample_dtype = getattr(jnp, 'int' + str(min(bitwidth(mlir_dtype_from), bitwidth(mlir_dtype_to))))\n    sample_iinfo = jnp.iinfo(int_sample_dtype)\n    expected_raw = self.prng.integers(low=sample_iinfo.min, high=sample_iinfo.max, size=(m, n), dtype=np.int32)\n    expected = lambda jax_dtype, tiling: expected_raw.reshape(m // tiling[0], tiling[0], n // tiling[1], tiling[1]).transpose(0, 2, 1, 3).astype(jax_dtype)\n    expected_from = expected(jax_dtype_from, from_tiling)\n    expected_to = expected(jax_dtype_to, to_tiling)\n    res = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), expected_from, expected_to, (expected_from, expected_to))(expected_from)\n    np.testing.assert_array_equal(res, expected_to)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def expected(M):\n    if n_sparse == 0:\n        return M\n    mask = (M != 0).any(range(M.ndim - n_dense, M.ndim), keepdims=True)\n    return jnp.where(mask, M, 0)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.product(ops=[(lambda x: mgpu.FragmentedArray.exp(x), np.exp), (lambda x: mgpu.FragmentedArray.sin(x), np.sin), (lambda x: mgpu.FragmentedArray.cos(x), np.cos), (lambda x: mgpu.FragmentedArray.rsqrt(x), jax.lax.rsqrt)], approx=[False, True])\n@jtu.ignore_warning(message='overflow encountered', category=RuntimeWarning)\ndef test_math(self, ops, approx, m=64, n=32):\n    op, np_op = ops\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, jnp.float32)\n        op(iota).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=jnp.float32).reshape(m, n)\n    atol = 0.005 if approx else 2e-07\n    rtol = 4e-06 if approx else 2e-07\n    np.testing.assert_allclose(result, np_op(x), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pass_is_registered(self):\n    ctx = mlir.make_ir_context()\n    ctx.allow_unregistered_dialects = True\n    with ir.Location.unknown(ctx):\n        module = ir.Module.create()\n        pipeline = passmanager.PassManager.parse('builtin.module(mosaic_gpu-serde{serialize=true})', ctx)\n        pipeline.run(module.operation)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def run(pos):\n    maxiter = 1000\n\n    def cond(v):\n        return v[0] < maxiter\n\n    def step(v):\n        i, pos = v\n        jax.debug.callback(print_it, i + 1, maxiter)\n        return (i + 1, pos + 1)\n    val = (jnp.array(0), pos)\n    val = jax.lax.while_loop(cond, step, val)\n    return val[1]"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pass_is_registered(self):\n    ctx = mlir.make_ir_context()\n    ctx.allow_unregistered_dialects = True\n    with ir.Location.unknown(ctx):\n        module = ir.Module.create()\n        pipeline = passmanager.PassManager.parse('builtin.module(mosaic_gpu-serde{serialize=true})', ctx)\n        pipeline.run(module.operation)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def run(primal_ins, cotangent_outs):\n    primal_outs, vjp = jax.vjp(g, *primal_ins)\n    cotangent_ins = vjp(cotangent_outs)\n    return (primal_outs, cotangent_ins)"
  },
  {
    "test_code": "@parameterized.product(ops=[(lambda x: mgpu.FragmentedArray.exp(x), np.exp), (lambda x: mgpu.FragmentedArray.sin(x), np.sin), (lambda x: mgpu.FragmentedArray.cos(x), np.cos), (lambda x: mgpu.FragmentedArray.rsqrt(x), jax.lax.rsqrt)], approx=[False, True])\n@jtu.ignore_warning(message='overflow encountered', category=RuntimeWarning)\ndef test_math(self, ops, approx, m=64, n=32):\n    op, np_op = ops\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, jnp.float32)\n        op(iota).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=jnp.float32).reshape(m, n)\n    atol = 0.005 if approx else 2e-07\n    rtol = 4e-06 if approx else 2e-07\n    np.testing.assert_allclose(result, np_op(x), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.float32, jnp.int32], m=[128], n=[32, 64])\ndef test_strided_reduce_sum(self, dtype, m, n):\n\n    def kernel(ctx, src, dst, scratch):\n        src = mgpu.FragmentedArray.load_strided(src, is_signed=utils.is_signed(dtype))\n        acc = src.reduce_sum(scratch).broadcast((m,))\n        acc.store_untiled(dst)\n    in_shape = jax.ShapeDtypeStruct((m, n), dtype)\n    out_shape = jax.ShapeDtypeStruct((m,), dtype)\n    kernel_fn = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), in_shape, out_shape, smem_scratch_shape=jax.ShapeDtypeStruct((4,), dtype))\n    x = np.arange(m * n, dtype=dtype).reshape(m, n)\n    np.testing.assert_array_equal(kernel_fn(x), jnp.full((m,), x.sum()))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@parameterized.product(op=(arith.addf, arith.maximumf), m=(64, 128), n=(8, 16, 32, 64, 80, 128, 256))\ndef test_reduce(self, op, m=64, n=32):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, jnp.float32)\n        iota.reduce(op, axis=1).broadcast_minor(n).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=jnp.float32).reshape(m, n)\n    if op == arith.addf:\n        expected = np.broadcast_to(x.sum(axis=1, keepdims=True), x.shape)\n    elif op == arith.maximumf:\n        expected = np.broadcast_to(x.max(axis=1, keepdims=True), x.shape)\n    else:\n        raise NotImplementedError(f'Unsupported op: {op}')\n    np.testing.assert_array_equal(result, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_warp_tree_reduce(self):\n\n    def kernel(ctx, out, *_):\n        del ctx\n        i32 = ir.IntegerType.get_signless(32)\n        tid = gpu.thread_id(gpu.Dimension.x)\n        value = arith.index_cast(i32, tid)\n        grp = warp_tree_reduce(value, arith.addi, 4)\n        memref.store(grp, out, [tid])\n    x = np.arange(128, dtype=jnp.int32)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), x, [])()\n    for i in range(0, 128, 4):\n        x[i:i + 4] = jnp.sum(x[i:i + 4])\n    np.testing.assert_array_equal(result, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@parameterized.product(m=(64, 128, 192), n=(64, 128, 192), k_steps=(1, 2), rhs_transpose=(False, True), swizzle=(32, 64, 128), dtype=[jnp.float16, jnp.bfloat16], tiled_layout=(False, True))\ndef test_wgmma_reg_lhs(self, m, n, k_steps, rhs_transpose, swizzle, dtype, tiled_layout):\n    index = ir.IndexType.get()\n    bytewidth = 2\n    nk_tile = swizzle // bytewidth\n    k = nk_tile * k_steps\n\n    def kernel(ctx, rhs, out, rhs_smem):\n        del ctx\n        for ki in range(k_steps):\n            for ni in range(n // nk_tile):\n                rhs_slice = (ds(c(ki * nk_tile, index), nk_tile), ds(c(ni * nk_tile, index), nk_tile))\n                if rhs_transpose:\n                    rhs_slice = rhs_slice[::-1]\n                copy(src=memref_slice(rhs, rhs_slice), dst=memref_slice(rhs_smem, (ki, ni)), swizzle=swizzle)\n        init_acc = mgpu.WGMMAAccumulator.zero(m=m, n=n)\n        lhs_regs = iota_tensor(m, k, dtype, tiled_layout)\n        if rhs_transpose:\n            rhs_smem = memref_transpose(rhs_smem, (0, 1, 3, 2))\n        acc = mgpu.wgmma(init_acc, lhs_regs, rhs_smem, swizzle=swizzle)\n        nvvm.wgmma_commit_group_sync_aligned()\n        nvvm.wgmma_wait_group_sync_aligned(0)\n        acc.value.store_untiled(out)\n    y_shape = (n, k) if rhs_transpose else (k, n)\n    y = self.prng.uniform(-1, 1, y_shape).astype(dtype)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    scratch_shape = jax.ShapeDtypeStruct((k_steps, n // nk_tile, nk_tile, nk_tile), dtype)\n    z = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), y, out_shape, scratch_shape)(y)\n    x = np.arange(m * k, dtype=dtype).reshape(m, k)\n    ref = jax.lax.dot(x, y.T if rhs_transpose else y, preferred_element_type=jnp.float32)\n    rtol = 0.0005\n    np.testing.assert_allclose(z, ref, rtol=rtol, atol=0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "@parameterized.product(rhs_transpose=(False, True), swizzle=(32, 64, 128), n=(8, 16), small_rhs_tile=(False, True))\ndef test_narrow_n(self, rhs_transpose, swizzle, n, small_rhs_tile):\n    m, k_steps = (64, 2)\n    bytewidth = 2\n    nk_tile = swizzle // bytewidth\n    k = nk_tile * k_steps\n    if small_rhs_tile and (not rhs_transpose):\n        self.skipTest('Small tiles only supported for transposed RHS')\n    n_tile = 8 if small_rhs_tile else nk_tile\n\n    def kernel(ctx, rhs, out, smem):\n        rhs_smem, barrier = smem\n        gmem_slice = (ds(0, k), ds(0, max(n_tile, n)))\n        transform = (mgpu.TileTransform((n_tile, nk_tile)),)\n        if rhs_transpose:\n            gmem_slice = gmem_slice[::-1]\n            transform += (mgpu.TransposeTransform((1, 0, 2, 3)),)\n        ctx.async_copy(src_ref=rhs, dst_ref=rhs_smem, swizzle=swizzle, gmem_slice=gmem_slice, gmem_transform=transform, barrier=barrier)\n        barrier.wait()\n        init_acc = mgpu.WGMMAAccumulator.zero(m=m, n=n)\n        lhs_regs = iota_tensor(m, k, jnp.float16)\n        if rhs_transpose:\n            rhs_smem = memref_transpose(rhs_smem, (0, 1, 3, 2))\n        if not small_rhs_tile:\n            smem_slice = (slice(None), slice(None), slice(None), ds(0, n))\n            rhs_smem = memref_slice(rhs_smem, smem_slice)\n        acc = mgpu.wgmma(init_acc, lhs_regs, rhs_smem, swizzle=swizzle)\n        nvvm.wgmma_commit_group_sync_aligned()\n        nvvm.wgmma_wait_group_sync_aligned(0)\n        acc.value.store_untiled(out)\n    jax_dtype = jnp.float16\n    y_shape = (n, k) if rhs_transpose else (k, n)\n    y = self.prng.uniform(-1, 1, y_shape).astype(jax_dtype)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    rhs_scratch_shape = jax.ShapeDtypeStruct((k_steps, (n + n_tile - 1) // n_tile, n_tile, nk_tile), jax_dtype)\n    z = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), y, out_shape, (rhs_scratch_shape, mgpu.TMABarrier()))(y)\n    x = np.arange(m * k, dtype=jax_dtype).reshape(m, k)\n    ref = jax.lax.dot(x, y.T if rhs_transpose else y, preferred_element_type=jnp.float32)\n    np.testing.assert_allclose(z, ref, rtol=0.0005, atol=0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128), shape=((128, 128), (5, 32, 128)), dtype=(jnp.float16, jnp.float32))\ndef test_tma_load_tiled(self, swizzle, shape, dtype):\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    tiled_shape = tile_shape(shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    smem = (jax.ShapeDtypeStruct(tile_shape(shape, tiling), dtype), mgpu.TMABarrier())\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.product(swizzle=(None, 128))\ndef test_tma_load_tiled_rounding(self, swizzle):\n    shape = (5, 32, 144)\n    dtype = jnp.float16\n    i1 = ir.IntegerType.get_signless(1)\n    index = ir.IndexType.get()\n    tiling = (32, (swizzle or 128) // jnp.dtype(dtype).itemsize)\n    rounded_shape = (*shape[:-1], shape[-1] // tiling[-1] * tiling[-1])\n    tiled_shape = tile_shape(rounded_shape, tiling)[:len(shape)]\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, swizzle=swizzle, barrier=barrier, gmem_transform=mgpu.TileTransform(tiling, rounding=mgpu.Rounding.DOWN))\n        barrier.wait_parity(c(0, i1))\n        for idxs in np.ndindex(tiled_shape):\n            untiled_idxs, tiled_idxs = (idxs[:-len(tiling)], idxs[-len(tiling):])\n            s = (*untiled_idxs, *(ds(c(ix * t, index), t) for ix, t in zip(tiled_idxs, tiling)))\n            copy(memref_slice(tmp, idxs), memref_slice(dst, s), swizzle=swizzle)\n    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    tmp_shape = jax.ShapeDtypeStruct(tile_shape(rounded_shape, tiling), dtype)\n    smem = (tmp_shape, mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct(rounded_shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    y = f(x)\n    np.testing.assert_array_equal(y, x[..., :rounded_shape[-1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_tma_load_indexed_tiled(self):\n    shape = (128, 2, 128)\n    tiling = mgpu.TileTransform((32, 32))\n\n    def kernel(ctx, src, dst, scratch):\n        tmp, barrier = scratch\n        ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier, gmem_transform=tiling, gmem_slice=(slice(None), 1, slice(None)))\n        barrier.wait()\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_transform=tiling)\n        ctx.await_async_copy(0)\n    x = np.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    smem = (jax.ShapeDtypeStruct((4, 4, 32, 32), jnp.float32), mgpu.TMABarrier())\n    out_shape = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, out_shape, smem)\n    np.testing.assert_array_equal(f(x), x[:, 1, :])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.float16, jnp.bfloat16)\ndef test_optimization_barrier(self, dtype):\n\n    def kernel(ctx, inp, out, smem):\n        del ctx, smem\n        arr = mgpu.FragmentedArray.load_strided(inp)\n        arr2 = arr * 2\n        arr, arr2 = mgpu.optimization_barrier(arr, arr2)\n        (arr + arr2).store_untiled(out)\n    x = jnp.arange(256, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n    np.testing.assert_array_equal(f(x), x * 3)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_profile(self):\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    spec = profiler.ProfilerSpec(1024)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, (), prof_spec=spec))\n    jax.block_until_ready(f(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_multigpu(self):\n    if len(jax.devices()) < 2:\n        self.skipTest('Need at least 2 devices')\n\n    def kernel(ctx, src, dst, _):\n        mgpu.FragmentedArray.load_strided(src).store_untiled(dst)\n    x = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n    f = jax.jit(mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, ()))\n    for xd in (jax.device_put(x, d) for d in jax.devices()[:2]):\n        jax.block_until_ready(f(xd))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_basic(self):\n\n    def kernel(ctx, i_gmem, o_gmem, _):\n        x = mgpu.FragmentedArray.load_strided(i_gmem)\n        (x + x).store_untiled(o_gmem)\n    ty = jax.ShapeDtypeStruct((128, 128), jnp.float32)\n    x = self.torch.randn((128, 128), dtype=self.torch.float, device='cuda')\n    f = mgpu.as_torch_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), ty, ty, ())\n    y = f(x)\n    np.testing.assert_allclose(y.cpu(), x.cpu() * 2)\n    del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_copy_for_upcast(self):\n    dtype = jnp.int8\n    swizzle = 128\n    col_tiling = swizzle // bytewidth(utils.dtype_to_ir_type(dtype))\n    m, n = (128, col_tiling * 2)\n    tiling = (64, col_tiling)\n    tiled_layout = fa._tiled_wgmma_layout_for_upcast((m, n))\n\n    def kernel(ctx, in_, out, smems):\n        smem_in, smem_out, barrier = smems\n        ctx.async_copy(src_ref=in_, dst_ref=smem_in, swizzle=swizzle, barrier=barrier)\n        barrier.wait()\n        t = mgpu.FragmentedArray.load_tiled(smem_in, swizzle=swizzle, is_signed=True, layout=tiled_layout)\n        t.store_tiled(smem_out, swizzle=swizzle)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem_out, dst_ref=out, swizzle=swizzle)\n        ctx.await_async_copy(0)\n    x = jax.random.randint(jax.random.key(42), tile_shape((m, n), tiling), -128, 127, dtype=dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, [x, x, mgpu.TMABarrier()])\n    np.testing.assert_array_equal(f(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.parameters(False, True)\ndef test_iota_tensor(self, tiled_layout):\n    m = n = 64\n\n    def kernel(ctx, dst, _):\n        f32 = ir.F32Type.get()\n        index = ir.IndexType.get()\n        registers = iota_tensor(m, n, jnp.float32, tiled_layout).registers\n        assert registers.size == 16, registers.size\n        for i, vec_reg in enumerate(registers.flat):\n            for j in range(2):\n                reg = vector.extractelement(vec_reg, position=c(j, index))\n                memref.store(reg, dst, [gpu.thread_id(gpu.Dimension.x), c(2 * i + j, index)])\n    out_shape = jax.ShapeDtypeStruct((128, 32), jnp.float32)\n    regs = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    thread_ids = np.arange(128)\n    warp_ids = thread_ids // 32\n    lane_ids = thread_ids % 32\n    thread_rows = warp_ids * 16 + lane_ids // 4\n    thread_start_cols = lane_ids % 4 * 2\n    thread_cols = thread_start_cols[:, None] + np.arange(n // 8)[None] * 8\n    regs = regs.reshape(128, 8, 2, 2)\n    for row_half in range(2):\n        for col_half in range(2):\n            np.testing.assert_array_equal(regs[..., row_half, col_half], (thread_rows[:, None] + row_half * 8) * n + thread_cols + col_half)",
    "assertions": [
      "assert registers.size == 16, registers.size"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.float16, jnp.float32], tiled_layout=[False, True], transposed_smem=[False, True])\ndef test_store_untiled(self, dtype, tiled_layout, transposed_smem):\n\n    def kernel(ctx, out, _):\n        del ctx\n        if transposed_smem:\n            out = memref_transpose(out, (1, 0))\n        iota_tensor(64, 64, dtype, tiled_layout=tiled_layout).store_untiled(out, vector_store=not transposed_smem)\n    expected = np.arange(64 * 64, dtype=dtype).reshape(64, 64)\n    if transposed_smem:\n        expected = expected.T\n    iota = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), expected, ())()\n    np.testing.assert_array_equal(iota, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.float32, jnp.float16, jnp.int8], swizzle=(32, 64, 128), num_col_tiles=(1, 2, 3), tiled_layout=(False, True))\ndef test_store_tiled(self, dtype, swizzle, num_col_tiles, tiled_layout):\n    mlir_dtype = utils.dtype_to_ir_type(dtype)\n    if bytewidth(mlir_dtype) > 2 and swizzle == 32:\n        self.skipTest('Not implemented')\n    col_tiling = swizzle // bytewidth(mlir_dtype)\n    m = 128\n    n = col_tiling * num_col_tiles\n    tiling = (64, col_tiling)\n\n    def kernel(ctx, out, smem):\n        del ctx\n        iota_tensor(m, n, dtype, tiled_layout).store_tiled(smem, swizzle=swizzle)\n        copy(smem, out, swizzle=swizzle)\n    expected = np.arange(m * n, dtype=dtype).reshape(m // tiling[0], tiling[0], n // tiling[1], tiling[1]).transpose(0, 2, 1, 3)\n    iota = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), expected, expected)()\n    np.testing.assert_array_equal(iota, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.float16, jnp.int8], swizzle=(32, 64, 128))\ndef test_store_tiled_short_n(self, dtype, swizzle):\n    mlir_dtype = utils.dtype_to_ir_type(dtype)\n    col_tiling = swizzle // bytewidth(mlir_dtype)\n    m = 128\n    n = 16 // bytewidth(mlir_dtype)\n    tiling = (64, col_tiling)\n\n    def kernel(ctx, out, smem):\n        iota_tensor(m, n, dtype).store_tiled(smem, swizzle=swizzle)\n        ctx.async_copy(src_ref=smem, dst_ref=out, swizzle=swizzle, gmem_slice=(ds(0, m), ds(0, col_tiling)), gmem_transform=mgpu.TileTransform(tiling))\n        ctx.await_async_copy(0)\n    smem_shape = jax.ShapeDtypeStruct((m // tiling[0], 1, *tiling), dtype)\n    expected = np.arange(m * n, dtype=dtype).reshape(m, n)\n    iota = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), expected, smem_shape)()\n    np.testing.assert_array_equal(iota, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(m=(64, 128, 192), n=(64, 128, 192), k_steps=(1, 2), rhs_transpose=(False, True), swizzle=(32, 64, 128), dtype=[jnp.float16, jnp.bfloat16], tiled_layout=(False, True))\ndef test_wgmma_reg_lhs(self, m, n, k_steps, rhs_transpose, swizzle, dtype, tiled_layout):\n    index = ir.IndexType.get()\n    bytewidth = 2\n    nk_tile = swizzle // bytewidth\n    k = nk_tile * k_steps\n\n    def kernel(ctx, rhs, out, rhs_smem):\n        del ctx\n        for ki in range(k_steps):\n            for ni in range(n // nk_tile):\n                rhs_slice = (ds(c(ki * nk_tile, index), nk_tile), ds(c(ni * nk_tile, index), nk_tile))\n                if rhs_transpose:\n                    rhs_slice = rhs_slice[::-1]\n                copy(src=memref_slice(rhs, rhs_slice), dst=memref_slice(rhs_smem, (ki, ni)), swizzle=swizzle)\n        init_acc = mgpu.WGMMAAccumulator.zero(m=m, n=n)\n        lhs_regs = iota_tensor(m, k, dtype, tiled_layout)\n        if rhs_transpose:\n            rhs_smem = memref_transpose(rhs_smem, (0, 1, 3, 2))\n        acc = mgpu.wgmma(init_acc, lhs_regs, rhs_smem, swizzle=swizzle)\n        nvvm.wgmma_commit_group_sync_aligned()\n        nvvm.wgmma_wait_group_sync_aligned(0)\n        acc.value.store_untiled(out)\n    y_shape = (n, k) if rhs_transpose else (k, n)\n    y = self.prng.uniform(-1, 1, y_shape).astype(dtype)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    scratch_shape = jax.ShapeDtypeStruct((k_steps, n // nk_tile, nk_tile, nk_tile), dtype)\n    z = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), y, out_shape, scratch_shape)(y)\n    x = np.arange(m * k, dtype=dtype).reshape(m, k)\n    ref = jax.lax.dot(x, y.T if rhs_transpose else y, preferred_element_type=jnp.float32)\n    rtol = 0.0005\n    np.testing.assert_allclose(z, ref, rtol=rtol, atol=0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(rhs_transpose=(False, True), swizzle=(32, 64, 128), n=(8, 16), small_rhs_tile=(False, True))\ndef test_narrow_n(self, rhs_transpose, swizzle, n, small_rhs_tile):\n    m, k_steps = (64, 2)\n    bytewidth = 2\n    nk_tile = swizzle // bytewidth\n    k = nk_tile * k_steps\n    if small_rhs_tile and (not rhs_transpose):\n        self.skipTest('Small tiles only supported for transposed RHS')\n    n_tile = 8 if small_rhs_tile else nk_tile\n\n    def kernel(ctx, rhs, out, smem):\n        rhs_smem, barrier = smem\n        gmem_slice = (ds(0, k), ds(0, max(n_tile, n)))\n        transform = (mgpu.TileTransform((n_tile, nk_tile)),)\n        if rhs_transpose:\n            gmem_slice = gmem_slice[::-1]\n            transform += (mgpu.TransposeTransform((1, 0, 2, 3)),)\n        ctx.async_copy(src_ref=rhs, dst_ref=rhs_smem, swizzle=swizzle, gmem_slice=gmem_slice, gmem_transform=transform, barrier=barrier)\n        barrier.wait()\n        init_acc = mgpu.WGMMAAccumulator.zero(m=m, n=n)\n        lhs_regs = iota_tensor(m, k, jnp.float16)\n        if rhs_transpose:\n            rhs_smem = memref_transpose(rhs_smem, (0, 1, 3, 2))\n        if not small_rhs_tile:\n            smem_slice = (slice(None), slice(None), slice(None), ds(0, n))\n            rhs_smem = memref_slice(rhs_smem, smem_slice)\n        acc = mgpu.wgmma(init_acc, lhs_regs, rhs_smem, swizzle=swizzle)\n        nvvm.wgmma_commit_group_sync_aligned()\n        nvvm.wgmma_wait_group_sync_aligned(0)\n        acc.value.store_untiled(out)\n    jax_dtype = jnp.float16\n    y_shape = (n, k) if rhs_transpose else (k, n)\n    y = self.prng.uniform(-1, 1, y_shape).astype(jax_dtype)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    rhs_scratch_shape = jax.ShapeDtypeStruct((k_steps, (n + n_tile - 1) // n_tile, n_tile, nk_tile), jax_dtype)\n    z = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), y, out_shape, (rhs_scratch_shape, mgpu.TMABarrier()))(y)\n    x = np.arange(m * k, dtype=jax_dtype).reshape(m, k)\n    ref = jax.lax.dot(x, y.T if rhs_transpose else y, preferred_element_type=jnp.float32)\n    np.testing.assert_allclose(z, ref, rtol=0.0005, atol=0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(small_dim=(0, 1), tiled_layout=(False, True))\ndef test_tma_small_tile_store(self, small_dim, tiled_layout):\n    if small_dim == 0:\n        shape = (4, 128)\n    elif small_dim == 1:\n        shape = (128, 8)\n    else:\n        raise ValueError('small_dim must be 0 or 1')\n    tiled_shape = ((shape[0] + 63) // 64, (shape[1] + 63) // 64, 64, 64)\n    m, n = (math.prod(tiled_shape[0::2]), math.prod(tiled_shape[1::2]))\n\n    def kernel(ctx, dst, tmp):\n        vals = iota_tensor(m, n, jnp.float16)\n        vals.store_tiled(tmp, swizzle=128)\n        ctx.async_copy(src_ref=tmp, dst_ref=dst, swizzle=128, gmem_transform=mgpu.TileTransform((64, 64)), gmem_slice=(ds(0, m), ds(0, n)))\n        ctx.await_async_copy(0)\n    tiled = jax.ShapeDtypeStruct(tiled_shape, jnp.float16)\n    out = jax.ShapeDtypeStruct(shape, jnp.float16)\n    y = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out, tiled)()\n    iota = np.arange(m * n, dtype=jnp.float16).reshape([m, n])\n    np.testing.assert_array_equal(y, iota[:shape[0], :shape[1]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(op=(operator.add, operator.mul, operator.sub, (lambda x, y: mgpu.FragmentedArray.min(x, y), np.minimum), (lambda x, y: mgpu.FragmentedArray.max(x, y), np.maximum)), dtype=[jnp.float32, jnp.int32, jnp.uint32], m=(64, 128), n=(8, 16, 32, 64, 80, 128, 256))\n@jtu.ignore_warning(message='(invalid value|divide by zero)', category=RuntimeWarning)\ndef test_binary(self, op, dtype, m=64, n=32):\n    if isinstance(op, tuple):\n        op, np_op = op\n    else:\n        np_op = op\n    for scalar_rhs in [None, 2]:\n\n        def kernel(ctx, dst, _):\n            mlir_dtype = utils.dtype_to_ir_type(dtype)\n            iota = iota_tensor(m, n, dtype)\n            rhs = iota if scalar_rhs is None else c(scalar_rhs, mlir_dtype)\n            op(iota, rhs).store_untiled(dst)\n        out_shape = jax.ShapeDtypeStruct((m, n), dtype)\n        result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n        ref_x = np.arange(m * n, dtype=dtype).reshape(m, n)\n        ref_rhs = scalar_rhs or ref_x\n        np.testing.assert_array_equal(result, np_op(ref_x, ref_rhs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(op=[operator.truediv, operator.floordiv, operator.mod], dtype=[jnp.float32, jnp.int32, jnp.uint32])\ndef test_division(self, op, dtype, m=64, n=32):\n    if jnp.issubdtype(dtype, jnp.integer) and op is operator.truediv:\n        self.skipTest('Unsupported for integer types')\n    if jnp.issubdtype(dtype, jnp.floating) and op is operator.mod:\n        self.skipTest('Unsupported for floating types')\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, dtype)\n        op(dtype(4.2).item() * iota, iota + 1).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), dtype)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    iota = np.arange(m * n, dtype=dtype).reshape(m, n)\n    np.testing.assert_allclose(result, op(dtype(4.2).item() * iota, iota + 1), atol=2e-07)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(op=[operator.lt, operator.le, operator.gt, operator.ge, operator.eq, operator.ne], dtype=[jnp.float32, jnp.int32, jnp.uint32], rhs_is_literal=[False, True])\ndef test_comparison(self, op, dtype, rhs_is_literal, m=64, n=32):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, dtype)\n        rhs = 0 if rhs_is_literal else iota + 1\n        op(iota, rhs).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.bool)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    iota = np.arange(m * n, dtype=dtype).reshape(m, n)\n    rhs = rhs = 0 if rhs_is_literal else iota + 1\n    np.testing.assert_array_equal(result, op(iota, rhs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(tiled_layout=(False, True))\ndef test_foreach(self, tiled_layout):\n    dtype = jnp.int32\n    swizzle = 128\n    tile = (64, swizzle // jnp.dtype(dtype).itemsize)\n    shape = (128, 192)\n    tiled_shape = mgpu.tile_shape(shape, tile)\n    mlir_dtype = utils.dtype_to_ir_type(dtype)\n    cst = 9999\n\n    def causal(val, idx):\n        row, col = idx\n        mask = arith.cmpi(arith.CmpIPredicate.uge, row, col)\n        return arith.select(mask, val, c(cst, mlir_dtype))\n    tiling = mgpu.TileTransform(tile)\n\n    def kernel(ctx, dst, smem):\n        x = iota_tensor(shape[0], shape[1], dtype, tiled_layout=tiled_layout)\n        x.foreach(causal, create_array=True, is_signed=False).store_untiled(smem)\n        mgpu.commit_shared()\n        ctx.async_copy(src_ref=smem, dst_ref=dst)\n        ctx.await_async_copy(0)\n    iota = np.arange(np.prod(shape), dtype=dtype).reshape(*shape)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), jax.ShapeDtypeStruct(shape=shape, dtype=dtype), jax.ShapeDtypeStruct(shape=shape, dtype=dtype))()\n    expected = jnp.tril(iota) + jnp.triu(jnp.ones(shape), k=1) * cst\n    np.testing.assert_array_equal(result, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(op=[operator.and_, operator.or_, operator.xor], dtype=[jnp.uint32])\ndef test_bitwise(self, op, dtype, m=64, n=8):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, dtype)\n        op(iota, iota + 1).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), dtype)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    iota = np.arange(m * n, dtype=dtype).reshape(m, n)\n    np.testing.assert_array_equal(result, op(iota, iota + 1))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(ops=((lambda x: -x, jax.lax.neg), (lambda x: x + 42, lambda x: x + 42), (lambda x: x.tanh(), jax.lax.tanh)), dtype=[jnp.float32, jnp.int32, jnp.uint32])\ndef test_unary(self, ops, dtype, m=64, n=32):\n    op, np_op = ops\n    if np_op is jax.lax.tanh and jnp.issubdtype(dtype, jnp.integer):\n        raise self.skipTest('Tanh not supported for integer types')\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, dtype)\n        op(iota).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), dtype)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=dtype).reshape(m, n)\n    np.testing.assert_allclose(result, np_op(x), atol=2e-07, rtol=2e-07)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "def test_select(self, m=64, n=32):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, jnp.int32)\n        (iota < 16).select(iota * 2, iota * 3).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.int32)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=jnp.int32).reshape(m, n)\n    np.testing.assert_array_equal(result, np.where(x < 16, x * 2, x * 3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(ops=[(lambda x: mgpu.FragmentedArray.exp(x), np.exp), (lambda x: mgpu.FragmentedArray.sin(x), np.sin), (lambda x: mgpu.FragmentedArray.cos(x), np.cos), (lambda x: mgpu.FragmentedArray.rsqrt(x), jax.lax.rsqrt)], approx=[False, True])\n@jtu.ignore_warning(message='overflow encountered', category=RuntimeWarning)\ndef test_math(self, ops, approx, m=64, n=32):\n    op, np_op = ops\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, jnp.float32)\n        op(iota).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=jnp.float32).reshape(m, n)\n    atol = 0.005 if approx else 2e-07\n    rtol = 4e-06 if approx else 2e-07\n    np.testing.assert_allclose(result, np_op(x), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(op=(arith.addf, arith.maximumf), m=(64, 128), n=(8, 16, 32, 64, 80, 128, 256))\ndef test_reduce(self, op, m=64, n=32):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, jnp.float32)\n        iota.reduce(op, axis=1).broadcast_minor(n).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    x = np.arange(m * n, dtype=jnp.float32).reshape(m, n)\n    if op == arith.addf:\n        expected = np.broadcast_to(x.sum(axis=1, keepdims=True), x.shape)\n    elif op == arith.maximumf:\n        expected = np.broadcast_to(x.max(axis=1, keepdims=True), x.shape)\n    else:\n        raise NotImplementedError(f'Unsupported op: {op}')\n    np.testing.assert_array_equal(result, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "def test_splat_layout(self):\n    m, n = (64, 8)\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(m, n, jnp.float32)\n        cte = c(1, iota.mlir_dtype)\n        cte_arr = mgpu.FragmentedArray.splat(cte, ())\n        cte_arr = cte_arr.reshape((1, 1)).broadcast((m, n))\n        (iota + cte_arr).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.float32)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    expected = np.arange(m * n, dtype=jnp.float32).reshape(m, n) + 1\n    np.testing.assert_array_equal(result, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "def test_convert_bool_to_u8(self):\n    m, n = (128, 128)\n\n    def kernel(ctx, dst, _):\n        i8 = ir.IntegerType.get_signless(8)\n        iota = iota_tensor(m, n, jnp.uint8)\n        (iota > 10).astype(i8, is_signed=False).store_untiled(dst)\n    out_shape = jax.ShapeDtypeStruct((m, n), jnp.int8)\n    result = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())()\n    iota = np.arange(m * n, dtype=jnp.uint8).reshape(m, n)\n    np.testing.assert_array_equal(result, (iota > 10).astype(jnp.uint8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  },
  {
    "test_code": "@parameterized.product(shape=((128, 128), (64, 8), (64, 256)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_wgmma_tiled_layout(self, shape, dtype):\n\n    def kernel(ctx, dst, _):\n        iota = iota_tensor(*shape, dtype)\n        tiled = iota.to_layout(fa._tiled_wgmma_layout(shape))\n        self.assertEqual(tiled.registers.shape, (shape[0] // 64, shape[1] // 8, 1, 1, 2, 1, 1, 1, 1, 1))\n        self.assertEqual(tiled.shape, shape)\n        self.assertEqual(tiled.mlir_dtype, iota.mlir_dtype)\n        tiled.store_untiled(dst)\n    ty = jax.ShapeDtypeStruct(shape, dtype)\n    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), ty, ())\n    expected = np.arange(math.prod(shape), dtype=dtype).reshape(shape)\n    np.testing.assert_array_equal(f(), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/mosaic/gpu_test.py",
    "function": "def iota_tensor(m, n, dtype: jax.typing.DTypeLike, tiled_layout=False):\n    assert m % 64 == 0\n    assert n % 8 == 0\n\n    def c(i):\n        return arith.constant(index, ir.IntegerAttr.get(index, i))\n    index = ir.IndexType.get()\n    i32 = ir.IntegerType.get_signless(32)\n    warp_id = arith.divui(gpu.thread_id(gpu.Dimension.x), c(32))\n    within_warp_id = arith.remui(gpu.thread_id(gpu.Dimension.x), c(32))\n    warp_row_start = arith.muli(warp_id, c(16))\n    within_warp_row = arith.divui(within_warp_id, c(4))\n    start_row = arith.addi(warp_row_start, within_warp_row)\n    start_col = arith.muli(arith.remui(within_warp_id, c(4)), c(2))\n    registers = np.empty((m // 64, n // 8, 2, 1), dtype=object)\n    for row_tile, col_tile, row_subtile, _ in np.ndindex(registers.shape):\n        row = arith.addi(start_row, c(row_tile * 64 + row_subtile * 8))\n        col = arith.addi(start_col, c(col_tile * 8))\n        row_value_base = arith.muli(row, c(n))\n        vec = llvm.mlir_undef(ir.VectorType.get((2,), i32))\n        for col_offset in range(2):\n            value = arith.addi(row_value_base, arith.addi(c(col_offset), col))\n            value = arith.index_cast(i32, value)\n            vec = vector.insertelement(value, vec, position=c(col_offset))\n        registers[row_tile, col_tile, row_subtile, 0] = vec\n    t = mgpu.FragmentedArray(_registers=registers, _layout=mgpu.WGMMA_LAYOUT, _is_signed=True)\n    if tiled_layout:\n        t = t.to_layout(mgpu.TILED_LAYOUT_WGMMA)\n    return t.astype(utils.dtype_to_ir_type(dtype), is_signed=utils.is_signed(dtype))"
  }
]