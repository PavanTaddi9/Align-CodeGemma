[
  {
    "test_code": "def test_vmap_jumble_over_sin_kernel(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Only tested on TPU')\n    row_count = 8\n    col_grid_size = 5\n    ragged_shape = [3, 1, 4]\n    sizes = lax.convert_element_type(jnp.array([128 * x for x in ragged_shape]), core.bint(col_grid_size * 128))\n    x = jax.vmap(lambda n: jnp.ones((row_count, n)), out_axes=batching.jumble_axis)(sizes)\n\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sin(x_ref[...])\n\n    def invoke_kernel(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda j, k: (j, k))], out_specs=pl.BlockSpec((8, 128), lambda j, k: (j, k)), out_shape=jax.ShapeDtypeStruct((8, col_grid_size * 128), dtype=jnp.float32), grid=(1, col_grid_size), interpret=self.INTERPRET, debug=True)(x)\n    res = jax.vmap(invoke_kernel, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)\n    ref = jax.vmap(jnp.sin, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)\n    _assert_ragged_equal_with_elementwise_mask(row_count, col_grid_size, ragged_shape, res.data, ref.data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_jumble_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_vmap_jumble_over_sin_kernel_grid_remapping(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Only tested on TPU')\n    row_count = 8\n    col_grid_size = 5\n    ragged_shape = [3, 1, 4]\n    sizes = lax.convert_element_type(jnp.array([128 * x for x in ragged_shape]), core.bint(col_grid_size * 128))\n    x = jax.vmap(lambda n: jnp.ones((row_count, n)), out_axes=batching.jumble_axis)(sizes)\n\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sin(x_ref[...]) * pl.program_id(2)\n\n    def invoke_kernel(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda j, k: (j, k))], out_specs=pl.BlockSpec((8, 128), lambda j, k: (j, k)), out_shape=jax.ShapeDtypeStruct((8, 640), dtype=jnp.float32), grid=(1, 5), interpret=self.INTERPRET)(x)\n    with self.assertRaisesRegex(ValueError, 'Axis 2 is out of bounds for grid'):\n        jax.vmap(invoke_kernel, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_jumble_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_vmap_jumble_ragged_boundary_unaligned_with_grid(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Only tested on TPU')\n    self.skipTest('Checkify NYI')\n    row_count = 8\n    col_grid_size = 5\n    ragged_shape = [3, 1, 4]\n    sizes = lax.convert_element_type(jnp.array([128 * x - 1 for x in ragged_shape]), core.bint(col_grid_size * 128))\n    x = jax.vmap(lambda n: jnp.ones((row_count, n)), out_axes=batching.jumble_axis)(sizes)\n\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sin(x_ref[...])\n\n    def invoke_kernel(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda j, k: (j, k))], out_specs=pl.BlockSpec((8, 128), lambda j, k: (j, k)), out_shape=jax.ShapeDtypeStruct((8, 640), dtype=jnp.float32), grid=(1, 5), interpret=False)(x)\n    with self.assertRaisesRegex(ValueError, 'Ragged input shape must be evenly divisble by the grid size at the ragged dimension 2'):\n        jax.vmap(invoke_kernel, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_jumble_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_vmap_jumble_over_sin_kernel(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Only tested on TPU')\n    row_count = 8\n    col_grid_size = 5\n    ragged_shape = [3, 1, 4]\n    sizes = lax.convert_element_type(jnp.array([128 * x for x in ragged_shape]), core.bint(col_grid_size * 128))\n    x = jax.vmap(lambda n: jnp.ones((row_count, n)), out_axes=batching.jumble_axis)(sizes)\n\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sin(x_ref[...])\n\n    def invoke_kernel(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda j, k: (j, k))], out_specs=pl.BlockSpec((8, 128), lambda j, k: (j, k)), out_shape=jax.ShapeDtypeStruct((8, col_grid_size * 128), dtype=jnp.float32), grid=(1, col_grid_size), interpret=self.INTERPRET, debug=True)(x)\n    res = jax.vmap(invoke_kernel, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)\n    ref = jax.vmap(jnp.sin, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)\n    _assert_ragged_equal_with_elementwise_mask(row_count, col_grid_size, ragged_shape, res.data, ref.data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_jumble_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_vmap_jumble_over_sin_kernel_grid_remapping(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Only tested on TPU')\n    row_count = 8\n    col_grid_size = 5\n    ragged_shape = [3, 1, 4]\n    sizes = lax.convert_element_type(jnp.array([128 * x for x in ragged_shape]), core.bint(col_grid_size * 128))\n    x = jax.vmap(lambda n: jnp.ones((row_count, n)), out_axes=batching.jumble_axis)(sizes)\n\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sin(x_ref[...]) * pl.program_id(2)\n\n    def invoke_kernel(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda j, k: (j, k))], out_specs=pl.BlockSpec((8, 128), lambda j, k: (j, k)), out_shape=jax.ShapeDtypeStruct((8, 640), dtype=jnp.float32), grid=(1, 5), interpret=self.INTERPRET)(x)\n    with self.assertRaisesRegex(ValueError, 'Axis 2 is out of bounds for grid'):\n        jax.vmap(invoke_kernel, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_jumble_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_vmap_jumble_ragged_boundary_unaligned_with_grid(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Only tested on TPU')\n    self.skipTest('Checkify NYI')\n    row_count = 8\n    col_grid_size = 5\n    ragged_shape = [3, 1, 4]\n    sizes = lax.convert_element_type(jnp.array([128 * x - 1 for x in ragged_shape]), core.bint(col_grid_size * 128))\n    x = jax.vmap(lambda n: jnp.ones((row_count, n)), out_axes=batching.jumble_axis)(sizes)\n\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sin(x_ref[...])\n\n    def invoke_kernel(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda j, k: (j, k))], out_specs=pl.BlockSpec((8, 128), lambda j, k: (j, k)), out_shape=jax.ShapeDtypeStruct((8, 640), dtype=jnp.float32), grid=(1, 5), interpret=False)(x)\n    with self.assertRaisesRegex(ValueError, 'Ragged input shape must be evenly divisble by the grid size at the ragged dimension 2'):\n        jax.vmap(invoke_kernel, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_jumble_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_vmap_jumble_over_sin_kernel(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Only tested on TPU')\n    row_count = 8\n    col_grid_size = 5\n    ragged_shape = [3, 1, 4]\n    sizes = lax.convert_element_type(jnp.array([128 * x for x in ragged_shape]), core.bint(col_grid_size * 128))\n    x = jax.vmap(lambda n: jnp.ones((row_count, n)), out_axes=batching.jumble_axis)(sizes)\n\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sin(x_ref[...])\n\n    def invoke_kernel(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda j, k: (j, k))], out_specs=pl.BlockSpec((8, 128), lambda j, k: (j, k)), out_shape=jax.ShapeDtypeStruct((8, col_grid_size * 128), dtype=jnp.float32), grid=(1, col_grid_size), interpret=self.INTERPRET, debug=True)(x)\n    res = jax.vmap(invoke_kernel, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)\n    ref = jax.vmap(jnp.sin, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)\n    _assert_ragged_equal_with_elementwise_mask(row_count, col_grid_size, ragged_shape, res.data, ref.data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_jumble_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_vmap_jumble_over_sin_kernel_grid_remapping(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Only tested on TPU')\n    row_count = 8\n    col_grid_size = 5\n    ragged_shape = [3, 1, 4]\n    sizes = lax.convert_element_type(jnp.array([128 * x for x in ragged_shape]), core.bint(col_grid_size * 128))\n    x = jax.vmap(lambda n: jnp.ones((row_count, n)), out_axes=batching.jumble_axis)(sizes)\n\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sin(x_ref[...]) * pl.program_id(2)\n\n    def invoke_kernel(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda j, k: (j, k))], out_specs=pl.BlockSpec((8, 128), lambda j, k: (j, k)), out_shape=jax.ShapeDtypeStruct((8, 640), dtype=jnp.float32), grid=(1, 5), interpret=self.INTERPRET)(x)\n    with self.assertRaisesRegex(ValueError, 'Axis 2 is out of bounds for grid'):\n        jax.vmap(invoke_kernel, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_jumble_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_vmap_jumble_ragged_boundary_unaligned_with_grid(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Only tested on TPU')\n    self.skipTest('Checkify NYI')\n    row_count = 8\n    col_grid_size = 5\n    ragged_shape = [3, 1, 4]\n    sizes = lax.convert_element_type(jnp.array([128 * x - 1 for x in ragged_shape]), core.bint(col_grid_size * 128))\n    x = jax.vmap(lambda n: jnp.ones((row_count, n)), out_axes=batching.jumble_axis)(sizes)\n\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sin(x_ref[...])\n\n    def invoke_kernel(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda j, k: (j, k))], out_specs=pl.BlockSpec((8, 128), lambda j, k: (j, k)), out_shape=jax.ShapeDtypeStruct((8, 640), dtype=jnp.float32), grid=(1, 5), interpret=False)(x)\n    with self.assertRaisesRegex(ValueError, 'Ragged input shape must be evenly divisble by the grid size at the ragged dimension 2'):\n        jax.vmap(invoke_kernel, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_jumble_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_vmap_jumble_over_matmul_kernel(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Only tested on TPU')\n    if jtu.is_device_tpu(version=4):\n        self.skipTest('Flaky 15% of the time on tpuv4?')\n    m = 128\n    k = 640\n    n = 640\n\n    def matmul_kernel(x_ref, y_ref, x_sentinel, z_ref):\n\n        @pl.when(x_sentinel[...][0][0] == 1.0)\n        def _():\n            z_ref[...] = jnp.zeros_like(z_ref)\n            x_sentinel[...] = jnp.zeros_like(x_sentinel)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul(x: jax.Array, y: jax.Array, x_sentinel: jax.Array, *, bm: int=128, bk: int=128, bn: int=640):\n        grid = (n // bn, k // bk)\n        return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((bm, bk), lambda j, k: (0, k)), pl.BlockSpec((bk, bn), lambda j, k: (k, j)), pl.BlockSpec((bm, bn), lambda j, k: (0, j))], out_specs=pl.BlockSpec((bm, bn), lambda j, k: (0, j)), grid=grid, input_output_aliases={2: 0}, interpret=self.INTERPRET)(x, y, x_sentinel)\n    ragged_shape = [3, 1, 4]\n    sizes = lax.convert_element_type(jnp.array([128 * x for x in ragged_shape]), core.bint(k))\n    x = jax.vmap(lambda k_: jnp.ones((m, k_)), out_axes=batching.jumble_axis)(sizes)\n    x_sentinel = jax.vmap(lambda k_: jnp.ones((m, k_)), out_axes=batching.jumble_axis)(sizes)\n    y = jax.vmap(lambda k_: jnp.ones((k_, n)), out_axes=batching.jumble_axis)(sizes)\n    res = jax.vmap(matmul, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x, y, x_sentinel)\n    ref = jax.vmap(jnp.dot, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x, y)\n    ref = ref.data\n    res = res.data\n    np.testing.assert_allclose(ref, res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_jumble_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_vmap_jumble_over_sin_kernel(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Only tested on TPU')\n    row_count = 8\n    col_grid_size = 5\n    ragged_shape = [3, 1, 4]\n    sizes = lax.convert_element_type(jnp.array([128 * x for x in ragged_shape]), core.bint(col_grid_size * 128))\n    x = jax.vmap(lambda n: jnp.ones((row_count, n)), out_axes=batching.jumble_axis)(sizes)\n\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sin(x_ref[...])\n\n    def invoke_kernel(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda j, k: (j, k))], out_specs=pl.BlockSpec((8, 128), lambda j, k: (j, k)), out_shape=jax.ShapeDtypeStruct((8, col_grid_size * 128), dtype=jnp.float32), grid=(1, col_grid_size), interpret=self.INTERPRET, debug=True)(x)\n    res = jax.vmap(invoke_kernel, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)\n    ref = jax.vmap(jnp.sin, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x)\n    _assert_ragged_equal_with_elementwise_mask(row_count, col_grid_size, ragged_shape, res.data, ref.data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_jumble_test.py",
    "function": "def _assert_ragged_equal_with_elementwise_mask(row_count, col_grid_size, ragged_shape, res, ref):\n    total_columns = col_grid_size * 128\n    mask = jnp.zeros((len(ragged_shape), row_count, total_columns), dtype=bool)\n    for i, r in enumerate(ragged_shape):\n        mask = mask.at[i, :, :r * 128].set(True)\n    res_valid = jnp.where(mask, res, -1)\n    ref_valid = jnp.where(mask, ref, -1)\n    np.testing.assert_allclose(res_valid, ref_valid)"
  },
  {
    "test_code": "def test_vmap_jumble_over_add_kernel(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Only tested on TPU')\n    row_count = 8\n    col_grid_size = 5\n    ragged_shape = [3, 1, 4]\n    sizes = lax.convert_element_type(jnp.array([128 * x for x in ragged_shape]), core.bint(col_grid_size * 128))\n    x = jax.vmap(lambda n: jnp.ones((row_count, n)), out_axes=batching.jumble_axis)(sizes)\n    y = jax.vmap(lambda n: jnp.ones((row_count, n)), out_axes=batching.jumble_axis)(sizes)\n\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = x_ref[...] + y_ref[...]\n\n    def invoke_kernel(x, y):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda j, k: (j, k)), pl.BlockSpec((8, 128), lambda j, k: (j, k))], out_specs=pl.BlockSpec((8, 128), lambda j, k: (j, k)), out_shape=jax.ShapeDtypeStruct((8, col_grid_size * 128), dtype=jnp.float32), grid=(1, col_grid_size), interpret=self.INTERPRET)(x, y)\n    for _ in range(4):\n        res = jax.vmap(invoke_kernel, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x, y)\n        res = res.data\n        total = len(ragged_shape) * row_count * col_grid_size * 128\n        res_total = np.prod(res.shape)\n        self.assertEqual(res_total, total)\n        ref = jax.vmap(lambda x, y: x + y, out_axes=batching.jumble_axis, in_axes=batching.jumble_axis, axis_size=3)(x, y)\n        _assert_ragged_equal_with_elementwise_mask(row_count, col_grid_size, ragged_shape, res, ref.data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_jumble_test.py",
    "function": "def _assert_ragged_equal_with_elementwise_mask(row_count, col_grid_size, ragged_shape, res, ref):\n    total_columns = col_grid_size * 128\n    mask = jnp.zeros((len(ragged_shape), row_count, total_columns), dtype=bool)\n    for i, r in enumerate(ragged_shape):\n        mask = mask.at[i, :, :r * 128].set(True)\n    res_valid = jnp.where(mask, res, -1)\n    ref_valid = jnp.where(mask, ref, -1)\n    np.testing.assert_allclose(res_valid, ref_valid)"
  }
]