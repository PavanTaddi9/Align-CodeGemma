[
  {
    "test_code": "def test_grid_shrinking(self):\n    \"\"\"Make sure that grid shrinking does not change the attention output.\"\"\"\n\n    class IdentityMask(mask_lib._ComputableMask):\n        \"\"\"Identity mask that is guaranteed to trigger grid shrinking.\"\"\"\n\n        def __init__(self, shape: tuple[int, int], shard_count: int=1):\n\n            def identity_mask_function(q_ids, kv_ids):\n                return q_ids == kv_ids\n            super().__init__(shape=shape, mask_function=identity_mask_function, shard_count=shard_count)\n\n        def __eq__(self, other: object):\n            if not isinstance(other, type(self)):\n                return NotImplemented\n            return self.shape == other.shape and np.array_equal(self.q_sequence, other.q_sequence)\n\n        def __hash__(self):\n            return hash((type(self), self.shape, self.q_sequence.tobytes() if self.q_sequence is not None else None))\n    seq_len = 256\n    head_dim = 128\n    key = random.key(42)\n    k1, k2, k3 = random.split(key, 3)\n    q = random.uniform(k1, (1, seq_len, head_dim), dtype=jnp.float32)\n    k = random.uniform(k2, (seq_len, head_dim), dtype=jnp.float32)\n    v = random.uniform(k3, (seq_len, head_dim), dtype=jnp.float32)\n    identity_mask = mask_lib.MultiHeadMask([IdentityMask((seq_len, seq_len))])\n    process_mask_path = 'jax.experimental.pallas.ops.tpu.splash_attention.splash_attention_mask_info.process_mask'\n    process_mask_shrink = lambda *args, **kwargs: process_mask(*args, **kwargs, shrink_grid=True)\n    process_mask_no_shrink = lambda *args, **kwargs: process_mask(*args, **kwargs, shrink_grid=False)\n    with unittest.mock.patch(process_mask_path, process_mask_shrink):\n        shrink_out = splash.make_splash_mqa_single_device(identity_mask)(q, k, v)\n    with unittest.mock.patch(process_mask_path, process_mask_no_shrink):\n        no_shrink_out = splash.make_splash_mqa_single_device(identity_mask)(q, k, v)\n    np.testing.assert_array_equal(shrink_out, no_shrink_out)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def __init__(self, key: jax.Array):\n    self.key = key"
  },
  {
    "test_code": "@parameterized.product(is_mqa=(False, True), is_segmented=(False, True), is_dynamic_mask=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention(self, is_mqa, is_segmented, is_dynamic_mask, data):\n    seed = data.draw(seed_strategy())\n    key = random.key(seed)\n    k1, k2, k3 = random.split(key, 3)\n    q_seq_len, kv_seq_len, num_q_heads, num_kv_heads, head_dim_qk, head_dim_v, dtype = data.draw(mha_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (num_q_heads, q_seq_len, head_dim_qk), dtype=dtype)\n    if is_mqa:\n        k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    else:\n        k = random.uniform(k2, (num_kv_heads, kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (num_kv_heads, kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy())\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, num_q_heads))\n    mask = mask_lib.MultiHeadMask(tuple((m.get_mask() for m in masks)))\n    if is_dynamic_mask:\n        mask = to_dynamic_mask(mask)\n    block_sizes = data.draw(block_sizes_strategy(q_seq_len, kv_seq_len))\n    if is_mqa:\n        attn_ref = splash.make_masked_mqa_reference(mask)\n        attn = splash.make_splash_mqa_single_device(mask, block_sizes=block_sizes, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    else:\n        attn_ref = splash.make_masked_mha_reference(mask)\n        attn = splash.make_splash_mha_single_device(mask, block_sizes=block_sizes, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o = attn(q, k, v, segment_ids)\n    o_ref = attn_ref(q.astype(np.float32), k.astype(np.float32), v.astype(np.float32), segment_ids, attn_logits_soft_cap=attn_logits_soft_cap)\n    self._assert_allclose(o, o_ref, atol=0.003, rtol=0.003)",
    "assertions": [
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.product(is_mqa=(False, True), is_segmented=(False, True), is_dynamic_mask=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_fwd(self, is_mqa, is_segmented, is_dynamic_mask, data):\n    seed = data.draw(seed_strategy())\n    key = random.key(seed)\n    k1, k2, k3 = random.split(key, 3)\n    q_seq_len, kv_seq_len, num_q_heads, num_kv_heads, head_dim_qk, head_dim_v, dtype = data.draw(mha_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (num_q_heads, q_seq_len, head_dim_qk), dtype=dtype)\n    if is_mqa:\n        k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    else:\n        k = random.uniform(k2, (num_kv_heads, kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (num_kv_heads, kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy())\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, num_q_heads))\n    mask = mask_lib.MultiHeadMask(tuple((m.get_mask() for m in masks)))\n    if is_dynamic_mask:\n        mask = to_dynamic_mask(mask)\n    block_sizes = data.draw(block_sizes_strategy(q_seq_len, kv_seq_len))\n    if is_mqa:\n        attn_ref = splash.make_masked_mqa_reference(mask)\n        attn = splash.make_splash_mqa_single_device(mask, block_sizes=block_sizes, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    else:\n        attn_ref = splash.make_masked_mha_reference(mask)\n        attn = splash.make_splash_mha_single_device(mask, block_sizes=block_sizes, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    attn_ref = partial(attn_ref, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap)\n    o, (logsumexp,) = attn(q, k, v, segment_ids)\n    o_ref, (logsumexp_ref,) = attn_ref(q.astype(jnp.float32), k.astype(jnp.float32), v.astype(jnp.float32), segment_ids)\n    self._assert_allclose(o, o_ref, atol=0.003, rtol=0.003)\n    self._assert_allclose(logsumexp, logsumexp_ref, atol=0.001, rtol=0.001)",
    "assertions": [
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.product(is_segmented=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_custom_bwd(self, is_segmented, data):\n    seed = data.draw(seed_strategy(), label='seed')\n    key = random.key(1 + seed)\n    k1, k2, k3, k4 = random.split(key, 4)\n    q_seq_len, kv_seq_len, head_dim_qk, head_dim_v, dtype = data.draw(attention_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (q_seq_len, head_dim_qk), dtype=dtype)\n    k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n    v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, 1))\n    mask = jnp.array(masks[0].get_mask()[:, :])\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy(), label='logit_cap')\n    attn_ref = partial(splash.attention_reference, mask, attn_logits_soft_cap=attn_logits_soft_cap)\n    attn_custom = partial(splash.attention_reference_custom, mask, attn_logits_soft_cap=attn_logits_soft_cap)\n    attn_custom_vanilla = partial(splash.attention_reference_custom, mask, custom_type='vanilla', attn_logits_soft_cap=attn_logits_soft_cap)\n    o_ref, attn_vjp_ref = jax.vjp(attn_ref, q, k, v, segment_ids)\n    q32, k32, v32 = jax.tree.map(lambda x: x.astype(jnp.float32), (q, k, v))\n    o_custom = attn_custom(q32, k32, v32, segment_ids)\n    _, attn_vjp = jax.vjp(attn_custom, q32, k32, v32, segment_ids)\n    _, attn_vanilla_vjp = jax.vjp(attn_custom_vanilla, q32, k32, v32, segment_ids)\n    do = random.uniform(k4, o_custom.shape, dtype=o_custom.dtype) / 10.0\n    self._assert_allclose(o_custom, o_ref, atol=1e-05, rtol=1e-05)\n    dq, dk, dv, _ = attn_vjp(do)\n    dq_vanilla, dk_vanilla, dv_vanilla, _ = attn_vanilla_vjp(do)\n    dq_ref, dk_ref, dv_ref, _ = attn_vjp_ref(do)\n    if dtype == jnp.bfloat16:\n        atols = {'dv': 0.004, 'dq': 0.05, 'dk': 0.05}\n        atols_v = {'dv': 0.008, 'dq': 0.02, 'dk': 0.02}\n        rtols = {'dv': 0.004, 'dq': 0.05, 'dk': 0.05}\n        rtols_v = {'dv': 0.008, 'dq': 0.02, 'dk': 0.02}\n        if jtu.is_device_tpu(version=5):\n            atols['dk'] = 0.065\n    elif dtype == jnp.float32:\n        atols = {'dv': 0.003, 'dq': 0.05, 'dk': 0.05}\n        atols_v = {'dv': 0.0004, 'dq': 0.002, 'dk': 0.003}\n        rtols = {'dv': 0.003, 'dq': 0.05, 'dk': 0.05}\n        rtols_v = {'dv': 0.008, 'dq': 0.0005, 'dk': 0.0005}\n        if jtu.is_device_tpu(version=4):\n            atols['dk'] = 0.09\n    else:\n        raise NotImplementedError\n    self._assert_allclose(dv_vanilla, dv_ref, atol=atols_v['dv'], rtol=rtols_v['dv'])\n    self._assert_allclose(dv, dv_ref, atol=atols['dv'], rtol=rtols['dv'])\n    self._assert_allclose(dq_vanilla, dq_ref, atol=atols_v['dq'], rtol=rtols_v['dq'])\n    self._assert_allclose(dq, dq_ref, atol=atols['dq'], rtol=rtols['dq'])\n    self._assert_allclose(dk_vanilla, dk_ref, atol=atols_v['dk'], rtol=rtols_v['dk'])\n    self._assert_allclose(dk, dk_ref, atol=atols['dk'], rtol=rtols['dk'])",
    "assertions": [
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.product(is_mqa=(False, True), is_segmented=(False, True), downcast_smem_data=(False, True), use_fused_bwd_kernel=(False, True), use_dynamic_mask=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_bwd(self, is_mqa, is_segmented, downcast_smem_data, use_fused_bwd_kernel, use_dynamic_mask, data):\n    seed = data.draw(seed_strategy())\n    key = random.key(seed)\n    k1, k2, k3, k4 = random.split(key, 4)\n    q_seq_len, kv_seq_len, num_q_heads, num_kv_heads, head_dim_qk, head_dim_v, dtype = data.draw(mha_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (num_q_heads, q_seq_len, head_dim_qk), dtype=dtype)\n    if is_mqa:\n        k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    else:\n        k = random.uniform(k2, (num_kv_heads, kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (num_kv_heads, kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy())\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, num_q_heads))\n    mask = mask_lib.MultiHeadMask(tuple((m.get_mask() for m in masks)))\n    if use_dynamic_mask:\n        mask = to_dynamic_mask(mask)\n    block_sizes = data.draw(block_sizes_strategy(q_seq_len, kv_seq_len, include_bwd_blocks=True, use_fused_bwd_kernel=use_fused_bwd_kernel))\n    if is_mqa:\n        attn_ref = splash.make_masked_mqa_reference(mask, backward_impl='custom')\n        attn = splash.make_splash_mqa_single_device(mask, block_sizes=block_sizes, downcast_smem_data=downcast_smem_data, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    else:\n        attn_ref = splash.make_masked_mha_reference(mask, backward_impl='custom')\n        attn = splash.make_splash_mha_single_device(mask, block_sizes=block_sizes, downcast_smem_data=downcast_smem_data, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o, attn_vjp = jax.vjp(attn, q, k, v, segment_ids)\n    q32, k32, v32 = jax.tree.map(lambda x: x.astype(jnp.float32), (q, k, v))\n    o_ref, (logsumexp,) = attn_ref(q32, k32, v32, segment_ids, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap)\n    self._assert_allclose(o, o_ref, atol=0.003, rtol=0.003)\n    do = random.uniform(k4, o.shape, dtype=o.dtype)\n    dq, dk, dv, _ = attn_vjp(do)\n\n    def bwd(mask, q, k, v, segment_ids, o, logsumexp, do) -> tuple[jax.Array, jax.Array, jax.Array]:\n        _, dq, dk, dv, _ = splash._attention_reference_custom_bwd(splash.DEFAULT_MASK_VALUE, False, 'flash', attn_logits_soft_cap, (mask, q, k, v, segment_ids, o, logsumexp), do)\n        return (dq, dk, dv)\n    is_grouped = not is_mqa and num_kv_heads < num_q_heads\n    assert num_q_heads % num_kv_heads == 0\n    head_multiplier = num_q_heads // num_kv_heads\n    if is_mqa:\n        bwd = jax.vmap(bwd, in_axes=(0, 0, None, None, None, 0, 0, 0))\n    else:\n        bwd = jax.vmap(bwd, in_axes=(0, 0, 0, 0, None, 0, 0, 0))\n        if is_grouped:\n            k32 = jnp.repeat(k32, head_multiplier, axis=0)\n            v32 = jnp.repeat(v32, head_multiplier, axis=0)\n    dq_ref, dk_ref, dv_ref = bwd(mask[:, :, :], q32, k32, v32, segment_ids, o.astype(jnp.float32), logsumexp, do.astype(jnp.float32))\n    if is_mqa:\n        dk_ref, dv_ref = (dk_ref.sum(axis=0), dv_ref.sum(axis=0))\n    elif is_grouped:\n        dk_ref = dk_ref.reshape(num_kv_heads, head_multiplier, *dk_ref.shape[1:])\n        dv_ref = dv_ref.reshape(num_kv_heads, head_multiplier, *dv_ref.shape[1:])\n        dk_ref, dv_ref = (dk_ref.sum(axis=1), dv_ref.sum(axis=1))\n    self._assert_allclose(dv, dv_ref, atol=0.02, rtol=0.03)\n    self._assert_allclose(dq, dq_ref, atol=0.02, rtol=0.03)\n    self._assert_allclose(dk, dk_ref, atol=0.02, rtol=0.03)",
    "assertions": [
      "assert num_q_heads % num_kv_heads == 0",
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "def test_grid_shrinking(self):\n    \"\"\"Make sure that grid shrinking does not change the attention output.\"\"\"\n\n    class IdentityMask(mask_lib._ComputableMask):\n        \"\"\"Identity mask that is guaranteed to trigger grid shrinking.\"\"\"\n\n        def __init__(self, shape: tuple[int, int], shard_count: int=1):\n\n            def identity_mask_function(q_ids, kv_ids):\n                return q_ids == kv_ids\n            super().__init__(shape=shape, mask_function=identity_mask_function, shard_count=shard_count)\n\n        def __eq__(self, other: object):\n            if not isinstance(other, type(self)):\n                return NotImplemented\n            return self.shape == other.shape and np.array_equal(self.q_sequence, other.q_sequence)\n\n        def __hash__(self):\n            return hash((type(self), self.shape, self.q_sequence.tobytes() if self.q_sequence is not None else None))\n    seq_len = 256\n    head_dim = 128\n    key = random.key(42)\n    k1, k2, k3 = random.split(key, 3)\n    q = random.uniform(k1, (1, seq_len, head_dim), dtype=jnp.float32)\n    k = random.uniform(k2, (seq_len, head_dim), dtype=jnp.float32)\n    v = random.uniform(k3, (seq_len, head_dim), dtype=jnp.float32)\n    identity_mask = mask_lib.MultiHeadMask([IdentityMask((seq_len, seq_len))])\n    process_mask_path = 'jax.experimental.pallas.ops.tpu.splash_attention.splash_attention_mask_info.process_mask'\n    process_mask_shrink = lambda *args, **kwargs: process_mask(*args, **kwargs, shrink_grid=True)\n    process_mask_no_shrink = lambda *args, **kwargs: process_mask(*args, **kwargs, shrink_grid=False)\n    with unittest.mock.patch(process_mask_path, process_mask_shrink):\n        shrink_out = splash.make_splash_mqa_single_device(identity_mask)(q, k, v)\n    with unittest.mock.patch(process_mask_path, process_mask_no_shrink):\n        no_shrink_out = splash.make_splash_mqa_single_device(identity_mask)(q, k, v)\n    np.testing.assert_array_equal(shrink_out, no_shrink_out)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "def test_grid_shrinking(self):\n    \"\"\"Make sure that grid shrinking does not change the attention output.\"\"\"\n\n    class IdentityMask(mask_lib._ComputableMask):\n        \"\"\"Identity mask that is guaranteed to trigger grid shrinking.\"\"\"\n\n        def __init__(self, shape: tuple[int, int], shard_count: int=1):\n\n            def identity_mask_function(q_ids, kv_ids):\n                return q_ids == kv_ids\n            super().__init__(shape=shape, mask_function=identity_mask_function, shard_count=shard_count)\n\n        def __eq__(self, other: object):\n            if not isinstance(other, type(self)):\n                return NotImplemented\n            return self.shape == other.shape and np.array_equal(self.q_sequence, other.q_sequence)\n\n        def __hash__(self):\n            return hash((type(self), self.shape, self.q_sequence.tobytes() if self.q_sequence is not None else None))\n    seq_len = 256\n    head_dim = 128\n    key = random.key(42)\n    k1, k2, k3 = random.split(key, 3)\n    q = random.uniform(k1, (1, seq_len, head_dim), dtype=jnp.float32)\n    k = random.uniform(k2, (seq_len, head_dim), dtype=jnp.float32)\n    v = random.uniform(k3, (seq_len, head_dim), dtype=jnp.float32)\n    identity_mask = mask_lib.MultiHeadMask([IdentityMask((seq_len, seq_len))])\n    process_mask_path = 'jax.experimental.pallas.ops.tpu.splash_attention.splash_attention_mask_info.process_mask'\n    process_mask_shrink = lambda *args, **kwargs: process_mask(*args, **kwargs, shrink_grid=True)\n    process_mask_no_shrink = lambda *args, **kwargs: process_mask(*args, **kwargs, shrink_grid=False)\n    with unittest.mock.patch(process_mask_path, process_mask_shrink):\n        shrink_out = splash.make_splash_mqa_single_device(identity_mask)(q, k, v)\n    with unittest.mock.patch(process_mask_path, process_mask_no_shrink):\n        no_shrink_out = splash.make_splash_mqa_single_device(identity_mask)(q, k, v)\n    np.testing.assert_array_equal(shrink_out, no_shrink_out)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def __init__(self):\n    self.amax_history = core.mutable_array(jnp.zeros(5))"
  },
  {
    "test_code": "@parameterized.product(is_segmented=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_custom_bwd(self, is_segmented, data):\n    seed = data.draw(seed_strategy(), label='seed')\n    key = random.key(1 + seed)\n    k1, k2, k3, k4 = random.split(key, 4)\n    q_seq_len, kv_seq_len, head_dim_qk, head_dim_v, dtype = data.draw(attention_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (q_seq_len, head_dim_qk), dtype=dtype)\n    k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n    v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, 1))\n    mask = jnp.array(masks[0].get_mask()[:, :])\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy(), label='logit_cap')\n    attn_ref = partial(splash.attention_reference, mask, attn_logits_soft_cap=attn_logits_soft_cap)\n    attn_custom = partial(splash.attention_reference_custom, mask, attn_logits_soft_cap=attn_logits_soft_cap)\n    attn_custom_vanilla = partial(splash.attention_reference_custom, mask, custom_type='vanilla', attn_logits_soft_cap=attn_logits_soft_cap)\n    o_ref, attn_vjp_ref = jax.vjp(attn_ref, q, k, v, segment_ids)\n    q32, k32, v32 = jax.tree.map(lambda x: x.astype(jnp.float32), (q, k, v))\n    o_custom = attn_custom(q32, k32, v32, segment_ids)\n    _, attn_vjp = jax.vjp(attn_custom, q32, k32, v32, segment_ids)\n    _, attn_vanilla_vjp = jax.vjp(attn_custom_vanilla, q32, k32, v32, segment_ids)\n    do = random.uniform(k4, o_custom.shape, dtype=o_custom.dtype) / 10.0\n    self._assert_allclose(o_custom, o_ref, atol=1e-05, rtol=1e-05)\n    dq, dk, dv, _ = attn_vjp(do)\n    dq_vanilla, dk_vanilla, dv_vanilla, _ = attn_vanilla_vjp(do)\n    dq_ref, dk_ref, dv_ref, _ = attn_vjp_ref(do)\n    if dtype == jnp.bfloat16:\n        atols = {'dv': 0.004, 'dq': 0.05, 'dk': 0.05}\n        atols_v = {'dv': 0.008, 'dq': 0.02, 'dk': 0.02}\n        rtols = {'dv': 0.004, 'dq': 0.05, 'dk': 0.05}\n        rtols_v = {'dv': 0.008, 'dq': 0.02, 'dk': 0.02}\n        if jtu.is_device_tpu(version=5):\n            atols['dk'] = 0.065\n    elif dtype == jnp.float32:\n        atols = {'dv': 0.003, 'dq': 0.05, 'dk': 0.05}\n        atols_v = {'dv': 0.0004, 'dq': 0.002, 'dk': 0.003}\n        rtols = {'dv': 0.003, 'dq': 0.05, 'dk': 0.05}\n        rtols_v = {'dv': 0.008, 'dq': 0.0005, 'dk': 0.0005}\n        if jtu.is_device_tpu(version=4):\n            atols['dk'] = 0.09\n    else:\n        raise NotImplementedError\n    self._assert_allclose(dv_vanilla, dv_ref, atol=atols_v['dv'], rtol=rtols_v['dv'])\n    self._assert_allclose(dv, dv_ref, atol=atols['dv'], rtol=rtols['dv'])\n    self._assert_allclose(dq_vanilla, dq_ref, atol=atols_v['dq'], rtol=rtols_v['dq'])\n    self._assert_allclose(dq, dq_ref, atol=atols['dq'], rtol=rtols['dq'])\n    self._assert_allclose(dk_vanilla, dk_ref, atol=atols_v['dk'], rtol=rtols_v['dk'])\n    self._assert_allclose(dk, dk_ref, atol=atols['dk'], rtol=rtols['dk'])",
    "assertions": [
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def attn_vjp(x, bias, mask, target_fn):\n    _, f_vjp = jax.vjp(target_fn, x, bias, mask)\n    return f_vjp(x)"
  },
  {
    "test_code": "@parameterized.product(is_mqa=(False, True), is_segmented=(False, True), downcast_smem_data=(False, True), use_fused_bwd_kernel=(False, True), use_dynamic_mask=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_bwd(self, is_mqa, is_segmented, downcast_smem_data, use_fused_bwd_kernel, use_dynamic_mask, data):\n    seed = data.draw(seed_strategy())\n    key = random.key(seed)\n    k1, k2, k3, k4 = random.split(key, 4)\n    q_seq_len, kv_seq_len, num_q_heads, num_kv_heads, head_dim_qk, head_dim_v, dtype = data.draw(mha_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (num_q_heads, q_seq_len, head_dim_qk), dtype=dtype)\n    if is_mqa:\n        k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    else:\n        k = random.uniform(k2, (num_kv_heads, kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (num_kv_heads, kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy())\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, num_q_heads))\n    mask = mask_lib.MultiHeadMask(tuple((m.get_mask() for m in masks)))\n    if use_dynamic_mask:\n        mask = to_dynamic_mask(mask)\n    block_sizes = data.draw(block_sizes_strategy(q_seq_len, kv_seq_len, include_bwd_blocks=True, use_fused_bwd_kernel=use_fused_bwd_kernel))\n    if is_mqa:\n        attn_ref = splash.make_masked_mqa_reference(mask, backward_impl='custom')\n        attn = splash.make_splash_mqa_single_device(mask, block_sizes=block_sizes, downcast_smem_data=downcast_smem_data, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    else:\n        attn_ref = splash.make_masked_mha_reference(mask, backward_impl='custom')\n        attn = splash.make_splash_mha_single_device(mask, block_sizes=block_sizes, downcast_smem_data=downcast_smem_data, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o, attn_vjp = jax.vjp(attn, q, k, v, segment_ids)\n    q32, k32, v32 = jax.tree.map(lambda x: x.astype(jnp.float32), (q, k, v))\n    o_ref, (logsumexp,) = attn_ref(q32, k32, v32, segment_ids, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap)\n    self._assert_allclose(o, o_ref, atol=0.003, rtol=0.003)\n    do = random.uniform(k4, o.shape, dtype=o.dtype)\n    dq, dk, dv, _ = attn_vjp(do)\n\n    def bwd(mask, q, k, v, segment_ids, o, logsumexp, do) -> tuple[jax.Array, jax.Array, jax.Array]:\n        _, dq, dk, dv, _ = splash._attention_reference_custom_bwd(splash.DEFAULT_MASK_VALUE, False, 'flash', attn_logits_soft_cap, (mask, q, k, v, segment_ids, o, logsumexp), do)\n        return (dq, dk, dv)\n    is_grouped = not is_mqa and num_kv_heads < num_q_heads\n    assert num_q_heads % num_kv_heads == 0\n    head_multiplier = num_q_heads // num_kv_heads\n    if is_mqa:\n        bwd = jax.vmap(bwd, in_axes=(0, 0, None, None, None, 0, 0, 0))\n    else:\n        bwd = jax.vmap(bwd, in_axes=(0, 0, 0, 0, None, 0, 0, 0))\n        if is_grouped:\n            k32 = jnp.repeat(k32, head_multiplier, axis=0)\n            v32 = jnp.repeat(v32, head_multiplier, axis=0)\n    dq_ref, dk_ref, dv_ref = bwd(mask[:, :, :], q32, k32, v32, segment_ids, o.astype(jnp.float32), logsumexp, do.astype(jnp.float32))\n    if is_mqa:\n        dk_ref, dv_ref = (dk_ref.sum(axis=0), dv_ref.sum(axis=0))\n    elif is_grouped:\n        dk_ref = dk_ref.reshape(num_kv_heads, head_multiplier, *dk_ref.shape[1:])\n        dv_ref = dv_ref.reshape(num_kv_heads, head_multiplier, *dv_ref.shape[1:])\n        dk_ref, dv_ref = (dk_ref.sum(axis=1), dv_ref.sum(axis=1))\n    self._assert_allclose(dv, dv_ref, atol=0.02, rtol=0.03)\n    self._assert_allclose(dq, dq_ref, atol=0.02, rtol=0.03)\n    self._assert_allclose(dk, dk_ref, atol=0.02, rtol=0.03)",
    "assertions": [
      "assert num_q_heads % num_kv_heads == 0",
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def attn_vjp(x, bias, mask, target_fn):\n    _, f_vjp = jax.vjp(target_fn, x, bias, mask)\n    return f_vjp(x)"
  },
  {
    "test_code": "def test_grid_shrinking(self):\n    \"\"\"Make sure that grid shrinking does not change the attention output.\"\"\"\n\n    class IdentityMask(mask_lib._ComputableMask):\n        \"\"\"Identity mask that is guaranteed to trigger grid shrinking.\"\"\"\n\n        def __init__(self, shape: tuple[int, int], shard_count: int=1):\n\n            def identity_mask_function(q_ids, kv_ids):\n                return q_ids == kv_ids\n            super().__init__(shape=shape, mask_function=identity_mask_function, shard_count=shard_count)\n\n        def __eq__(self, other: object):\n            if not isinstance(other, type(self)):\n                return NotImplemented\n            return self.shape == other.shape and np.array_equal(self.q_sequence, other.q_sequence)\n\n        def __hash__(self):\n            return hash((type(self), self.shape, self.q_sequence.tobytes() if self.q_sequence is not None else None))\n    seq_len = 256\n    head_dim = 128\n    key = random.key(42)\n    k1, k2, k3 = random.split(key, 3)\n    q = random.uniform(k1, (1, seq_len, head_dim), dtype=jnp.float32)\n    k = random.uniform(k2, (seq_len, head_dim), dtype=jnp.float32)\n    v = random.uniform(k3, (seq_len, head_dim), dtype=jnp.float32)\n    identity_mask = mask_lib.MultiHeadMask([IdentityMask((seq_len, seq_len))])\n    process_mask_path = 'jax.experimental.pallas.ops.tpu.splash_attention.splash_attention_mask_info.process_mask'\n    process_mask_shrink = lambda *args, **kwargs: process_mask(*args, **kwargs, shrink_grid=True)\n    process_mask_no_shrink = lambda *args, **kwargs: process_mask(*args, **kwargs, shrink_grid=False)\n    with unittest.mock.patch(process_mask_path, process_mask_shrink):\n        shrink_out = splash.make_splash_mqa_single_device(identity_mask)(q, k, v)\n    with unittest.mock.patch(process_mask_path, process_mask_no_shrink):\n        no_shrink_out = splash.make_splash_mqa_single_device(identity_mask)(q, k, v)\n    np.testing.assert_array_equal(shrink_out, no_shrink_out)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def __init__(self, group_name: str, name: str, fun: Callable[..., Any], *, arg_descriptors: Sequence[test_harnesses.ArgDescriptor]=(), polymorphic_shapes: Sequence[str | None]=(), symbolic_constraints: Sequence[str]=(), expect_error: tuple[Any, str] | None=None, check_result: bool=True, tol: float | None=None, limitations: Sequence[test_harnesses.Limitation]=(), override_jax_config_flags: dict[str, Any]={}):\n    \"\"\"Args:\n\n      group_name, name: The name for the harness. See `Harness.__init__`.\n      fun: the function to be converted. See `Harness.__init__`.\n      arg_descriptors: The argument descriptors. See `Harness.__init__`.\n      polymorphic_shapes: For `export.args_specs`.\n      symbolic_constraints: For `export.args_specs`.\n      expect_error: an optional pair of an Exception type and a regular\n        expression to match the expected exception string.\n        We expect this error during tracing and exporting with shape\n        polymorphism.\n      check_result: specifies if we want to check that the result of invoking\n        the shape polymorphic export produces the same result as the\n        native JAX function.\n      tol: the tolerance to use for checking results.\n      limitations: a sequence of Limitation(s), used for obtaining the default\n        tolerance (if `tol` is not specified).\n      override_jax_config_flags: jax.config flags to override for the duration\n        of the test.\n    \"\"\"\n    super().__init__(group_name, name, fun, arg_descriptors, dtype=np.float32)\n    self.polymorphic_shapes = polymorphic_shapes\n    self.symbolic_constraints = symbolic_constraints\n    self.expect_error = expect_error\n    self.tol = tol\n    self.check_result = check_result\n    self.limitations = limitations\n    self.override_jax_config_flags = override_jax_config_flags"
  },
  {
    "test_code": "def test_grid_shrinking(self):\n    \"\"\"Make sure that grid shrinking does not change the attention output.\"\"\"\n\n    class IdentityMask(mask_lib._ComputableMask):\n        \"\"\"Identity mask that is guaranteed to trigger grid shrinking.\"\"\"\n\n        def __init__(self, shape: tuple[int, int], shard_count: int=1):\n\n            def identity_mask_function(q_ids, kv_ids):\n                return q_ids == kv_ids\n            super().__init__(shape=shape, mask_function=identity_mask_function, shard_count=shard_count)\n\n        def __eq__(self, other: object):\n            if not isinstance(other, type(self)):\n                return NotImplemented\n            return self.shape == other.shape and np.array_equal(self.q_sequence, other.q_sequence)\n\n        def __hash__(self):\n            return hash((type(self), self.shape, self.q_sequence.tobytes() if self.q_sequence is not None else None))\n    seq_len = 256\n    head_dim = 128\n    key = random.key(42)\n    k1, k2, k3 = random.split(key, 3)\n    q = random.uniform(k1, (1, seq_len, head_dim), dtype=jnp.float32)\n    k = random.uniform(k2, (seq_len, head_dim), dtype=jnp.float32)\n    v = random.uniform(k3, (seq_len, head_dim), dtype=jnp.float32)\n    identity_mask = mask_lib.MultiHeadMask([IdentityMask((seq_len, seq_len))])\n    process_mask_path = 'jax.experimental.pallas.ops.tpu.splash_attention.splash_attention_mask_info.process_mask'\n    process_mask_shrink = lambda *args, **kwargs: process_mask(*args, **kwargs, shrink_grid=True)\n    process_mask_no_shrink = lambda *args, **kwargs: process_mask(*args, **kwargs, shrink_grid=False)\n    with unittest.mock.patch(process_mask_path, process_mask_shrink):\n        shrink_out = splash.make_splash_mqa_single_device(identity_mask)(q, k, v)\n    with unittest.mock.patch(process_mask_path, process_mask_no_shrink):\n        no_shrink_out = splash.make_splash_mqa_single_device(identity_mask)(q, k, v)\n    np.testing.assert_array_equal(shrink_out, no_shrink_out)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def __init__(self):\n    self._foo = jax.jit(self.foo)"
  },
  {
    "test_code": "@parameterized.product(is_mqa=(False, True), is_segmented=(False, True), downcast_smem_data=(False, True), use_fused_bwd_kernel=(False, True), use_dynamic_mask=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_bwd(self, is_mqa, is_segmented, downcast_smem_data, use_fused_bwd_kernel, use_dynamic_mask, data):\n    seed = data.draw(seed_strategy())\n    key = random.key(seed)\n    k1, k2, k3, k4 = random.split(key, 4)\n    q_seq_len, kv_seq_len, num_q_heads, num_kv_heads, head_dim_qk, head_dim_v, dtype = data.draw(mha_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (num_q_heads, q_seq_len, head_dim_qk), dtype=dtype)\n    if is_mqa:\n        k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    else:\n        k = random.uniform(k2, (num_kv_heads, kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (num_kv_heads, kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy())\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, num_q_heads))\n    mask = mask_lib.MultiHeadMask(tuple((m.get_mask() for m in masks)))\n    if use_dynamic_mask:\n        mask = to_dynamic_mask(mask)\n    block_sizes = data.draw(block_sizes_strategy(q_seq_len, kv_seq_len, include_bwd_blocks=True, use_fused_bwd_kernel=use_fused_bwd_kernel))\n    if is_mqa:\n        attn_ref = splash.make_masked_mqa_reference(mask, backward_impl='custom')\n        attn = splash.make_splash_mqa_single_device(mask, block_sizes=block_sizes, downcast_smem_data=downcast_smem_data, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    else:\n        attn_ref = splash.make_masked_mha_reference(mask, backward_impl='custom')\n        attn = splash.make_splash_mha_single_device(mask, block_sizes=block_sizes, downcast_smem_data=downcast_smem_data, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o, attn_vjp = jax.vjp(attn, q, k, v, segment_ids)\n    q32, k32, v32 = jax.tree.map(lambda x: x.astype(jnp.float32), (q, k, v))\n    o_ref, (logsumexp,) = attn_ref(q32, k32, v32, segment_ids, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap)\n    self._assert_allclose(o, o_ref, atol=0.003, rtol=0.003)\n    do = random.uniform(k4, o.shape, dtype=o.dtype)\n    dq, dk, dv, _ = attn_vjp(do)\n\n    def bwd(mask, q, k, v, segment_ids, o, logsumexp, do) -> tuple[jax.Array, jax.Array, jax.Array]:\n        _, dq, dk, dv, _ = splash._attention_reference_custom_bwd(splash.DEFAULT_MASK_VALUE, False, 'flash', attn_logits_soft_cap, (mask, q, k, v, segment_ids, o, logsumexp), do)\n        return (dq, dk, dv)\n    is_grouped = not is_mqa and num_kv_heads < num_q_heads\n    assert num_q_heads % num_kv_heads == 0\n    head_multiplier = num_q_heads // num_kv_heads\n    if is_mqa:\n        bwd = jax.vmap(bwd, in_axes=(0, 0, None, None, None, 0, 0, 0))\n    else:\n        bwd = jax.vmap(bwd, in_axes=(0, 0, 0, 0, None, 0, 0, 0))\n        if is_grouped:\n            k32 = jnp.repeat(k32, head_multiplier, axis=0)\n            v32 = jnp.repeat(v32, head_multiplier, axis=0)\n    dq_ref, dk_ref, dv_ref = bwd(mask[:, :, :], q32, k32, v32, segment_ids, o.astype(jnp.float32), logsumexp, do.astype(jnp.float32))\n    if is_mqa:\n        dk_ref, dv_ref = (dk_ref.sum(axis=0), dv_ref.sum(axis=0))\n    elif is_grouped:\n        dk_ref = dk_ref.reshape(num_kv_heads, head_multiplier, *dk_ref.shape[1:])\n        dv_ref = dv_ref.reshape(num_kv_heads, head_multiplier, *dv_ref.shape[1:])\n        dk_ref, dv_ref = (dk_ref.sum(axis=1), dv_ref.sum(axis=1))\n    self._assert_allclose(dv, dv_ref, atol=0.02, rtol=0.03)\n    self._assert_allclose(dq, dq_ref, atol=0.02, rtol=0.03)\n    self._assert_allclose(dk, dk_ref, atol=0.02, rtol=0.03)",
    "assertions": [
      "assert num_q_heads % num_kv_heads == 0",
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@parameterized.product(is_mqa=(False, True), is_segmented=(False, True), downcast_smem_data=(False, True), use_fused_bwd_kernel=(False, True), use_dynamic_mask=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_bwd(self, is_mqa, is_segmented, downcast_smem_data, use_fused_bwd_kernel, use_dynamic_mask, data):\n    seed = data.draw(seed_strategy())\n    key = random.key(seed)\n    k1, k2, k3, k4 = random.split(key, 4)\n    q_seq_len, kv_seq_len, num_q_heads, num_kv_heads, head_dim_qk, head_dim_v, dtype = data.draw(mha_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (num_q_heads, q_seq_len, head_dim_qk), dtype=dtype)\n    if is_mqa:\n        k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    else:\n        k = random.uniform(k2, (num_kv_heads, kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (num_kv_heads, kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy())\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, num_q_heads))\n    mask = mask_lib.MultiHeadMask(tuple((m.get_mask() for m in masks)))\n    if use_dynamic_mask:\n        mask = to_dynamic_mask(mask)\n    block_sizes = data.draw(block_sizes_strategy(q_seq_len, kv_seq_len, include_bwd_blocks=True, use_fused_bwd_kernel=use_fused_bwd_kernel))\n    if is_mqa:\n        attn_ref = splash.make_masked_mqa_reference(mask, backward_impl='custom')\n        attn = splash.make_splash_mqa_single_device(mask, block_sizes=block_sizes, downcast_smem_data=downcast_smem_data, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    else:\n        attn_ref = splash.make_masked_mha_reference(mask, backward_impl='custom')\n        attn = splash.make_splash_mha_single_device(mask, block_sizes=block_sizes, downcast_smem_data=downcast_smem_data, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o, attn_vjp = jax.vjp(attn, q, k, v, segment_ids)\n    q32, k32, v32 = jax.tree.map(lambda x: x.astype(jnp.float32), (q, k, v))\n    o_ref, (logsumexp,) = attn_ref(q32, k32, v32, segment_ids, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap)\n    self._assert_allclose(o, o_ref, atol=0.003, rtol=0.003)\n    do = random.uniform(k4, o.shape, dtype=o.dtype)\n    dq, dk, dv, _ = attn_vjp(do)\n\n    def bwd(mask, q, k, v, segment_ids, o, logsumexp, do) -> tuple[jax.Array, jax.Array, jax.Array]:\n        _, dq, dk, dv, _ = splash._attention_reference_custom_bwd(splash.DEFAULT_MASK_VALUE, False, 'flash', attn_logits_soft_cap, (mask, q, k, v, segment_ids, o, logsumexp), do)\n        return (dq, dk, dv)\n    is_grouped = not is_mqa and num_kv_heads < num_q_heads\n    assert num_q_heads % num_kv_heads == 0\n    head_multiplier = num_q_heads // num_kv_heads\n    if is_mqa:\n        bwd = jax.vmap(bwd, in_axes=(0, 0, None, None, None, 0, 0, 0))\n    else:\n        bwd = jax.vmap(bwd, in_axes=(0, 0, 0, 0, None, 0, 0, 0))\n        if is_grouped:\n            k32 = jnp.repeat(k32, head_multiplier, axis=0)\n            v32 = jnp.repeat(v32, head_multiplier, axis=0)\n    dq_ref, dk_ref, dv_ref = bwd(mask[:, :, :], q32, k32, v32, segment_ids, o.astype(jnp.float32), logsumexp, do.astype(jnp.float32))\n    if is_mqa:\n        dk_ref, dv_ref = (dk_ref.sum(axis=0), dv_ref.sum(axis=0))\n    elif is_grouped:\n        dk_ref = dk_ref.reshape(num_kv_heads, head_multiplier, *dk_ref.shape[1:])\n        dv_ref = dv_ref.reshape(num_kv_heads, head_multiplier, *dv_ref.shape[1:])\n        dk_ref, dv_ref = (dk_ref.sum(axis=1), dv_ref.sum(axis=1))\n    self._assert_allclose(dv, dv_ref, atol=0.02, rtol=0.03)\n    self._assert_allclose(dq, dq_ref, atol=0.02, rtol=0.03)\n    self._assert_allclose(dk, dk_ref, atol=0.02, rtol=0.03)",
    "assertions": [
      "assert num_q_heads % num_kv_heads == 0",
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def bwd(y, g):\n    return (2.0 * jnp.cos(y) * g,)"
  },
  {
    "test_code": "@parameterized.product(is_mqa=(False, True), is_segmented=(False, True), is_dynamic_mask=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention(self, is_mqa, is_segmented, is_dynamic_mask, data):\n    seed = data.draw(seed_strategy())\n    key = random.key(seed)\n    k1, k2, k3 = random.split(key, 3)\n    q_seq_len, kv_seq_len, num_q_heads, num_kv_heads, head_dim_qk, head_dim_v, dtype = data.draw(mha_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (num_q_heads, q_seq_len, head_dim_qk), dtype=dtype)\n    if is_mqa:\n        k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    else:\n        k = random.uniform(k2, (num_kv_heads, kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (num_kv_heads, kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy())\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, num_q_heads))\n    mask = mask_lib.MultiHeadMask(tuple((m.get_mask() for m in masks)))\n    if is_dynamic_mask:\n        mask = to_dynamic_mask(mask)\n    block_sizes = data.draw(block_sizes_strategy(q_seq_len, kv_seq_len))\n    if is_mqa:\n        attn_ref = splash.make_masked_mqa_reference(mask)\n        attn = splash.make_splash_mqa_single_device(mask, block_sizes=block_sizes, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    else:\n        attn_ref = splash.make_masked_mha_reference(mask)\n        attn = splash.make_splash_mha_single_device(mask, block_sizes=block_sizes, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o = attn(q, k, v, segment_ids)\n    o_ref = attn_ref(q.astype(np.float32), k.astype(np.float32), v.astype(np.float32), segment_ids, attn_logits_soft_cap=attn_logits_soft_cap)\n    self._assert_allclose(o, o_ref, atol=0.003, rtol=0.003)",
    "assertions": [
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def get_mask(x_ref):\n    x = x_ref[...] == 1\n    iota = jax.lax.broadcasted_iota(jnp.int32, x_ref.shape, 1)\n    iota = iota > 7\n    return jnp.logical_and(x, iota)"
  },
  {
    "test_code": "@parameterized.product(is_mqa=(False, True), is_segmented=(False, True), is_dynamic_mask=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_fwd(self, is_mqa, is_segmented, is_dynamic_mask, data):\n    seed = data.draw(seed_strategy())\n    key = random.key(seed)\n    k1, k2, k3 = random.split(key, 3)\n    q_seq_len, kv_seq_len, num_q_heads, num_kv_heads, head_dim_qk, head_dim_v, dtype = data.draw(mha_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (num_q_heads, q_seq_len, head_dim_qk), dtype=dtype)\n    if is_mqa:\n        k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    else:\n        k = random.uniform(k2, (num_kv_heads, kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (num_kv_heads, kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy())\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, num_q_heads))\n    mask = mask_lib.MultiHeadMask(tuple((m.get_mask() for m in masks)))\n    if is_dynamic_mask:\n        mask = to_dynamic_mask(mask)\n    block_sizes = data.draw(block_sizes_strategy(q_seq_len, kv_seq_len))\n    if is_mqa:\n        attn_ref = splash.make_masked_mqa_reference(mask)\n        attn = splash.make_splash_mqa_single_device(mask, block_sizes=block_sizes, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    else:\n        attn_ref = splash.make_masked_mha_reference(mask)\n        attn = splash.make_splash_mha_single_device(mask, block_sizes=block_sizes, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    attn_ref = partial(attn_ref, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap)\n    o, (logsumexp,) = attn(q, k, v, segment_ids)\n    o_ref, (logsumexp_ref,) = attn_ref(q.astype(jnp.float32), k.astype(jnp.float32), v.astype(jnp.float32), segment_ids)\n    self._assert_allclose(o, o_ref, atol=0.003, rtol=0.003)\n    self._assert_allclose(logsumexp, logsumexp_ref, atol=0.001, rtol=0.001)",
    "assertions": [
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def get_mask(x_ref):\n    x = x_ref[...] == 1\n    iota = jax.lax.broadcasted_iota(jnp.int32, x_ref.shape, 1)\n    iota = iota > 7\n    return jnp.logical_and(x, iota)"
  },
  {
    "test_code": "@parameterized.product(is_segmented=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_custom_bwd(self, is_segmented, data):\n    seed = data.draw(seed_strategy(), label='seed')\n    key = random.key(1 + seed)\n    k1, k2, k3, k4 = random.split(key, 4)\n    q_seq_len, kv_seq_len, head_dim_qk, head_dim_v, dtype = data.draw(attention_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (q_seq_len, head_dim_qk), dtype=dtype)\n    k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n    v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, 1))\n    mask = jnp.array(masks[0].get_mask()[:, :])\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy(), label='logit_cap')\n    attn_ref = partial(splash.attention_reference, mask, attn_logits_soft_cap=attn_logits_soft_cap)\n    attn_custom = partial(splash.attention_reference_custom, mask, attn_logits_soft_cap=attn_logits_soft_cap)\n    attn_custom_vanilla = partial(splash.attention_reference_custom, mask, custom_type='vanilla', attn_logits_soft_cap=attn_logits_soft_cap)\n    o_ref, attn_vjp_ref = jax.vjp(attn_ref, q, k, v, segment_ids)\n    q32, k32, v32 = jax.tree.map(lambda x: x.astype(jnp.float32), (q, k, v))\n    o_custom = attn_custom(q32, k32, v32, segment_ids)\n    _, attn_vjp = jax.vjp(attn_custom, q32, k32, v32, segment_ids)\n    _, attn_vanilla_vjp = jax.vjp(attn_custom_vanilla, q32, k32, v32, segment_ids)\n    do = random.uniform(k4, o_custom.shape, dtype=o_custom.dtype) / 10.0\n    self._assert_allclose(o_custom, o_ref, atol=1e-05, rtol=1e-05)\n    dq, dk, dv, _ = attn_vjp(do)\n    dq_vanilla, dk_vanilla, dv_vanilla, _ = attn_vanilla_vjp(do)\n    dq_ref, dk_ref, dv_ref, _ = attn_vjp_ref(do)\n    if dtype == jnp.bfloat16:\n        atols = {'dv': 0.004, 'dq': 0.05, 'dk': 0.05}\n        atols_v = {'dv': 0.008, 'dq': 0.02, 'dk': 0.02}\n        rtols = {'dv': 0.004, 'dq': 0.05, 'dk': 0.05}\n        rtols_v = {'dv': 0.008, 'dq': 0.02, 'dk': 0.02}\n        if jtu.is_device_tpu(version=5):\n            atols['dk'] = 0.065\n    elif dtype == jnp.float32:\n        atols = {'dv': 0.003, 'dq': 0.05, 'dk': 0.05}\n        atols_v = {'dv': 0.0004, 'dq': 0.002, 'dk': 0.003}\n        rtols = {'dv': 0.003, 'dq': 0.05, 'dk': 0.05}\n        rtols_v = {'dv': 0.008, 'dq': 0.0005, 'dk': 0.0005}\n        if jtu.is_device_tpu(version=4):\n            atols['dk'] = 0.09\n    else:\n        raise NotImplementedError\n    self._assert_allclose(dv_vanilla, dv_ref, atol=atols_v['dv'], rtol=rtols_v['dv'])\n    self._assert_allclose(dv, dv_ref, atol=atols['dv'], rtol=rtols['dv'])\n    self._assert_allclose(dq_vanilla, dq_ref, atol=atols_v['dq'], rtol=rtols_v['dq'])\n    self._assert_allclose(dq, dq_ref, atol=atols['dq'], rtol=rtols['dq'])\n    self._assert_allclose(dk_vanilla, dk_ref, atol=atols_v['dk'], rtol=rtols_v['dk'])\n    self._assert_allclose(dk, dk_ref, atol=atols['dk'], rtol=rtols['dk'])",
    "assertions": [
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def get_mask(x_ref):\n    x = x_ref[...] == 1\n    iota = jax.lax.broadcasted_iota(jnp.int32, x_ref.shape, 1)\n    iota = iota > 7\n    return jnp.logical_and(x, iota)"
  },
  {
    "test_code": "@parameterized.product(is_mqa=(False, True), is_segmented=(False, True), downcast_smem_data=(False, True), use_fused_bwd_kernel=(False, True), use_dynamic_mask=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_bwd(self, is_mqa, is_segmented, downcast_smem_data, use_fused_bwd_kernel, use_dynamic_mask, data):\n    seed = data.draw(seed_strategy())\n    key = random.key(seed)\n    k1, k2, k3, k4 = random.split(key, 4)\n    q_seq_len, kv_seq_len, num_q_heads, num_kv_heads, head_dim_qk, head_dim_v, dtype = data.draw(mha_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (num_q_heads, q_seq_len, head_dim_qk), dtype=dtype)\n    if is_mqa:\n        k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    else:\n        k = random.uniform(k2, (num_kv_heads, kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (num_kv_heads, kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy())\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, num_q_heads))\n    mask = mask_lib.MultiHeadMask(tuple((m.get_mask() for m in masks)))\n    if use_dynamic_mask:\n        mask = to_dynamic_mask(mask)\n    block_sizes = data.draw(block_sizes_strategy(q_seq_len, kv_seq_len, include_bwd_blocks=True, use_fused_bwd_kernel=use_fused_bwd_kernel))\n    if is_mqa:\n        attn_ref = splash.make_masked_mqa_reference(mask, backward_impl='custom')\n        attn = splash.make_splash_mqa_single_device(mask, block_sizes=block_sizes, downcast_smem_data=downcast_smem_data, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    else:\n        attn_ref = splash.make_masked_mha_reference(mask, backward_impl='custom')\n        attn = splash.make_splash_mha_single_device(mask, block_sizes=block_sizes, downcast_smem_data=downcast_smem_data, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o, attn_vjp = jax.vjp(attn, q, k, v, segment_ids)\n    q32, k32, v32 = jax.tree.map(lambda x: x.astype(jnp.float32), (q, k, v))\n    o_ref, (logsumexp,) = attn_ref(q32, k32, v32, segment_ids, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap)\n    self._assert_allclose(o, o_ref, atol=0.003, rtol=0.003)\n    do = random.uniform(k4, o.shape, dtype=o.dtype)\n    dq, dk, dv, _ = attn_vjp(do)\n\n    def bwd(mask, q, k, v, segment_ids, o, logsumexp, do) -> tuple[jax.Array, jax.Array, jax.Array]:\n        _, dq, dk, dv, _ = splash._attention_reference_custom_bwd(splash.DEFAULT_MASK_VALUE, False, 'flash', attn_logits_soft_cap, (mask, q, k, v, segment_ids, o, logsumexp), do)\n        return (dq, dk, dv)\n    is_grouped = not is_mqa and num_kv_heads < num_q_heads\n    assert num_q_heads % num_kv_heads == 0\n    head_multiplier = num_q_heads // num_kv_heads\n    if is_mqa:\n        bwd = jax.vmap(bwd, in_axes=(0, 0, None, None, None, 0, 0, 0))\n    else:\n        bwd = jax.vmap(bwd, in_axes=(0, 0, 0, 0, None, 0, 0, 0))\n        if is_grouped:\n            k32 = jnp.repeat(k32, head_multiplier, axis=0)\n            v32 = jnp.repeat(v32, head_multiplier, axis=0)\n    dq_ref, dk_ref, dv_ref = bwd(mask[:, :, :], q32, k32, v32, segment_ids, o.astype(jnp.float32), logsumexp, do.astype(jnp.float32))\n    if is_mqa:\n        dk_ref, dv_ref = (dk_ref.sum(axis=0), dv_ref.sum(axis=0))\n    elif is_grouped:\n        dk_ref = dk_ref.reshape(num_kv_heads, head_multiplier, *dk_ref.shape[1:])\n        dv_ref = dv_ref.reshape(num_kv_heads, head_multiplier, *dv_ref.shape[1:])\n        dk_ref, dv_ref = (dk_ref.sum(axis=1), dv_ref.sum(axis=1))\n    self._assert_allclose(dv, dv_ref, atol=0.02, rtol=0.03)\n    self._assert_allclose(dq, dq_ref, atol=0.02, rtol=0.03)\n    self._assert_allclose(dk, dk_ref, atol=0.02, rtol=0.03)",
    "assertions": [
      "assert num_q_heads % num_kv_heads == 0",
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def get_mask(x_ref):\n    x = x_ref[...] == 1\n    iota = jax.lax.broadcasted_iota(jnp.int32, x_ref.shape, 1)\n    iota = iota > 7\n    return jnp.logical_and(x, iota)"
  },
  {
    "test_code": "@parameterized.product(is_segmented=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_custom_bwd(self, is_segmented, data):\n    seed = data.draw(seed_strategy(), label='seed')\n    key = random.key(1 + seed)\n    k1, k2, k3, k4 = random.split(key, 4)\n    q_seq_len, kv_seq_len, head_dim_qk, head_dim_v, dtype = data.draw(attention_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (q_seq_len, head_dim_qk), dtype=dtype)\n    k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n    v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, 1))\n    mask = jnp.array(masks[0].get_mask()[:, :])\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy(), label='logit_cap')\n    attn_ref = partial(splash.attention_reference, mask, attn_logits_soft_cap=attn_logits_soft_cap)\n    attn_custom = partial(splash.attention_reference_custom, mask, attn_logits_soft_cap=attn_logits_soft_cap)\n    attn_custom_vanilla = partial(splash.attention_reference_custom, mask, custom_type='vanilla', attn_logits_soft_cap=attn_logits_soft_cap)\n    o_ref, attn_vjp_ref = jax.vjp(attn_ref, q, k, v, segment_ids)\n    q32, k32, v32 = jax.tree.map(lambda x: x.astype(jnp.float32), (q, k, v))\n    o_custom = attn_custom(q32, k32, v32, segment_ids)\n    _, attn_vjp = jax.vjp(attn_custom, q32, k32, v32, segment_ids)\n    _, attn_vanilla_vjp = jax.vjp(attn_custom_vanilla, q32, k32, v32, segment_ids)\n    do = random.uniform(k4, o_custom.shape, dtype=o_custom.dtype) / 10.0\n    self._assert_allclose(o_custom, o_ref, atol=1e-05, rtol=1e-05)\n    dq, dk, dv, _ = attn_vjp(do)\n    dq_vanilla, dk_vanilla, dv_vanilla, _ = attn_vanilla_vjp(do)\n    dq_ref, dk_ref, dv_ref, _ = attn_vjp_ref(do)\n    if dtype == jnp.bfloat16:\n        atols = {'dv': 0.004, 'dq': 0.05, 'dk': 0.05}\n        atols_v = {'dv': 0.008, 'dq': 0.02, 'dk': 0.02}\n        rtols = {'dv': 0.004, 'dq': 0.05, 'dk': 0.05}\n        rtols_v = {'dv': 0.008, 'dq': 0.02, 'dk': 0.02}\n        if jtu.is_device_tpu(version=5):\n            atols['dk'] = 0.065\n    elif dtype == jnp.float32:\n        atols = {'dv': 0.003, 'dq': 0.05, 'dk': 0.05}\n        atols_v = {'dv': 0.0004, 'dq': 0.002, 'dk': 0.003}\n        rtols = {'dv': 0.003, 'dq': 0.05, 'dk': 0.05}\n        rtols_v = {'dv': 0.008, 'dq': 0.0005, 'dk': 0.0005}\n        if jtu.is_device_tpu(version=4):\n            atols['dk'] = 0.09\n    else:\n        raise NotImplementedError\n    self._assert_allclose(dv_vanilla, dv_ref, atol=atols_v['dv'], rtol=rtols_v['dv'])\n    self._assert_allclose(dv, dv_ref, atol=atols['dv'], rtol=rtols['dv'])\n    self._assert_allclose(dq_vanilla, dq_ref, atol=atols_v['dq'], rtol=rtols_v['dq'])\n    self._assert_allclose(dq, dq_ref, atol=atols['dq'], rtol=rtols['dq'])\n    self._assert_allclose(dk_vanilla, dk_ref, atol=atols_v['dk'], rtol=rtols_v['dk'])\n    self._assert_allclose(dk, dk_ref, atol=atols['dk'], rtol=rtols['dk'])",
    "assertions": [
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "@hps.composite\ndef attention_strategy(draw: Draw) -> tuple[int, int, int, np.dtype]:\n    q_seq_len, kv_seq_len = draw(sequence_length_strategy())\n    head_dim_qk, head_dim_v = draw(hps.sampled_from([(128, 128), (256, 256), (192, 128)]))\n    if q_seq_len >= 4096 and kv_seq_len >= 4096:\n        dtype = np.dtype('float32')\n    else:\n        dtype = draw(hps.sampled_from([np.dtype('float32'), np.dtype(jnp.bfloat16)]))\n    return (q_seq_len, kv_seq_len, head_dim_qk, head_dim_v, dtype)"
  },
  {
    "test_code": "@parameterized.product(is_mqa=(False, True), is_segmented=(False, True), is_dynamic_mask=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention(self, is_mqa, is_segmented, is_dynamic_mask, data):\n    seed = data.draw(seed_strategy())\n    key = random.key(seed)\n    k1, k2, k3 = random.split(key, 3)\n    q_seq_len, kv_seq_len, num_q_heads, num_kv_heads, head_dim_qk, head_dim_v, dtype = data.draw(mha_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (num_q_heads, q_seq_len, head_dim_qk), dtype=dtype)\n    if is_mqa:\n        k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    else:\n        k = random.uniform(k2, (num_kv_heads, kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (num_kv_heads, kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy())\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, num_q_heads))\n    mask = mask_lib.MultiHeadMask(tuple((m.get_mask() for m in masks)))\n    if is_dynamic_mask:\n        mask = to_dynamic_mask(mask)\n    block_sizes = data.draw(block_sizes_strategy(q_seq_len, kv_seq_len))\n    if is_mqa:\n        attn_ref = splash.make_masked_mqa_reference(mask)\n        attn = splash.make_splash_mqa_single_device(mask, block_sizes=block_sizes, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    else:\n        attn_ref = splash.make_masked_mha_reference(mask)\n        attn = splash.make_splash_mha_single_device(mask, block_sizes=block_sizes, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o = attn(q, k, v, segment_ids)\n    o_ref = attn_ref(q.astype(np.float32), k.astype(np.float32), v.astype(np.float32), segment_ids, attn_logits_soft_cap=attn_logits_soft_cap)\n    self._assert_allclose(o, o_ref, atol=0.003, rtol=0.003)",
    "assertions": [
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def to_dynamic_mask(mask: mask_lib.MultiHeadMask) -> jax.Array:\n    q_seq_len, kv_seq_len = mask.masks[0].shape\n    full_mask_slice = (slice(0, q_seq_len), slice(0, kv_seq_len))\n    dynamic_mask = jnp.stack([m[full_mask_slice] for m in mask.masks], axis=0)\n    return dynamic_mask"
  },
  {
    "test_code": "@parameterized.product(is_mqa=(False, True), is_segmented=(False, True), is_dynamic_mask=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_fwd(self, is_mqa, is_segmented, is_dynamic_mask, data):\n    seed = data.draw(seed_strategy())\n    key = random.key(seed)\n    k1, k2, k3 = random.split(key, 3)\n    q_seq_len, kv_seq_len, num_q_heads, num_kv_heads, head_dim_qk, head_dim_v, dtype = data.draw(mha_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (num_q_heads, q_seq_len, head_dim_qk), dtype=dtype)\n    if is_mqa:\n        k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    else:\n        k = random.uniform(k2, (num_kv_heads, kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (num_kv_heads, kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy())\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, num_q_heads))\n    mask = mask_lib.MultiHeadMask(tuple((m.get_mask() for m in masks)))\n    if is_dynamic_mask:\n        mask = to_dynamic_mask(mask)\n    block_sizes = data.draw(block_sizes_strategy(q_seq_len, kv_seq_len))\n    if is_mqa:\n        attn_ref = splash.make_masked_mqa_reference(mask)\n        attn = splash.make_splash_mqa_single_device(mask, block_sizes=block_sizes, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    else:\n        attn_ref = splash.make_masked_mha_reference(mask)\n        attn = splash.make_splash_mha_single_device(mask, block_sizes=block_sizes, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    attn_ref = partial(attn_ref, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap)\n    o, (logsumexp,) = attn(q, k, v, segment_ids)\n    o_ref, (logsumexp_ref,) = attn_ref(q.astype(jnp.float32), k.astype(jnp.float32), v.astype(jnp.float32), segment_ids)\n    self._assert_allclose(o, o_ref, atol=0.003, rtol=0.003)\n    self._assert_allclose(logsumexp, logsumexp_ref, atol=0.001, rtol=0.001)",
    "assertions": [
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def to_dynamic_mask(mask: mask_lib.MultiHeadMask) -> jax.Array:\n    q_seq_len, kv_seq_len = mask.masks[0].shape\n    full_mask_slice = (slice(0, q_seq_len), slice(0, kv_seq_len))\n    dynamic_mask = jnp.stack([m[full_mask_slice] for m in mask.masks], axis=0)\n    return dynamic_mask"
  },
  {
    "test_code": "@parameterized.product(is_mqa=(False, True), is_segmented=(False, True), downcast_smem_data=(False, True), use_fused_bwd_kernel=(False, True), use_dynamic_mask=(False, True))\n@hp.given(hps.data())\ndef test_splash_attention_bwd(self, is_mqa, is_segmented, downcast_smem_data, use_fused_bwd_kernel, use_dynamic_mask, data):\n    seed = data.draw(seed_strategy())\n    key = random.key(seed)\n    k1, k2, k3, k4 = random.split(key, 4)\n    q_seq_len, kv_seq_len, num_q_heads, num_kv_heads, head_dim_qk, head_dim_v, dtype = data.draw(mha_strategy())\n    hp.assume(q_seq_len == kv_seq_len or not is_segmented)\n    q = random.uniform(k1, (num_q_heads, q_seq_len, head_dim_qk), dtype=dtype)\n    if is_mqa:\n        k = random.uniform(k2, (kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (kv_seq_len, head_dim_v), dtype=dtype)\n    else:\n        k = random.uniform(k2, (num_kv_heads, kv_seq_len, head_dim_qk), dtype=dtype)\n        v = random.uniform(k3, (num_kv_heads, kv_seq_len, head_dim_v), dtype=dtype)\n    segment_ids = None\n    if is_segmented:\n        assert q_seq_len == kv_seq_len\n        segment_ids = data.draw(segment_ids_strategy(q_seq_len))\n    attn_logits_soft_cap = data.draw(attn_logits_soft_cap_strategy())\n    masks = data.draw(mha_mask_strategy(q_seq_len, kv_seq_len, num_q_heads))\n    mask = mask_lib.MultiHeadMask(tuple((m.get_mask() for m in masks)))\n    if use_dynamic_mask:\n        mask = to_dynamic_mask(mask)\n    block_sizes = data.draw(block_sizes_strategy(q_seq_len, kv_seq_len, include_bwd_blocks=True, use_fused_bwd_kernel=use_fused_bwd_kernel))\n    if is_mqa:\n        attn_ref = splash.make_masked_mqa_reference(mask, backward_impl='custom')\n        attn = splash.make_splash_mqa_single_device(mask, block_sizes=block_sizes, downcast_smem_data=downcast_smem_data, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    else:\n        attn_ref = splash.make_masked_mha_reference(mask, backward_impl='custom')\n        attn = splash.make_splash_mha_single_device(mask, block_sizes=block_sizes, downcast_smem_data=downcast_smem_data, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o, attn_vjp = jax.vjp(attn, q, k, v, segment_ids)\n    q32, k32, v32 = jax.tree.map(lambda x: x.astype(jnp.float32), (q, k, v))\n    o_ref, (logsumexp,) = attn_ref(q32, k32, v32, segment_ids, save_residuals=True, attn_logits_soft_cap=attn_logits_soft_cap)\n    self._assert_allclose(o, o_ref, atol=0.003, rtol=0.003)\n    do = random.uniform(k4, o.shape, dtype=o.dtype)\n    dq, dk, dv, _ = attn_vjp(do)\n\n    def bwd(mask, q, k, v, segment_ids, o, logsumexp, do) -> tuple[jax.Array, jax.Array, jax.Array]:\n        _, dq, dk, dv, _ = splash._attention_reference_custom_bwd(splash.DEFAULT_MASK_VALUE, False, 'flash', attn_logits_soft_cap, (mask, q, k, v, segment_ids, o, logsumexp), do)\n        return (dq, dk, dv)\n    is_grouped = not is_mqa and num_kv_heads < num_q_heads\n    assert num_q_heads % num_kv_heads == 0\n    head_multiplier = num_q_heads // num_kv_heads\n    if is_mqa:\n        bwd = jax.vmap(bwd, in_axes=(0, 0, None, None, None, 0, 0, 0))\n    else:\n        bwd = jax.vmap(bwd, in_axes=(0, 0, 0, 0, None, 0, 0, 0))\n        if is_grouped:\n            k32 = jnp.repeat(k32, head_multiplier, axis=0)\n            v32 = jnp.repeat(v32, head_multiplier, axis=0)\n    dq_ref, dk_ref, dv_ref = bwd(mask[:, :, :], q32, k32, v32, segment_ids, o.astype(jnp.float32), logsumexp, do.astype(jnp.float32))\n    if is_mqa:\n        dk_ref, dv_ref = (dk_ref.sum(axis=0), dv_ref.sum(axis=0))\n    elif is_grouped:\n        dk_ref = dk_ref.reshape(num_kv_heads, head_multiplier, *dk_ref.shape[1:])\n        dv_ref = dv_ref.reshape(num_kv_heads, head_multiplier, *dv_ref.shape[1:])\n        dk_ref, dv_ref = (dk_ref.sum(axis=1), dv_ref.sum(axis=1))\n    self._assert_allclose(dv, dv_ref, atol=0.02, rtol=0.03)\n    self._assert_allclose(dq, dq_ref, atol=0.02, rtol=0.03)\n    self._assert_allclose(dk, dk_ref, atol=0.02, rtol=0.03)",
    "assertions": [
      "assert num_q_heads % num_kv_heads == 0",
      "assert q_seq_len == kv_seq_len"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_splash_attention_kernel_test.py",
    "function": "def to_dynamic_mask(mask: mask_lib.MultiHeadMask) -> jax.Array:\n    q_seq_len, kv_seq_len = mask.masks[0].shape\n    full_mask_slice = (slice(0, q_seq_len), slice(0, kv_seq_len))\n    dynamic_mask = jnp.stack([m[full_mask_slice] for m in mask.masks], axis=0)\n    return dynamic_mask"
  }
]