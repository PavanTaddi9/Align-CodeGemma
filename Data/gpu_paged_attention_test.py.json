[
  {
    "test_code": "@jtu.sample_product(dtype=(jnp.float16,), page_size=(8, 16, 32), num_kv_heads=(1, 2), q_kv_head_ratio=(2, 16, 20), head_dim=(32, 64), block_h=(16, 32), pages_per_compute_block=(4, 8), k_splits=(4, 16), attn_logits_soft_cap=(None,))\ndef test_paged_attention(self, dtype, page_size, num_kv_heads, q_kv_head_ratio, head_dim, block_h, pages_per_compute_block, k_splits, attn_logits_soft_cap):\n    max_kv_len = 2048\n    seq_lens = np.asarray([3, 256, 513, 1023, 2048], dtype=jnp.int32)\n    q, k_pages, v_pages, block_tables = _generate_qkv(seq_lens.shape[0], page_size, max_kv_len, num_kv_heads, num_kv_heads * q_kv_head_ratio, head_dim, jax.random.key(0), dtype)\n    k = _reconstruct_kv(block_tables, k_pages)\n    v = _reconstruct_kv(block_tables, v_pages)\n    o = paged_attention.paged_attention(q, k_pages, v_pages, block_tables, seq_lens, block_h=block_h, pages_per_compute_block=pages_per_compute_block, k_splits=k_splits, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o_ref = paged_attention.paged_attention_reference(q, k, v, lengths=seq_lens)\n    self.assertArraysAllClose(o, o_ref, rtol=0.05, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_paged_attention_test.py",
    "function": "def _generate_qkv(seq_lens, page_size, max_seq_len, num_kv_heads, num_heads, head_dim, prng_key, dtype=jnp.float32, are_kv_quantized=False):\n    assert max_seq_len % page_size == 0\n    pages_per_sequence = max_seq_len // page_size\n    batch_size = len(seq_lens)\n    total_pages = batch_size * pages_per_sequence\n    k1, k2, k3, k4 = jax.random.split(prng_key, 4)\n    k_pages = jax.random.normal(k1, (num_kv_heads, total_pages, page_size, head_dim), dtype=dtype)\n    v_pages = jax.random.normal(k2, (num_kv_heads, total_pages, page_size, head_dim), dtype=dtype)\n    if are_kv_quantized:\n        k_pages = quantization_utils.quantize_to_int8(k_pages)\n        v_pages = quantization_utils.quantize_to_int8(v_pages)\n    page_indices = jnp.arange(batch_size * pages_per_sequence, dtype=jnp.int32)\n    page_indices = jax.random.permutation(k3, page_indices, independent=True)\n    page_indices = page_indices.reshape(batch_size, pages_per_sequence)\n    q = jax.random.normal(k4, (batch_size, num_heads, head_dim), dtype=dtype)\n    return (q, k_pages, v_pages, page_indices)"
  },
  {
    "test_code": "@jtu.sample_product(dtype=(jnp.float16,), page_size=(8, 16, 32), num_kv_heads=(1, 2), q_kv_head_ratio=(2, 16, 20), head_dim=(32, 64), block_h=(16, 32), pages_per_compute_block=(4, 8), k_splits=(4, 16), attn_logits_soft_cap=(None,))\ndef test_paged_attention(self, dtype, page_size, num_kv_heads, q_kv_head_ratio, head_dim, block_h, pages_per_compute_block, k_splits, attn_logits_soft_cap):\n    max_kv_len = 2048\n    seq_lens = np.asarray([3, 256, 513, 1023, 2048], dtype=jnp.int32)\n    q, k_pages, v_pages, block_tables = _generate_qkv(seq_lens.shape[0], page_size, max_kv_len, num_kv_heads, num_kv_heads * q_kv_head_ratio, head_dim, jax.random.key(0), dtype)\n    k = _reconstruct_kv(block_tables, k_pages)\n    v = _reconstruct_kv(block_tables, v_pages)\n    o = paged_attention.paged_attention(q, k_pages, v_pages, block_tables, seq_lens, block_h=block_h, pages_per_compute_block=pages_per_compute_block, k_splits=k_splits, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o_ref = paged_attention.paged_attention_reference(q, k, v, lengths=seq_lens)\n    self.assertArraysAllClose(o, o_ref, rtol=0.05, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_paged_attention_test.py",
    "function": "def _reconstruct_kv(page_indices, pages):\n    if isinstance(pages, quantization_utils.QuantizedTensor):\n        pages = quantization_utils.unquantize_from_int8(pages, dtype=jnp.float32)\n    batch_size = page_indices.shape[0]\n    num_heads, _, _, head_dim = pages.shape\n\n    def per_sequence_page_gather(pages, page_indices):\n        return jnp.take(pages, page_indices, 1)\n    gathered = jax.vmap(per_sequence_page_gather, in_axes=(None, 0))(pages, page_indices)\n    return gathered.reshape(batch_size, num_heads, -1, head_dim)"
  },
  {
    "test_code": "@jtu.sample_product(dtype=(jnp.float16,), page_size=(8, 16, 32), num_kv_heads=(1, 2), q_kv_head_ratio=(2, 16, 20), head_dim=(32, 64), block_h=(16, 32), pages_per_compute_block=(4, 8), k_splits=(4, 16), attn_logits_soft_cap=(None,))\ndef test_paged_attention(self, dtype, page_size, num_kv_heads, q_kv_head_ratio, head_dim, block_h, pages_per_compute_block, k_splits, attn_logits_soft_cap):\n    max_kv_len = 2048\n    seq_lens = np.asarray([3, 256, 513, 1023, 2048], dtype=jnp.int32)\n    q, k_pages, v_pages, block_tables = _generate_qkv(seq_lens.shape[0], page_size, max_kv_len, num_kv_heads, num_kv_heads * q_kv_head_ratio, head_dim, jax.random.key(0), dtype)\n    k = _reconstruct_kv(block_tables, k_pages)\n    v = _reconstruct_kv(block_tables, v_pages)\n    o = paged_attention.paged_attention(q, k_pages, v_pages, block_tables, seq_lens, block_h=block_h, pages_per_compute_block=pages_per_compute_block, k_splits=k_splits, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o_ref = paged_attention.paged_attention_reference(q, k, v, lengths=seq_lens)\n    self.assertArraysAllClose(o, o_ref, rtol=0.05, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_paged_attention_test.py",
    "function": "def _generate_qkv(batch_size, page_size, max_seq_len, num_kv_heads, num_heads, head_dim, prng_key, dtype=jnp.float32):\n    assert max_seq_len % page_size == 0\n    max_num_blocks_per_seq = max_seq_len // page_size\n    total_pages = batch_size * max_num_blocks_per_seq\n    k1, k2, k3, k4 = jax.random.split(prng_key, 4)\n    k_pages = jax.random.normal(k1, (num_kv_heads, total_pages, page_size, head_dim), dtype=dtype)\n    v_pages = jax.random.normal(k2, (num_kv_heads, total_pages, page_size, head_dim), dtype=dtype)\n    block_tables = jnp.arange(batch_size * max_num_blocks_per_seq, dtype=jnp.int32)\n    block_tables = jax.random.permutation(k3, block_tables, independent=True)\n    block_tables = block_tables.reshape(batch_size, max_num_blocks_per_seq)\n    q = jax.random.normal(k4, (batch_size, num_heads, head_dim), dtype=dtype)\n    return (q, k_pages, v_pages, block_tables)"
  },
  {
    "test_code": "@jtu.sample_product(dtype=(jnp.float16,), page_size=(8, 16, 32), num_kv_heads=(1, 2), q_kv_head_ratio=(2, 16, 20), head_dim=(32, 64), block_h=(16, 32), pages_per_compute_block=(4, 8), k_splits=(4, 16), attn_logits_soft_cap=(None,))\ndef test_paged_attention(self, dtype, page_size, num_kv_heads, q_kv_head_ratio, head_dim, block_h, pages_per_compute_block, k_splits, attn_logits_soft_cap):\n    max_kv_len = 2048\n    seq_lens = np.asarray([3, 256, 513, 1023, 2048], dtype=jnp.int32)\n    q, k_pages, v_pages, block_tables = _generate_qkv(seq_lens.shape[0], page_size, max_kv_len, num_kv_heads, num_kv_heads * q_kv_head_ratio, head_dim, jax.random.key(0), dtype)\n    k = _reconstruct_kv(block_tables, k_pages)\n    v = _reconstruct_kv(block_tables, v_pages)\n    o = paged_attention.paged_attention(q, k_pages, v_pages, block_tables, seq_lens, block_h=block_h, pages_per_compute_block=pages_per_compute_block, k_splits=k_splits, attn_logits_soft_cap=attn_logits_soft_cap, interpret=self.INTERPRET)\n    o_ref = paged_attention.paged_attention_reference(q, k, v, lengths=seq_lens)\n    self.assertArraysAllClose(o, o_ref, rtol=0.05, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_paged_attention_test.py",
    "function": "def _reconstruct_kv(block_tables: jax.Array, pages: jax.Array) -> jax.Array:\n\n    def fn(_block_tables, _pages):\n        head_dim = _pages.shape[-1]\n        out = _pages[_block_tables]\n        return out.reshape(-1, head_dim)\n    with_batch = jax.vmap(fn, (0, None), 0)\n    attn_fn = jax.vmap(with_batch, (None, 0), 1)\n    out = attn_fn(block_tables, pages)\n    out = jnp.swapaxes(out, 1, 2)\n    return out"
  }
]