[
  {
    "test_code": "@parameterized.product(dma_execution_mode=['eager', 'on_wait'], detect_races=[True, False])\ndef test_right_permute_example(self, dma_execution_mode, detect_races):\n    num_devices = jax.device_count()\n    partition = P(None, 'x')\n    mesh = jax.make_mesh((num_devices,), ('x',))\n    sharding = jax.sharding.NamedSharding(mesh, partition)\n    input_arr = jax.random.uniform(jax.random.key(0), (8, 128 * num_devices), dtype=jnp.float32)\n    input_arr = jax.device_put(input_arr, sharding)\n\n    def right_permute_kernel(input_ref, output_ref, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        left_neighbor = lax.rem(my_id + num_devices - 1, jnp.int32(num_devices))\n        right_neighbor = lax.rem(my_id + 1, jnp.int32(num_devices))\n        barrier_sem = pltpu.get_barrier_semaphore()\n        pltpu.semaphore_signal(barrier_sem, device_id=(left_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        pltpu.semaphore_signal(barrier_sem, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        pltpu.semaphore_wait(barrier_sem, 2)\n        remote_copy_op = pltpu.make_async_remote_copy(src_ref=input_ref, dst_ref=output_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        remote_copy_op.start()\n        remote_copy_op.wait()\n    out_shape = jax.ShapeDtypeStruct((8, 128), jnp.float32)\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY), scratch_shapes=[pltpu.SemaphoreType.DMA] * 2)\n    right_permute = pl.pallas_call(right_permute_kernel, out_shape=out_shape, grid_spec=grid_spec, compiler_params=pltpu.TPUCompilerParams(collective_id=13), interpret=mosaic_interpret.TPUInterpretParams(dma_execution_mode=dma_execution_mode, detect_races=detect_races))\n    pallas_result = jax.jit(shard_map.shard_map(right_permute, mesh=mesh, in_specs=partition, out_specs=partition, check_rep=False))(input_arr)\n    perm = tuple(((src, (src + 1) % num_devices) for src in range(num_devices)))\n    xla_result = jax.jit(shard_map.shard_map(lambda x: lax.ppermute(x, 'x', perm), mesh=mesh, in_specs=partition, out_specs=partition))(input_arr)\n    np.testing.assert_allclose(xla_result, pallas_result)\n    if detect_races:\n        self.assertFalse(mosaic_interpret.races.races_found)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_interpret_distributed_test.py",
    "function": "def ppermute(input):\n    return jax.lax.ppermute(input, axis_name='i', perm=[[0, 1], [1, 0]])"
  },
  {
    "test_code": "def test_race_detection(self):\n    num_devices = 4\n    mesh = jax.sharding.Mesh(np.array(jax.devices()[:4]), ('x',))\n    sharding = jax.sharding.NamedSharding(mesh, P('x', None))\n    input_arr = jax.random.uniform(jax.random.key(0), (8 * num_devices, 128))\n    input_arr = jax.device_put(input_arr, sharding)\n\n    def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n        barrier_sem = pltpu.get_barrier_semaphore()\n\n        @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n        def _(i, _):\n            pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n            return None\n        pltpu.semaphore_wait(barrier_sem, num_devices)\n        my_id = lax.axis_index('x')\n        src_dst_ids = src_dst_ids_ref[:]\n        recv_count = 0\n        for i in range(src_dst_ids.shape[0]):\n            src_id = src_dst_ids[i, 0]\n            dst_id = src_dst_ids[i, 1]\n\n            @pl.when(src_id == my_id)\n            def _():\n                dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n                dma.start()\n                dma.wait_send()\n            recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n        @pl.when(recv_count > 0)\n        def _():\n            fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            fake_dma.wait_recv()\n\n    @jax.jit\n    def run(src_dst_ids):\n        return shard_map.shard_map(pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), input_arr.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY), scratch_shapes=[pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA], compiler_params=pltpu.TPUCompilerParams(collective_id=0), interpret=mosaic_interpret.TPUInterpretParams(dma_execution_mode='eager', detect_races=True)), mesh=mesh, in_specs=(P(None), P('x', None)), out_specs=P('x', None), check_rep=False)(src_dst_ids, input_arr)\n    run(jnp.array([[0, 1], [1, 2], [2, 3]], jnp.int32)).block_until_ready()\n    self.assertFalse(mosaic_interpret.races.races_found)\n    run(jnp.array([[0, 1], [1, 2], [3, 2], [3, 0]], jnp.int32)).block_until_ready()\n    self.assertTrue(mosaic_interpret.races.races_found)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_interpret_distributed_test.py",
    "function": "def run(pos):\n    maxiter = 1000\n\n    def cond(v):\n        return v[0] < maxiter\n\n    def step(v):\n        i, pos = v\n        jax.debug.callback(print_it, i + 1, maxiter)\n        return (i + 1, pos + 1)\n    val = (jnp.array(0), pos)\n    val = jax.lax.while_loop(cond, step, val)\n    return val[1]"
  },
  {
    "test_code": "def test_race_detection(self):\n    num_devices = 4\n    mesh = jax.sharding.Mesh(np.array(jax.devices()[:4]), ('x',))\n    sharding = jax.sharding.NamedSharding(mesh, P('x', None))\n    input_arr = jax.random.uniform(jax.random.key(0), (8 * num_devices, 128))\n    input_arr = jax.device_put(input_arr, sharding)\n\n    def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n        barrier_sem = pltpu.get_barrier_semaphore()\n\n        @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n        def _(i, _):\n            pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n            return None\n        pltpu.semaphore_wait(barrier_sem, num_devices)\n        my_id = lax.axis_index('x')\n        src_dst_ids = src_dst_ids_ref[:]\n        recv_count = 0\n        for i in range(src_dst_ids.shape[0]):\n            src_id = src_dst_ids[i, 0]\n            dst_id = src_dst_ids[i, 1]\n\n            @pl.when(src_id == my_id)\n            def _():\n                dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n                dma.start()\n                dma.wait_send()\n            recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n        @pl.when(recv_count > 0)\n        def _():\n            fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            fake_dma.wait_recv()\n\n    @jax.jit\n    def run(src_dst_ids):\n        return shard_map.shard_map(pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), input_arr.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY), scratch_shapes=[pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA], compiler_params=pltpu.TPUCompilerParams(collective_id=0), interpret=mosaic_interpret.TPUInterpretParams(dma_execution_mode='eager', detect_races=True)), mesh=mesh, in_specs=(P(None), P('x', None)), out_specs=P('x', None), check_rep=False)(src_dst_ids, input_arr)\n    run(jnp.array([[0, 1], [1, 2], [2, 3]], jnp.int32)).block_until_ready()\n    self.assertFalse(mosaic_interpret.races.races_found)\n    run(jnp.array([[0, 1], [1, 2], [3, 2], [3, 0]], jnp.int32)).block_until_ready()\n    self.assertTrue(mosaic_interpret.races.races_found)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_interpret_distributed_test.py",
    "function": "def run(primal_ins, cotangent_outs):\n    primal_outs, vjp = jax.vjp(g, *primal_ins)\n    cotangent_ins = vjp(cotangent_outs)\n    return (primal_outs, cotangent_ins)"
  },
  {
    "test_code": "@parameterized.product(dma_execution_mode=['eager', 'on_wait'], detect_races=[True, False])\ndef test_right_permute_example(self, dma_execution_mode, detect_races):\n    num_devices = jax.device_count()\n    partition = P(None, 'x')\n    mesh = jax.make_mesh((num_devices,), ('x',))\n    sharding = jax.sharding.NamedSharding(mesh, partition)\n    input_arr = jax.random.uniform(jax.random.key(0), (8, 128 * num_devices), dtype=jnp.float32)\n    input_arr = jax.device_put(input_arr, sharding)\n\n    def right_permute_kernel(input_ref, output_ref, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        left_neighbor = lax.rem(my_id + num_devices - 1, jnp.int32(num_devices))\n        right_neighbor = lax.rem(my_id + 1, jnp.int32(num_devices))\n        barrier_sem = pltpu.get_barrier_semaphore()\n        pltpu.semaphore_signal(barrier_sem, device_id=(left_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        pltpu.semaphore_signal(barrier_sem, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        pltpu.semaphore_wait(barrier_sem, 2)\n        remote_copy_op = pltpu.make_async_remote_copy(src_ref=input_ref, dst_ref=output_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        remote_copy_op.start()\n        remote_copy_op.wait()\n    out_shape = jax.ShapeDtypeStruct((8, 128), jnp.float32)\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY), scratch_shapes=[pltpu.SemaphoreType.DMA] * 2)\n    right_permute = pl.pallas_call(right_permute_kernel, out_shape=out_shape, grid_spec=grid_spec, compiler_params=pltpu.TPUCompilerParams(collective_id=13), interpret=mosaic_interpret.TPUInterpretParams(dma_execution_mode=dma_execution_mode, detect_races=detect_races))\n    pallas_result = jax.jit(shard_map.shard_map(right_permute, mesh=mesh, in_specs=partition, out_specs=partition, check_rep=False))(input_arr)\n    perm = tuple(((src, (src + 1) % num_devices) for src in range(num_devices)))\n    xla_result = jax.jit(shard_map.shard_map(lambda x: lax.ppermute(x, 'x', perm), mesh=mesh, in_specs=partition, out_specs=partition))(input_arr)\n    np.testing.assert_allclose(xla_result, pallas_result)\n    if detect_races:\n        self.assertFalse(mosaic_interpret.races.races_found)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_interpret_distributed_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.product(dma_execution_mode=['eager', 'on_wait'], detect_races=[True, False])\ndef test_all_gather_example(self, dma_execution_mode, detect_races):\n    num_devices = jax.device_count()\n    partition = P('x', None)\n    mesh = jax.make_mesh((num_devices,), ('x',))\n    sharding = jax.sharding.NamedSharding(mesh, partition)\n    input_arr = jax.random.uniform(jax.random.key(0), (8 * num_devices, 128), dtype=jnp.float32)\n    input_arr = jax.device_put(input_arr, sharding)\n\n    def all_gather_kernel(input_ref, output_ref, local_copy_sem, send_sem, recv_sems):\n        outer_step = pl.program_id(0)\n        my_id = lax.axis_index('x')\n        left_neighbor = lax.rem(my_id + num_devices - 1, jnp.int32(num_devices))\n        right_neighbor = lax.rem(my_id + 1, jnp.int32(num_devices))\n        copy_slot = my_id - outer_step\n        copy_slot = lax.rem(copy_slot + num_devices, jnp.int32(num_devices))\n\n        @pl.when(outer_step == 0)\n        def _():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(left_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n            pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            local_copy_op = pltpu.make_async_copy(src_ref=input_ref, dst_ref=output_ref.at[my_id], sem=local_copy_sem)\n            local_copy_op.start()\n            local_copy_op.wait()\n        remote_copy_op = pltpu.make_async_remote_copy(src_ref=output_ref.at[copy_slot], dst_ref=output_ref.at[copy_slot], send_sem=send_sem, recv_sem=recv_sems.at[outer_step], device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        remote_copy_op.start()\n        remote_copy_op.wait()\n    out_shape = jax.ShapeDtypeStruct((num_devices, 8, 128), jnp.float32)\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY), scratch_shapes=[pltpu.SemaphoreType.DMA] * 2 + [pltpu.SemaphoreType.DMA((num_devices - 1,))], grid=(num_devices - 1,))\n    all_gather = pl.pallas_call(all_gather_kernel, out_shape=out_shape, grid_spec=grid_spec, interpret=mosaic_interpret.TPUInterpretParams(dma_execution_mode=dma_execution_mode, detect_races=detect_races), compiler_params=pltpu.TPUCompilerParams(collective_id=0))\n    pallas_result = jax.jit(shard_map.shard_map(all_gather, mesh=mesh, in_specs=partition, out_specs=partition, check_rep=False))(input_arr)\n    xla_result = jax.jit(shard_map.shard_map(lambda x: lax.all_gather(x, 'x'), mesh=mesh, in_specs=partition, out_specs=partition))(input_arr)\n    np.testing.assert_allclose(xla_result, pallas_result)\n    if detect_races:\n        self.assertFalse(mosaic_interpret.races.races_found)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_interpret_distributed_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.product(dma_execution_mode=['eager', 'on_wait'], detect_races=[True, False])\ndef test_all_reduce_sum_example(self, dma_execution_mode, detect_races):\n    num_devices = jax.device_count()\n    partition = P(None, 'x')\n    mesh = jax.make_mesh((num_devices,), ('x',))\n    sharding = jax.sharding.NamedSharding(mesh, partition)\n    input_arr = jax.random.uniform(jax.random.key(0), shape=(8, 128 * num_devices))\n    input_arr = jax.device_put(input_arr, sharding)\n\n    def all_reduce_kernel(x_ref, o_ref, hbm_scratch, copy_sem, remote_recv_sem, remote_send_sem, capacity_sem, receive_scratch):\n        outer_step = pl.program_id(0)\n        working_slot = lax.rem(outer_step, jnp.int32(2))\n        receiving_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = lax.rem(my_id + 1, jnp.int32(num_devices))\n        left_neighbor = lax.rem(my_id - 1 + num_devices, jnp.int32(num_devices))\n\n        @pl.when(outer_step == 0)\n        def _():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(left_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n            pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            o_ref[...] = jnp.zeros_like(o_ref)\n            receive_scratch[...] = jnp.zeros_like(receive_scratch)\n            initial_copy = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=hbm_scratch.at[working_slot], send_sem=remote_send_sem, recv_sem=remote_recv_sem, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n            initial_copy.start()\n            initial_copy.wait()\n        pltpu.semaphore_signal(capacity_sem, inc=1, device_id=(left_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        local_copy = pltpu.make_async_copy(src_ref=hbm_scratch.at[working_slot], dst_ref=receive_scratch, sem=copy_sem)\n        local_copy.start()\n        pltpu.semaphore_wait(capacity_sem, 1)\n        remote_copy = pltpu.make_async_remote_copy(src_ref=hbm_scratch.at[working_slot], dst_ref=hbm_scratch.at[receiving_slot], send_sem=remote_send_sem, recv_sem=remote_recv_sem, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        remote_copy.start()\n        local_copy.wait()\n        o_ref[...] += receive_scratch[...]\n        remote_copy.wait()\n    out_shape = (jax.ShapeDtypeStruct((8, 128), input_arr.dtype), jax.ShapeDtypeStruct((2, 8, 128), input_arr.dtype))\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM)], out_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], grid=(num_devices,), scratch_shapes=[pltpu.SemaphoreType.DMA] * 3 + [pltpu.SemaphoreType.REGULAR] + [pltpu.VMEM((8, 128), input_arr.dtype)])\n    kernel = pl.pallas_call(all_reduce_kernel, out_shape=out_shape, grid_spec=grid_spec, interpret=mosaic_interpret.TPUInterpretParams(dma_execution_mode=dma_execution_mode, detect_races=detect_races), compiler_params=pltpu.TPUCompilerParams(collective_id=0))\n    pallas_result = jax.jit(shard_map.shard_map(kernel, mesh=mesh, in_specs=partition, out_specs=partition, check_rep=False))(input_arr)\n    pallas_result = jax.block_until_ready(pallas_result)[0]\n\n    def lax_sum(x):\n        return lax.psum(x, 'x')\n    xla_result = jax.jit(shard_map.shard_map(lax_sum, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x')))(input_arr)\n    np.testing.assert_allclose(xla_result, pallas_result, atol=1e-05)\n    if detect_races:\n        self.assertFalse(mosaic_interpret.races.races_found)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_interpret_distributed_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.product(dma_execution_mode=['eager', 'on_wait'], detect_races=[True, False])\ndef test_reduce_scatter_sum_example(self, dma_execution_mode, detect_races):\n    num_devices = jax.device_count()\n    partition = P(None, 'x')\n    mesh = jax.make_mesh((num_devices,), ('x',))\n    sharding = jax.sharding.NamedSharding(mesh, partition)\n    block_size = (16, 128)\n    input_arr = jax.random.uniform(jax.random.key(0), shape=(block_size[0] * num_devices, block_size[1] * num_devices), dtype=jnp.float32)\n    input_arr = jax.device_put(input_arr, sharding)\n    LEFT = 0\n    RIGHT = 1\n\n    def mod(x, n):\n        return lax.rem(x + n, n)\n\n    def signal(left_or_right, semaphore):\n        my_id = lax.axis_index('x')\n        if left_or_right == LEFT:\n            neighbor = mod(my_id - 1, jnp.int32(num_devices))\n        else:\n            neighbor = mod(my_id + 1, jnp.int32(num_devices))\n        pltpu.semaphore_signal(semaphore, inc=1, device_id=(neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n\n    def reduce_scatter_kernel(x_ref, o_ref, hbm_scratch, local_copy_sem, left_recv_sem, left_send_sem, right_recv_sem, right_send_sem, left_capacity_sem, right_capacity_sem, accum_scratch):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        last_iteration = outer_step == pl.num_programs(0) - 1\n        working_slot = lax.rem(outer_step, jnp.int32(2))\n        receiving_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, jnp.int32(num_devices))\n        left_neighbor = mod(my_id - 1, jnp.int32(num_devices))\n        left_copy_device = mod(my_id + outer_step + 1, jnp.int32(num_devices))\n        right_copy_device = mod(my_id - outer_step - 1, jnp.int32(num_devices))\n        left_copy_slice = pl.ds(0, block_size[0] // 2)\n        right_copy_slice = pl.ds(block_size[0] // 2, block_size[0] // 2)\n        current_phase_slice = pl.ds(phase * (block_size[0] // 2), block_size[0] // 2)\n\n        @pl.when(is_start)\n        def _():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(left_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n            pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n            pltpu.semaphore_wait(barrier_sem, 2)\n        initial_left_copy = pltpu.make_async_remote_copy(src_ref=x_ref.at[my_id, left_copy_slice], dst_ref=hbm_scratch.at[working_slot, left_copy_slice], send_sem=left_send_sem, recv_sem=left_recv_sem, device_id=(left_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        initial_right_copy = pltpu.make_async_remote_copy(src_ref=x_ref.at[my_id, right_copy_slice], dst_ref=hbm_scratch.at[working_slot, right_copy_slice], send_sem=right_send_sem, recv_sem=right_recv_sem, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        left_copy = pltpu.make_async_remote_copy(src_ref=hbm_scratch.at[working_slot, left_copy_slice], dst_ref=hbm_scratch.at[receiving_slot, left_copy_slice], send_sem=left_send_sem, recv_sem=left_recv_sem, device_id=(left_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        right_copy = pltpu.make_async_remote_copy(src_ref=hbm_scratch.at[receiving_slot, right_copy_slice], dst_ref=hbm_scratch.at[working_slot, right_copy_slice], send_sem=right_send_sem, recv_sem=right_recv_sem, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n\n        @pl.when(is_start)\n        def _():\n            o_ref[...] = jnp.zeros_like(o_ref[...])\n            accum_scratch[...] = jnp.zeros_like(accum_scratch[...])\n            initial_left_copy.start()\n            initial_left_copy.wait()\n            initial_right_copy.start()\n            signal(LEFT, right_capacity_sem)\n            signal(RIGHT, left_capacity_sem)\n\n        @pl.when(~is_start)\n        def _():\n\n            @pl.when(phase == LEFT)\n            def _():\n                pltpu.semaphore_wait(right_capacity_sem, 1)\n                right_copy.start()\n\n            @pl.when(phase == RIGHT)\n            def _():\n                pltpu.semaphore_wait(left_capacity_sem, 1)\n                left_copy.start()\n        local_copy = pltpu.make_async_copy(src_ref=hbm_scratch.at[working_slot, current_phase_slice], dst_ref=accum_scratch, sem=local_copy_sem)\n        local_copy.start()\n        local_copy.wait()\n\n        @pl.when(~last_iteration)\n        def _():\n\n            @pl.when(phase == LEFT)\n            def _():\n                accum_scratch[...] += x_ref[left_copy_device, left_copy_slice]\n\n            @pl.when(phase == RIGHT)\n            def _():\n                accum_scratch[...] += x_ref[right_copy_device, right_copy_slice]\n        local_copy = pltpu.make_async_copy(src_ref=accum_scratch, dst_ref=hbm_scratch.at[working_slot, current_phase_slice], sem=local_copy_sem)\n        local_copy.start()\n        local_copy.wait()\n\n        @pl.when(is_start)\n        def _():\n            initial_right_copy.wait()\n\n        @pl.when(~is_start)\n        def _():\n\n            @pl.when(phase == LEFT)\n            def _():\n                right_copy.wait()\n                signal(LEFT, right_capacity_sem)\n\n            @pl.when(phase == RIGHT)\n            def _():\n                left_copy.wait()\n                signal(RIGHT, left_capacity_sem)\n\n        @pl.when(last_iteration)\n        def _():\n\n            @pl.when(phase == LEFT)\n            def _():\n                o_ref[left_copy_slice, ...] = accum_scratch[...]\n                pltpu.semaphore_wait(right_capacity_sem, 1)\n\n            @pl.when(phase == RIGHT)\n            def _():\n                o_ref[right_copy_slice, ...] = accum_scratch[...]\n                pltpu.semaphore_wait(left_capacity_sem, 1)\n    out_shape = (jax.ShapeDtypeStruct((block_size[0], block_size[1]), jnp.float32), jax.ShapeDtypeStruct((2, block_size[0], block_size[1]), jnp.float32))\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM)], out_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], grid=(num_devices, 2), scratch_shapes=[pltpu.SemaphoreType.DMA] * 5 + [pltpu.SemaphoreType.REGULAR] * 2 + [pltpu.VMEM((block_size[0] // 2, block_size[1]), jnp.float32)])\n\n    def pallas_reduce_scatter(input_arr):\n        input_arr = input_arr.reshape(num_devices, block_size[0], block_size[1])\n        return pl.pallas_call(reduce_scatter_kernel, out_shape=out_shape, grid_spec=grid_spec, interpret=mosaic_interpret.TPUInterpretParams(dma_execution_mode=dma_execution_mode, detect_races=True), compiler_params=pltpu.TPUCompilerParams(collective_id=7))(input_arr)[0]\n    pallas_result = jax.jit(shard_map.shard_map(pallas_reduce_scatter, mesh=mesh, in_specs=P(None, 'x'), out_specs=P('x', None), check_rep=False))(input_arr)\n    pallas_result = jax.block_until_ready(pallas_result)\n\n    def lax_reduce_sum_scatter(x):\n        x = x.reshape(num_devices, block_size[0], block_size[1])\n        return lax.psum_scatter(x, 'x')\n    xla_result = jax.jit(shard_map.shard_map(lax_reduce_sum_scatter, mesh=mesh, in_specs=P(None, 'x'), out_specs=P('x', None)))(input_arr)\n    np.testing.assert_allclose(xla_result, pallas_result, atol=1e-05)\n    if detect_races:\n        self.assertFalse(mosaic_interpret.races.races_found)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_interpret_distributed_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.product(dma_execution_mode=['eager', 'on_wait'], detect_races=[True, False])\ndef test_reduce_scatter_sum_with_emit_pipeline_example(self, dma_execution_mode, detect_races):\n    self.skipTest('requires a patched pallas.emit_pipeline to specify/fake the TPU generation')\n    if jax.config.jax_enable_x64:\n        self.skipTest('pallas.emit_pipeline + x64 is not currently supported')\n    num_devices = jax.device_count()\n    partition = P(None, 'x')\n    mesh = jax.make_mesh((num_devices,), ('x',))\n    sharding = jax.sharding.NamedSharding(mesh, partition)\n    outer_block_size = (512, 512)\n    inner_block_size = (128, 128)\n    input_arr = jax.random.uniform(jax.random.key(0), shape=(outer_block_size[0] * num_devices, outer_block_size[1] * num_devices), dtype=jnp.float32)\n    input_arr = jax.device_put(input_arr, sharding)\n    inner_grid = (outer_block_size[0] // inner_block_size[0] // 2, outer_block_size[1] // inner_block_size[1])\n    inner_block_spec = pl.BlockSpec(index_map=lambda i, j: (i, j), block_shape=inner_block_size, memory_space=pltpu.TPUMemorySpace.ANY)\n    LEFT = 0\n    RIGHT = 1\n\n    def mod(x, n):\n        return lax.rem(x + n, n)\n\n    def signal(left_or_right, semaphore):\n        my_id = lax.axis_index('x')\n        if left_or_right == LEFT:\n            neighbor = mod(my_id - 1, num_devices)\n        else:\n            neighbor = mod(my_id + 1, num_devices)\n        pltpu.semaphore_signal(semaphore, inc=1, device_id=(neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n\n    def reduce_scatter_kernel(x_ref, o_ref, hbm_scratch, left_recv_sem, left_send_sem, copy_sem, right_recv_sem, right_send_sem, left_capacity_sem, right_capacity_sem):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        last_iteration = outer_step == pl.num_programs(0) - 1\n        working_slot = lax.rem(outer_step, 2)\n        receiving_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        left_copy_device = mod(my_id + outer_step + 1, num_devices)\n        right_copy_device = mod(my_id - outer_step - 1, num_devices)\n        left_copy_slice = pl.ds(0, outer_block_size[0] // 2)\n        right_copy_slice = pl.ds(outer_block_size[0] // 2, outer_block_size[0] // 2)\n        current_phase_slice = pl.ds(phase * (outer_block_size[0] // 2), outer_block_size[0] // 2)\n        initial_left_copy = pltpu.make_async_remote_copy(src_ref=x_ref.at[my_id, left_copy_slice], dst_ref=hbm_scratch.at[working_slot, left_copy_slice], send_sem=left_send_sem, recv_sem=left_recv_sem, device_id=(left_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        initial_right_copy = pltpu.make_async_remote_copy(src_ref=x_ref.at[my_id, right_copy_slice], dst_ref=hbm_scratch.at[working_slot, right_copy_slice], send_sem=right_send_sem, recv_sem=right_recv_sem, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        left_copy = pltpu.make_async_remote_copy(src_ref=hbm_scratch.at[working_slot, left_copy_slice], dst_ref=hbm_scratch.at[receiving_slot, left_copy_slice], send_sem=left_send_sem, recv_sem=left_recv_sem, device_id=(left_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n        right_copy = pltpu.make_async_remote_copy(src_ref=hbm_scratch.at[receiving_slot, right_copy_slice], dst_ref=hbm_scratch.at[working_slot, right_copy_slice], send_sem=right_send_sem, recv_sem=right_recv_sem, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n\n        @pl.when(is_start)\n        def _():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(left_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n            pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(right_neighbor,), device_id_type=pltpu.DeviceIdType.MESH)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_left_copy.start()\n            initial_left_copy.wait()\n            initial_right_copy.start()\n            signal(LEFT, right_capacity_sem)\n            signal(RIGHT, left_capacity_sem)\n\n        @pl.when(~is_start)\n        def _():\n\n            @pl.when(phase == LEFT)\n            def _():\n                pltpu.semaphore_wait(right_capacity_sem, 1)\n                right_copy.start()\n\n            @pl.when(phase == RIGHT)\n            def _():\n                pltpu.semaphore_wait(left_capacity_sem, 1)\n                left_copy.start()\n\n        def inner_kernel(input_ref, accum_ref):\n            accum_ref[...] = input_ref[...]\n        accum_pipeline = pltpu.emit_pipeline(inner_kernel, in_specs=[inner_block_spec], out_specs=inner_block_spec, should_accumulate_out=True, grid=inner_grid)\n\n        @pl.when(~last_iteration)\n        def _():\n\n            @pl.when(phase == LEFT)\n            def _():\n                accum_pipeline(x_ref.at[left_copy_device, left_copy_slice], hbm_scratch.at[working_slot, left_copy_slice])\n\n            @pl.when(phase == RIGHT)\n            def _():\n                accum_pipeline(x_ref.at[right_copy_device, right_copy_slice], hbm_scratch.at[working_slot, right_copy_slice])\n\n        @pl.when(is_start)\n        def _():\n            initial_right_copy.wait()\n\n        @pl.when(~is_start)\n        def _():\n\n            @pl.when(phase == LEFT)\n            def _():\n                right_copy.wait()\n                signal(LEFT, right_capacity_sem)\n\n            @pl.when(phase == RIGHT)\n            def _():\n                left_copy.wait()\n                signal(RIGHT, left_capacity_sem)\n\n        @pl.when(last_iteration)\n        def _():\n            output_copy = pltpu.make_async_copy(src_ref=hbm_scratch.at[working_slot, current_phase_slice], dst_ref=o_ref.at[current_phase_slice], sem=copy_sem)\n            output_copy.start()\n            output_copy.wait()\n\n            @pl.when(phase == LEFT)\n            def _():\n                pltpu.semaphore_wait(right_capacity_sem, 1)\n\n            @pl.when(phase == RIGHT)\n            def _():\n                pltpu.semaphore_wait(left_capacity_sem, 1)\n    out_shape = (jax.ShapeDtypeStruct((outer_block_size[0], outer_block_size[1]), jnp.float32), jax.ShapeDtypeStruct((2, outer_block_size[0], outer_block_size[1]), jnp.float32))\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], out_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], grid=(num_devices, 2), scratch_shapes=[pltpu.SemaphoreType.DMA] * 5 + [pltpu.SemaphoreType.REGULAR] * 2)\n\n    def pallas_reduce_scatter(input_arr):\n        input_arr = input_arr.reshape(num_devices, outer_block_size[0], outer_block_size[1])\n        return pl.pallas_call(reduce_scatter_kernel, out_shape=out_shape, grid_spec=grid_spec, interpret=mosaic_interpret.TPUInterpretParams(dma_execution_mode=dma_execution_mode, detect_races=detect_races), compiler_params=pltpu.TPUCompilerParams(collective_id=19))(input_arr)[0]\n    pallas_result = jax.jit(shard_map.shard_map(pallas_reduce_scatter, mesh=mesh, in_specs=P(None, 'x'), out_specs=P('x', None), check_rep=False))(input_arr)\n    pallas_result = jax.block_until_ready(pallas_result)\n\n    def lax_reduce_sum_scatter(x):\n        x = x.reshape(num_devices, outer_block_size[0], outer_block_size[1])\n        return lax.psum_scatter(x, 'x')\n    xla_result = jax.jit(shard_map.shard_map(lax_reduce_sum_scatter, mesh=mesh, in_specs=P(None, 'x'), out_specs=P('x', None)))(input_arr)\n    np.testing.assert_allclose(xla_result, pallas_result, atol=1e-05)\n    if detect_races:\n        self.assertFalse(mosaic_interpret.races.races_found)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_interpret_distributed_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  }
]