[
  {
    "test_code": "def test_select_with_scalar_condition(self):\n\n    def kernel(cond, lhs, rhs, out):\n        out[:] = jax.lax.select(cond[0] != 0, lhs[:], rhs[:])\n\n    def run(cond, lhs, rhs):\n        return self.pallas_call(kernel, out_shape=lhs, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM), pl.BlockSpec(memory_space=pltpu.VMEM), pl.BlockSpec(memory_space=pltpu.VMEM)]), name='select_kernel')(cond, lhs, rhs)\n    cond = jnp.array([1], dtype=jnp.int32)\n    lhs = jnp.zeros((8, 128), dtype=jnp.float32)\n    rhs = jnp.ones((8, 128), dtype=jnp.float32)\n    assert (run(cond, lhs, rhs) == lhs).all()",
    "assertions": [
      "assert (run(cond, lhs, rhs) == lhs).all()"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@parameterized.product(is_dynamic=(False, True))\n@hp.given(axis=hps.integers(0, 3), shift=hps.integers(0, 3), stride=hps.one_of(hps.just(None), hps.integers(0, 2)), stride_axis=hps.one_of(hps.just(None), hps.integers(0, 2)))\n@hp.example(3, 9, 1, 2)\n@hp.example(3, 9, 2, 2)\n@hp.example(0, 9, 0, 1)\n@hp.example(0, 9, 1, 1)\ndef test_roll(self, is_dynamic, axis, shift, stride, stride_axis):\n    if (stride is None) != (stride_axis is None):\n        self.skipTest('Roll op requires both stride and stride_axis to be either specified or not specified.')\n    if not jtu.is_device_tpu(version=5) and stride_axis == 2:\n        self.skipTest('Roll op with stride axis on 2nd minor requires at least TPU v5')\n    shape = (4, 4, 32, 512)\n\n    def kernel(s_ref, x_ref, y_ref):\n        amt = s_ref[0] if is_dynamic else shift\n        y_ref[...] = pltpu.roll(x_ref[...], amt, axis, stride=stride, stride_axis=stride_axis)\n\n    def roll(x, shift, axis, stride=None, stride_axis=None):\n        assert (stride is None) == (stride_axis is None)\n        if stride is None:\n            return np.roll(x, shift, axis)\n        outputs = [np.roll(xs, shift + i * stride, axis) for i, xs in enumerate(np.split(x, x.shape[stride_axis], stride_axis))]\n        return np.concatenate(outputs, stride_axis)\n    inp = np.arange(np.prod(shape), dtype=jnp.int32).reshape(shape)\n    ref = roll(inp, shift, axis, stride, stride_axis)\n    dynamic_shift = jnp.array([abs(shift)], jnp.int32)\n    for interpret in [False, True]:\n        out = pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(shape, jnp.int32), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1), interpret=interpret)(dynamic_shift, inp)\n        np.testing.assert_array_equal(out, ref, err_msg=f'interpret={interpret!r}')",
    "assertions": [
      "assert (stride is None) == (stride_axis is None)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "def test_interleave_vectors(self):\n    if not jtu.is_device_tpu_at_least(version=4):\n        self.skipTest('Expect TPUv4+')\n\n    def kernel(x_ref, y_ref, out_ref):\n        x = pltpu.bitcast(x_ref[...].astype(jnp.float32), jnp.int32)\n        y = pltpu.bitcast(y_ref[...].astype(jnp.float32), jnp.int32)\n        shift = jax.lax.broadcast(16, x.shape)\n        out_ref[...] = pltpu.bitcast(y | jax.lax.shift_right_logical(x, shift), jnp.bfloat16)\n    m, n = (16, 128)\n    inp = np.arange(m * n * 2, dtype=jnp.bfloat16).reshape(m, n * 2)\n    x, y = np.split(inp, 2, axis=1)\n    out = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((m * 2, n), jnp.bfloat16))(x, y)\n    np.testing.assert_array_equal(out, inp.reshape(m * 2, n))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.float32, jnp.bfloat16])\ndef test_float_div(self, dtype):\n    if not jtu.if_cloud_tpu_at_least(2025, 2, 13):\n        self.skipTest('Requires libtpu built after 2025-02-13')\n    if not jtu.is_device_tpu_at_least(version=4):\n        self.skipTest('Requires TPUv4+')\n    kwargs = {}\n    if jtu.get_tpu_version() == 6:\n        kwargs.update(dict(rtol=0.01))\n\n    def kernel(x, y, out):\n        out[:] = jax.lax.div(x[:], y[:])\n    run = pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    k1, k2 = jax.random.split(jax.random.key(1234), 2)\n    x = jax.random.normal(k1, (8, 128), dtype=dtype)\n    y = jax.random.normal(k2, (8, 128), dtype=dtype)\n    np.testing.assert_allclose(run(x, y), jax.lax.div(x, y), **kwargs)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.float32, jnp.bfloat16, jnp.int16, jnp.int8])\ndef test_cast_vector_to_mask(self, dtype):\n    if not jtu.if_cloud_tpu_at_least(2025, 1, 22):\n        self.skipTest('Requires libtpu built after 2025-01-22')\n    shape = (128, 128)\n    bitwidth = pallas_utils.dtype_bitwidth(dtype)\n    if jtu.get_tpu_version() < 5 and bitwidth < 32:\n        self.skipTest(f'Not implemented: cast vector to mask with bitwidth == {bitwidth}')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, mask_ref, o_ref):\n        zeros = jnp.zeros_like(x_ref)\n        o_ref[...] = jnp.where(mask_ref[...], x_ref[...], zeros)\n    mask = jax.random.bernoulli(jax.random.key(1234), 0.5, shape).astype(dtype)\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape) + 1\n    out = kernel(x, mask)\n    expected = jnp.where(mask, x, jnp.zeros_like(x))\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.product(msk_dtype=[jnp.float32, jnp.bfloat16, jnp.int8], dtype=[jnp.float32, jnp.bfloat16])\ndef test_i1_relayout_with_bitwidth_change(self, msk_dtype, dtype):\n    if not jtu.if_cloud_tpu_at_least(2025, 1, 25):\n        self.skipTest('Requires libtpu built after 2025-01-25')\n    shape = (129, 129)\n    msk_bitwidth = pallas_utils.dtype_bitwidth(msk_dtype)\n    bitwidth = pallas_utils.dtype_bitwidth(dtype)\n    if jtu.get_tpu_version() < 5 and msk_bitwidth < 32:\n        self.skipTest(f'Not implemented: cast vector to mask with bitwidth == {msk_bitwidth}')\n    if jtu.get_tpu_version() < 5 and bitwidth < 32:\n        self.skipTest(f'Not implemented: comparison with bitwidth == {bitwidth}')\n\n    @functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, mask_ref, o_ref):\n        zeros = jnp.zeros_like(x_ref)\n        o_ref[...] = jnp.where(mask_ref[...], x_ref[...], zeros)\n    mask = jax.random.bernoulli(jax.random.key(1234), 0.5, shape).astype(msk_dtype)\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape) + 1\n    out = kernel(x, mask)\n    expected = jnp.where(mask, x, jnp.zeros_like(x))\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.product(is_dynamic=(False, True))\n@hp.given(axis=hps.integers(0, 3), shift=hps.integers(0, 3), stride=hps.one_of(hps.just(None), hps.integers(0, 2)), stride_axis=hps.one_of(hps.just(None), hps.integers(0, 2)))\n@hp.example(3, 9, 1, 2)\n@hp.example(3, 9, 2, 2)\n@hp.example(0, 9, 0, 1)\n@hp.example(0, 9, 1, 1)\ndef test_roll(self, is_dynamic, axis, shift, stride, stride_axis):\n    if (stride is None) != (stride_axis is None):\n        self.skipTest('Roll op requires both stride and stride_axis to be either specified or not specified.')\n    if not jtu.is_device_tpu(version=5) and stride_axis == 2:\n        self.skipTest('Roll op with stride axis on 2nd minor requires at least TPU v5')\n    shape = (4, 4, 32, 512)\n\n    def kernel(s_ref, x_ref, y_ref):\n        amt = s_ref[0] if is_dynamic else shift\n        y_ref[...] = pltpu.roll(x_ref[...], amt, axis, stride=stride, stride_axis=stride_axis)\n\n    def roll(x, shift, axis, stride=None, stride_axis=None):\n        assert (stride is None) == (stride_axis is None)\n        if stride is None:\n            return np.roll(x, shift, axis)\n        outputs = [np.roll(xs, shift + i * stride, axis) for i, xs in enumerate(np.split(x, x.shape[stride_axis], stride_axis))]\n        return np.concatenate(outputs, stride_axis)\n    inp = np.arange(np.prod(shape), dtype=jnp.int32).reshape(shape)\n    ref = roll(inp, shift, axis, stride, stride_axis)\n    dynamic_shift = jnp.array([abs(shift)], jnp.int32)\n    for interpret in [False, True]:\n        out = pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(shape, jnp.int32), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1), interpret=interpret)(dynamic_shift, inp)\n        np.testing.assert_array_equal(out, ref, err_msg=f'interpret={interpret!r}')",
    "assertions": [
      "assert (stride is None) == (stride_axis is None)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "@partial(jax.jit, in_shardings=s, out_shardings=jax.sharding.NamedSharding(mesh, P()))\ndef example(x, y):\n    return jax.grad(f_wrapper, allow_int=True, argnums=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_select_with_scalar_condition(self):\n\n    def kernel(cond, lhs, rhs, out):\n        out[:] = jax.lax.select(cond[0] != 0, lhs[:], rhs[:])\n\n    def run(cond, lhs, rhs):\n        return self.pallas_call(kernel, out_shape=lhs, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM), pl.BlockSpec(memory_space=pltpu.VMEM), pl.BlockSpec(memory_space=pltpu.VMEM)]), name='select_kernel')(cond, lhs, rhs)\n    cond = jnp.array([1], dtype=jnp.int32)\n    lhs = jnp.zeros((8, 128), dtype=jnp.float32)\n    rhs = jnp.ones((8, 128), dtype=jnp.float32)\n    assert (run(cond, lhs, rhs) == lhs).all()",
    "assertions": [
      "assert (run(cond, lhs, rhs) == lhs).all()"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def run(pos):\n    maxiter = 1000\n\n    def cond(v):\n        return v[0] < maxiter\n\n    def step(v):\n        i, pos = v\n        jax.debug.callback(print_it, i + 1, maxiter)\n        return (i + 1, pos + 1)\n    val = (jnp.array(0), pos)\n    val = jax.lax.while_loop(cond, step, val)\n    return val[1]"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.float32, jnp.bfloat16])\ndef test_float_div(self, dtype):\n    if not jtu.if_cloud_tpu_at_least(2025, 2, 13):\n        self.skipTest('Requires libtpu built after 2025-02-13')\n    if not jtu.is_device_tpu_at_least(version=4):\n        self.skipTest('Requires TPUv4+')\n    kwargs = {}\n    if jtu.get_tpu_version() == 6:\n        kwargs.update(dict(rtol=0.01))\n\n    def kernel(x, y, out):\n        out[:] = jax.lax.div(x[:], y[:])\n    run = pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    k1, k2 = jax.random.split(jax.random.key(1234), 2)\n    x = jax.random.normal(k1, (8, 128), dtype=dtype)\n    y = jax.random.normal(k2, (8, 128), dtype=dtype)\n    np.testing.assert_allclose(run(x, y), jax.lax.div(x, y), **kwargs)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def run(pos):\n    maxiter = 1000\n\n    def cond(v):\n        return v[0] < maxiter\n\n    def step(v):\n        i, pos = v\n        jax.debug.callback(print_it, i + 1, maxiter)\n        return (i + 1, pos + 1)\n    val = (jnp.array(0), pos)\n    val = jax.lax.while_loop(cond, step, val)\n    return val[1]"
  },
  {
    "test_code": "def test_select_with_scalar_condition(self):\n\n    def kernel(cond, lhs, rhs, out):\n        out[:] = jax.lax.select(cond[0] != 0, lhs[:], rhs[:])\n\n    def run(cond, lhs, rhs):\n        return self.pallas_call(kernel, out_shape=lhs, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM), pl.BlockSpec(memory_space=pltpu.VMEM), pl.BlockSpec(memory_space=pltpu.VMEM)]), name='select_kernel')(cond, lhs, rhs)\n    cond = jnp.array([1], dtype=jnp.int32)\n    lhs = jnp.zeros((8, 128), dtype=jnp.float32)\n    rhs = jnp.ones((8, 128), dtype=jnp.float32)\n    assert (run(cond, lhs, rhs) == lhs).all()",
    "assertions": [
      "assert (run(cond, lhs, rhs) == lhs).all()"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def run(primal_ins, cotangent_outs):\n    primal_outs, vjp = jax.vjp(g, *primal_ins)\n    cotangent_ins = vjp(cotangent_outs)\n    return (primal_outs, cotangent_ins)"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.float32, jnp.bfloat16])\ndef test_float_div(self, dtype):\n    if not jtu.if_cloud_tpu_at_least(2025, 2, 13):\n        self.skipTest('Requires libtpu built after 2025-02-13')\n    if not jtu.is_device_tpu_at_least(version=4):\n        self.skipTest('Requires TPUv4+')\n    kwargs = {}\n    if jtu.get_tpu_version() == 6:\n        kwargs.update(dict(rtol=0.01))\n\n    def kernel(x, y, out):\n        out[:] = jax.lax.div(x[:], y[:])\n    run = pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    k1, k2 = jax.random.split(jax.random.key(1234), 2)\n    x = jax.random.normal(k1, (8, 128), dtype=dtype)\n    y = jax.random.normal(k2, (8, 128), dtype=dtype)\n    np.testing.assert_allclose(run(x, y), jax.lax.div(x, y), **kwargs)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def run(primal_ins, cotangent_outs):\n    primal_outs, vjp = jax.vjp(g, *primal_ins)\n    cotangent_ins = vjp(cotangent_outs)\n    return (primal_outs, cotangent_ins)"
  },
  {
    "test_code": "def test_tpu_signed_int_upcast(self):\n    if not jtu.is_device_tpu_at_least(version=5):\n        self.skipTest('TPUv5+ needed for integer matmuls')\n\n    def body(x_ref, o_ref):\n        ux = lax.convert_element_type(x_ref[...], jnp.int8)\n        o_ref[...] = jax.lax.dot(ux, ux, preferred_element_type=jnp.int32)\n    out = jax.ShapeDtypeStruct((128, 128), jnp.int32)\n    x = jnp.arange(128 * 128, dtype=jnp.int4).reshape((128, 128))\n    result = self.pallas_call(body, out_shape=out)(x)\n    np.testing.assert_array_equal(result, jax.lax.dot(x.astype(jnp.int8), x.astype(jnp.int8), preferred_element_type=jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.float32, jnp.bfloat16, jnp.int16, jnp.int8])\ndef test_cast_vector_to_mask(self, dtype):\n    if not jtu.if_cloud_tpu_at_least(2025, 1, 22):\n        self.skipTest('Requires libtpu built after 2025-01-22')\n    shape = (128, 128)\n    bitwidth = pallas_utils.dtype_bitwidth(dtype)\n    if jtu.get_tpu_version() < 5 and bitwidth < 32:\n        self.skipTest(f'Not implemented: cast vector to mask with bitwidth == {bitwidth}')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, mask_ref, o_ref):\n        zeros = jnp.zeros_like(x_ref)\n        o_ref[...] = jnp.where(mask_ref[...], x_ref[...], zeros)\n    mask = jax.random.bernoulli(jax.random.key(1234), 0.5, shape).astype(dtype)\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape) + 1\n    out = kernel(x, mask)\n    expected = jnp.where(mask, x, jnp.zeros_like(x))\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.product(msk_dtype=[jnp.float32, jnp.bfloat16, jnp.int8], dtype=[jnp.float32, jnp.bfloat16])\ndef test_i1_relayout_with_bitwidth_change(self, msk_dtype, dtype):\n    if not jtu.if_cloud_tpu_at_least(2025, 1, 25):\n        self.skipTest('Requires libtpu built after 2025-01-25')\n    shape = (129, 129)\n    msk_bitwidth = pallas_utils.dtype_bitwidth(msk_dtype)\n    bitwidth = pallas_utils.dtype_bitwidth(dtype)\n    if jtu.get_tpu_version() < 5 and msk_bitwidth < 32:\n        self.skipTest(f'Not implemented: cast vector to mask with bitwidth == {msk_bitwidth}')\n    if jtu.get_tpu_version() < 5 and bitwidth < 32:\n        self.skipTest(f'Not implemented: comparison with bitwidth == {bitwidth}')\n\n    @functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, mask_ref, o_ref):\n        zeros = jnp.zeros_like(x_ref)\n        o_ref[...] = jnp.where(mask_ref[...], x_ref[...], zeros)\n    mask = jax.random.bernoulli(jax.random.key(1234), 0.5, shape).astype(msk_dtype)\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape) + 1\n    out = kernel(x, mask)\n    expected = jnp.where(mask, x, jnp.zeros_like(x))\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.float32, jnp.bfloat16, jnp.int16, jnp.int8])\ndef test_cast_vector_to_mask(self, dtype):\n    if not jtu.if_cloud_tpu_at_least(2025, 1, 22):\n        self.skipTest('Requires libtpu built after 2025-01-22')\n    shape = (128, 128)\n    bitwidth = pallas_utils.dtype_bitwidth(dtype)\n    if jtu.get_tpu_version() < 5 and bitwidth < 32:\n        self.skipTest(f'Not implemented: cast vector to mask with bitwidth == {bitwidth}')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, mask_ref, o_ref):\n        zeros = jnp.zeros_like(x_ref)\n        o_ref[...] = jnp.where(mask_ref[...], x_ref[...], zeros)\n    mask = jax.random.bernoulli(jax.random.key(1234), 0.5, shape).astype(dtype)\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape) + 1\n    out = kernel(x, mask)\n    expected = jnp.where(mask, x, jnp.zeros_like(x))\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.product(msk_dtype=[jnp.float32, jnp.bfloat16, jnp.int8], dtype=[jnp.float32, jnp.bfloat16])\ndef test_i1_relayout_with_bitwidth_change(self, msk_dtype, dtype):\n    if not jtu.if_cloud_tpu_at_least(2025, 1, 25):\n        self.skipTest('Requires libtpu built after 2025-01-25')\n    shape = (129, 129)\n    msk_bitwidth = pallas_utils.dtype_bitwidth(msk_dtype)\n    bitwidth = pallas_utils.dtype_bitwidth(dtype)\n    if jtu.get_tpu_version() < 5 and msk_bitwidth < 32:\n        self.skipTest(f'Not implemented: cast vector to mask with bitwidth == {msk_bitwidth}')\n    if jtu.get_tpu_version() < 5 and bitwidth < 32:\n        self.skipTest(f'Not implemented: comparison with bitwidth == {bitwidth}')\n\n    @functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, mask_ref, o_ref):\n        zeros = jnp.zeros_like(x_ref)\n        o_ref[...] = jnp.where(mask_ref[...], x_ref[...], zeros)\n    mask = jax.random.bernoulli(jax.random.key(1234), 0.5, shape).astype(msk_dtype)\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape) + 1\n    out = kernel(x, mask)\n    expected = jnp.where(mask, x, jnp.zeros_like(x))\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "def test_tpu_signed_int_upcast(self):\n    if not jtu.is_device_tpu_at_least(version=5):\n        self.skipTest('TPUv5+ needed for integer matmuls')\n\n    def body(x_ref, o_ref):\n        ux = lax.convert_element_type(x_ref[...], jnp.int8)\n        o_ref[...] = jax.lax.dot(ux, ux, preferred_element_type=jnp.int32)\n    out = jax.ShapeDtypeStruct((128, 128), jnp.int32)\n    x = jnp.arange(128 * 128, dtype=jnp.int4).reshape((128, 128))\n    result = self.pallas_call(body, out_shape=out)(x)\n    np.testing.assert_array_equal(result, jax.lax.dot(x.astype(jnp.int8), x.astype(jnp.int8), preferred_element_type=jnp.int32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.float32, jnp.bfloat16])\ndef test_float_div(self, dtype):\n    if not jtu.if_cloud_tpu_at_least(2025, 2, 13):\n        self.skipTest('Requires libtpu built after 2025-02-13')\n    if not jtu.is_device_tpu_at_least(version=4):\n        self.skipTest('Requires TPUv4+')\n    kwargs = {}\n    if jtu.get_tpu_version() == 6:\n        kwargs.update(dict(rtol=0.01))\n\n    def kernel(x, y, out):\n        out[:] = jax.lax.div(x[:], y[:])\n    run = pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    k1, k2 = jax.random.split(jax.random.key(1234), 2)\n    x = jax.random.normal(k1, (8, 128), dtype=dtype)\n    y = jax.random.normal(k2, (8, 128), dtype=dtype)\n    np.testing.assert_allclose(run(x, y), jax.lax.div(x, y), **kwargs)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  }
]