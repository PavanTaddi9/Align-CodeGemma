[
  {
    "test_code": "def test_cross_platform(self):\n    if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n        self.skipTest('Requires libtpu built after 2024-12-19')\n\n    def add_vectors_kernel(x_ref, y_ref, o_ref):\n        x, y = (x_ref[...], y_ref[...])\n        o_ref[...] = x + y\n\n    @jax.jit\n    def add_vectors(x: jax.Array, y: jax.Array) -> jax.Array:\n        return pl.pallas_call(add_vectors_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), name='my_custom_kernel_name')(x, y)\n    platforms = ['tpu']\n    if True or triton.has_compilation_handler('cuda'):\n        platforms.append('cuda')\n    a = np.arange(8 * 16, dtype=np.int32).reshape((8, 16))\n    exp = export.export(add_vectors, platforms=platforms, disabled_checks=[export.DisabledSafetyCheck.custom_call('triton_kernel_call'), export.DisabledSafetyCheck.custom_call('__gpu$xla.gpu.triton')])(a, a)\n    if jtu.device_under_test() == 'tpu' or (jtu.device_under_test() == 'gpu' and jtu.is_cuda_compute_capability_at_least('8.0')):\n        res = exp.call(a, a)\n        self.assertAllClose(res, a + a)\n    if 'tpu' in platforms:\n        self.assertRegex(exp.mlir_module(), 'stablehlo.custom_call @tpu_custom_call.+kernel_name\\\\s*=\\\\s*\\\\\"my_custom_kernel_name\\\\\"')\n    if 'cuda' in platforms:\n        self.assertRegex(exp.mlir_module(), 'stablehlo.custom_call @__gpu\\\\$xla\\\\.gpu\\\\.triton.+name\\\\s*=\\\\s*\\\\\"my_custom_kernel_name\\\\\"')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/export_pallas_test.py",
    "function": "def call(platform, x):\n    target_name = dict(cpu='lapack_sgeqrf_ffi', rocm='hipsolver_geqrf_ffi', cuda='cusolver_geqrf_ffi')[platform]\n    f = jex.ffi.ffi_call if _use_extend else jax.ffi.ffi_call\n    return f(target_name, output_types, input_output_aliases={0: 0}, input_layouts=[x_major_to_minor], output_layouts=[x_major_to_minor, None], **kwargs)(x)"
  },
  {
    "test_code": "def test_cross_platform(self):\n    if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n        self.skipTest('Requires libtpu built after 2024-12-19')\n\n    def add_vectors_kernel(x_ref, y_ref, o_ref):\n        x, y = (x_ref[...], y_ref[...])\n        o_ref[...] = x + y\n\n    @jax.jit\n    def add_vectors(x: jax.Array, y: jax.Array) -> jax.Array:\n        return pl.pallas_call(add_vectors_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), name='my_custom_kernel_name')(x, y)\n    platforms = ['tpu']\n    if True or triton.has_compilation_handler('cuda'):\n        platforms.append('cuda')\n    a = np.arange(8 * 16, dtype=np.int32).reshape((8, 16))\n    exp = export.export(add_vectors, platforms=platforms, disabled_checks=[export.DisabledSafetyCheck.custom_call('triton_kernel_call'), export.DisabledSafetyCheck.custom_call('__gpu$xla.gpu.triton')])(a, a)\n    if jtu.device_under_test() == 'tpu' or (jtu.device_under_test() == 'gpu' and jtu.is_cuda_compute_capability_at_least('8.0')):\n        res = exp.call(a, a)\n        self.assertAllClose(res, a + a)\n    if 'tpu' in platforms:\n        self.assertRegex(exp.mlir_module(), 'stablehlo.custom_call @tpu_custom_call.+kernel_name\\\\s*=\\\\s*\\\\\"my_custom_kernel_name\\\\\"')\n    if 'cuda' in platforms:\n        self.assertRegex(exp.mlir_module(), 'stablehlo.custom_call @__gpu\\\\$xla\\\\.gpu\\\\.triton.+name\\\\s*=\\\\s*\\\\\"my_custom_kernel_name\\\\\"')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/export_pallas_test.py",
    "function": "def call(f, *args):\n    f = jax.custom_jvp(f)\n    f.defjvp(lambda primals, tangents: (f(*primals), sum(tangents)))\n    return f(*args)"
  }
]