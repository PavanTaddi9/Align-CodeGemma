[
  {
    "test_code": "@parameterized.product(batch_size=(1, 4), q_seq_len=(4096,), kv_seq_len=(4096,), num_q_and_kv_heads=((4, 1), (6, 3), (4, 4)), head_dim=(64, 128, 256), attention_impl=(attention_mgpu.attention, attention_mgpu.attention_with_pipeline_emitter))\ndef test_flash_attention(self, batch_size, q_seq_len, kv_seq_len, num_q_and_kv_heads, head_dim, attention_impl):\n    num_q_heads, num_kv_heads = num_q_and_kv_heads\n    k1, k2, k3 = jax.random.split(jax.random.key(42), 3)\n    q = jax.random.normal(k1, (batch_size, q_seq_len, num_q_heads, head_dim), jnp.float16)\n    k = jax.random.normal(k2, (batch_size, kv_seq_len, num_kv_heads, head_dim), jnp.float16)\n    v = jax.random.normal(k3, (batch_size, kv_seq_len, num_kv_heads, head_dim), jnp.float16)\n    out = attention_impl(q, k, v, attention_mgpu.TuningConfig(block_q=64, block_kv=64, max_concurrent_steps=2))\n    out_ref = attention_mgpu.attention_reference(q, k, v)\n    np.testing.assert_allclose(out, out_ref, atol=0.002, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/mgpu_attention_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.product(batch_size=(1, 4), q_seq_len=(4096,), kv_seq_len=(4096,), num_q_and_kv_heads=((4, 1), (6, 3), (4, 4)), head_dim=(64, 128, 256), attention_impl=(attention_mgpu.attention, attention_mgpu.attention_with_pipeline_emitter))\ndef test_flash_attention(self, batch_size, q_seq_len, kv_seq_len, num_q_and_kv_heads, head_dim, attention_impl):\n    num_q_heads, num_kv_heads = num_q_and_kv_heads\n    k1, k2, k3 = jax.random.split(jax.random.key(42), 3)\n    q = jax.random.normal(k1, (batch_size, q_seq_len, num_q_heads, head_dim), jnp.float16)\n    k = jax.random.normal(k2, (batch_size, kv_seq_len, num_kv_heads, head_dim), jnp.float16)\n    v = jax.random.normal(k3, (batch_size, kv_seq_len, num_kv_heads, head_dim), jnp.float16)\n    out = attention_impl(q, k, v, attention_mgpu.TuningConfig(block_q=64, block_kv=64, max_concurrent_steps=2))\n    out_ref = attention_mgpu.attention_reference(q, k, v)\n    np.testing.assert_allclose(out, out_ref, atol=0.002, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/mgpu_attention_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  }
]