[
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@parameterized.named_parameters(*AD_TEST_CASES)\ndef test_jvp_slice(self, impl):\n    grad_tol = self.grad_tol\n    if jtu.test_device_matches(['tpu']) and 'tanh' in self._testMethodName:\n        grad_tol = 0.1\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx))\n    def pallas_impl(x_ref, o_ref):\n        x = x_ref[jnp.arange(2)]\n        o_ref[jnp.arange(2)] = jnp.zeros(2)\n        o_ref[2 + jnp.arange(2)] = impl(x)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (8,))\n    t = random.normal(k2, (8,))\n    out_primal, out_tangent = jax.jvp(pallas_impl, (x,), (t,))\n    out_primal_ref, out_tangent_ref = jax.jvp(lambda x: jnp.concatenate([jnp.zeros(2), impl(x[:2])]), (x,), (t,))\n    np.testing.assert_allclose(out_primal, out_primal_ref, atol=self.tol, rtol=self.tol)\n    np.testing.assert_allclose(out_tangent, out_tangent_ref, atol=self.tol, rtol=self.tol)\n    jtu.check_grads(pallas_impl, (x,), modes=['fwd'], order=2, atol=grad_tol, rtol=grad_tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "def test_can_query_named_dynamic_grid_size_in_kernel_via_psum(self):\n    self.skipTest('Not supported.')\n\n    def kernel(x_ref, y_ref):\n        self.assertEqual(lax.psum(1, 'i'), 2)\n        self.assertEqual(lax.psum(1, 'j'), 4)\n        y_ref[...] = x_ref[...]\n    x = jnp.arange(4 * 8 * 128, dtype=np.int32).reshape((4, 8, 128))\n\n    @jax.jit\n    def foo(n):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0)), grid=(('i', n),))(x)\n    y = foo(4)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def foo(x, y):\n    z = jax.vmap(jax.vmap(jnp.sin))(x) * y\n    return jax.vmap(jax.vmap(jnp.add))(x, z)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_nested_conds(self):\n\n    def kernel(y_ref):\n\n        def select(pred, x, y, nesting=0):\n\n            def _true():\n                if nesting == 0:\n                    return x + 1\n                return select(x == nesting, x, y, nesting=nesting - 1)\n\n            def _false():\n                if nesting == 0:\n                    return y + 1\n                return select(y == nesting, x, y, nesting=nesting - 1)\n            return jax.lax.cond(pred, _true, _false)\n        j = pl.program_id(0)\n        j = select(j == 0, j, j, nesting=4)\n        y_ref[...] = j * jnp.ones_like(y_ref)\n    pl.pallas_call(kernel, grid=(1,), out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))()\n    return",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_can_query_named_dynamic_grid_size_in_kernel_via_psum(self):\n    self.skipTest('Not supported.')\n\n    def kernel(x_ref, y_ref):\n        self.assertEqual(lax.psum(1, 'i'), 2)\n        self.assertEqual(lax.psum(1, 'j'), 4)\n        y_ref[...] = x_ref[...]\n    x = jnp.arange(4 * 8 * 128, dtype=np.int32).reshape((4, 8, 128))\n\n    @jax.jit\n    def foo(n):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0)), grid=(('i', n),))(x)\n    y = foo(4)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_vjp\ndef foo(x):\n    return x"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}_gm_{group_size_m}', m, n, k, dtype, block_size_m, block_size_n, block_size_k, group_size_m) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] for group_size_m in [8] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul(self, m, n, k, dtype, bm, bn, bk, gm):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul(x, y, bm=bm, bn=bn, bk=bk, gm=gm, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}', m, n, k, dtype, block_size_m, block_size_n, block_size_k) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul_block_spec(self, m, n, k, dtype, bm, bn, bk):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul_block_spec(x, y, bm=bm, bn=bn, bk=bk, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.parameters(('float32', None), ('float32', jax.lax.Precision.DEFAULT), ('float32', jax.lax.Precision.HIGH), ('float32', jax.lax.Precision.HIGHEST), ('float32', jax.lax.DotAlgorithmPreset.DEFAULT), ('float32', jax.lax.DotAlgorithmPreset.F16_F16_F32), ('float32', jax.lax.DotAlgorithmPreset.BF16_BF16_F32), ('float32', jax.lax.DotAlgorithmPreset.TF32_TF32_F32), ('float32', jax.lax.DotAlgorithmPreset.TF32_TF32_F32_X3), ('float32', jax.lax.DotAlgorithmPreset.F32_F32_F32), ('bfloat16', None), ('bfloat16', jax.lax.Precision.DEFAULT), ('bfloat16', jax.lax.Precision.HIGHEST), ('bfloat16', jax.lax.DotAlgorithmPreset.DEFAULT), ('bfloat16', jax.lax.DotAlgorithmPreset.BF16_BF16_F32))\ndef test_dot_precision(self, dtype, precision):\n    if not jtu.test_device_matches(['gpu']):\n        self.skipTest('`DotAlgorithmPreset` only supported on GPU.')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((32, 64), jnp.float32))\n    def dot_kernel(x_ref, y_ref, o_ref):\n        o_ref[()] = pl.dot(x_ref[()], y_ref[()], precision=precision)\n    key0, key1 = random.split(random.key(0))\n    x = random.normal(key0, (32, 16), dtype=dtype)\n    y = random.normal(key1, (16, 64), dtype=dtype)\n    expected = jnp.dot(x, y, precision=jax.lax.Precision.HIGHEST, preferred_element_type=jnp.float32)\n    self.assertAllClose(dot_kernel(x, y), expected, atol=0.05, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.parameters(jnp.int8, jnp.uint8)\ndef test_integer_dot(self, dtype):\n    if jtu.test_device_matches(['tpu']) and (not jtu.is_device_tpu_at_least(5)):\n        self.skipTest('`int8` dot is only supported on v5 TPUs and newer.')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((32, 64), jnp.int32))\n    def dot_kernel(x_ref, y_ref, o_ref):\n        o_ref[()] = pl.dot(x_ref[()], y_ref[()])\n    key0, key1 = random.split(random.key(0))\n    kwargs = dict(minval=jnp.iinfo(dtype).min, maxval=128, dtype=dtype)\n    x = random.randint(key0, (32, 128), **kwargs)\n    y = random.randint(key1, (128, 64), **kwargs)\n    expected = jnp.dot(x, y, preferred_element_type=jnp.int32)\n    self.assertAllClose(dot_kernel(x, y), expected, atol=0.0, rtol=0.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "def test_dot_with_vector(self):\n    if not jtu.test_device_matches(['gpu']) or self.INTERPRET:\n        self.skipTest('jnp.dot is only restricted to 2D on GPU in non-interpret mode.')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((32,), jnp.float32))\n    def dot_kernel(x_ref, y_ref, o_ref):\n        o_ref[()] = jnp.dot(x_ref[()], y_ref[()])\n    key0, key1 = random.split(random.key(0))\n    x = random.normal(key0, (32, 64), dtype=jnp.float32)\n    y = random.normal(key1, (64,), dtype=jnp.float32)\n    with self.assertRaisesRegex(Exception, 'must be 2D'):\n        dot_kernel(x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(*AD_TEST_CASES)\ndef test_jvp(self, impl):\n    grad_tol = self.grad_tol\n    if jtu.test_device_matches(['tpu']) and 'recip_exp_sq' in self._testMethodName:\n        grad_tol = 0.1\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), floatx))\n    def pallas_impl(x_ref, o_ref):\n        x = x_ref[()]\n        o_ref[()] = impl(x)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1)\n    t = random.normal(k2)\n    out_primal, out_tangent = jax.jvp(pallas_impl, (x,), (t,))\n    out_primal_ref, out_tangent_ref = jax.jvp(impl, (x,), (t,))\n    np.testing.assert_allclose(out_primal, out_primal_ref, atol=self.tol, rtol=self.tol)\n    np.testing.assert_allclose(out_tangent, out_tangent_ref, atol=self.tol, rtol=self.tol)\n    jtu.check_grads(pallas_impl, (x,), modes=['fwd'], order=2, atol=grad_tol, rtol=grad_tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(*AD_TEST_CASES)\ndef test_pallas_around_grad(self, impl):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), floatx), name=self.id().split('.')[-1])\n    def pallas_impl(x_ref, o_ref):\n        x = x_ref[()]\n        o_ref[()] = jax.grad(impl)(x)\n    x = random.normal(random.key(0))\n    out_grad = pallas_impl(x)\n    out_grad_ref = jax.grad(impl)(x)\n    np.testing.assert_allclose(out_grad, out_grad_ref, atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(*AD_TEST_CASES)\ndef test_jvp_slice(self, impl):\n    grad_tol = self.grad_tol\n    if jtu.test_device_matches(['tpu']) and 'tanh' in self._testMethodName:\n        grad_tol = 0.1\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx))\n    def pallas_impl(x_ref, o_ref):\n        x = x_ref[jnp.arange(2)]\n        o_ref[jnp.arange(2)] = jnp.zeros(2)\n        o_ref[2 + jnp.arange(2)] = impl(x)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (8,))\n    t = random.normal(k2, (8,))\n    out_primal, out_tangent = jax.jvp(pallas_impl, (x,), (t,))\n    out_primal_ref, out_tangent_ref = jax.jvp(lambda x: jnp.concatenate([jnp.zeros(2), impl(x[:2])]), (x,), (t,))\n    np.testing.assert_allclose(out_primal, out_primal_ref, atol=self.tol, rtol=self.tol)\n    np.testing.assert_allclose(out_tangent, out_tangent_ref, atol=self.tol, rtol=self.tol)\n    jtu.check_grads(pallas_impl, (x,), modes=['fwd'], order=2, atol=grad_tol, rtol=grad_tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_nested_conds(self):\n\n    def kernel(y_ref):\n\n        def select(pred, x, y, nesting=0):\n\n            def _true():\n                if nesting == 0:\n                    return x + 1\n                return select(x == nesting, x, y, nesting=nesting - 1)\n\n            def _false():\n                if nesting == 0:\n                    return y + 1\n                return select(y == nesting, x, y, nesting=nesting - 1)\n            return jax.lax.cond(pred, _true, _false)\n        j = pl.program_id(0)\n        j = select(j == 0, j, j, nesting=4)\n        y_ref[...] = j * jnp.ones_like(y_ref)\n    pl.pallas_call(kernel, grid=(1,), out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))()\n    return",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_hoisted_consts(self):\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    x = np.arange(16 * 128, dtype=np.float32).reshape((16, 128))\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\n    def kernel(src, dst):\n        dst[0:1] = to_store\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* captures constants'):\n        kernel(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_can_query_named_dynamic_grid_size_in_kernel_via_psum(self):\n    self.skipTest('Not supported.')\n\n    def kernel(x_ref, y_ref):\n        self.assertEqual(lax.psum(1, 'i'), 2)\n        self.assertEqual(lax.psum(1, 'j'), 4)\n        y_ref[...] = x_ref[...]\n    x = jnp.arange(4 * 8 * 128, dtype=np.int32).reshape((4, 8, 128))\n\n    @jax.jit\n    def foo(n):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0)), grid=(('i', n),))(x)\n    y = foo(4)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef foo(x):\n    return jnp.concatenate(x)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_nested_conds(self):\n\n    def kernel(y_ref):\n\n        def select(pred, x, y, nesting=0):\n\n            def _true():\n                if nesting == 0:\n                    return x + 1\n                return select(x == nesting, x, y, nesting=nesting - 1)\n\n            def _false():\n                if nesting == 0:\n                    return y + 1\n                return select(y == nesting, x, y, nesting=nesting - 1)\n            return jax.lax.cond(pred, _true, _false)\n        j = pl.program_id(0)\n        j = select(j == 0, j, j, nesting=4)\n        y_ref[...] = j * jnp.ones_like(y_ref)\n    pl.pallas_call(kernel, grid=(1,), out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))()\n    return",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_can_query_named_dynamic_grid_size_in_kernel_via_psum(self):\n    self.skipTest('Not supported.')\n\n    def kernel(x_ref, y_ref):\n        self.assertEqual(lax.psum(1, 'i'), 2)\n        self.assertEqual(lax.psum(1, 'j'), 4)\n        y_ref[...] = x_ref[...]\n    x = jnp.arange(4 * 8 * 128, dtype=np.int32).reshape((4, 8, 128))\n\n    @jax.jit\n    def foo(n):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0)), grid=(('i', n),))(x)\n    y = foo(4)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef foo(x):\n    return jnp.concatenate(x)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_can_query_named_dynamic_grid_size_in_kernel_via_psum(self):\n    self.skipTest('Not supported.')\n\n    def kernel(x_ref, y_ref):\n        self.assertEqual(lax.psum(1, 'i'), 2)\n        self.assertEqual(lax.psum(1, 'j'), 4)\n        y_ref[...] = x_ref[...]\n    x = jnp.arange(4 * 8 * 128, dtype=np.int32).reshape((4, 8, 128))\n\n    @jax.jit\n    def foo(n):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0)), grid=(('i', n),))(x)\n    y = foo(4)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def foo(x):\n    i = jax.lax.axis_index('x')\n    return jnp.exp(x) + i.astype(x.dtype)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_can_query_named_dynamic_grid_size_in_kernel_via_psum(self):\n    self.skipTest('Not supported.')\n\n    def kernel(x_ref, y_ref):\n        self.assertEqual(lax.psum(1, 'i'), 2)\n        self.assertEqual(lax.psum(1, 'j'), 4)\n        y_ref[...] = x_ref[...]\n    x = jnp.arange(4 * 8 * 128, dtype=np.int32).reshape((4, 8, 128))\n\n    @jax.jit\n    def foo(n):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0)), grid=(('i', n),))(x)\n    y = foo(4)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def foo(x):\n\n    def bar(y):\n        return jnp.multiply(x, y)\n    return jvp(bar, (3.0,), (1.0,))[1]"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_nested_conds(self):\n\n    def kernel(y_ref):\n\n        def select(pred, x, y, nesting=0):\n\n            def _true():\n                if nesting == 0:\n                    return x + 1\n                return select(x == nesting, x, y, nesting=nesting - 1)\n\n            def _false():\n                if nesting == 0:\n                    return y + 1\n                return select(y == nesting, x, y, nesting=nesting - 1)\n            return jax.lax.cond(pred, _true, _false)\n        j = pl.program_id(0)\n        j = select(j == 0, j, j, nesting=4)\n        y_ref[...] = j * jnp.ones_like(y_ref)\n    pl.pallas_call(kernel, grid=(1,), out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))()\n    return",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def cond(x):\n    return jax.pure_callback(_cond_callback, jax.ShapeDtypeStruct((), np.bool_), x)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def cond(x):\n    return jax.pure_callback(_cond_callback, jax.ShapeDtypeStruct((), np.bool_), x)"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def cond(x):\n    return jax.pure_callback(_cond_callback, jax.ShapeDtypeStruct((), np.bool_), x)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.named_parameters(*AD_TEST_CASES)\ndef test_jvp(self, impl):\n    grad_tol = self.grad_tol\n    if jtu.test_device_matches(['tpu']) and 'recip_exp_sq' in self._testMethodName:\n        grad_tol = 0.1\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), floatx))\n    def pallas_impl(x_ref, o_ref):\n        x = x_ref[()]\n        o_ref[()] = impl(x)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1)\n    t = random.normal(k2)\n    out_primal, out_tangent = jax.jvp(pallas_impl, (x,), (t,))\n    out_primal_ref, out_tangent_ref = jax.jvp(impl, (x,), (t,))\n    np.testing.assert_allclose(out_primal, out_primal_ref, atol=self.tol, rtol=self.tol)\n    np.testing.assert_allclose(out_tangent, out_tangent_ref, atol=self.tol, rtol=self.tol)\n    jtu.check_grads(pallas_impl, (x,), modes=['fwd'], order=2, atol=grad_tol, rtol=grad_tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@parameterized.named_parameters(*AD_TEST_CASES)\ndef test_jvp_slice(self, impl):\n    grad_tol = self.grad_tol\n    if jtu.test_device_matches(['tpu']) and 'tanh' in self._testMethodName:\n        grad_tol = 0.1\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx))\n    def pallas_impl(x_ref, o_ref):\n        x = x_ref[jnp.arange(2)]\n        o_ref[jnp.arange(2)] = jnp.zeros(2)\n        o_ref[2 + jnp.arange(2)] = impl(x)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (8,))\n    t = random.normal(k2, (8,))\n    out_primal, out_tangent = jax.jvp(pallas_impl, (x,), (t,))\n    out_primal_ref, out_tangent_ref = jax.jvp(lambda x: jnp.concatenate([jnp.zeros(2), impl(x[:2])]), (x,), (t,))\n    np.testing.assert_allclose(out_primal, out_primal_ref, atol=self.tol, rtol=self.tol)\n    np.testing.assert_allclose(out_tangent, out_tangent_ref, atol=self.tol, rtol=self.tol)\n    jtu.check_grads(pallas_impl, (x,), modes=['fwd'], order=2, atol=grad_tol, rtol=grad_tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def fn(data, segment_ids):\n    return jax.ops.segment_sum(data, segment_ids, num_segments).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def fn(data, segment_ids):\n    return jax.ops.segment_sum(data, segment_ids, num_segments).sum()"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def fn(data, segment_ids):\n    return jax.ops.segment_sum(data, segment_ids, num_segments).sum()"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.named_parameters(*AD_TEST_CASES)\ndef test_jvp(self, impl):\n    grad_tol = self.grad_tol\n    if jtu.test_device_matches(['tpu']) and 'recip_exp_sq' in self._testMethodName:\n        grad_tol = 0.1\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), floatx))\n    def pallas_impl(x_ref, o_ref):\n        x = x_ref[()]\n        o_ref[()] = impl(x)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1)\n    t = random.normal(k2)\n    out_primal, out_tangent = jax.jvp(pallas_impl, (x,), (t,))\n    out_primal_ref, out_tangent_ref = jax.jvp(impl, (x,), (t,))\n    np.testing.assert_allclose(out_primal, out_primal_ref, atol=self.tol, rtol=self.tol)\n    np.testing.assert_allclose(out_tangent, out_tangent_ref, atol=self.tol, rtol=self.tol)\n    jtu.check_grads(pallas_impl, (x,), modes=['fwd'], order=2, atol=grad_tol, rtol=grad_tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def impl(x):\n    return spec.call((x, jnp.zeros_like(x)))[1]"
  },
  {
    "test_code": "@parameterized.named_parameters(*AD_TEST_CASES)\ndef test_jvp_slice(self, impl):\n    grad_tol = self.grad_tol\n    if jtu.test_device_matches(['tpu']) and 'tanh' in self._testMethodName:\n        grad_tol = 0.1\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx))\n    def pallas_impl(x_ref, o_ref):\n        x = x_ref[jnp.arange(2)]\n        o_ref[jnp.arange(2)] = jnp.zeros(2)\n        o_ref[2 + jnp.arange(2)] = impl(x)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (8,))\n    t = random.normal(k2, (8,))\n    out_primal, out_tangent = jax.jvp(pallas_impl, (x,), (t,))\n    out_primal_ref, out_tangent_ref = jax.jvp(lambda x: jnp.concatenate([jnp.zeros(2), impl(x[:2])]), (x,), (t,))\n    np.testing.assert_allclose(out_primal, out_primal_ref, atol=self.tol, rtol=self.tol)\n    np.testing.assert_allclose(out_tangent, out_tangent_ref, atol=self.tol, rtol=self.tol)\n    jtu.check_grads(pallas_impl, (x,), modes=['fwd'], order=2, atol=grad_tol, rtol=grad_tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def impl(x):\n    return spec.call((x, jnp.zeros_like(x)))[1]"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_nested_conds(self):\n\n    def kernel(y_ref):\n\n        def select(pred, x, y, nesting=0):\n\n            def _true():\n                if nesting == 0:\n                    return x + 1\n                return select(x == nesting, x, y, nesting=nesting - 1)\n\n            def _false():\n                if nesting == 0:\n                    return y + 1\n                return select(y == nesting, x, y, nesting=nesting - 1)\n            return jax.lax.cond(pred, _true, _false)\n        j = pl.program_id(0)\n        j = select(j == 0, j, j, nesting=4)\n        y_ref[...] = j * jnp.ones_like(y_ref)\n    pl.pallas_call(kernel, grid=(1,), out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))()\n    return",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def cond(state):\n    idx, x, _ = state\n    chunk = jax.lax.dynamic_slice_in_dim(x, idx * chunk_size, chunk_size)\n    return (idx * chunk_size < x.shape[0]) & jnp.any(chunk > 0)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def cond(state):\n    idx, x, _ = state\n    chunk = jax.lax.dynamic_slice_in_dim(x, idx * chunk_size, chunk_size)\n    return (idx * chunk_size < x.shape[0]) & jnp.any(chunk > 0)"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def cond(state):\n    idx, x, _ = state\n    chunk = jax.lax.dynamic_slice_in_dim(x, idx * chunk_size, chunk_size)\n    return (idx * chunk_size < x.shape[0]) & jnp.any(chunk > 0)"
  },
  {
    "test_code": "def test_can_query_named_dynamic_grid_size_in_kernel_via_psum(self):\n    self.skipTest('Not supported.')\n\n    def kernel(x_ref, y_ref):\n        self.assertEqual(lax.psum(1, 'i'), 2)\n        self.assertEqual(lax.psum(1, 'j'), 4)\n        y_ref[...] = x_ref[...]\n    x = jnp.arange(4 * 8 * 128, dtype=np.int32).reshape((4, 8, 128))\n\n    @jax.jit\n    def foo(n):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0)), grid=(('i', n),))(x)\n    y = foo(4)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def foo(x):\n    output = jnp.zeros_like(x, device=cpu_sharding)\n    _, _, cpu_x = jax.lax.while_loop(cond, inner, (0, x, output))\n    return cpu_x"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@compute_on('device_host')\ndef fn():\n    k = jax.random.key(0)\n    return jax.nn.initializers.lecun_normal()(k, (2, 2), jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@compute_on('device_host')\ndef fn():\n    k = jax.random.key(0)\n    return jax.nn.initializers.lecun_normal()(k, (2, 2), jnp.float32)"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@compute_on('device_host')\ndef fn():\n    k = jax.random.key(0)\n    return jax.nn.initializers.lecun_normal()(k, (2, 2), jnp.float32)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_triton_params_consistent_across_double_jit(self):\n    if not jtu.test_device_matches(['gpu']):\n        self.skipTest('Triton backend only works on GPU.')\n    params = plgpu.TritonCompilerParams(num_warps=8)\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32), compiler_params=params)\n    def copy_kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n\n    @functools.partial(jax.jit, static_argnames=['z'])\n    def plus_z(x, z):\n        return copy_kernel(x + z)\n    x = 0.0\n    extracted_params = _find_pallas_call_in_jaxpr(plus_z.trace(x, 1).jaxpr).params['compiler_params']\n    self.assertEqual(plus_z(0.0, 1.0), 1.0)\n    self.assertEqual(extracted_params['triton']['num_warps'], 8)\n    extracted_params = _find_pallas_call_in_jaxpr(plus_z.trace(x, 2).jaxpr).params['compiler_params']\n    self.assertEqual(plus_z(0.0, 2.0), 2.0)\n    self.assertEqual(extracted_params['triton']['num_warps'], 8)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def trace(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).trace(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def fn(indices):\n    return jnp.equal(indices, jnp.arange(3)).astype(jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def fn(indices):\n    return jnp.equal(indices, jnp.arange(3)).astype(jnp.float32)"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def fn(indices):\n    return jnp.equal(indices, jnp.arange(3)).astype(jnp.float32)"
  },
  {
    "test_code": "def test_can_query_named_dynamic_grid_size_in_kernel_via_psum(self):\n    self.skipTest('Not supported.')\n\n    def kernel(x_ref, y_ref):\n        self.assertEqual(lax.psum(1, 'i'), 2)\n        self.assertEqual(lax.psum(1, 'j'), 4)\n        y_ref[...] = x_ref[...]\n    x = jnp.arange(4 * 8 * 128, dtype=np.int32).reshape((4, 8, 128))\n\n    @jax.jit\n    def foo(n):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0)), grid=(('i', n),))(x)\n    y = foo(4)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_vjp\ndef foo(x):\n    return jnp.exp(x)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef fn(key):\n    x = jnp.arange(113003)\n    x = with_sharding_constraint(x, P('data'))\n    y = jnp.arange(65536)\n    y = with_sharding_constraint(y.reshape(-1), P('data'))\n    z = jnp.concatenate([x, y], axis=0)\n    z = with_sharding_constraint(z, P('data'))\n    return (x, y, z)"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef fn(key):\n    x = jnp.arange(113003)\n    x = with_sharding_constraint(x, P('data'))\n    y = jnp.arange(65536)\n    y = with_sharding_constraint(y.reshape(-1), P('data'))\n    z = jnp.concatenate([x, y], axis=0)\n    z = with_sharding_constraint(z, P('data'))\n    return (x, y, z)"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef fn(key):\n    x = jnp.arange(113003)\n    x = with_sharding_constraint(x, P('data'))\n    y = jnp.arange(65536)\n    y = with_sharding_constraint(y.reshape(-1), P('data'))\n    z = jnp.concatenate([x, y], axis=0)\n    z = with_sharding_constraint(z, P('data'))\n    return (x, y, z)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_add_one(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), floatx))\n    def add_one(x_ref, o_ref):\n        o_ref[()] = x_ref[()] + 1.0\n    x = 0.0\n    self.assertEqual(add_one(x), 1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef add_one(x):\n    return jax.tree.map(lambda x: x + 1, x)"
  },
  {
    "test_code": "def test_add_singleton_vector(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((1,), jnp.float32))\n    def add_one(x_ref, o_ref):\n        o_ref[0] = x_ref[0] + 1.0\n    x = jnp.array([0.0], jnp.float32)\n    np.testing.assert_allclose(add_one(x), jnp.array([1.0], jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef add_one(x):\n    return jax.tree.map(lambda x: x + 1, x)"
  },
  {
    "test_code": "def test_add_vector_block_spec(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), intx), in_specs=[pl.BlockSpec((1,), lambda i: i)], out_specs=pl.BlockSpec((1,), lambda i: i), grid=8)\n    def add_one(x_ref, o_ref):\n        o_ref[0] = x_ref[0] + 1\n    np.testing.assert_allclose(add_one(jnp.arange(8)), jnp.arange(8) + 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef add_one(x):\n    return jax.tree.map(lambda x: x + 1, x)"
  },
  {
    "test_code": "def test_add_matrix_block_spec(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), intx), in_specs=[pl.BlockSpec((2, 2), lambda i, j: (i, j))], out_specs=pl.BlockSpec((2, 2), lambda i, j: (i, j)), grid=(4, 4))\n    def add_one(x_ref, o_ref):\n        o_ref[:, :] = x_ref[:, :] + 1\n    x = jnp.arange(64).reshape((8, 8))\n    np.testing.assert_allclose(add_one(x), x + 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef add_one(x):\n    return jax.tree.map(lambda x: x + 1, x)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef add_one(x):\n    return jax.tree.map(lambda x: x + 1, x)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\ndef fn(x):\n    R1 = jnp.array([[x[0], 0, 0], [0, x[0], 0], [0, 0, x[0]]])\n    R2 = jnp.array([[x[0], 0, 0], [0, x[1], 0], [0, 0, x[2]]])\n    H = jnp.eye(4)\n    H = H.at[:3, :3].set(R2.T)\n    pos = H @ jnp.concatenate([x, jnp.array([1.0])])\n    return (pos, R1)"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\ndef fn(x):\n    R1 = jnp.array([[x[0], 0, 0], [0, x[0], 0], [0, 0, x[0]]])\n    R2 = jnp.array([[x[0], 0, 0], [0, x[1], 0], [0, 0, x[2]]])\n    H = jnp.eye(4)\n    H = H.at[:3, :3].set(R2.T)\n    pos = H @ jnp.concatenate([x, jnp.array([1.0])])\n    return (pos, R1)"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\ndef fn(x):\n    R1 = jnp.array([[x[0], 0, 0], [0, x[0], 0], [0, 0, x[0]]])\n    R2 = jnp.array([[x[0], 0, 0], [0, x[1], 0], [0, 0, x[2]]])\n    H = jnp.eye(4)\n    H = H.at[:3, :3].set(R2.T)\n    pos = H @ jnp.concatenate([x, jnp.array([1.0])])\n    return (pos, R1)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def fn(x: jax.Array):\n    checkify.check(jnp.all(x > 0), 'x must be positive')\n    return x + 1"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def fn(x: jax.Array):\n    checkify.check(jnp.all(x > 0), 'x must be positive')\n    return x + 1"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def fn(x: jax.Array):\n    checkify.check(jnp.all(x > 0), 'x must be positive')\n    return x + 1"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef fn(x):\n    return x * x"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef fn(x):\n    return x * x"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef fn(x):\n    return x * x"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@custom_transpose(jnp.ones(2))\ndef fn(r, x):\n    return x / r"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@custom_transpose(jnp.ones(2))\ndef fn(r, x):\n    return x / r"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@custom_transpose(jnp.ones(2))\ndef fn(r, x):\n    return x / r"
  },
  {
    "test_code": "def test_can_query_named_dynamic_grid_size_in_kernel_via_psum(self):\n    self.skipTest('Not supported.')\n\n    def kernel(x_ref, y_ref):\n        self.assertEqual(lax.psum(1, 'i'), 2)\n        self.assertEqual(lax.psum(1, 'j'), 4)\n        y_ref[...] = x_ref[...]\n    x = jnp.arange(4 * 8 * 128, dtype=np.int32).reshape((4, 8, 128))\n\n    @jax.jit\n    def foo(n):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0)), grid=(('i', n),))(x)\n    y = foo(4)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_vjp\ndef foo(x, y):\n    return x"
  },
  {
    "test_code": "def test_add_one(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), floatx))\n    def add_one(x_ref, o_ref):\n        o_ref[()] = x_ref[()] + 1.0\n    x = 0.0\n    self.assertEqual(add_one(x), 1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef add_one(x: int) -> int:\n    return x + 1"
  },
  {
    "test_code": "def test_add_singleton_vector(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((1,), jnp.float32))\n    def add_one(x_ref, o_ref):\n        o_ref[0] = x_ref[0] + 1.0\n    x = jnp.array([0.0], jnp.float32)\n    np.testing.assert_allclose(add_one(x), jnp.array([1.0], jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef add_one(x: int) -> int:\n    return x + 1"
  },
  {
    "test_code": "def test_add_vector_block_spec(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), intx), in_specs=[pl.BlockSpec((1,), lambda i: i)], out_specs=pl.BlockSpec((1,), lambda i: i), grid=8)\n    def add_one(x_ref, o_ref):\n        o_ref[0] = x_ref[0] + 1\n    np.testing.assert_allclose(add_one(jnp.arange(8)), jnp.arange(8) + 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef add_one(x: int) -> int:\n    return x + 1"
  },
  {
    "test_code": "def test_add_matrix_block_spec(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), intx), in_specs=[pl.BlockSpec((2, 2), lambda i, j: (i, j))], out_specs=pl.BlockSpec((2, 2), lambda i, j: (i, j)), grid=(4, 4))\n    def add_one(x_ref, o_ref):\n        o_ref[:, :] = x_ref[:, :] + 1\n    x = jnp.arange(64).reshape((8, 8))\n    np.testing.assert_allclose(add_one(x), x + 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef add_one(x: int) -> int:\n    return x + 1"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef add_one(x: int) -> int:\n    return x + 1"
  },
  {
    "test_code": "@parameterized.named_parameters(*(dict(testcase_name=f'{batch_size}_{size}_{block_size}_{dtype}', batch_size=batch_size, size=size, block_size=block_size, dtype=dtype) for batch_size in [1, 2, 4, 23] for size in [1, 2, 129, 255, 256] for block_size in [1, 2, 32, 64, 128, 256] for dtype in ['float32'] if size < block_size))\ndef test_softmax(self, batch_size, size, block_size, dtype):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((batch_size, size), dtype), grid=batch_size)\n    def softmax(x_ref, o_ref):\n        row_idx = pl.program_id(0)\n        x_idx = jnp.arange(block_size)\n        row_idxs = (row_idx, x_idx)\n        mask = x_idx < x_ref.shape[1]\n        row = pl.load(x_ref, row_idxs, mask=mask, other=-float('inf'))\n        row_minus_max = row - jnp.max(row, axis=0)\n        numerator = jnp.exp(row_minus_max)\n        denominator = jnp.sum(numerator, axis=0)\n        softmax_output = numerator / denominator\n        pl.store(o_ref, row_idxs, softmax_output, mask=mask)\n    key = random.key(0)\n    x = random.normal(key, [batch_size, size], dtype=dtype)\n    np.testing.assert_allclose(softmax(x), jax.nn.softmax(x, axis=-1), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_range_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which can reduce to a fori_loop.\"\"\"\n\n    def kernel(x_ref, r_ref):\n\n        @pl.when(pl.program_id(0) == 0)\n        def _():\n            pl.store(r_ref, (0, 0), 0)\n\n        def cond(carry):\n            i, j = carry\n            return i < j\n\n        def body(carry):\n            io, j = carry\n            i = io - 128\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            v = x_ref[0, sl, l]\n            s = pl.load(r_ref, (0, 0))\n            pl.store(r_ref, (0, 0), s + v)\n            return (io + 1, j)\n        i = 128\n        j = 128 + 1024\n        i, j = jax.lax.while_loop(cond, body, (i, j))\n    x = jnp.arange(4096)\n    x = jnp.reshape(x, [4, 8, 128])\n    r = pl.pallas_call(kernel, grid=(1,), out_specs=pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct([1, 1], intx), in_specs=[pl.BlockSpec((1, 8, 128), lambda i: (i, 0, 0), memory_space=smem_on_tpu())])(x)\n    expected = jnp.sum(jnp.arange(1024))\n    np.testing.assert_array_equal(r, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_custom_jvp_call(self):\n\n    @functools.partial(jax.custom_jvp, nondiff_argnums=(1,))\n    def softmax(x, axis=-1):\n        unnormalized = jnp.exp(x - jnp.max(x, axis, keepdims=True))\n        return unnormalized / jnp.sum(unnormalized, axis, keepdims=True)\n\n    @softmax.defjvp\n    def softmax_jvp(axis, primals, tangents):\n        (x,), (x_dot,) = (primals, tangents)\n        y = softmax(x, axis)\n        return (y, y * (x_dot - (y * x_dot).sum(axis, keepdims=True)))\n    m, n = (16, 32)\n    x = random.normal(random.key(0), (m, n))\n\n    @functools.partial(self.pallas_call, out_shape=x)\n    def softmax_kernel(x_ref, y_ref):\n        y_ref[:] = softmax(x_ref[:])\n    np.testing.assert_allclose(softmax_kernel(x), jax.nn.softmax(x), atol=1e-07)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@parameterized.parameters(('float32', None), ('float32', jax.lax.Precision.DEFAULT), ('float32', jax.lax.Precision.HIGH), ('float32', jax.lax.Precision.HIGHEST), ('float32', jax.lax.DotAlgorithmPreset.DEFAULT), ('float32', jax.lax.DotAlgorithmPreset.F16_F16_F32), ('float32', jax.lax.DotAlgorithmPreset.BF16_BF16_F32), ('float32', jax.lax.DotAlgorithmPreset.TF32_TF32_F32), ('float32', jax.lax.DotAlgorithmPreset.TF32_TF32_F32_X3), ('float32', jax.lax.DotAlgorithmPreset.F32_F32_F32), ('bfloat16', None), ('bfloat16', jax.lax.Precision.DEFAULT), ('bfloat16', jax.lax.Precision.HIGHEST), ('bfloat16', jax.lax.DotAlgorithmPreset.DEFAULT), ('bfloat16', jax.lax.DotAlgorithmPreset.BF16_BF16_F32))\ndef test_dot_precision(self, dtype, precision):\n    if not jtu.test_device_matches(['gpu']):\n        self.skipTest('`DotAlgorithmPreset` only supported on GPU.')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((32, 64), jnp.float32))\n    def dot_kernel(x_ref, y_ref, o_ref):\n        o_ref[()] = pl.dot(x_ref[()], y_ref[()], precision=precision)\n    key0, key1 = random.split(random.key(0))\n    x = random.normal(key0, (32, 16), dtype=dtype)\n    y = random.normal(key1, (16, 64), dtype=dtype)\n    expected = jnp.dot(x, y, precision=jax.lax.Precision.HIGHEST, preferred_element_type=jnp.float32)\n    self.assertAllClose(dot_kernel(x, y), expected, atol=0.05, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.int8, jnp.uint8)\ndef test_integer_dot(self, dtype):\n    if jtu.test_device_matches(['tpu']) and (not jtu.is_device_tpu_at_least(5)):\n        self.skipTest('`int8` dot is only supported on v5 TPUs and newer.')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((32, 64), jnp.int32))\n    def dot_kernel(x_ref, y_ref, o_ref):\n        o_ref[()] = pl.dot(x_ref[()], y_ref[()])\n    key0, key1 = random.split(random.key(0))\n    kwargs = dict(minval=jnp.iinfo(dtype).min, maxval=128, dtype=dtype)\n    x = random.randint(key0, (32, 128), **kwargs)\n    y = random.randint(key1, (128, 64), **kwargs)\n    expected = jnp.dot(x, y, preferred_element_type=jnp.int32)\n    self.assertAllClose(dot_kernel(x, y), expected, atol=0.0, rtol=0.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "def test_dot_with_vector(self):\n    if not jtu.test_device_matches(['gpu']) or self.INTERPRET:\n        self.skipTest('jnp.dot is only restricted to 2D on GPU in non-interpret mode.')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((32,), jnp.float32))\n    def dot_kernel(x_ref, y_ref, o_ref):\n        o_ref[()] = jnp.dot(x_ref[()], y_ref[()])\n    key0, key1 = random.split(random.key(0))\n    x = random.normal(key0, (32, 64), dtype=jnp.float32)\n    y = random.normal(key1, (64,), dtype=jnp.float32)\n    with self.assertRaisesRegex(Exception, 'must be 2D'):\n        dot_kernel(x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def fn(*args, split_transpose=False):\n    v, fn_transpose = jax.vjp(partial(loss, split_transpose=split_transpose), *args)\n    grads = fn_transpose(1.0)\n    return (*grads, v)"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def fn(*args, split_transpose=False):\n    v, fn_transpose = jax.vjp(partial(loss, split_transpose=split_transpose), *args)\n    grads = fn_transpose(1.0)\n    return (*grads, v)"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def fn(*args, split_transpose=False):\n    v, fn_transpose = jax.vjp(partial(loss, split_transpose=split_transpose), *args)\n    grads = fn_transpose(1.0)\n    return (*grads, v)"
  },
  {
    "test_code": "def test_vector_carry_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which carries a vector quantity.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: slice not implemented on GPU')\n\n    def kernel(x_ref, r_ref):\n\n        def cond(v):\n            return v[0, 0] < 16\n\n        def body(v):\n            return v * 2\n        r_ref[:] = jax.lax.while_loop(cond, body, x_ref[:])\n    x = jnp.full((8, 128), 3, dtype=jnp.int32)\n    fn = pl.pallas_call(kernel, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0))], out_specs=pl.BlockSpec((8, 128), lambda i: (0, 0)), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.int32))\n    r = fn(x)\n    reduced = jnp.sum(r)\n    np.testing.assert_array_equal(reduced, 1024 * 24)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef fn(x: int, static_y: BlackBox):\n    nonlocal num_called\n    num_called += 1\n    return x + static_y.value"
  },
  {
    "test_code": "@parameterized.named_parameters(('1x128', (1, 128)), ('2x128', (2, 128)), ('4x128', (4, 128)), ('8x128', (8, 128)), ('8x256', (8, 256)))\ndef test_while_loop_carry_memref(self, shape):\n    \"\"\"Tests a while loop carrying a memref.\"\"\"\n    if shape == (1, 128):\n        self.skipTest('memref<1x128> inexplicably doubles to 2x128.')\n\n    def kernel(out_ref, bound):\n\n        def cond(i):\n            return i < bound\n\n        def body(i):\n            out_ref[0, i] = 2\n            return i + 1\n        jax.lax.while_loop(cond, body, 0)\n    x = jnp.asarray([1, 1, 1, 1])\n    x = jnp.asarray(x)\n    x = jnp.pad(x, (0, np.prod(shape) - 4), constant_values=0)\n    x = jnp.reshape(x, shape)\n    kernel = functools.partial(kernel, bound=x.shape[1])\n    fn = pl.pallas_call(kernel, grid=(1,), out_specs=[pl.BlockSpec(shape, lambda i: (0, 0), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct(shape, jnp.int32)])\n    y = fn()[0]\n    np.testing.assert_array_equal(y[0, 0], 2)\n    np.testing.assert_array_equal(y[0, 1], 2)\n    np.testing.assert_array_equal(y[0, 2], 2)\n    np.testing.assert_array_equal(y[0, 3], 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef fn(x: int, static_y: BlackBox):\n    nonlocal num_called\n    num_called += 1\n    return x + static_y.value"
  },
  {
    "test_code": "def test_nested_while_loop(self):\n    \"\"\"Tests lowering a nested while_loop.\"\"\"\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: assertion error on GPU')\n\n    def kernel(in_key_ref, out_segment_count, out_size_ref, key_count):\n\n        def inner_cond(carry):\n            i, prev_key = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = jax.lax.cond(i < key_count, lambda i: in_key_ref[sl, l], lambda i: -1, i)\n            return jnp.logical_and(i < key_count, key == prev_key)\n\n        def inner_body(carry):\n            i, key = carry\n            return (i + 1, key)\n\n        def outer_cond(carry):\n            i, _ = carry\n            return i < key_count\n\n        def outer_body(carry):\n            i, next_out_idx = carry\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            key = in_key_ref[sl, l]\n            end, _ = jax.lax.while_loop(inner_cond, inner_body, (i + 1, key))\n            sl = jax.lax.div(next_out_idx, 128)\n            l = jax.lax.rem(next_out_idx, 128)\n            out_size_ref[sl, l] = end - i\n            return (end, next_out_idx + 1)\n        _, count = jax.lax.while_loop(outer_cond, outer_body, (0, 0))\n        out_segment_count[0, 0] = count\n    keys = [4, 4, 4, 3, 2, 2, 7, 7, 7, 7]\n    keys = jnp.asarray(keys)\n    real_keys = keys.shape[0]\n    key_count = 1024\n    keys = jnp.pad(keys, (0, key_count - real_keys), constant_values=32768)\n    keys = jnp.reshape(keys, (8, 128))\n    kernel_fn = functools.partial(kernel, key_count=key_count)\n    fn = pl.pallas_call(kernel_fn, grid=(1,), in_specs=[pl.BlockSpec((8, 128), lambda i: (0, 0), memory_space=smem_on_tpu())], out_specs=[pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), pl.BlockSpec((8, 128), memory_space=smem_on_tpu())], out_shape=[jax.ShapeDtypeStruct((1, 1), jnp.int32), jax.ShapeDtypeStruct((8, 128), jnp.int32)])\n    count, sizes = fn(keys)\n    np.testing.assert_equal(count[0, 0], jnp.asarray(5))\n    np.testing.assert_equal(sizes[0, 0], jnp.asarray(3))\n    np.testing.assert_equal(sizes[0, 1], jnp.asarray(1))\n    np.testing.assert_equal(sizes[0, 2], jnp.asarray(2))\n    np.testing.assert_equal(sizes[0, 3], jnp.asarray(4))\n    np.testing.assert_equal(sizes[0, 4], jnp.asarray(key_count - real_keys))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef fn(x: int, static_y: BlackBox):\n    nonlocal num_called\n    num_called += 1\n    return x + static_y.value"
  },
  {
    "test_code": "def test_can_query_named_dynamic_grid_size_in_kernel_via_psum(self):\n    self.skipTest('Not supported.')\n\n    def kernel(x_ref, y_ref):\n        self.assertEqual(lax.psum(1, 'i'), 2)\n        self.assertEqual(lax.psum(1, 'j'), 4)\n        y_ref[...] = x_ref[...]\n    x = jnp.arange(4 * 8 * 128, dtype=np.int32).reshape((4, 8, 128))\n\n    @jax.jit\n    def foo(n):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0)), grid=(('i', n),))(x)\n    y = foo(4)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef foo(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_hoisted_consts(self):\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    x = np.arange(16 * 128, dtype=np.float32).reshape((16, 128))\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\n    def kernel(src, dst):\n        dst[0:1] = to_store\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* captures constants'):\n        kernel(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "def test_can_query_named_dynamic_grid_size_in_kernel_via_psum(self):\n    self.skipTest('Not supported.')\n\n    def kernel(x_ref, y_ref):\n        self.assertEqual(lax.psum(1, 'i'), 2)\n        self.assertEqual(lax.psum(1, 'j'), 4)\n        y_ref[...] = x_ref[...]\n    x = jnp.arange(4 * 8 * 128, dtype=np.int32).reshape((4, 8, 128))\n\n    @jax.jit\n    def foo(n):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, 8, 128), lambda i: (i, 0, 0)), grid=(('i', n),))(x)\n    y = foo(4)\n    np.testing.assert_array_equal(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@print_ir(np.empty([7], np.int32))\n@jax.jit\ndef foo(x):\n    return x + 2"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_hoisted_consts(self):\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    x = np.arange(16 * 128, dtype=np.float32).reshape((16, 128))\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\n    def kernel(src, dst):\n        dst[0:1] = to_store\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* captures constants'):\n        kernel(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}_gm_{group_size_m}', m, n, k, dtype, block_size_m, block_size_n, block_size_k, group_size_m) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] for group_size_m in [8] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul(self, m, n, k, dtype, bm, bn, bk, gm):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul(x, y, bm=bm, bn=bn, bk=bk, gm=gm, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def matmul(impl, x, y):\n    z = impl(x, y)\n    return jnp.exp(jnp.tanh(z)).astype(x.dtype)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}', m, n, k, dtype, block_size_m, block_size_n, block_size_k) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul_block_spec(self, m, n, k, dtype, bm, bn, bk):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul_block_spec(x, y, bm=bm, bn=bn, bk=bk, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def matmul(impl, x, y):\n    z = impl(x, y)\n    return jnp.exp(jnp.tanh(z)).astype(x.dtype)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}_gm_{group_size_m}', m, n, k, dtype, block_size_m, block_size_n, block_size_k, group_size_m) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] for group_size_m in [8] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul(self, m, n, k, dtype, bm, bn, bk, gm):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul(x, y, bm=bm, bn=bn, bk=bk, gm=gm, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef matmul(x: jax.Array, y: jax.Array):\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype), grid=(2, 2), in_specs=[pl.BlockSpec((x.shape[0] // 2, x.shape[1]), lambda i, j: (i, 0)), pl.BlockSpec((y.shape[0], y.shape[1] // 2), lambda i, j: (0, j))], out_specs=pl.BlockSpec((x.shape[0] // 2, y.shape[1] // 2), lambda i, j: (i, j)), interpret=mosaic_interpret.TPUInterpretParams())(x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}', m, n, k, dtype, block_size_m, block_size_n, block_size_k) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul_block_spec(self, m, n, k, dtype, bm, bn, bk):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul_block_spec(x, y, bm=bm, bn=bn, bk=bk, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef matmul(x: jax.Array, y: jax.Array):\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype), grid=(2, 2), in_specs=[pl.BlockSpec((x.shape[0] // 2, x.shape[1]), lambda i, j: (i, 0)), pl.BlockSpec((y.shape[0], y.shape[1] // 2), lambda i, j: (0, j))], out_specs=pl.BlockSpec((x.shape[0] // 2, y.shape[1] // 2), lambda i, j: (i, j)), interpret=mosaic_interpret.TPUInterpretParams())(x, y)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_block_spec_mapped_dimension(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float32), in_specs=[pl.BlockSpec((None, 4), lambda _: (0, 0)), pl.BlockSpec((None, 4), lambda _: (1, 0))], grid=1)\n    def add_vectors(x_ref, y_ref, o_ref):\n        o_ref[:] = x_ref[:] + y_ref[:]\n    xy = jnp.arange(8.0, dtype=np.float32).reshape((2, 4))\n    out = add_vectors(xy, xy)\n    out_ref = xy[0] + xy[1]\n    np.testing.assert_allclose(out, out_ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef add_vectors(x: jax.Array, y: jax.Array) -> jax.Array:\n    return pl.pallas_call(add_vectors_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), name='my_custom_kernel_name')(x, y)"
  },
  {
    "test_code": "@parameterized.parameters(('float32', None), ('float32', jax.lax.Precision.DEFAULT), ('float32', jax.lax.Precision.HIGH), ('float32', jax.lax.Precision.HIGHEST), ('float32', jax.lax.DotAlgorithmPreset.DEFAULT), ('float32', jax.lax.DotAlgorithmPreset.F16_F16_F32), ('float32', jax.lax.DotAlgorithmPreset.BF16_BF16_F32), ('float32', jax.lax.DotAlgorithmPreset.TF32_TF32_F32), ('float32', jax.lax.DotAlgorithmPreset.TF32_TF32_F32_X3), ('float32', jax.lax.DotAlgorithmPreset.F32_F32_F32), ('bfloat16', None), ('bfloat16', jax.lax.Precision.DEFAULT), ('bfloat16', jax.lax.Precision.HIGHEST), ('bfloat16', jax.lax.DotAlgorithmPreset.DEFAULT), ('bfloat16', jax.lax.DotAlgorithmPreset.BF16_BF16_F32))\ndef test_dot_precision(self, dtype, precision):\n    if not jtu.test_device_matches(['gpu']):\n        self.skipTest('`DotAlgorithmPreset` only supported on GPU.')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((32, 64), jnp.float32))\n    def dot_kernel(x_ref, y_ref, o_ref):\n        o_ref[()] = pl.dot(x_ref[()], y_ref[()], precision=precision)\n    key0, key1 = random.split(random.key(0))\n    x = random.normal(key0, (32, 16), dtype=dtype)\n    y = random.normal(key1, (16, 64), dtype=dtype)\n    expected = jnp.dot(x, y, precision=jax.lax.Precision.HIGHEST, preferred_element_type=jnp.float32)\n    self.assertAllClose(dot_kernel(x, y), expected, atol=0.05, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.int8, jnp.uint8)\ndef test_integer_dot(self, dtype):\n    if jtu.test_device_matches(['tpu']) and (not jtu.is_device_tpu_at_least(5)):\n        self.skipTest('`int8` dot is only supported on v5 TPUs and newer.')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((32, 64), jnp.int32))\n    def dot_kernel(x_ref, y_ref, o_ref):\n        o_ref[()] = pl.dot(x_ref[()], y_ref[()])\n    key0, key1 = random.split(random.key(0))\n    kwargs = dict(minval=jnp.iinfo(dtype).min, maxval=128, dtype=dtype)\n    x = random.randint(key0, (32, 128), **kwargs)\n    y = random.randint(key1, (128, 64), **kwargs)\n    expected = jnp.dot(x, y, preferred_element_type=jnp.int32)\n    self.assertAllClose(dot_kernel(x, y), expected, atol=0.0, rtol=0.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "def test_dot_with_vector(self):\n    if not jtu.test_device_matches(['gpu']) or self.INTERPRET:\n        self.skipTest('jnp.dot is only restricted to 2D on GPU in non-interpret mode.')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((32,), jnp.float32))\n    def dot_kernel(x_ref, y_ref, o_ref):\n        o_ref[()] = jnp.dot(x_ref[()], y_ref[()])\n    key0, key1 = random.split(random.key(0))\n    x = random.normal(key0, (32, 64), dtype=jnp.float32)\n    y = random.normal(key1, (64,), dtype=jnp.float32)\n    with self.assertRaisesRegex(Exception, 'must be 2D'):\n        dot_kernel(x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "def test_add_singleton_vector(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((1,), jnp.float32))\n    def add_one(x_ref, o_ref):\n        o_ref[0] = x_ref[0] + 1.0\n    x = jnp.array([0.0], jnp.float32)\n    np.testing.assert_allclose(add_one(x), jnp.array([1.0], jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_add_vector_block_spec(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), intx), in_specs=[pl.BlockSpec((1,), lambda i: i)], out_specs=pl.BlockSpec((1,), lambda i: i), grid=8)\n    def add_one(x_ref, o_ref):\n        o_ref[0] = x_ref[0] + 1\n    np.testing.assert_allclose(add_one(jnp.arange(8)), jnp.arange(8) + 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_add_matrix_block_spec(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), intx), in_specs=[pl.BlockSpec((2, 2), lambda i, j: (i, j))], out_specs=pl.BlockSpec((2, 2), lambda i, j: (i, j)), grid=(4, 4))\n    def add_one(x_ref, o_ref):\n        o_ref[:, :] = x_ref[:, :] + 1\n    x = jnp.arange(64).reshape((8, 8))\n    np.testing.assert_allclose(add_one(x), x + 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_vector_indexing(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), floatx))\n    def index(x_ref, i_ref, o_ref):\n        o_ref[()] = x_ref[i_ref[()]]\n    x = jnp.arange(5.0)\n    for i in range(5):\n        np.testing.assert_allclose(index(x, i), x[i])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_block_spec_mapped_dimension(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float32), in_specs=[pl.BlockSpec((None, 4), lambda _: (0, 0)), pl.BlockSpec((None, 4), lambda _: (1, 0))], grid=1)\n    def add_vectors(x_ref, y_ref, o_ref):\n        o_ref[:] = x_ref[:] + y_ref[:]\n    xy = jnp.arange(8.0, dtype=np.float32).reshape((2, 4))\n    out = add_vectors(xy, xy)\n    out_ref = xy[0] + xy[1]\n    np.testing.assert_allclose(out, out_ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_vector_slicing(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), floatx))\n    def index(x_ref, idx_ref, o_ref):\n        idx = idx_ref[()]\n        o_ref[:] = x_ref[idx]\n    x = jnp.arange(5.0)\n    for i in range(4):\n        idx = jnp.arange(i, i + 2)\n        np.testing.assert_allclose(index(x, idx), x[idx])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}_gm_{group_size_m}', m, n, k, dtype, block_size_m, block_size_n, block_size_k, group_size_m) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] for group_size_m in [8] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul(self, m, n, k, dtype, bm, bn, bk, gm):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul(x, y, bm=bm, bn=bn, bk=bk, gm=gm, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}', m, n, k, dtype, block_size_m, block_size_n, block_size_k) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul_block_spec(self, m, n, k, dtype, bm, bn, bk):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul_block_spec(x, y, bm=bm, bn=bn, bk=bk, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(*(dict(testcase_name=f'{batch_size}_{size}_{block_size}_{dtype}', batch_size=batch_size, size=size, block_size=block_size, dtype=dtype) for batch_size in [1, 2, 4, 23] for size in [1, 2, 129, 255, 256] for block_size in [1, 2, 32, 64, 128, 256] for dtype in ['float32'] if size < block_size))\ndef test_softmax(self, batch_size, size, block_size, dtype):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((batch_size, size), dtype), grid=batch_size)\n    def softmax(x_ref, o_ref):\n        row_idx = pl.program_id(0)\n        x_idx = jnp.arange(block_size)\n        row_idxs = (row_idx, x_idx)\n        mask = x_idx < x_ref.shape[1]\n        row = pl.load(x_ref, row_idxs, mask=mask, other=-float('inf'))\n        row_minus_max = row - jnp.max(row, axis=0)\n        numerator = jnp.exp(row_minus_max)\n        denominator = jnp.sum(numerator, axis=0)\n        softmax_output = numerator / denominator\n        pl.store(o_ref, row_idxs, softmax_output, mask=mask)\n    key = random.key(0)\n    x = random.normal(key, [batch_size, size], dtype=dtype)\n    np.testing.assert_allclose(softmax(x), jax.nn.softmax(x, axis=-1), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_unused_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    m, n = (16, 32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32))\n    def dummy(_, o_ref):\n        pl.store(o_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]), jnp.ones_like(o_ref))\n    key = random.key(0)\n    x = random.normal(key, (m, n))\n    np.testing.assert_allclose(dummy(x), jnp.ones_like(x), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_with_input_output_aliasing(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def add_inplace_kernel(_, o_ref, *, block_size):\n        pid = pl.program_id(axis=0)\n        block_start = pid * block_size\n        offsets = block_start + jnp.arange(block_size, dtype=jnp.int32)\n        mask = offsets < o_ref.shape[0]\n        x = pl.load(o_ref, (offsets,), mask=mask)\n        output = x + 1\n        pl.store(o_ref, (offsets,), output, mask=mask)\n    grid = (8,)\n    size = 8\n    dtype = 'float32'\n    k1 = random.key(0)\n    block_size = 1\n    x = random.normal(k1, [size], dtype=dtype)\n    kernel = functools.partial(add_inplace_kernel, block_size=block_size)\n    out = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=grid, input_output_aliases={0: 0})(x)\n    expected = x + 1\n    np.testing.assert_allclose(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_using_pallas_slice(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    m, n = (32, 4)\n    out_shape = jax.ShapeDtypeStruct((4, n), floatx)\n\n    @functools.partial(self.pallas_call, out_shape=out_shape)\n    def slice_kernel(x_ref, y_ref):\n        x = pl.load(x_ref, (pl.dslice(0, 4), pl.dslice(0, 4)))\n        pl.store(y_ref, (pl.dslice(4), pl.dslice(4)), x)\n    x = random.normal(random.key(0), (m, n))\n    y = slice_kernel(x)\n    y_ref = x[:4]\n    np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(*AD_TEST_CASES)\ndef test_jvp(self, impl):\n    grad_tol = self.grad_tol\n    if jtu.test_device_matches(['tpu']) and 'recip_exp_sq' in self._testMethodName:\n        grad_tol = 0.1\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), floatx))\n    def pallas_impl(x_ref, o_ref):\n        x = x_ref[()]\n        o_ref[()] = impl(x)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1)\n    t = random.normal(k2)\n    out_primal, out_tangent = jax.jvp(pallas_impl, (x,), (t,))\n    out_primal_ref, out_tangent_ref = jax.jvp(impl, (x,), (t,))\n    np.testing.assert_allclose(out_primal, out_primal_ref, atol=self.tol, rtol=self.tol)\n    np.testing.assert_allclose(out_tangent, out_tangent_ref, atol=self.tol, rtol=self.tol)\n    jtu.check_grads(pallas_impl, (x,), modes=['fwd'], order=2, atol=grad_tol, rtol=grad_tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(*AD_TEST_CASES)\ndef test_pallas_around_grad(self, impl):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), floatx), name=self.id().split('.')[-1])\n    def pallas_impl(x_ref, o_ref):\n        x = x_ref[()]\n        o_ref[()] = jax.grad(impl)(x)\n    x = random.normal(random.key(0))\n    out_grad = pallas_impl(x)\n    out_grad_ref = jax.grad(impl)(x)\n    np.testing.assert_allclose(out_grad, out_grad_ref, atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(*AD_TEST_CASES)\ndef test_jvp_slice(self, impl):\n    grad_tol = self.grad_tol\n    if jtu.test_device_matches(['tpu']) and 'tanh' in self._testMethodName:\n        grad_tol = 0.1\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx))\n    def pallas_impl(x_ref, o_ref):\n        x = x_ref[jnp.arange(2)]\n        o_ref[jnp.arange(2)] = jnp.zeros(2)\n        o_ref[2 + jnp.arange(2)] = impl(x)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (8,))\n    t = random.normal(k2, (8,))\n    out_primal, out_tangent = jax.jvp(pallas_impl, (x,), (t,))\n    out_primal_ref, out_tangent_ref = jax.jvp(lambda x: jnp.concatenate([jnp.zeros(2), impl(x[:2])]), (x,), (t,))\n    np.testing.assert_allclose(out_primal, out_primal_ref, atol=self.tol, rtol=self.tol)\n    np.testing.assert_allclose(out_tangent, out_tangent_ref, atol=self.tol, rtol=self.tol)\n    jtu.check_grads(pallas_impl, (x,), modes=['fwd'], order=2, atol=grad_tol, rtol=grad_tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_custom_jvp_call(self):\n\n    @functools.partial(jax.custom_jvp, nondiff_argnums=(1,))\n    def softmax(x, axis=-1):\n        unnormalized = jnp.exp(x - jnp.max(x, axis, keepdims=True))\n        return unnormalized / jnp.sum(unnormalized, axis, keepdims=True)\n\n    @softmax.defjvp\n    def softmax_jvp(axis, primals, tangents):\n        (x,), (x_dot,) = (primals, tangents)\n        y = softmax(x, axis)\n        return (y, y * (x_dot - (y * x_dot).sum(axis, keepdims=True)))\n    m, n = (16, 32)\n    x = random.normal(random.key(0), (m, n))\n\n    @functools.partial(self.pallas_call, out_shape=x)\n    def softmax_kernel(x_ref, y_ref):\n        y_ref[:] = softmax(x_ref[:])\n    np.testing.assert_allclose(softmax_kernel(x), jax.nn.softmax(x), atol=1e-07)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_interpret_mode_out_of_bounds_access(self):\n    block_size = 32\n    dtype = jnp.float32\n    x = jax.random.normal(jax.random.key(0), (block_size, block_size + 1), dtype=dtype)\n    y = jax.random.normal(jax.random.key(1), (block_size + 1, block_size), dtype=dtype)\n    expected = x @ y\n    in_specs = [pl.BlockSpec((block_size, block_size), lambda i, j, k: (i, k)), pl.BlockSpec((block_size, block_size), lambda i, j, k: (k, j))]\n    out_spec = pl.BlockSpec((block_size, block_size), lambda i, j, k: (i, j))\n\n    def _unmasked_matmul_kernel(x_ref, y_ref, o_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            o_ref[...] = jnp.zeros_like(o_ref)\n        o_ref[...] += x_ref[...] @ y_ref[...]\n    out = self.pallas_call(_unmasked_matmul_kernel, out_shape=expected, grid=(1, 1, 2), in_specs=in_specs, out_specs=out_spec)(x, y)\n    with self.subTest('UnmaskedIsNaN'):\n        np.testing.assert_allclose(np.isnan(out), jnp.ones_like(out, dtype=jnp.bool_))\n\n    def _masked_matmul_kernel(x_ref, y_ref, o_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            o_ref[:, :] = jnp.zeros_like(o_ref)\n        num_valid = x.shape[1] - pl.program_id(2) * block_size\n        num_valid = jnp.minimum(num_valid, block_size)\n        mask = jnp.tril(jnp.ones_like(x_ref[:, :]))[num_valid - 1][jnp.newaxis, :]\n        mask = jnp.repeat(mask, block_size, axis=0)\n        masked_x = jnp.where(mask, x_ref[:, :], 0.0)\n        masked_y = jnp.where(mask.T, y_ref[:, :], 0.0)\n        o_ref[:, :] += masked_x @ masked_y\n    out = self.pallas_call(_masked_matmul_kernel, out_shape=expected, grid=(1, 1, 2), in_specs=in_specs, out_specs=out_spec)(x, y)\n    if jtu.test_device_matches(['gpu']):\n        atol = 0.01\n    else:\n        atol = 1e-05\n    with self.subTest('MaskedOutputIsCorrect'):\n        np.testing.assert_allclose(out, expected, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_runtime_assert_is_noop_when_not_enabled(self):\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Runtime check only implemented on TPU.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n        checkify.check(False, 'failed check', debug=True)\n    input_ = jnp.arange(4, dtype=jnp.int32)\n    out_shape = jax.ShapeDtypeStruct(input_.shape, input_.dtype)\n    with pltpu.enable_runtime_assert(False):\n        pallas_call = pl.pallas_call(kernel, out_shape=out_shape)\n        result = pallas_call(input_)\n    np.testing.assert_allclose(result, input_)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_no_checkify(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU.')\n\n    def kernel(y_ref):\n        y_ref[...] = jnp.zeros_like(y_ref[...])\n    out_shape = jax.ShapeDtypeStruct((2, 2), jnp.float32)\n    pallas_call = self.pallas_call(kernel, out_shape=out_shape)\n    checked_call = checkify.checkify(pallas_call)\n    err, result = checked_call()\n    err.throw()\n    np.testing.assert_allclose(result, jnp.zeros_like(result))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_does_not_clobber_previous_error(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU.')\n\n    def kernel(y_ref):\n        y_ref[...] = jnp.zeros_like(y_ref[...])\n        checkify.check(False, 'error in kernel')\n    out_shape = jax.ShapeDtypeStruct((2, 2), jnp.float32)\n    pallas_call = self.pallas_call(kernel, out_shape=out_shape)\n\n    def error_before_call():\n        checkify.check(False, 'error before call')\n        return pallas_call()\n    checked_call = checkify.checkify(error_before_call)\n    err, result = checked_call()\n    with self.assertRaisesRegex(checkify.JaxRuntimeError, 'error before call'):\n        err.throw()\n    np.testing.assert_allclose(result, jnp.zeros_like(result))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters((False,), (True,))\ndef test_trivial_check(self, assert_cond):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n        checkify.check(assert_cond, 'pallas check failed')\n    input = jnp.arange(4, dtype=jnp.int32)\n    out_shape = jax.ShapeDtypeStruct(input.shape, input.dtype)\n    pallas_call = self.pallas_call(kernel, out_shape=out_shape)\n    checked_call = checkify.checkify(pallas_call)\n    err, result = checked_call(input)\n    if not assert_cond:\n        with self.assertRaisesRegex(checkify.JaxRuntimeError, 'pallas check failed'):\n            err.throw()\n    np.testing.assert_allclose(result, input)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_nan_error(self):\n    if not self.INTERPRET:\n        self.skipTest('Not supported in non-interpret mode.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = jnp.log(x_ref[...])\n    input = jnp.arange(4, dtype=jnp.float32) - 2\n    out_shape = jax.ShapeDtypeStruct(input.shape, input.dtype)\n    pallas_call = self.pallas_call(kernel, out_shape=out_shape)\n    checked_call = checkify.checkify(pallas_call, errors=checkify.nan_checks)\n    err, result = checked_call(input)\n    with self.assertRaisesRegex(checkify.JaxRuntimeError, 'nan generated by primitive: log'):\n        err.throw()\n    is_nan = jnp.isnan(result)\n    np.testing.assert_allclose(is_nan, input < 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}_gm_{group_size_m}', m, n, k, dtype, block_size_m, block_size_n, block_size_k, group_size_m) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] for group_size_m in [8] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul(self, m, n, k, dtype, bm, bn, bk, gm):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul(x, y, bm=bm, bn=bn, bk=bk, gm=gm, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def matmul(x, y):\n\n    def run_matmul(refs):\n        x_ref, y_ref, o_ref = refs\n\n        def matmul_pipeline_kernel(acc_ref):\n            pltpu.emit_pipeline(functools.partial(matmul_kernel, acc_ref), grid=(m // bm, n // bn, k // bk), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)))(x_ref, y_ref, o_ref)\n        pl.pallas_call(matmul_pipeline_kernel, out_shape=[], scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)])()\n    _, _, o = pl.run_state(run_matmul)((x, y, jnp.ones((m, n), dtype=x.dtype)))\n    return o"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}', m, n, k, dtype, block_size_m, block_size_n, block_size_k) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul_block_spec(self, m, n, k, dtype, bm, bn, bk):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul_block_spec(x, y, bm=bm, bn=bn, bk=bk, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def matmul(x, y):\n\n    def run_matmul(refs):\n        x_ref, y_ref, o_ref = refs\n\n        def matmul_pipeline_kernel(acc_ref):\n            pltpu.emit_pipeline(functools.partial(matmul_kernel, acc_ref), grid=(m // bm, n // bn, k // bk), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)))(x_ref, y_ref, o_ref)\n        pl.pallas_call(matmul_pipeline_kernel, out_shape=[], scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)])()\n    _, _, o = pl.run_state(run_matmul)((x, y, jnp.ones((m, n), dtype=x.dtype)))\n    return o"
  },
  {
    "test_code": "def test_hoisted_consts(self):\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    x = np.arange(16 * 128, dtype=np.float32).reshape((16, 128))\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\n    def kernel(src, dst):\n        dst[0:1] = to_store\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* captures constants'):\n        kernel(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_hoisted_consts(self):\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    x = np.arange(16 * 128, dtype=np.float32).reshape((16, 128))\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\n    def kernel(src, dst):\n        dst[0:1] = to_store\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* captures constants'):\n        kernel(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}_gm_{group_size_m}', m, n, k, dtype, block_size_m, block_size_n, block_size_k, group_size_m) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] for group_size_m in [8] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul(self, m, n, k, dtype, bm, bn, bk, gm):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul(x, y, bm=bm, bn=bn, bk=bk, gm=gm, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def matmul(a, b):\n    return jnp.matmul(a, b)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}', m, n, k, dtype, block_size_m, block_size_n, block_size_k) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul_block_spec(self, m, n, k, dtype, bm, bn, bk):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul_block_spec(x, y, bm=bm, bn=bn, bk=bk, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def matmul(a, b):\n    return jnp.matmul(a, b)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_hoisted_consts(self):\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    x = np.arange(16 * 128, dtype=np.float32).reshape((16, 128))\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\n    def kernel(src, dst):\n        dst[0:1] = to_store\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* captures constants'):\n        kernel(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "def test_hoisted_consts(self):\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    x = np.arange(16 * 128, dtype=np.float32).reshape((16, 128))\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\n    def kernel(src, dst):\n        dst[0:1] = to_store\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* captures constants'):\n        kernel(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}_gm_{group_size_m}', m, n, k, dtype, block_size_m, block_size_n, block_size_k, group_size_m) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] for group_size_m in [8] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul(self, m, n, k, dtype, bm, bn, bk, gm):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul(x, y, bm=bm, bn=bn, bk=bk, gm=gm, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def matmul(x: jax.Array, y: jax.Array, x_sentinel: jax.Array, *, bm: int=128, bk: int=128, bn: int=640):\n    grid = (n // bn, k // bk)\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((bm, bk), lambda j, k: (0, k)), pl.BlockSpec((bk, bn), lambda j, k: (k, j)), pl.BlockSpec((bm, bn), lambda j, k: (0, j))], out_specs=pl.BlockSpec((bm, bn), lambda j, k: (0, j)), grid=grid, input_output_aliases={2: 0}, interpret=self.INTERPRET)(x, y, x_sentinel)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}', m, n, k, dtype, block_size_m, block_size_n, block_size_k) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul_block_spec(self, m, n, k, dtype, bm, bn, bk):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul_block_spec(x, y, bm=bm, bn=bn, bk=bk, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def matmul(x: jax.Array, y: jax.Array, x_sentinel: jax.Array, *, bm: int=128, bk: int=128, bn: int=640):\n    grid = (n // bn, k // bk)\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((bm, bk), lambda j, k: (0, k)), pl.BlockSpec((bk, bn), lambda j, k: (k, j)), pl.BlockSpec((bm, bn), lambda j, k: (0, j))], out_specs=pl.BlockSpec((bm, bn), lambda j, k: (0, j)), grid=grid, input_output_aliases={2: 0}, interpret=self.INTERPRET)(x, y, x_sentinel)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}_gm_{group_size_m}', m, n, k, dtype, block_size_m, block_size_n, block_size_k, group_size_m) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] for group_size_m in [8] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul(self, m, n, k, dtype, bm, bn, bk, gm):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul(x, y, bm=bm, bn=bn, bk=bk, gm=gm, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, static_argnames=['bm', 'bk', 'bn'])\ndef matmul(x: jax.Array, y: jax.Array, *, bm: int, bk: int, bn: int):\n    m, k = x.shape\n    _, n = y.shape\n\n    def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n        grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n        def run(acc_scratch_ref):\n            pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n        accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n        pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))\n    num_cores = jax.devices()[0].num_cores\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((m, n), x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,))(x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}', m, n, k, dtype, block_size_m, block_size_n, block_size_k) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul_block_spec(self, m, n, k, dtype, bm, bn, bk):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul_block_spec(x, y, bm=bm, bn=bn, bk=bk, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@partial(jax.jit, static_argnames=['bm', 'bk', 'bn'])\ndef matmul(x: jax.Array, y: jax.Array, *, bm: int, bk: int, bn: int):\n    m, k = x.shape\n    _, n = y.shape\n\n    def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n        grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n        def run(acc_scratch_ref):\n            pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n        accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n        pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))\n    num_cores = jax.devices()[0].num_cores\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((m, n), x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,))(x, y)"
  },
  {
    "test_code": "def test_hoisted_consts(self):\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    x = np.arange(16 * 128, dtype=np.float32).reshape((16, 128))\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\n    def kernel(src, dst):\n        dst[0:1] = to_store\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* captures constants'):\n        kernel(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}_gm_{group_size_m}', m, n, k, dtype, block_size_m, block_size_n, block_size_k, group_size_m) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] for group_size_m in [8] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul(self, m, n, k, dtype, bm, bn, bk, gm):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul(x, y, bm=bm, bn=bn, bk=bk, gm=gm, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['block_shape'])\ndef matmul(x: jax.Array, y: jax.Array, *, block_shape=(128, 128, 128)):\n    m, l = x.shape\n    l2, n = y.shape\n    assert l2 == l\n    block_m, block_n, block_l = block_shape\n    assert l % block_l == 0, f'l={l!r}, block_l={block_l!r}'\n    assert m % block_m == 0, f'm={m!r}, block_m={block_m!r}'\n    assert n % block_n == 0, f'n={n!r}, block_n={block_n!r}'\n    grid = (m // block_m, n // block_n, l // block_l)\n    fused_matmul = pl.pallas_call(functools.partial(matmul_kernel), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((block_m, block_l), lambda i, j, k: (i, k)), pl.BlockSpec((block_l, block_n), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((block_m, block_n), lambda i, j, k: (i, j)), grid=grid, interpret=jtu.test_device_matches(['cpu']))\n    return fused_matmul(x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}', m, n, k, dtype, block_size_m, block_size_n, block_size_k) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul_block_spec(self, m, n, k, dtype, bm, bn, bk):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul_block_spec(x, y, bm=bm, bn=bn, bk=bk, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['block_shape'])\ndef matmul(x: jax.Array, y: jax.Array, *, block_shape=(128, 128, 128)):\n    m, l = x.shape\n    l2, n = y.shape\n    assert l2 == l\n    block_m, block_n, block_l = block_shape\n    assert l % block_l == 0, f'l={l!r}, block_l={block_l!r}'\n    assert m % block_m == 0, f'm={m!r}, block_m={block_m!r}'\n    assert n % block_n == 0, f'n={n!r}, block_n={block_n!r}'\n    grid = (m // block_m, n // block_n, l // block_l)\n    fused_matmul = pl.pallas_call(functools.partial(matmul_kernel), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((block_m, block_l), lambda i, j, k: (i, k)), pl.BlockSpec((block_l, block_n), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((block_m, block_n), lambda i, j, k: (i, j)), grid=grid, interpret=jtu.test_device_matches(['cpu']))\n    return fused_matmul(x, y)"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_hoisted_consts(self):\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    x = np.arange(16 * 128, dtype=np.float32).reshape((16, 128))\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\n    def kernel(src, dst):\n        dst[0:1] = to_store\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* captures constants'):\n        kernel(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\ndef kernel(x_ref, o_ref):\n    jax.debug.print('x = {}', x_ref)"
  },
  {
    "test_code": "def test_interpret_mode_out_of_bounds_access(self):\n    block_size = 32\n    dtype = jnp.float32\n    x = jax.random.normal(jax.random.key(0), (block_size, block_size + 1), dtype=dtype)\n    y = jax.random.normal(jax.random.key(1), (block_size + 1, block_size), dtype=dtype)\n    expected = x @ y\n    in_specs = [pl.BlockSpec((block_size, block_size), lambda i, j, k: (i, k)), pl.BlockSpec((block_size, block_size), lambda i, j, k: (k, j))]\n    out_spec = pl.BlockSpec((block_size, block_size), lambda i, j, k: (i, j))\n\n    def _unmasked_matmul_kernel(x_ref, y_ref, o_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            o_ref[...] = jnp.zeros_like(o_ref)\n        o_ref[...] += x_ref[...] @ y_ref[...]\n    out = self.pallas_call(_unmasked_matmul_kernel, out_shape=expected, grid=(1, 1, 2), in_specs=in_specs, out_specs=out_spec)(x, y)\n    with self.subTest('UnmaskedIsNaN'):\n        np.testing.assert_allclose(np.isnan(out), jnp.ones_like(out, dtype=jnp.bool_))\n\n    def _masked_matmul_kernel(x_ref, y_ref, o_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            o_ref[:, :] = jnp.zeros_like(o_ref)\n        num_valid = x.shape[1] - pl.program_id(2) * block_size\n        num_valid = jnp.minimum(num_valid, block_size)\n        mask = jnp.tril(jnp.ones_like(x_ref[:, :]))[num_valid - 1][jnp.newaxis, :]\n        mask = jnp.repeat(mask, block_size, axis=0)\n        masked_x = jnp.where(mask, x_ref[:, :], 0.0)\n        masked_y = jnp.where(mask.T, y_ref[:, :], 0.0)\n        o_ref[:, :] += masked_x @ masked_y\n    out = self.pallas_call(_masked_matmul_kernel, out_shape=expected, grid=(1, 1, 2), in_specs=in_specs, out_specs=out_spec)(x, y)\n    if jtu.test_device_matches(['gpu']):\n        atol = 0.01\n    else:\n        atol = 1e-05\n    with self.subTest('MaskedOutputIsCorrect'):\n        np.testing.assert_allclose(out, expected, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\ndef isnan(x_ref, o_ref):\n    o_ref[:] = jnp.isnan(x_ref[...])"
  },
  {
    "test_code": "def test_nan_error(self):\n    if not self.INTERPRET:\n        self.skipTest('Not supported in non-interpret mode.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = jnp.log(x_ref[...])\n    input = jnp.arange(4, dtype=jnp.float32) - 2\n    out_shape = jax.ShapeDtypeStruct(input.shape, input.dtype)\n    pallas_call = self.pallas_call(kernel, out_shape=out_shape)\n    checked_call = checkify.checkify(pallas_call, errors=checkify.nan_checks)\n    err, result = checked_call(input)\n    with self.assertRaisesRegex(checkify.JaxRuntimeError, 'nan generated by primitive: log'):\n        err.throw()\n    is_nan = jnp.isnan(result)\n    np.testing.assert_allclose(is_nan, input < 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\ndef isnan(x_ref, o_ref):\n    o_ref[:] = jnp.isnan(x_ref[...])"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "@parameterized.parameters(('float32', None), ('float32', jax.lax.Precision.DEFAULT), ('float32', jax.lax.Precision.HIGH), ('float32', jax.lax.Precision.HIGHEST), ('float32', jax.lax.DotAlgorithmPreset.DEFAULT), ('float32', jax.lax.DotAlgorithmPreset.F16_F16_F32), ('float32', jax.lax.DotAlgorithmPreset.BF16_BF16_F32), ('float32', jax.lax.DotAlgorithmPreset.TF32_TF32_F32), ('float32', jax.lax.DotAlgorithmPreset.TF32_TF32_F32_X3), ('float32', jax.lax.DotAlgorithmPreset.F32_F32_F32), ('bfloat16', None), ('bfloat16', jax.lax.Precision.DEFAULT), ('bfloat16', jax.lax.Precision.HIGHEST), ('bfloat16', jax.lax.DotAlgorithmPreset.DEFAULT), ('bfloat16', jax.lax.DotAlgorithmPreset.BF16_BF16_F32))\ndef test_dot_precision(self, dtype, precision):\n    if not jtu.test_device_matches(['gpu']):\n        self.skipTest('`DotAlgorithmPreset` only supported on GPU.')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((32, 64), jnp.float32))\n    def dot_kernel(x_ref, y_ref, o_ref):\n        o_ref[()] = pl.dot(x_ref[()], y_ref[()], precision=precision)\n    key0, key1 = random.split(random.key(0))\n    x = random.normal(key0, (32, 16), dtype=dtype)\n    y = random.normal(key1, (16, 64), dtype=dtype)\n    expected = jnp.dot(x, y, precision=jax.lax.Precision.HIGHEST, preferred_element_type=jnp.float32)\n    self.assertAllClose(dot_kernel(x, y), expected, atol=0.05, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, dtype))\ndef dot(x_ref, y_ref, o_ref):\n    x = x_ref[:, :]\n    y = y_ref[:, :]\n    o_ref[:, :] = pl.dot(x, y, trans_x, trans_y).astype(o_ref.dtype)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.int8, jnp.uint8)\ndef test_integer_dot(self, dtype):\n    if jtu.test_device_matches(['tpu']) and (not jtu.is_device_tpu_at_least(5)):\n        self.skipTest('`int8` dot is only supported on v5 TPUs and newer.')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((32, 64), jnp.int32))\n    def dot_kernel(x_ref, y_ref, o_ref):\n        o_ref[()] = pl.dot(x_ref[()], y_ref[()])\n    key0, key1 = random.split(random.key(0))\n    kwargs = dict(minval=jnp.iinfo(dtype).min, maxval=128, dtype=dtype)\n    x = random.randint(key0, (32, 128), **kwargs)\n    y = random.randint(key1, (128, 64), **kwargs)\n    expected = jnp.dot(x, y, preferred_element_type=jnp.int32)\n    self.assertAllClose(dot_kernel(x, y), expected, atol=0.0, rtol=0.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, dtype))\ndef dot(x_ref, y_ref, o_ref):\n    x = x_ref[:, :]\n    y = y_ref[:, :]\n    o_ref[:, :] = pl.dot(x, y, trans_x, trans_y).astype(o_ref.dtype)"
  },
  {
    "test_code": "def test_dot_with_vector(self):\n    if not jtu.test_device_matches(['gpu']) or self.INTERPRET:\n        self.skipTest('jnp.dot is only restricted to 2D on GPU in non-interpret mode.')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((32,), jnp.float32))\n    def dot_kernel(x_ref, y_ref, o_ref):\n        o_ref[()] = jnp.dot(x_ref[()], y_ref[()])\n    key0, key1 = random.split(random.key(0))\n    x = random.normal(key0, (32, 64), dtype=jnp.float32)\n    y = random.normal(key1, (64,), dtype=jnp.float32)\n    with self.assertRaisesRegex(Exception, 'must be 2D'):\n        dot_kernel(x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, dtype))\ndef dot(x_ref, y_ref, o_ref):\n    x = x_ref[:, :]\n    y = y_ref[:, :]\n    o_ref[:, :] = pl.dot(x, y, trans_x, trans_y).astype(o_ref.dtype)"
  },
  {
    "test_code": "@parameterized.named_parameters(*(dict(testcase_name=f'{batch_size}_{size}_{block_size}_{dtype}', batch_size=batch_size, size=size, block_size=block_size, dtype=dtype) for batch_size in [1, 2, 4, 23] for size in [1, 2, 129, 255, 256] for block_size in [1, 2, 32, 64, 128, 256] for dtype in ['float32'] if size < block_size))\ndef test_softmax(self, batch_size, size, block_size, dtype):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((batch_size, size), dtype), grid=batch_size)\n    def softmax(x_ref, o_ref):\n        row_idx = pl.program_id(0)\n        x_idx = jnp.arange(block_size)\n        row_idxs = (row_idx, x_idx)\n        mask = x_idx < x_ref.shape[1]\n        row = pl.load(x_ref, row_idxs, mask=mask, other=-float('inf'))\n        row_minus_max = row - jnp.max(row, axis=0)\n        numerator = jnp.exp(row_minus_max)\n        denominator = jnp.sum(numerator, axis=0)\n        softmax_output = numerator / denominator\n        pl.store(o_ref, row_idxs, softmax_output, mask=mask)\n    key = random.key(0)\n    x = random.normal(key, [batch_size, size], dtype=dtype)\n    np.testing.assert_allclose(softmax(x), jax.nn.softmax(x, axis=-1), atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), floatx))\ndef load(x_ref, o_ref):\n    x = pl.load(x_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]))\n    pl.store(o_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]), x + 1.0)"
  },
  {
    "test_code": "def test_with_input_output_aliasing(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def add_inplace_kernel(_, o_ref, *, block_size):\n        pid = pl.program_id(axis=0)\n        block_start = pid * block_size\n        offsets = block_start + jnp.arange(block_size, dtype=jnp.int32)\n        mask = offsets < o_ref.shape[0]\n        x = pl.load(o_ref, (offsets,), mask=mask)\n        output = x + 1\n        pl.store(o_ref, (offsets,), output, mask=mask)\n    grid = (8,)\n    size = 8\n    dtype = 'float32'\n    k1 = random.key(0)\n    block_size = 1\n    x = random.normal(k1, [size], dtype=dtype)\n    kernel = functools.partial(add_inplace_kernel, block_size=block_size)\n    out = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=grid, input_output_aliases={0: 0})(x)\n    expected = x + 1\n    np.testing.assert_allclose(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), floatx))\ndef load(x_ref, o_ref):\n    x = pl.load(x_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]))\n    pl.store(o_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]), x + 1.0)"
  },
  {
    "test_code": "def test_using_pallas_slice(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    m, n = (32, 4)\n    out_shape = jax.ShapeDtypeStruct((4, n), floatx)\n\n    @functools.partial(self.pallas_call, out_shape=out_shape)\n    def slice_kernel(x_ref, y_ref):\n        x = pl.load(x_ref, (pl.dslice(0, 4), pl.dslice(0, 4)))\n        pl.store(y_ref, (pl.dslice(4), pl.dslice(4)), x)\n    x = random.normal(random.key(0), (m, n))\n    y = slice_kernel(x)\n    y_ref = x[:4]\n    np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), floatx))\ndef load(x_ref, o_ref):\n    x = pl.load(x_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]))\n    pl.store(o_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]), x + 1.0)"
  },
  {
    "test_code": "def test_range_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which can reduce to a fori_loop.\"\"\"\n\n    def kernel(x_ref, r_ref):\n\n        @pl.when(pl.program_id(0) == 0)\n        def _():\n            pl.store(r_ref, (0, 0), 0)\n\n        def cond(carry):\n            i, j = carry\n            return i < j\n\n        def body(carry):\n            io, j = carry\n            i = io - 128\n            sl = jax.lax.div(i, 128)\n            l = jax.lax.rem(i, 128)\n            v = x_ref[0, sl, l]\n            s = pl.load(r_ref, (0, 0))\n            pl.store(r_ref, (0, 0), s + v)\n            return (io + 1, j)\n        i = 128\n        j = 128 + 1024\n        i, j = jax.lax.while_loop(cond, body, (i, j))\n    x = jnp.arange(4096)\n    x = jnp.reshape(x, [4, 8, 128])\n    r = pl.pallas_call(kernel, grid=(1,), out_specs=pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct([1, 1], intx), in_specs=[pl.BlockSpec((1, 8, 128), lambda i: (i, 0, 0), memory_space=smem_on_tpu())])(x)\n    expected = jnp.sum(jnp.arange(1024))\n    np.testing.assert_array_equal(r, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), floatx))\ndef load(x_ref, o_ref):\n    x = pl.load(x_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]))\n    pl.store(o_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]), x + 1.0)"
  },
  {
    "test_code": "def test_non_range_while_loop(self):\n    \"\"\"Tests lowering of a while_loop which cannot reduce to a fori_loop.\"\"\"\n\n    def kernel(x_ref, r_ref):\n\n        @pl.when(pl.program_id(0) == 0)\n        def _():\n            pl.store(r_ref, (0, 0), 0)\n\n        def cond(state):\n            i, s = state\n            return jnp.logical_and(i < 1024, s < 1024)\n\n        def body(state):\n            i, s = state\n            sl = jax.lax.div(i, jnp.astype(128, i.dtype))\n            l = jax.lax.rem(i, jnp.astype(128, i.dtype))\n            v = pl.load(x_ref, (0, sl, l))\n            return (i + 1, s + v)\n        i = jnp.int32(0)\n        s = pl.load(r_ref, (0, 0))\n        i, s = jax.lax.while_loop(cond, body, (i, s))\n        pl.store(r_ref, (0, 0), s)\n    x = jnp.arange(4096)\n    x = jnp.reshape(x, [4, 8, 128])\n    r = pl.pallas_call(kernel, grid=(4,), out_specs=pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct([1, 1], intx), in_specs=[pl.BlockSpec((1, 8, 128), lambda i: (i, 0, 0), memory_space=smem_on_tpu())])(x)\n    np.testing.assert_array_equal(r, [[1035]])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), floatx))\ndef load(x_ref, o_ref):\n    x = pl.load(x_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]))\n    pl.store(o_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]), x + 1.0)"
  },
  {
    "test_code": "def test_add_one(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), floatx))\n    def add_one(x_ref, o_ref):\n        o_ref[()] = x_ref[()] + 1.0\n    x = 0.0\n    self.assertEqual(add_one(x), 1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), intx), grid=(2,))\ndef add_one(x_ref, o_ref):\n    o_ref[()] = x_ref[()] + 1"
  },
  {
    "test_code": "def test_add_singleton_vector(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((1,), jnp.float32))\n    def add_one(x_ref, o_ref):\n        o_ref[0] = x_ref[0] + 1.0\n    x = jnp.array([0.0], jnp.float32)\n    np.testing.assert_allclose(add_one(x), jnp.array([1.0], jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), intx), grid=(2,))\ndef add_one(x_ref, o_ref):\n    o_ref[()] = x_ref[()] + 1"
  },
  {
    "test_code": "def test_add_vector_block_spec(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), intx), in_specs=[pl.BlockSpec((1,), lambda i: i)], out_specs=pl.BlockSpec((1,), lambda i: i), grid=8)\n    def add_one(x_ref, o_ref):\n        o_ref[0] = x_ref[0] + 1\n    np.testing.assert_allclose(add_one(jnp.arange(8)), jnp.arange(8) + 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), intx), grid=(2,))\ndef add_one(x_ref, o_ref):\n    o_ref[()] = x_ref[()] + 1"
  },
  {
    "test_code": "def test_add_matrix_block_spec(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), intx), in_specs=[pl.BlockSpec((2, 2), lambda i, j: (i, j))], out_specs=pl.BlockSpec((2, 2), lambda i, j: (i, j)), grid=(4, 4))\n    def add_one(x_ref, o_ref):\n        o_ref[:, :] = x_ref[:, :] + 1\n    x = jnp.arange(64).reshape((8, 8))\n    np.testing.assert_allclose(add_one(x), x + 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), intx), grid=(2,))\ndef add_one(x_ref, o_ref):\n    o_ref[()] = x_ref[()] + 1"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), intx), grid=(2,))\ndef add_one(x_ref, o_ref):\n    o_ref[()] = x_ref[()] + 1"
  },
  {
    "test_code": "def test_hoisted_consts(self):\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    x = np.arange(16 * 128, dtype=np.float32).reshape((16, 128))\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\n    def kernel(src, dst):\n        dst[0:1] = to_store\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* captures constants'):\n        kernel(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.vmap\n@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\ndef kernel(src, dst):\n    dst[0:1] = to_store"
  },
  {
    "test_code": "def test_hoisted_consts(self):\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    x = np.arange(16 * 128, dtype=np.float32).reshape((16, 128))\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\n    def kernel(src, dst):\n        dst[0:1] = to_store\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* captures constants'):\n        kernel(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.zeros_like(o_ref)\n    new_o_ref = o_ref.at[pl.ds(0, 8)].at[0].at[pl.ds(0, 4), pl.ds(0, 4)]\n    new_o_ref[...] = x_ref[...]"
  },
  {
    "test_code": "def test_hoisted_consts(self):\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    x = np.arange(16 * 128, dtype=np.float32).reshape((16, 128))\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid=(2,), in_specs=[pl.BlockSpec((8, 128), lambda i: (i, 0))], out_specs=pl.BlockSpec((32, 128), lambda i: (i, 0)))\n    def kernel(src, dst):\n        dst[0:1] = to_store\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* captures constants'):\n        kernel(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@pl.core_map(mesh)\ndef kernel():\n\n    def scoped(barrier):\n        plgpu.barrier_arrive(barrier)\n        plgpu.barrier_wait(barrier)\n        wg_idx = jax.lax.axis_index('wg')\n        y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n    pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))"
  },
  {
    "test_code": "def test_pallas_call_no_outputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, ())\n    self.assertAllClose((), f(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_singleton_tuple(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=(a,))\n    res = f(a)\n    self.assertIsInstance(res, tuple)\n    self.assertLen(res, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_out_shape_is_list(self):\n    a = np.arange(1024, dtype=np.int32).reshape((8, 128))\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a])\n    res = f(a)\n    self.assertIsInstance(res, tuple)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_block_spec_with_padding(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n\n    def f(*, shape, block_shape):\n\n        def kernel(o1_ref):\n            assert o1_ref.shape == block_shape\n            o1_ref[...] = jnp.full(o1_ref.shape, pl.program_id(0))\n        return self.pallas_call(kernel, jax.ShapeDtypeStruct(shape, dtype=np.int32), grid=((shape[0] + block_shape[0] - 1) // block_shape[0],), out_specs=pl.BlockSpec(block_shape, lambda i: i))()\n    pids = f(shape=(8,), block_shape=(2,))\n    self.assertAllClose(pids, np.array([0, 0, 1, 1, 2, 2, 3, 3], dtype=np.int32))\n    pids = f(shape=(8,), block_shape=(3,))\n    self.assertAllClose(pids, np.array([0, 0, 0, 1, 1, 1, 2, 2], dtype=np.int32))\n    pids = f(shape=(3,), block_shape=(8,))\n    self.assertAllClose(pids, np.array([0, 0, 0], dtype=np.int32))",
    "assertions": [
      "assert o1_ref.shape == block_shape"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_trace_cache(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    trace_count = 0\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def add_one(x_ref, o_ref):\n        nonlocal trace_count\n        o_ref[()] = x_ref[()] + 1.0\n        trace_count += 1\n\n    @jax.jit\n    def f(x):\n        return add_one(add_one(x))\n    x = jnp.array(0.0, dtype=jnp.float32)\n    self.assertEqual(f(x), 2.0)\n    self.assertEqual(trace_count, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_kernel_args_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref: None, out_shape=a)\n    with self.assertRaisesRegex(TypeError, 'takes 1 positional argument but 2 were given'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "@parameterized.named_parameters(('array', 0), ('empty_tuple', ()))\ndef test_pallas_call_error_kernel_returns_something(self, returns):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_kernel(x_ref, o1_ref, o2_ref):\n        return returns\n    f = self.pallas_call(my_kernel, out_shape=(a, a))\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* my_kernel at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_kernel_with_no_signature_returns_something(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda *args: 0, out_shape=a)\n    with self.assertRaisesRegex(ValueError, 'The kernel function .* at .*pallas_test.py:.* should return None'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_in_specs_mismatch_inputs(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda: 0), pl.BlockSpec((4,), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:.* at \\\\[1\\\\], `in_specs` is a pytree leaf but inputs is a.*', re.DOTALL)):\n        f(a, dict(a=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_arguments(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), lambda i, j: 0)])\n    with self.assertRaisesRegex(TypeError, \"missing 2 required positional arguments: 'i' and 'j'\"):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return 1 values to match .*Currently returning 2 values.'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_index_map_pytree_input_wrong_number_of_results(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map():\n        return (0, 0)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, in_specs=[dict(one=pl.BlockSpec((4,), my_index_map), two=pl.BlockSpec((8,), my_index_map))])\n    with self.assertRaisesRegex(ValueError, \"Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\]\\\\['one'\\\\] must return 1 values to match .*Currently returning 2 values.\"):\n        f(dict(one=a, two=a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_type(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return 5.0\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*float'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_index_map_wrong_return_shape(self):\n    a = np.arange(256, dtype=np.int32)\n\n    def my_index_map(i):\n        return jnp.arange(4, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), my_index_map)])\n    with self.assertRaisesRegex(ValueError, 'Index map function my_index_map at .*pallas_test.py.* for args\\\\[0\\\\] must return integer scalars. Output\\\\[0\\\\] has type .*int32\\\\[4\\\\]'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_index_map_captures_consts(self):\n    a = np.arange(256, dtype=np.int32)\n    index_map_result = np.array([0], dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=a, grid=(1,), in_specs=[pl.BlockSpec((4,), lambda i: jnp.array(index_map_result)[i])])\n    with self.assertRaisesRegex(ValueError, 'Index map function .* for args\\\\[0\\\\] must not capture constants:'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_out_specs_mismatch_shape(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a, a], out_specs=[pl.BlockSpec((6,), lambda i: i)])\n    with self.assertRaisesRegex(ValueError, re.compile('Pytree for `out_specs` and `out_shape` do not match. There are 1 mismatches, including:.* `out_specs` is a tuple of length 1 but `out_shape` is a tuple of length 2.*', re.DOTALL)):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_pallas_call_block_shape_ndim_mismatch(self):\n    a = np.arange(256, dtype=np.int32)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], in_specs=[pl.BlockSpec((1, 1), lambda: (0, 0))])\n    with self.assertRaisesRegex(ValueError, 'Block shape for args\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)\n    f = self.pallas_call(lambda x_ref, o1_ref: None, out_shape=[a], out_specs=[pl.BlockSpec((1, 1), lambda: 0)])\n    with self.assertRaisesRegex(ValueError, 'Block shape for outputs\\\\[0\\\\] .* must have the same number of dimensions as the array shape'):\n        f(a)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_basic_input_output_aliasing(self):\n    size = 1024\n    if jtu.is_device_cuda():\n        size = 256\n    x = jnp.ones((32, size, size))\n    expected = x + 1\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @functools.partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, in_specs=[pl.BlockSpec((None, size, size), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((None, size, size), lambda i: (i, 0, 0)), grid=(x.shape[0],), input_output_aliases={0: 0})(x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)\n    self.assertEqual(mem_analysis.temp_size_in_bytes, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_loop_with_float64_carry(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    with config.enable_x64(True):\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\n        def f(x_ref, y_ref):\n\n            def body(i, acc):\n                return acc + x_ref[...] + i * 0\n            y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))\n        np.testing.assert_allclose(np.arange(1, 5.0) * 3, f(jnp.arange(1, 5.0, dtype=jnp.float64)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_cond_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_cond_threebranch(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32(0.0)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    self.assertEqual(y, 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    self.assertEqual(y, -2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    self.assertEqual(y, -16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 4, 8)\ndef test_cond_threebranch_vectors(self, block_size):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.float32([0.0] * 8)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32), in_specs=[pl.BlockSpec((), lambda _: ()), pl.BlockSpec((block_size,), lambda i: i)], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=pl.cdiv(arg.shape[0], block_size))\n    def f(branch_ref, x_ref, y_ref):\n        y_ref[...] = lax.switch(branch_ref[...], (lambda x: x ** 2, lambda x: -x, lambda x: -x ** 2), x_ref[...])\n    y = f(jnp.int32(0), arg + 3.0)\n    np.testing.assert_allclose(y, arg + 9.0)\n    y = f(jnp.int32(1), arg + 2.0)\n    np.testing.assert_allclose(y, arg - 2.0)\n    y = f(jnp.int32(2), arg + 4.0)\n    np.testing.assert_allclose(y, arg - 16.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product([1, 8], [1, 2, 4]))\ndef test_cond_threebranch_matrix_out(self, bx, by):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    x = jnp.arange(64.0)[:, None]\n    y = jnp.arange(128.0)[None, :]\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), floatx), in_specs=[pl.BlockSpec((), lambda _, __: ()), pl.BlockSpec((bx, 1), lambda i, _: (i, 0)), pl.BlockSpec((1, by), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bx, by), lambda i, j: (i, j)), grid=(pl.cdiv(x.shape[0], bx), pl.cdiv(y.shape[1], by)))\n    def f(branch_ref, x_ref, y_ref, o_ref):\n        o_ref[...] = lax.switch(branch_ref[...], (lambda x, y: (x - y) ** 2, lambda x, y: -jnp.abs(x - y), lambda x, y: jnp.sqrt(jnp.abs(x - y))), x_ref[...], y_ref[...])\n    np.testing.assert_allclose(f(jnp.int32(0), x, y), (x - y) ** 2)\n    np.testing.assert_allclose(f(jnp.int32(1), x, y), -jnp.abs(x - y))\n    np.testing.assert_allclose(f(jnp.int32(2), x, y), jnp.sqrt(jnp.abs(x - y)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_conditional_write(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    arg = jnp.arange(8, dtype=jnp.float32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(arg.shape, jnp.float32))\n    def f(branch_ref, x_ref, out_ref):\n        out_ref[...] = -x_ref[...]\n\n        def if_true(z):\n            out_ref[4] = z\n            return ()\n        jax.lax.cond(branch_ref[...], if_true, lambda z: (), x_ref[6])\n    np.testing.assert_allclose(f(jnp.bool_(True), arg), jnp.float32([0.0, -1, -2, -3, 6, -5, -6, -7]))\n    np.testing.assert_allclose(f(jnp.bool_(False), arg), -arg)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        _ = jax.grad(lambda x: jnp.sum(f(jnp.bool_(True), x) ** 2))(arg)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_scan_cond_vm_explicit_ref_arg(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i]\n            state = jax.lax.switch(opcode, (lambda state, params, i: state + params[i, 0] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x, lambda state, params, i: state + params[i, 2] * 2.0 ** i * x, lambda state, params, i: state + params[i, 1] * 2.0 ** i * x), state, params_ref, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_scan_cond_vm_closing_over_ref(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n    program = jnp.int32([0, 1, 2, 3, 2, -1])\n    params = jnp.arange(len(program) * 3.0, dtype=jnp.float32)\n    params = params.reshape(len(program), 3)\n    x = jnp.arange(7.0, dtype=jnp.float32)\n    bx = 4\n\n    @jax.jit\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((x.shape[0],), jnp.float32), in_specs=[pl.BlockSpec(program.shape, lambda _: (0,)), pl.BlockSpec(params.shape, lambda _: (0, 0)), pl.BlockSpec((bx,), lambda i: (i,))], out_specs=pl.BlockSpec((bx,), lambda i: (i,)), grid=pl.cdiv(x.shape[0], bx))\n    def f(program_ref, params_ref, x_ref, out_ref):\n        x = x_ref[...]\n\n        def body_fn(i, args):\n            state, program_ref, params_ref = args\n            opcode = program_ref[i] + 1\n            state = jax.lax.switch(opcode, (lambda state, *_: state, lambda state, i: state + params_ref[i, 0] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 2] * 2.0 ** i * x, lambda state, i: state + params_ref[i, 1] * 2.0 ** i * x), state, i)\n            return (state, program_ref, params_ref)\n        out_ref[...] = jax.lax.fori_loop(0, len(program), body_fn, (jnp.zeros(x.shape, dtype=jnp.float32), program_ref, params_ref))[0]\n    expected = x * params[0, 0] + 2 * x * params[1, 1] + 4 * x * params[2, 2] + 8 * x * params[3, 1] + 16 * x * params[4, 2]\n    np.testing.assert_allclose(f(program, params, x), expected)\n    with self.assertRaisesRegex(ValueError, 'Linearization failed'):\n        jax.value_and_grad(lambda params, x: f(program, params, x).sum())(params, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_fori_loop_simple(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += 1\n        lax.fori_loop(0, 5, body, None)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_fori_loop_with_nonzero_lower_bound(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n        def body(i, _):\n            y_ref[...] += i\n        lax.fori_loop(2, 5, body, None)\n    y = f(6)\n    self.assertEqual(y, 6 + 2 + 3 + 4)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_fori_loop_accumulates(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + 1\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_fori_loop_accumulates_with_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n\n        def body(i, acc):\n            return acc + i\n        acc = lax.fori_loop(0, 5, body, 0)\n        y_ref[...] = acc\n    y = f(0)\n    self.assertEqual(y, 10)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_fori_loop_with_writing_to_index(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.int32))\n    def f(y_ref):\n\n        def body(i, _):\n            y_ref[i] = i\n        lax.fori_loop(0, y_ref.shape[0], body, None)\n    y = f()\n    np.testing.assert_allclose(y, jnp.arange(8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_fori_loop_with_dynamic_indices(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(lb_ref, ub_ref, y_ref):\n        y_ref[...] = 0\n\n        def body(i, a):\n            y_ref[...] += i\n            return a\n        lax.fori_loop(lb_ref[...], ub_ref[...], body, 1)\n    y = f(2, 5)\n    np.testing.assert_allclose(y, 2 + 3 + 4)\n    y = f(1, 8)\n    np.testing.assert_allclose(y, sum(range(1, 8)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_simple_while(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = 0\n\n        def cond(x):\n            return x < 5\n\n        def body(x):\n            y_ref[...] += 1\n            return x + 1\n        lax.while_loop(cond, body, x)\n    y = f(0)\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_simple_while_with_only_values(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(y_ref):\n\n        def cond(acc):\n            return acc < 5\n\n        def body(acc):\n            acc += 1\n            return acc\n        acc = lax.while_loop(cond, body, 0)\n        y_ref[...] = acc\n    y = f()\n    self.assertEqual(y, 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_while_with_dynamic_condition(self):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('TODO: error on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.int32))\n    def f(i_ref, y_ref):\n        y_ref[...] = 0\n        n_iter = i_ref[...]\n\n        def cond(i):\n            return i < n_iter\n\n        def body(i):\n            y_ref[...] += 1\n            return i + 1\n        _ = lax.while_loop(cond, body, 0)\n    self.assertEqual(f(1), 1)\n    self.assertEqual(f(4), 4)\n    self.assertEqual(f(100), 100)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}_gm_{group_size_m}', m, n, k, dtype, block_size_m, block_size_n, block_size_k, group_size_m) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] for group_size_m in [8] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul(self, m, n, k, dtype, bm, bn, bk, gm):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul(x, y, bm=bm, bn=bn, bk=bk, gm=gm, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['bm', 'bn', 'gm', 'bk', 'interpret', 'debug'])\ndef matmul(x, y, *, bm, bn, gm, bk, interpret, debug=False):\n    m, n, k = (x.shape[0], y.shape[1], x.shape[1])\n\n    @functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), interpret=interpret, debug=debug, grid=pl.cdiv(m, bm) * pl.cdiv(n, bn))\n    def matmul_kernel(x_ref, y_ref, o_ref):\n        pid = pl.program_id(axis=0).astype(intx)\n        num_pid_m = m // bm\n        num_pid_n = n // bn\n        num_pid_in_group = gm * num_pid_n\n        group_id = lax.div(pid, num_pid_in_group)\n        first_pid_m = group_id * gm\n        group_size_m = jnp.minimum(num_pid_m - first_pid_m, gm)\n        pid_m = first_pid_m + lax.rem(pid, group_size_m)\n        pid_n = lax.div(lax.rem(pid, num_pid_in_group), group_size_m)\n        idx_m = pid_m * bm + jnp.arange(bm)\n        idx_n = pid_n * bn + jnp.arange(bn)\n        idx_m = pl.max_contiguous(pl.multiple_of(idx_m, bm), bm)\n        idx_n = pl.max_contiguous(pl.multiple_of(idx_n, bn), bn)\n        acc = jnp.zeros((bm, bn), dtype=jnp.float32)\n\n        def body(i, acc_ref):\n            idx_k = i * bk + jnp.arange(bk)\n            x_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bk), (0,)), jax.lax.broadcast_in_dim(idx_k, (bm, bk), (1,)))\n            y_idx = (jax.lax.broadcast_in_dim(idx_k, (bk, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bk, bn), (1,)))\n            x_block, y_block = (x_ref[x_idx], y_ref[y_idx])\n            out = pl.dot(x_block, y_block)\n            acc_ref[:, :] += out\n        acc = for_loop(k // bk, body, acc).astype(o_ref.dtype)\n        o_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bm, bn), (1,)))\n        o_ref[o_idx] = acc\n    return matmul_kernel(x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}', m, n, k, dtype, block_size_m, block_size_n, block_size_k) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul_block_spec(self, m, n, k, dtype, bm, bn, bk):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul_block_spec(x, y, bm=bm, bn=bn, bk=bk, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['bm', 'bn', 'gm', 'bk', 'interpret', 'debug'])\ndef matmul(x, y, *, bm, bn, gm, bk, interpret, debug=False):\n    m, n, k = (x.shape[0], y.shape[1], x.shape[1])\n\n    @functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), interpret=interpret, debug=debug, grid=pl.cdiv(m, bm) * pl.cdiv(n, bn))\n    def matmul_kernel(x_ref, y_ref, o_ref):\n        pid = pl.program_id(axis=0).astype(intx)\n        num_pid_m = m // bm\n        num_pid_n = n // bn\n        num_pid_in_group = gm * num_pid_n\n        group_id = lax.div(pid, num_pid_in_group)\n        first_pid_m = group_id * gm\n        group_size_m = jnp.minimum(num_pid_m - first_pid_m, gm)\n        pid_m = first_pid_m + lax.rem(pid, group_size_m)\n        pid_n = lax.div(lax.rem(pid, num_pid_in_group), group_size_m)\n        idx_m = pid_m * bm + jnp.arange(bm)\n        idx_n = pid_n * bn + jnp.arange(bn)\n        idx_m = pl.max_contiguous(pl.multiple_of(idx_m, bm), bm)\n        idx_n = pl.max_contiguous(pl.multiple_of(idx_n, bn), bn)\n        acc = jnp.zeros((bm, bn), dtype=jnp.float32)\n\n        def body(i, acc_ref):\n            idx_k = i * bk + jnp.arange(bk)\n            x_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bk), (0,)), jax.lax.broadcast_in_dim(idx_k, (bm, bk), (1,)))\n            y_idx = (jax.lax.broadcast_in_dim(idx_k, (bk, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bk, bn), (1,)))\n            x_block, y_block = (x_ref[x_idx], y_ref[y_idx])\n            out = pl.dot(x_block, y_block)\n            acc_ref[:, :] += out\n        acc = for_loop(k // bk, body, acc).astype(o_ref.dtype)\n        o_idx = (jax.lax.broadcast_in_dim(idx_m, (bm, bn), (0,)), jax.lax.broadcast_in_dim(idx_n, (bm, bn), (1,)))\n        o_ref[o_idx] = acc\n    return matmul_kernel(x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'm_{m}_n_{n}_k_{k}_dtype_{dtype}_bm_{block_size_m}_bn_{block_size_n}_bk_{block_size_k}', m, n, k, dtype, block_size_m, block_size_n, block_size_k) for m in [512, 1024] for k in [512] for n in [512, 1024] for dtype in ['float32', 'float16'] for block_size_m in [64, 128] for block_size_n in [64, 128] for block_size_k in [32] if block_size_m <= m and block_size_n <= n and (block_size_k <= k)])\ndef test_matmul_block_spec(self, m, n, k, dtype, bm, bn, bk):\n    if jtu.test_device_matches(['tpu']) and (not self.INTERPRET):\n        self.skipTest('On TPU the test works only in interpret mode')\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, (m, k), dtype=dtype)\n    y = random.normal(k2, (k, n), dtype=dtype)\n    out = matmul_block_spec(x, y, bm=bm, bn=bn, bk=bk, interpret=self.INTERPRET)\n    expected = jnp.matmul(x, y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out, expected, atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['bm', 'bn', 'bk', 'interpret', 'debug'])\ndef matmul_block_spec(x, y, *, bm, bn, bk, interpret, debug=False):\n    m, n, k = (x.shape[0], y.shape[1], x.shape[1])\n\n    @functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), interpret=interpret, debug=debug, in_specs=[pl.BlockSpec((bm, x.shape[1]), lambda i, _: (i, 0)), pl.BlockSpec((y.shape[0], bn), lambda _, j: (0, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j: (i, j)), grid=(pl.cdiv(m, bm), pl.cdiv(n, bn)))\n    def matmul_kernel(x_ref, y_ref, o_ref):\n        acc = jnp.zeros(o_ref.shape, dtype=jnp.float32)\n\n        def body(i, acc_ref):\n            x_block = pl.load(x_ref, (slice(None), pl.ds(i * bk, bk)))\n            y_block = pl.load(y_ref, (pl.ds(i * bk, bk), slice(None)))\n            acc_ref[:, :] += pl.dot(x_block, y_block)\n        acc = for_loop(k // bk, body, acc).astype(o_ref.dtype)\n        o_ref[:, :] = acc\n    return matmul_kernel(x, y)"
  }
]