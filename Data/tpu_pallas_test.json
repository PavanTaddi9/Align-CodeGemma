[
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_dynamic_dma_on_2nd_minor(self):\n\n    def kernel(array, data, index, size, _, sem):\n        pltpu.async_copy(data.at[pl.ds(0, size[0])], array.at[pl.ds(index[0], size[0])], sem).wait()\n\n    def run(array, data, index, size):\n        return pl.pallas_call(kernel, out_shape=array, in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.VMEM), pl.BlockSpec(memory_space=pltpu.SMEM), pl.BlockSpec(memory_space=pltpu.SMEM)], scratch_shapes=[pltpu.SemaphoreType.DMA], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), input_output_aliases={0: 0})(array, data, index, size)\n    array = jnp.zeros((1024, 128), jnp.int32)\n    data = jnp.ones((8, 128), jnp.int32)\n    index = jnp.array([3], jnp.int32)\n    size = jnp.array([5], jnp.int32)\n    expected = array.at[index[0]:index[0] + size[0]].set(data[index[0]:index[0] + size[0]])\n    result = run(array, data, index, size)\n    np.testing.assert_array_equal(result, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_simple_tile_aligned_dynamic_size_dma(self):\n\n    def kernel(size_smem_ref, x_hbm_ref, _, o_hbm_ref, sem):\n        size = size_smem_ref[0]\n        pltpu.async_copy(x_hbm_ref.at[pl.ds(0, size)], o_hbm_ref.at[pl.ds(0, size)], sem).wait()\n    x = jnp.tile(jnp.arange(8, dtype=jnp.int32)[:, None, None], [1, 8, 128])\n    o = jnp.zeros((8, 8, 128), dtype=jnp.int32)\n    size = jnp.array([4], dtype=jnp.int32)\n    out = pl.pallas_call(kernel, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM), pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), scratch_shapes=[pltpu.SemaphoreType.DMA]), out_shape=o, input_output_aliases={2: 0})(size, x, o)\n    expected = o.at[:4].set(x.at[:4].get())\n    np.testing.assert_array_equal(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_simple_dynamic_size_dma(self):\n    self.skipTest(\"doesn't work yet.\")\n\n    def kernel(size_smem_ref, x_hbm_ref, _, o_hbm_ref, sem):\n        size = size_smem_ref[0]\n        pltpu.async_copy(x_hbm_ref.at[pl.ds(0, size)], o_hbm_ref.at[pl.ds(0, size)], sem).wait()\n    x = jnp.arange(8, dtype=jnp.int32)\n    o = jnp.zeros(8, dtype=jnp.int32)\n    size = jnp.array([4], dtype=jnp.int32)\n    out = pl.pallas_call(kernel, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM), pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), scratch_shapes=[pltpu.SemaphoreType.DMA]), out_shape=o, input_output_aliases={2: 0})(size, x, o)\n    expected = o.at[:4].set(x.at[:4].get())\n    np.testing.assert_array_equal(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_mixed_strides(self):\n    x = np.zeros((8, 128), dtype=jnp.float32)\n    y = np.zeros((8, 2, 128), dtype=jnp.bfloat16)\n\n    def kernel(x_ref, y_ref, out_ref):\n        out_ref[:, :] = x_ref[:, :] + y_ref[:, 1, :].astype(jnp.float32)\n    out = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32))(x, y)\n    np.testing.assert_array_equal(out, np.zeros((8, 128), dtype=jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_sum(self):\n    x = np.zeros((8, 2, 8, 128), dtype=jnp.float32)\n\n    def kernel(x_ref, out_ref):\n        out_ref[:, :, :] = jnp.sum(x_ref[:, :, :, :], 2)\n    out = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 2, 128), jnp.float32))(x)\n    np.testing.assert_array_equal(out, np.zeros((8, 2, 128), dtype=jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@only_passes_in_interpret()\ndef test_transpose(self):\n    \"\"\"b/356475128\"\"\"\n    x = np.zeros((8, 2, 8, 128), dtype=jnp.float32)\n\n    def kernel(x_ref, out_ref):\n        out_ref[:, :, :, :] = jnp.transpose(x_ref[:, :, :, :], (0, 2, 1, 3))\n    out = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 8, 2, 128), jnp.float32))(x)\n    np.testing.assert_array_equal(out, np.zeros((8, 8, 2, 128), dtype=jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(i, _):\n    return (i + 1, jnp.sum(x[:i + 1]))"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(state):\n    it, x, fx, dfx = state\n    step = jnp.linalg.solve(dfx.reshape((-1, fx.size)), fx.ravel()).reshape(fx.shape)\n    x_next = x - step\n    fx, dfx = (func(x_next), jax.jacobian(func)(x_next))\n    return (it + 1, x_next, fx, dfx)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(carry, x):\n    return (carry, jnp.dot(x, x))"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(i, refs):\n    x_ref, y_ref = refs\n    y_ref[i] = s * x_ref[i] * jnp.cos(s)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_megacore_splitting(self):\n\n    def matmul_kernel(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros_like(z_ref)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n    k1, k2 = jax.random.split(jax.random.key(0))\n    x = jax.random.uniform(k1, (3, 3, 512, 512))\n    y = jax.random.uniform(k2, (3, 3, 512, 512))\n    z = jax.vmap(jax.vmap(pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), grid=(4, 4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j, k: (i, k)), pl.BlockSpec((128, 128), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j, k: (i, j)), debug=True)))(x, y)\n    np.testing.assert_allclose(z, jax.vmap(jax.vmap(jnp.dot))(x, y), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "def test_vector_bool_masking_with_indexing(self):\n\n    def kernel(mask_ref, true_ref, false_ref, o_ref):\n        o_ref[0, ...] = jnp.where(mask_ref[0, ...], true_ref[0, ...], false_ref[0, ...])\n    key = jax.random.key(0)\n    k1, k2, k3 = jax.random.split(key, 3)\n    values_1 = jax.random.normal(k1, (1, 256, 256), jnp.float32)\n    values_2 = jax.random.normal(k2, (1, 256, 256), jnp.float32)\n    mask = jax.random.bernoulli(k3, p=0.5, shape=(1, 256, 256))\n    output_shape = jax.ShapeDtypeStruct((1, 256, 256), jnp.float32)\n    result = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.VMEM), pl.BlockSpec(memory_space=pltpu.VMEM), pl.BlockSpec(memory_space=pltpu.VMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.VMEM), out_shape=output_shape)(mask, values_1, values_2)\n    expected = jnp.where(mask, values_1, values_2)\n    np.testing.assert_array_equal(result, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.named_scope('scan_body')\ndef body(carry, x):\n    return (carry * x, carry + x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "def test_with_unhashable_grid_spec(self):\n\n    @functools.partial(self.pallas_call, out_shape=[[jax.ShapeDtypeStruct((8, 128), np.int32)]], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(1,), in_specs=[[pl.BlockSpec((8, 128), lambda i, s_ref: (0, 0))]], out_specs=[[pl.BlockSpec((8, 128), lambda i, s_ref: (0, 0))]], scratch_shapes=[[pltpu.SemaphoreType.REGULAR((3,))]]))\n    def kernel(s_ref, x_ref, o_ref, scratch_ref):\n        assert isinstance(s_ref, list)\n        assert isinstance(x_ref, list)\n        assert isinstance(o_ref, list)\n        assert isinstance(scratch_ref, list)\n        o_ref[0][...] = x_ref[0][...]\n    x_shape = (8, 128)\n    s = np.array([0, 1], np.int32)\n    x = np.arange(math.prod(x_shape), dtype=np.int32).reshape(x_shape)\n    res = kernel([s, s], [x])\n    self.assertIsInstance(res, tuple)\n    self.assertAllClose(res[0][0], x)",
    "assertions": [
      "assert isinstance(s_ref, list)",
      "assert isinstance(x_ref, list)",
      "assert isinstance(o_ref, list)",
      "assert isinstance(scratch_ref, list)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "def test_nontrivial_vmap_scalar_prefetch(self):\n\n    def body(_, x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    s = jnp.array([4, 3, 2, 5, 3, 5, 2, 7], jnp.int32)\n    x = jnp.arange(2 * 8 * 8 * 128, dtype=jnp.int32).reshape((2, 8 * 8, 128))\n\n    def _x_transform(i, s_ref):\n        s = pl.load(s_ref, (i,))\n        return (s, 0)\n    s = jnp.tile(s[None], [2, 1])\n\n    @jax.jit\n    @jax.vmap\n    def kernel(s, x):\n        return self.pallas_call(body, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((x.shape[0] // 8, x.shape[1]), _x_transform)], out_specs=pl.BlockSpec((x.shape[0] // 8, x.shape[1]), lambda i, _: (i, 0)), grid=8), compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[False, True]))(s, x)\n    first = x[0, ...].reshape((1, 8, 8, -1))[:, s[0, ...]].reshape(x.shape[1:])\n    second = x[1, ...].reshape((1, 8, 8, -1))[:, s[1, ...]].reshape(x.shape[1:])\n    expected = jnp.stack([first, second])\n    np.testing.assert_allclose(kernel(s, x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "def test_interpret_local_dma(self):\n\n    def test_kernel(x_ref, o_ref, sem_out_ref, copy_sem):\n        o_ref[...] = jnp.zeros_like(o_ref[...])\n        input_to_output_copy = pltpu.make_async_copy(src_ref=x_ref.at[0:8], dst_ref=o_ref.at[0:8], sem=copy_sem.at[0])\n        input_to_output_copy.start()\n        sem_out_ref[0, :] = jnp.ones_like(sem_out_ref[0, :]) * pltpu.semaphore_read(copy_sem.at[0])\n        input_to_output_copy.wait()\n        sem_out_ref[1, :] = jnp.ones_like(sem_out_ref[0, :]) * pltpu.semaphore_read(copy_sem.at[0])\n    out_shape = (jax.ShapeDtypeStruct((16, 128), jnp.int32), jax.ShapeDtypeStruct((2, 1), jnp.int32))\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], scratch_shapes=[pltpu.SemaphoreType.DMA(2)])\n    kernel = pl.pallas_call(test_kernel, out_shape=out_shape, grid_spec=grid_spec, interpret=True)\n    x = jax.random.randint(jax.random.key(0), shape=(16, 128), minval=0, maxval=128)\n    result, semaphores = kernel(x)\n    np.testing.assert_array_equal(result[0:8], x[0:8])\n    np.testing.assert_array_equal(result[8:], jnp.zeros_like(result[8:]))\n    result_sem_pre_wait = semaphores[0, 0]\n    np.testing.assert_array_equal(result_sem_pre_wait, result[0:8].size)\n    result_sem_post_wait = semaphores[1, 0]\n    np.testing.assert_array_equal(result_sem_post_wait, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(i, xy):\n    x, y = xy\n    y = y + jax.grad(lambda z: jnp.sum(jnp.maximum(z, 0.0)))(x)\n    return (x, y)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(state):\n    it, x, fx, dfx = state\n    step = jnp.linalg.solve(dfx.reshape((-1, fx.size)), fx.ravel()).reshape(fx.shape)\n    x_next = x - step\n    fx, dfx = (func(x_next), jax.jacobian(func)(x_next))\n    return (it + 1, x_next, fx, dfx)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_dynamic_grid_has_dynamic_size(self):\n\n    def kernel(_):\n        num_programs = pl.num_programs(0)\n        self.assertIsInstance(num_programs, int, msg=type(num_programs))\n        self.assertEqual(num_programs, 2)\n        num_programs = pl.num_programs(1)\n        self.assertIsInstance(num_programs, jax.Array)\n\n    @jax.jit\n    def outer(x):\n        self.pallas_call(kernel, out_shape=None, grid=(2, x))()\n    outer(2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def outer(x, scale_f32):\n    scale = jax.lax.convert_element_type(scale_f32, sc32)\n\n    def body_fun(carry, _):\n        carry = inner(carry, scale)\n        return (carry, None)\n    x, _ = jax.lax.scan(body_fun, x, None, length=3)\n    return x"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@staticmethod\ndef add(dt, x, y):\n    fromscale = partial(jax.lax.convert_element_type, new_dtype=dt.float_dtype)\n    toscale = partial(jax.lax.convert_element_type, new_dtype=dt)\n    return toscale(jax.lax.max(fromscale(x), fromscale(y)))"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(i, xy):\n    x, y = xy\n    y = y + jax.grad(lambda z: jnp.sum(jnp.maximum(z, 0.0)))(x)\n    return (x, y)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(init, xs):\n    return jax.lax.scan(f, init=init, xs=xs)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(x):\n    return jax.pure_callback(_callback, x, x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.bfloat16, jnp.float32])\ndef test_pltpu_repeat(self, dtype):\n\n    def test_kernel(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = pltpu.repeat(x, 2, axis=1)\n\n    @jax.jit\n    def test(x: jax.Array) -> jax.Array:\n        return pl.pallas_call(test_kernel, out_shape=jax.ShapeDtypeStruct([x.shape[0], x.shape[1] * 2], x.dtype))(x)\n    x = jnp.arange(2048, dtype=dtype).reshape((8, 256))\n    y = test(x)\n    np.testing.assert_array_equal(y, jnp.concatenate([x, x], axis=1))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def test(x):\n    val, vec = jnp.linalg.eigh(x)\n    return jnp.real(jnp.sum(val))"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(refs):\n    x_ref, y_ref, z_ref = refs\n    a = jnp.array(2.0)\n\n    def true_fun():\n        b = jnp.array(2.0)\n\n        def inner_true_fun():\n            x_ref[()] = 1.0 * a\n            y_ref[()] = 1.0\n\n        def inner_false_fun():\n            z_ref[()] = 1.0\n            x_ref[()] = 2.0\n            x_ref[()] = 2.0 * b\n        lax.cond(pred2, inner_true_fun, inner_false_fun)\n\n    def false_fun():\n        a = jnp.array(2.0)\n        b = jnp.array(2.0)\n\n        def inner_true_fun():\n            x_ref[()] = 1.0 * a\n            z_ref[()] = 4.0\n\n        def inner_false_fun():\n            x_ref[()] = 2.0 * b\n        lax.cond(pred2, inner_true_fun, inner_false_fun)\n    lax.cond(pred1, true_fun, false_fun)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(carry, x):\n    out_tpu = x + carry\n    return (carry, jax.device_put(out_tpu, NamedSharding(mesh, P('y', 'z'), memory_kind='pinned_host')))"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_dynamic_grid_has_dynamic_size(self):\n\n    def kernel(_):\n        num_programs = pl.num_programs(0)\n        self.assertIsInstance(num_programs, int, msg=type(num_programs))\n        self.assertEqual(num_programs, 2)\n        num_programs = pl.num_programs(1)\n        self.assertIsInstance(num_programs, jax.Array)\n\n    @jax.jit\n    def outer(x):\n        self.pallas_call(kernel, out_shape=None, grid=(2, x))()\n    outer(2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef outer(x):\n    x = exp_inner.call(x)\n    return inner(x)"
  },
  {
    "test_code": "@parameterized.parameters((pl.Buffered(2), pl.Buffered(2)), (pl.Buffered(2), pl.Buffered(1)), (pl.Buffered(1), pl.Buffered(1)))\ndef test_two_input_vadd(self, x_pmode: pl.Buffered, y_pmode: pl.Buffered):\n    if not jtu.if_cloud_tpu_at_least(2025, 2, 11):\n        self.skipTest('Needs a newer libTPU')\n\n    def body(x_ref, y_ref, o_ref):\n        x = x_ref[:]\n        y = y_ref[:]\n        o_ref[:] = x + y\n    size_in_vregs = 128\n    data_size = size_in_vregs * 1024\n    block_size = 1024\n    x = jnp.arange(data_size, dtype=jnp.float32)\n    y = jnp.arange(data_size, dtype=jnp.float32)\n    in_specs = [pl.BlockSpec((block_size,), lambda i: i, pipeline_mode=pmode) for pmode in [x_pmode, y_pmode]]\n    out_specs = pl.BlockSpec((block_size,), lambda i: i)\n\n    @jax.jit\n    def vadd(x, y):\n        return self.pallas_call(body, out_shape=jax.ShapeDtypeStruct(x.shape, jnp.float32), in_specs=in_specs, out_specs=out_specs, grid=data_size // block_size)(x, y)\n    compiled = vadd.lower(jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct(y.shape, y.dtype)).compile().as_text()\n    pattern = '\"used_scoped_memory_configs\":\\\\[\\\\{\"memory_space\":\"1\",.*?\"size\":\"(\\\\d+)\"'\n    expected_vmem_usage = block_size * 4 * (2 + x_pmode.buffer_count + y_pmode.buffer_count)\n    vmem_usage = int(re.search(pattern, compiled).group(1))\n    self.assertEqual(vmem_usage, expected_vmem_usage)\n    z = vadd(x, y)\n    np.testing.assert_allclose(z, x + y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_cost_analysis(self):\n\n    def kernel(x, y):\n        y[:] = x[:]\n    x = jnp.arange(1024.0).reshape(8, 128)\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), cost_estimate=pl.CostEstimate(flops=1234, transcendentals=21, bytes_accessed=12345))\n    analysis_result = jax.jit(f).lower(x).compile().cost_analysis()\n    self.assertEqual(analysis_result['flops'], 1234)\n    self.assertEqual(analysis_result['transcendentals'], 21)\n    self.assertEqual(analysis_result['bytes accessed'], 12345)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_cost_analysis_vmap(self):\n\n    def kernel(x, y):\n        y[:] = x[:]\n    batch_size = 3\n    x = jnp.arange(batch_size * 1024.0).reshape(batch_size, 8, 128)\n    f = pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), cost_estimate=pl.CostEstimate(flops=1234, transcendentals=21, bytes_accessed=12345))\n    f = jax.vmap(f)\n    analysis_result = jax.jit(f).lower(x).compile().cost_analysis()\n    self.assertEqual(analysis_result['flops'], batch_size * 1234)\n    self.assertEqual(analysis_result['transcendentals'], batch_size * 21)\n    self.assertEqual(analysis_result['bytes accessed'], batch_size * 12345)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_mlir_location(self):\n    args = (jax.ShapeDtypeStruct((8, 128), jnp.float32),)\n    f = example_kernel.double\n    as_tpu_kernel = mosaic.as_tpu_kernel\n\n    def capture_as_tpu_kernel(module, *args, **kwargs):\n        asm = module.operation.get_asm(enable_debug_info=True)\n        self.assertIn('example_kernel.py\":25', asm)\n        return as_tpu_kernel(module, *args, **kwargs)\n    mosaic.as_tpu_kernel = capture_as_tpu_kernel\n    try:\n        jax.jit(f).lower(*args)\n    finally:\n        mosaic.as_tpu_kernel = as_tpu_kernel",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_debug_print(self):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('It works!')\n    x = jnp.arange(8 * 128, dtype=jnp.float32).reshape((8, 128))\n    compiled_kernel = jax.jit(kernel).lower(x).compile({'xla_tpu_enable_log_recorder': 'true'})\n    with jtu.capture_stderr() as get_output:\n        jax.block_until_ready(compiled_kernel(x))\n    self.assertIn('It works!', get_output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_debug_print_with_values(self):\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=pltpu.SMEM),), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('x[0] == {}', x_ref[0])\n    x = jnp.array([42, 24]).astype(jnp.int32)\n    compiled_kernel = jax.jit(kernel).lower(x).compile({'xla_tpu_enable_log_recorder': 'true'})\n    with jtu.capture_stderr() as get_output:\n        jax.block_until_ready(compiled_kernel(x))\n    self.assertIn('x[0] == 42', get_output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{'_'.join(map(str, shape))}_{dtype.__name__}', shape, dtype) for shape in ((2, 8, 128), (3,), (3, 4), (2, 3, 4), (2, 9, 129)) for dtype in (jnp.int32, jnp.uint32, jnp.float32)))\ndef test_debug_print_vector(self, shape, dtype):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('{}', x_ref[...])\n        o_ref[...] = x_ref[...]\n    n = np.prod(shape)\n    x = jnp.arange(n, dtype=dtype).reshape(shape)\n    compiled_kernel = jax.jit(kernel).lower(x).compile({'xla_tpu_enable_log_recorder': 'true'})\n    with jtu.capture_stderr() as get_output:\n        jax.block_until_ready(compiled_kernel(x))\n    output = get_output()\n    numbers = [int(num) for line in output.splitlines() if (match := re.search('\\\\{(.*)', line)) for num in re.findall('\\\\d+', match.group(1))]\n    self.assertLen(numbers, n)\n    self.assertTrue(all((num == i for i, num in enumerate(numbers))))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.bfloat16, jnp.float32])\ndef test_pltpu_repeat(self, dtype):\n\n    def test_kernel(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = pltpu.repeat(x, 2, axis=1)\n\n    @jax.jit\n    def test(x: jax.Array) -> jax.Array:\n        return pl.pallas_call(test_kernel, out_shape=jax.ShapeDtypeStruct([x.shape[0], x.shape[1] * 2], x.dtype))(x)\n    x = jnp.arange(2048, dtype=dtype).reshape((8, 256))\n    y = test(x)\n    np.testing.assert_array_equal(y, jnp.concatenate([x, x], axis=1))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def test(x):\n    return jax.lax.psum(x, axis_name='batch')"
  },
  {
    "test_code": "def test_dynamic_grid_has_dynamic_size(self):\n\n    def kernel(_):\n        num_programs = pl.num_programs(0)\n        self.assertIsInstance(num_programs, int, msg=type(num_programs))\n        self.assertEqual(num_programs, 2)\n        num_programs = pl.num_programs(1)\n        self.assertIsInstance(num_programs, jax.Array)\n\n    @jax.jit\n    def outer(x):\n        self.pallas_call(kernel, out_shape=None, grid=(2, x))()\n    outer(2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def outer(params):\n    meta_params = jnp.array(4.0)\n    return jax.grad(inner)(meta_params, params)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def add(self, x: jax.Array) -> jax.Array:\n    self.value += np.asarray(x)\n    return jax.device_put(self.value, x.sharding)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(carry, x):\n    checkify.check(jnp.all(x > 0), 'should be positive')\n    return (carry, x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_large_array_indexing(self):\n    n = 6\n    dtype = jnp.bfloat16\n    gc.collect()\n    x = jax.lax.broadcasted_iota(dtype, (n, 1024 * 1024, 512), 0)\n\n    def kernel(index, x, y, sem):\n        pltpu.async_copy(x.at[index[0]], y.at[:], sem).wait()\n    run = self.pallas_call(kernel, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=pl.BlockSpec(memory_space=pl.ANY), scratch_shapes=[pltpu.SemaphoreType.DMA]), out_shape=jax.ShapeDtypeStruct(x.shape[1:], dtype))\n    for i in range(x.shape[0]):\n        y = run(jnp.array([i], dtype=jnp.int32), x)\n        np.testing.assert_array_equal(y, i)\n        del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def run(pos):\n    maxiter = 1000\n\n    def cond(v):\n        return v[0] < maxiter\n\n    def step(v):\n        i, pos = v\n        jax.debug.callback(print_it, i + 1, maxiter)\n        return (i + 1, pos + 1)\n    val = (jnp.array(0), pos)\n    val = jax.lax.while_loop(cond, step, val)\n    return val[1]"
  },
  {
    "test_code": "def test_dynamic_dma_on_2nd_minor(self):\n\n    def kernel(array, data, index, size, _, sem):\n        pltpu.async_copy(data.at[pl.ds(0, size[0])], array.at[pl.ds(index[0], size[0])], sem).wait()\n\n    def run(array, data, index, size):\n        return pl.pallas_call(kernel, out_shape=array, in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.VMEM), pl.BlockSpec(memory_space=pltpu.SMEM), pl.BlockSpec(memory_space=pltpu.SMEM)], scratch_shapes=[pltpu.SemaphoreType.DMA], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), input_output_aliases={0: 0})(array, data, index, size)\n    array = jnp.zeros((1024, 128), jnp.int32)\n    data = jnp.ones((8, 128), jnp.int32)\n    index = jnp.array([3], jnp.int32)\n    size = jnp.array([5], jnp.int32)\n    expected = array.at[index[0]:index[0] + size[0]].set(data[index[0]:index[0] + size[0]])\n    result = run(array, data, index, size)\n    np.testing.assert_array_equal(result, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def run(pos):\n    maxiter = 1000\n\n    def cond(v):\n        return v[0] < maxiter\n\n    def step(v):\n        i, pos = v\n        jax.debug.callback(print_it, i + 1, maxiter)\n        return (i + 1, pos + 1)\n    val = (jnp.array(0), pos)\n    val = jax.lax.while_loop(cond, step, val)\n    return val[1]"
  },
  {
    "test_code": "def test_mixed_precision_dot(self):\n    if not jtu.if_cloud_tpu_at_least(2025, 2, 27):\n        self.skipTest('Needs a newer libTPU')\n    if not jtu.is_device_tpu_at_least(5):\n        self.skipTest('float8_e4m3b11fnuz not supported on TPU generations <= 4')\n\n    def kernel(x_ref, w_ref, o_ref):\n        o_ref[:] = jax.lax.dot_general(x_ref[:], w_ref[:], dimension_numbers=(((1,), (0,)), ((), ())), preferred_element_type=jnp.float32)\n    x = jnp.ones((64, 128), dtype=jnp.bfloat16)\n    w = jnp.full((128, 128), jnp.nan, jnp.float8_e4m3b11fnuz)\n    run = pl.pallas_call(kernel, jax.ShapeDtypeStruct((64, 128), jnp.float32))\n    run = jax.named_call(run, name='run')\n    run = jax.jit(run)\n    expected = jax.lax.dot_general(x, w, dimension_numbers=(((1,), (0,)), ((), ())), preferred_element_type=jnp.float32)\n    jax_nans = jnp.isnan(expected).sum()\n    mosaic_nans = jnp.isnan(run(x, w)).sum()\n    self.assertEqual(jax_nans, mosaic_nans)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def run(pos):\n    maxiter = 1000\n\n    def cond(v):\n        return v[0] < maxiter\n\n    def step(v):\n        i, pos = v\n        jax.debug.callback(print_it, i + 1, maxiter)\n        return (i + 1, pos + 1)\n    val = (jnp.array(0), pos)\n    val = jax.lax.while_loop(cond, step, val)\n    return val[1]"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_large_array_indexing(self):\n    n = 6\n    dtype = jnp.bfloat16\n    gc.collect()\n    x = jax.lax.broadcasted_iota(dtype, (n, 1024 * 1024, 512), 0)\n\n    def kernel(index, x, y, sem):\n        pltpu.async_copy(x.at[index[0]], y.at[:], sem).wait()\n    run = self.pallas_call(kernel, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=pl.BlockSpec(memory_space=pl.ANY), scratch_shapes=[pltpu.SemaphoreType.DMA]), out_shape=jax.ShapeDtypeStruct(x.shape[1:], dtype))\n    for i in range(x.shape[0]):\n        y = run(jnp.array([i], dtype=jnp.int32), x)\n        np.testing.assert_array_equal(y, i)\n        del y",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def run(primal_ins, cotangent_outs):\n    primal_outs, vjp = jax.vjp(g, *primal_ins)\n    cotangent_ins = vjp(cotangent_outs)\n    return (primal_outs, cotangent_ins)"
  },
  {
    "test_code": "def test_dynamic_dma_on_2nd_minor(self):\n\n    def kernel(array, data, index, size, _, sem):\n        pltpu.async_copy(data.at[pl.ds(0, size[0])], array.at[pl.ds(index[0], size[0])], sem).wait()\n\n    def run(array, data, index, size):\n        return pl.pallas_call(kernel, out_shape=array, in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.VMEM), pl.BlockSpec(memory_space=pltpu.SMEM), pl.BlockSpec(memory_space=pltpu.SMEM)], scratch_shapes=[pltpu.SemaphoreType.DMA], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), input_output_aliases={0: 0})(array, data, index, size)\n    array = jnp.zeros((1024, 128), jnp.int32)\n    data = jnp.ones((8, 128), jnp.int32)\n    index = jnp.array([3], jnp.int32)\n    size = jnp.array([5], jnp.int32)\n    expected = array.at[index[0]:index[0] + size[0]].set(data[index[0]:index[0] + size[0]])\n    result = run(array, data, index, size)\n    np.testing.assert_array_equal(result, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def run(primal_ins, cotangent_outs):\n    primal_outs, vjp = jax.vjp(g, *primal_ins)\n    cotangent_ins = vjp(cotangent_outs)\n    return (primal_outs, cotangent_ins)"
  },
  {
    "test_code": "def test_mixed_precision_dot(self):\n    if not jtu.if_cloud_tpu_at_least(2025, 2, 27):\n        self.skipTest('Needs a newer libTPU')\n    if not jtu.is_device_tpu_at_least(5):\n        self.skipTest('float8_e4m3b11fnuz not supported on TPU generations <= 4')\n\n    def kernel(x_ref, w_ref, o_ref):\n        o_ref[:] = jax.lax.dot_general(x_ref[:], w_ref[:], dimension_numbers=(((1,), (0,)), ((), ())), preferred_element_type=jnp.float32)\n    x = jnp.ones((64, 128), dtype=jnp.bfloat16)\n    w = jnp.full((128, 128), jnp.nan, jnp.float8_e4m3b11fnuz)\n    run = pl.pallas_call(kernel, jax.ShapeDtypeStruct((64, 128), jnp.float32))\n    run = jax.named_call(run, name='run')\n    run = jax.jit(run)\n    expected = jax.lax.dot_general(x, w, dimension_numbers=(((1,), (0,)), ((), ())), preferred_element_type=jnp.float32)\n    jax_nans = jnp.isnan(expected).sum()\n    mosaic_nans = jnp.isnan(run(x, w)).sum()\n    self.assertEqual(jax_nans, mosaic_nans)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def run(primal_ins, cotangent_outs):\n    primal_outs, vjp = jax.vjp(g, *primal_ins)\n    cotangent_ins = vjp(cotangent_outs)\n    return (primal_outs, cotangent_ins)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef add(x):\n    return x * 2"
  },
  {
    "test_code": "def test_dynamic_grid_overflow(self):\n    shape = (8, 128)\n    result_ty = jax.ShapeDtypeStruct(shape, jnp.float32)\n\n    def kernel(y_ref):\n\n        @pl.when(sum((pl.program_id(i) for i in range(3))) == 0)\n        def _init():\n            y_ref[...] = jnp.zeros_like(y_ref)\n        y_ref[...] += 1\n\n    @jax.jit\n    def dynamic_kernel(steps):\n        return self.pallas_call(kernel, grid=(steps * 2, steps + 1, 3), out_specs=pl.BlockSpec(shape, lambda *_: (0, 0)), out_shape=result_ty)()\n    np.testing.assert_array_equal(dynamic_kernel(jnp.int32(4)), np.full(shape, 120.0, np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@parameterized.parameters([dict(shape=shape, dty=dty) for shape, dty in itertools.product([(4, 2, 9), (1, 1025), (1024, 1024)], [jnp.float32, jnp.int32])])\ndef test_double_replicated_reduction(self, shape, dty):\n    if not jtu.if_cloud_tpu_at_least(2025, 2, 19):\n        self.skipTest('Needs a newer libTPU')\n\n    def body(o_ref):\n        x = jnp.full(shape, 2.0, dtype=dty)\n        reduction = jnp.sum(x, axis=None)\n        bcast = jnp.full((vregs_in_block * 1024,), reduction)\n        o_ref[:] = bcast\n    vregs_in_block = 2\n    total_vregs = 4\n    data_size = total_vregs * 1024\n    block_size = vregs_in_block * 1024\n\n    @jax.jit\n    def reduce():\n        return self.pallas_call(body, out_shape=jax.ShapeDtypeStruct((data_size,), dty), in_specs=[], out_specs=pl.BlockSpec((block_size,), lambda i: i), grid=data_size // block_size)()\n    x = jnp.full(shape, 2.0, dtype=dty)\n    z = jax.block_until_ready(reduce())\n    reduce_value = jnp.sum(jnp.full(shape, x), dtype=dty)\n    np.testing.assert_allclose(z, reduce_value)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_mixed_precision_dot(self):\n    if not jtu.if_cloud_tpu_at_least(2025, 2, 27):\n        self.skipTest('Needs a newer libTPU')\n    if not jtu.is_device_tpu_at_least(5):\n        self.skipTest('float8_e4m3b11fnuz not supported on TPU generations <= 4')\n\n    def kernel(x_ref, w_ref, o_ref):\n        o_ref[:] = jax.lax.dot_general(x_ref[:], w_ref[:], dimension_numbers=(((1,), (0,)), ((), ())), preferred_element_type=jnp.float32)\n    x = jnp.ones((64, 128), dtype=jnp.bfloat16)\n    w = jnp.full((128, 128), jnp.nan, jnp.float8_e4m3b11fnuz)\n    run = pl.pallas_call(kernel, jax.ShapeDtypeStruct((64, 128), jnp.float32))\n    run = jax.named_call(run, name='run')\n    run = jax.jit(run)\n    expected = jax.lax.dot_general(x, w, dimension_numbers=(((1,), (0,)), ((), ())), preferred_element_type=jnp.float32)\n    jax_nans = jnp.isnan(expected).sum()\n    mosaic_nans = jnp.isnan(run(x, w)).sum()\n    self.assertEqual(jax_nans, mosaic_nans)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_sum(self):\n    x = np.zeros((8, 2, 8, 128), dtype=jnp.float32)\n\n    def kernel(x_ref, out_ref):\n        out_ref[:, :, :] = jnp.sum(x_ref[:, :, :, :], 2)\n    out = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 2, 128), jnp.float32))(x)\n    np.testing.assert_array_equal(out, np.zeros((8, 2, 128), dtype=jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_dynamic_grid_has_dynamic_size(self):\n\n    def kernel(_):\n        num_programs = pl.num_programs(0)\n        self.assertIsInstance(num_programs, int, msg=type(num_programs))\n        self.assertEqual(num_programs, 2)\n        num_programs = pl.num_programs(1)\n        self.assertIsInstance(num_programs, jax.Array)\n\n    @jax.jit\n    def outer(x):\n        self.pallas_call(kernel, out_shape=None, grid=(2, x))()\n    outer(2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def outer(x):\n\n    @jax.custom_vjp\n    def f(y):\n        return x * y\n\n    def f_fwd(y):\n        return (f(y), (x, jnp.cos(y)))\n\n    def f_rev(res, g):\n        x, cos_y = res\n        return (cos_y * g * x,)\n    f.defvjp(f_fwd, f_rev)\n    return api.grad(f)"
  },
  {
    "test_code": "def test_output_dma_semaphore_ref(self):\n    if self.INTERPRET:\n        self.skipTest('TODO(sharadmv, justinfu): Add interpret support for DMA.')\n\n    def kernel(x_hbm_ref, y_hbm_ref, sem_out):\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_out).start()\n\n    def kernel2(x_hbm_ref, y_hbm_ref, sem_in, y_hbm_out):\n        del y_hbm_out\n        pltpu.make_async_copy(x_hbm_ref.at[pl.ds(8), :], y_hbm_ref.at[:, pl.ds(128)], sem_in).wait()\n    x = jnp.arange(8 * 128.0).reshape((8, 128))\n\n    @jax.jit\n    def body(x):\n        y, sem_out = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], out_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_shape=[jax.ShapeDtypeStruct((8, 128), jnp.float32), pltpu.SemaphoreType.DMA])(x)\n        y = self.pallas_call(kernel2, in_specs=[pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pl.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pl.ANY), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), input_output_aliases={1: 0})(x, y, sem_out)\n        return y\n    np.testing.assert_array_equal(body(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def body(c, x):\n    return ((c[0], c[0] + c[1], jnp.arange(3.0)), x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "def test_with_unhashable_grid_spec(self):\n\n    @functools.partial(self.pallas_call, out_shape=[[jax.ShapeDtypeStruct((8, 128), np.int32)]], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(1,), in_specs=[[pl.BlockSpec((8, 128), lambda i, s_ref: (0, 0))]], out_specs=[[pl.BlockSpec((8, 128), lambda i, s_ref: (0, 0))]], scratch_shapes=[[pltpu.SemaphoreType.REGULAR((3,))]]))\n    def kernel(s_ref, x_ref, o_ref, scratch_ref):\n        assert isinstance(s_ref, list)\n        assert isinstance(x_ref, list)\n        assert isinstance(o_ref, list)\n        assert isinstance(scratch_ref, list)\n        o_ref[0][...] = x_ref[0][...]\n    x_shape = (8, 128)\n    s = np.array([0, 1], np.int32)\n    x = np.arange(math.prod(x_shape), dtype=np.int32).reshape(x_shape)\n    res = kernel([s, s], [x])\n    self.assertIsInstance(res, tuple)\n    self.assertAllClose(res[0][0], x)",
    "assertions": [
      "assert isinstance(s_ref, list)",
      "assert isinstance(x_ref, list)",
      "assert isinstance(o_ref, list)",
      "assert isinstance(scratch_ref, list)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "def test_nontrivial_vmap_scalar_prefetch(self):\n\n    def body(_, x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    s = jnp.array([4, 3, 2, 5, 3, 5, 2, 7], jnp.int32)\n    x = jnp.arange(2 * 8 * 8 * 128, dtype=jnp.int32).reshape((2, 8 * 8, 128))\n\n    def _x_transform(i, s_ref):\n        s = pl.load(s_ref, (i,))\n        return (s, 0)\n    s = jnp.tile(s[None], [2, 1])\n\n    @jax.jit\n    @jax.vmap\n    def kernel(s, x):\n        return self.pallas_call(body, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((x.shape[0] // 8, x.shape[1]), _x_transform)], out_specs=pl.BlockSpec((x.shape[0] // 8, x.shape[1]), lambda i, _: (i, 0)), grid=8), compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[False, True]))(s, x)\n    first = x[0, ...].reshape((1, 8, 8, -1))[:, s[0, ...]].reshape(x.shape[1:])\n    second = x[1, ...].reshape((1, 8, 8, -1))[:, s[1, ...]].reshape(x.shape[1:])\n    expected = jnp.stack([first, second])\n    np.testing.assert_allclose(kernel(s, x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "def test_interpret_local_dma(self):\n\n    def test_kernel(x_ref, o_ref, sem_out_ref, copy_sem):\n        o_ref[...] = jnp.zeros_like(o_ref[...])\n        input_to_output_copy = pltpu.make_async_copy(src_ref=x_ref.at[0:8], dst_ref=o_ref.at[0:8], sem=copy_sem.at[0])\n        input_to_output_copy.start()\n        sem_out_ref[0, :] = jnp.ones_like(sem_out_ref[0, :]) * pltpu.semaphore_read(copy_sem.at[0])\n        input_to_output_copy.wait()\n        sem_out_ref[1, :] = jnp.ones_like(sem_out_ref[0, :]) * pltpu.semaphore_read(copy_sem.at[0])\n    out_shape = (jax.ShapeDtypeStruct((16, 128), jnp.int32), jax.ShapeDtypeStruct((2, 1), jnp.int32))\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pl.ANY)], scratch_shapes=[pltpu.SemaphoreType.DMA(2)])\n    kernel = pl.pallas_call(test_kernel, out_shape=out_shape, grid_spec=grid_spec, interpret=True)\n    x = jax.random.randint(jax.random.key(0), shape=(16, 128), minval=0, maxval=128)\n    result, semaphores = kernel(x)\n    np.testing.assert_array_equal(result[0:8], x[0:8])\n    np.testing.assert_array_equal(result[8:], jnp.zeros_like(result[8:]))\n    result_sem_pre_wait = semaphores[0, 0]\n    np.testing.assert_array_equal(result_sem_pre_wait, result[0:8].size)\n    result_sem_post_wait = semaphores[1, 0]\n    np.testing.assert_array_equal(result_sem_post_wait, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.parameters((pl.Buffered(2), pl.Buffered(2)), (pl.Buffered(2), pl.Buffered(1)), (pl.Buffered(1), pl.Buffered(1)))\ndef test_two_input_vadd(self, x_pmode: pl.Buffered, y_pmode: pl.Buffered):\n    if not jtu.if_cloud_tpu_at_least(2025, 2, 11):\n        self.skipTest('Needs a newer libTPU')\n\n    def body(x_ref, y_ref, o_ref):\n        x = x_ref[:]\n        y = y_ref[:]\n        o_ref[:] = x + y\n    size_in_vregs = 128\n    data_size = size_in_vregs * 1024\n    block_size = 1024\n    x = jnp.arange(data_size, dtype=jnp.float32)\n    y = jnp.arange(data_size, dtype=jnp.float32)\n    in_specs = [pl.BlockSpec((block_size,), lambda i: i, pipeline_mode=pmode) for pmode in [x_pmode, y_pmode]]\n    out_specs = pl.BlockSpec((block_size,), lambda i: i)\n\n    @jax.jit\n    def vadd(x, y):\n        return self.pallas_call(body, out_shape=jax.ShapeDtypeStruct(x.shape, jnp.float32), in_specs=in_specs, out_specs=out_specs, grid=data_size // block_size)(x, y)\n    compiled = vadd.lower(jax.ShapeDtypeStruct(x.shape, x.dtype), jax.ShapeDtypeStruct(y.shape, y.dtype)).compile().as_text()\n    pattern = '\"used_scoped_memory_configs\":\\\\[\\\\{\"memory_space\":\"1\",.*?\"size\":\"(\\\\d+)\"'\n    expected_vmem_usage = block_size * 4 * (2 + x_pmode.buffer_count + y_pmode.buffer_count)\n    vmem_usage = int(re.search(pattern, compiled).group(1))\n    self.assertEqual(vmem_usage, expected_vmem_usage)\n    z = vadd(x, y)\n    np.testing.assert_allclose(z, x + y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_cost_analysis(self):\n\n    def kernel(x, y):\n        y[:] = x[:]\n    x = jnp.arange(1024.0).reshape(8, 128)\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), cost_estimate=pl.CostEstimate(flops=1234, transcendentals=21, bytes_accessed=12345))\n    analysis_result = jax.jit(f).lower(x).compile().cost_analysis()\n    self.assertEqual(analysis_result['flops'], 1234)\n    self.assertEqual(analysis_result['transcendentals'], 21)\n    self.assertEqual(analysis_result['bytes accessed'], 12345)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_cost_analysis_vmap(self):\n\n    def kernel(x, y):\n        y[:] = x[:]\n    batch_size = 3\n    x = jnp.arange(batch_size * 1024.0).reshape(batch_size, 8, 128)\n    f = pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32), cost_estimate=pl.CostEstimate(flops=1234, transcendentals=21, bytes_accessed=12345))\n    f = jax.vmap(f)\n    analysis_result = jax.jit(f).lower(x).compile().cost_analysis()\n    self.assertEqual(analysis_result['flops'], batch_size * 1234)\n    self.assertEqual(analysis_result['transcendentals'], batch_size * 21)\n    self.assertEqual(analysis_result['bytes accessed'], batch_size * 12345)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_mlir_location(self):\n    args = (jax.ShapeDtypeStruct((8, 128), jnp.float32),)\n    f = example_kernel.double\n    as_tpu_kernel = mosaic.as_tpu_kernel\n\n    def capture_as_tpu_kernel(module, *args, **kwargs):\n        asm = module.operation.get_asm(enable_debug_info=True)\n        self.assertIn('example_kernel.py\":25', asm)\n        return as_tpu_kernel(module, *args, **kwargs)\n    mosaic.as_tpu_kernel = capture_as_tpu_kernel\n    try:\n        jax.jit(f).lower(*args)\n    finally:\n        mosaic.as_tpu_kernel = as_tpu_kernel",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_debug_print(self):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('It works!')\n    x = jnp.arange(8 * 128, dtype=jnp.float32).reshape((8, 128))\n    compiled_kernel = jax.jit(kernel).lower(x).compile({'xla_tpu_enable_log_recorder': 'true'})\n    with jtu.capture_stderr() as get_output:\n        jax.block_until_ready(compiled_kernel(x))\n    self.assertIn('It works!', get_output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_debug_print_with_values(self):\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=pltpu.SMEM),), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('x[0] == {}', x_ref[0])\n    x = jnp.array([42, 24]).astype(jnp.int32)\n    compiled_kernel = jax.jit(kernel).lower(x).compile({'xla_tpu_enable_log_recorder': 'true'})\n    with jtu.capture_stderr() as get_output:\n        jax.block_until_ready(compiled_kernel(x))\n    self.assertIn('x[0] == 42', get_output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{'_'.join(map(str, shape))}_{dtype.__name__}', shape, dtype) for shape in ((2, 8, 128), (3,), (3, 4), (2, 3, 4), (2, 9, 129)) for dtype in (jnp.int32, jnp.uint32, jnp.float32)))\ndef test_debug_print_vector(self, shape, dtype):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('{}', x_ref[...])\n        o_ref[...] = x_ref[...]\n    n = np.prod(shape)\n    x = jnp.arange(n, dtype=dtype).reshape(shape)\n    compiled_kernel = jax.jit(kernel).lower(x).compile({'xla_tpu_enable_log_recorder': 'true'})\n    with jtu.capture_stderr() as get_output:\n        jax.block_until_ready(compiled_kernel(x))\n    output = get_output()\n    numbers = [int(num) for line in output.splitlines() if (match := re.search('\\\\{(.*)', line)) for num in re.findall('\\\\d+', match.group(1))]\n    self.assertLen(numbers, n)\n    self.assertTrue(all((num == i for i, num in enumerate(numbers))))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(scratch=scratch, vmap=vmap, dyn_grid=dyn_grid) for scratch in [True, False] for vmap in [False, True] for dyn_grid in [False, True]])\ndef test_scalar_prefetch_calling_convention(self, *, scratch: bool, vmap: bool, dyn_grid: bool):\n    if jtu.test_device_matches(['cpu']) and jax.config.x64_enabled:\n        self.skipTest('TODO: dslice(start, 1) raises error about slice inputs being int32 and int64')\n    to_store = np.arange(128, dtype=np.float32).reshape((1, 128))\n    if vmap:\n        x_shape = (4, 16, 128)\n    else:\n        x_shape = (16, 128)\n    x = np.arange(math.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f(x, grid_size, to_store):\n        s = jnp.array([1, 0], jnp.int32)\n\n        @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\n        def kernel(s_refs, src, to_store, dst, *scratch_refs):\n            s_ref, s2, s3 = s_refs\n            assert s_ref.shape == (2,)\n            assert s2.shape == (3,)\n            assert s3 is None\n            store_idx = s_ref[pl.program_id(0)]\n            pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])\n        return kernel((s, np.arange(3, dtype=np.int32), None), x, to_store)\n    if dyn_grid:\n        f = jax.jit(f)\n    if vmap:\n        res = jax.vmap(lambda x: f(x, 2, to_store))(x)\n    else:\n        res = f(x, 2, to_store)\n    if vmap:\n        for i in range(x.shape[0]):\n            self.assertAllClose(res[i, 0:1], to_store)\n            self.assertAllClose(res[i, 33:34], to_store)\n    else:\n        self.assertAllClose(res[0:1], to_store)\n        self.assertAllClose(res[33:34], to_store)",
    "assertions": [
      "assert s_ref.shape == (2,)",
      "assert s2.shape == (3,)",
      "assert s3 is None"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_input_output_aliasing_with_scalar_prefetch(self):\n    x = jnp.ones((32, 1024, 1024))\n    expected = x + 1\n\n    def kernel(_, x_ref, y_ref):\n        y_ref[...] = x_ref[...] + 1.0\n\n    @partial(jax.jit, donate_argnums=(0,))\n    def f(x):\n        return self.pallas_call(kernel, out_shape=x, grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0))], out_specs=pl.BlockSpec((None, 1024, 1024), lambda i, _: (i, 0, 0)), grid=(x.shape[0],)), input_output_aliases={1: 0})(jnp.array([1, 2, 3]), x)\n    o = f(x)\n    np.testing.assert_array_equal(o, expected)\n    compiled = f.lower(jax.ShapeDtypeStruct(x.shape, x.dtype)).compile()\n    mem_analysis = compiled.memory_analysis()\n    expected_num_bytes = np.prod(x.shape) * x.dtype.itemsize\n    self.assertEqual(mem_analysis.alias_size_in_bytes, expected_num_bytes)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_allow_input_fusion(self):\n    shape = (3, 128, 128)\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = x_ref[...]\n\n    def f(x, y):\n        z = jax.numpy.add(x, y)\n        return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)\n    x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    y = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    out = f(x, y)\n    expected = x + y\n    np.testing.assert_array_equal(out, expected)\n    compiled = jax.jit(f).lower(x, y).compile().as_text()\n    assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)",
    "assertions": [
      "assert re.search('fusion.*kind=kCustom.*fused_computation', compiled)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  }
]