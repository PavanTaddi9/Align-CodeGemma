[
  {
    "test_code": "def test_can_partition_nondivisible_grid_with_dynamic_dimensions(self):\n    self.skipTest('Broken test.')\n\n    def mul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def mul_kernel(iters_ref, x_ref, y_ref):\n        pltpu.emit_pipeline(mul_pipeline, grid=(iters_ref[0], 5), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(mul_kernel, out_shape=jax.ShapeDtypeStruct((640, 640), jnp.float32), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,)), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    x = jax.random.uniform(jax.random.key(0), (640, 640))\n    np.testing.assert_allclose(func(jnp.array([5]), x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(x):\n    x, = promote_dtypes_complex(x)\n    return jnp.fft.irfft(jnp.concatenate([jnp.zeros_like(x, shape=1), x[:2] + 1j * x[2:]]))"
  },
  {
    "test_code": "def test_megacore_mul(self):\n    self.skipTest('Broken test.')\n    x = jax.random.uniform(jax.random.key(0), (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def matmul_kernel(x_ref, y_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.ARBITRARY, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(x):\n    x, = promote_dtypes_complex(x)\n    return jnp.fft.irfft(jnp.concatenate([jnp.zeros_like(x, shape=1), x[:2] + 1j * x[2:]]))"
  },
  {
    "test_code": "@parameterized.parameters((1024, 1024, 1024, 256, 512, 256), (768, 1024, 1024, 256, 512, 256), (1024, 1024, 768, 256, 512, 256), (768, 1024, 768, 256, 512, 256))\ndef test_megacore_matmul(self, m, k, n, bm, bk, bn):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k))\n    y = jax.random.uniform(k2, (k, n))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros_like(z_ref)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref, *, bm, bk, bn):\n        m, k = x_ref.shape\n        _, n = y_ref.shape\n        assert k % bk == 0\n        pltpu.emit_pipeline(matmul_pipeline, grid=(pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk)), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_ref, y_ref, z_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(functools.partial(matmul_kernel, bm=bm, bk=bk, bn=bn), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x, y), x @ y, atol=7e-05)",
    "assertions": [
      "assert k % bk == 0"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(x):\n    x, = promote_dtypes_complex(x)\n    return jnp.fft.irfft(jnp.concatenate([jnp.zeros_like(x, shape=1), x[:2] + 1j * x[2:]]))"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM), ('hbm', pltpu.TPUMemorySpace.ANY))\ndef test_pipeline_matmul(self, memory_space):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(0))\n    x = jax.random.uniform(k1, (512, 512))\n    y = jax.random.uniform(k2, (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros(z_ref.shape, jnp.float32)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j, k: (i, k)), pl.BlockSpec((128, 128), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j, k: (i, j)))(x_ref, y_ref, z_ref)\n    z = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space))\n    jax.block_until_ready(z(x, y))\n    jax.block_until_ready(jnp.dot(x, y))\n    out = jax.block_until_ready(z(x, y))\n    expected_out = jax.block_until_ready(jnp.dot(x, y))\n    np.testing.assert_allclose(out, expected_out)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM), ('hbm', pltpu.TPUMemorySpace.ANY))\ndef test_double_pipeline_matmul(self, memory_space):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(0))\n    x = jax.random.uniform(k1, (512, 512))\n    y = jax.random.uniform(k2, (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros(z_ref.shape, jnp.float32)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref):\n\n        def emit_pipeline(should_accumulate_out):\n            pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j, k: (i, k)), pl.BlockSpec((128, 128), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j, k: (i, j)), should_accumulate_out=should_accumulate_out)(x_ref, y_ref, z_ref)\n        emit_pipeline(False)\n        emit_pipeline(True)\n    z = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space))(x, y)\n    np.testing.assert_allclose(z, jnp.dot(x, y) + jnp.dot(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_can_partition_nondivisible_grid_with_dynamic_dimensions(self):\n    self.skipTest('Broken test.')\n\n    def mul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def mul_kernel(iters_ref, x_ref, y_ref):\n        pltpu.emit_pipeline(mul_pipeline, grid=(iters_ref[0], 5), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(mul_kernel, out_shape=jax.ShapeDtypeStruct((640, 640), jnp.float32), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,)), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    x = jax.random.uniform(jax.random.key(0), (640, 640))\n    np.testing.assert_allclose(func(jnp.array([5]), x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(size):\n    lhs_one_d = jnp.arange(size, dtype='int32') + 1\n    lhs_two_d = jax.lax.broadcast_in_dim(lhs_one_d, (size, 2), (0,))\n    rhs = jax.lax.broadcasted_iota('int32', (2, 4), 0) + 1\n    return jnp.dot(lhs_two_d, rhs)"
  },
  {
    "test_code": "def test_megacore_mul(self):\n    self.skipTest('Broken test.')\n    x = jax.random.uniform(jax.random.key(0), (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def matmul_kernel(x_ref, y_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.ARBITRARY, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(size):\n    lhs_one_d = jnp.arange(size, dtype='int32') + 1\n    lhs_two_d = jax.lax.broadcast_in_dim(lhs_one_d, (size, 2), (0,))\n    rhs = jax.lax.broadcasted_iota('int32', (2, 4), 0) + 1\n    return jnp.dot(lhs_two_d, rhs)"
  },
  {
    "test_code": "@parameterized.parameters((1024, 1024, 1024, 256, 512, 256), (768, 1024, 1024, 256, 512, 256), (1024, 1024, 768, 256, 512, 256), (768, 1024, 768, 256, 512, 256))\ndef test_megacore_matmul(self, m, k, n, bm, bk, bn):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k))\n    y = jax.random.uniform(k2, (k, n))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros_like(z_ref)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref, *, bm, bk, bn):\n        m, k = x_ref.shape\n        _, n = y_ref.shape\n        assert k % bk == 0\n        pltpu.emit_pipeline(matmul_pipeline, grid=(pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk)), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_ref, y_ref, z_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(functools.partial(matmul_kernel, bm=bm, bk=bk, bn=bn), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x, y), x @ y, atol=7e-05)",
    "assertions": [
      "assert k % bk == 0"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(size):\n    lhs_one_d = jnp.arange(size, dtype='int32') + 1\n    lhs_two_d = jax.lax.broadcast_in_dim(lhs_one_d, (size, 2), (0,))\n    rhs = jax.lax.broadcasted_iota('int32', (2, 4), 0) + 1\n    return jnp.dot(lhs_two_d, rhs)"
  },
  {
    "test_code": "def test_can_partition_nondivisible_grid_with_dynamic_dimensions(self):\n    self.skipTest('Broken test.')\n\n    def mul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def mul_kernel(iters_ref, x_ref, y_ref):\n        pltpu.emit_pipeline(mul_pipeline, grid=(iters_ref[0], 5), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(mul_kernel, out_shape=jax.ShapeDtypeStruct((640, 640), jnp.float32), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,)), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    x = jax.random.uniform(jax.random.key(0), (640, 640))\n    np.testing.assert_allclose(func(jnp.array([5]), x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(x):\n    return jax.random.uniform(x, (2, 4), dtype=np.float32)"
  },
  {
    "test_code": "def test_megacore_mul(self):\n    self.skipTest('Broken test.')\n    x = jax.random.uniform(jax.random.key(0), (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def matmul_kernel(x_ref, y_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.ARBITRARY, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(x):\n    return jax.random.uniform(x, (2, 4), dtype=np.float32)"
  },
  {
    "test_code": "@parameterized.parameters((1024, 1024, 1024, 256, 512, 256), (768, 1024, 1024, 256, 512, 256), (1024, 1024, 768, 256, 512, 256), (768, 1024, 768, 256, 512, 256))\ndef test_megacore_matmul(self, m, k, n, bm, bk, bn):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k))\n    y = jax.random.uniform(k2, (k, n))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros_like(z_ref)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref, *, bm, bk, bn):\n        m, k = x_ref.shape\n        _, n = y_ref.shape\n        assert k % bk == 0\n        pltpu.emit_pipeline(matmul_pipeline, grid=(pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk)), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_ref, y_ref, z_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(functools.partial(matmul_kernel, bm=bm, bk=bk, bn=bn), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x, y), x @ y, atol=7e-05)",
    "assertions": [
      "assert k % bk == 0"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(x):\n    return jax.random.uniform(x, (2, 4), dtype=np.float32)"
  },
  {
    "test_code": "def test_can_partition_nondivisible_grid_with_dynamic_dimensions(self):\n    self.skipTest('Broken test.')\n\n    def mul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def mul_kernel(iters_ref, x_ref, y_ref):\n        pltpu.emit_pipeline(mul_pipeline, grid=(iters_ref[0], 5), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(mul_kernel, out_shape=jax.ShapeDtypeStruct((640, 640), jnp.float32), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,)), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    x = jax.random.uniform(jax.random.key(0), (640, 640))\n    np.testing.assert_allclose(func(jnp.array([5]), x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(xs):\n    return jnp.array(list(xs))"
  },
  {
    "test_code": "def test_megacore_mul(self):\n    self.skipTest('Broken test.')\n    x = jax.random.uniform(jax.random.key(0), (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def matmul_kernel(x_ref, y_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.ARBITRARY, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(xs):\n    return jnp.array(list(xs))"
  },
  {
    "test_code": "@parameterized.parameters((1024, 1024, 1024, 256, 512, 256), (768, 1024, 1024, 256, 512, 256), (1024, 1024, 768, 256, 512, 256), (768, 1024, 768, 256, 512, 256))\ndef test_megacore_matmul(self, m, k, n, bm, bk, bn):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k))\n    y = jax.random.uniform(k2, (k, n))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros_like(z_ref)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref, *, bm, bk, bn):\n        m, k = x_ref.shape\n        _, n = y_ref.shape\n        assert k % bk == 0\n        pltpu.emit_pipeline(matmul_pipeline, grid=(pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk)), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_ref, y_ref, z_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(functools.partial(matmul_kernel, bm=bm, bk=bk, bn=bn), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x, y), x @ y, atol=7e-05)",
    "assertions": [
      "assert k % bk == 0"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(xs):\n    return jnp.array(list(xs))"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM), ('hbm', pltpu.TPUMemorySpace.ANY))\ndef test_pipeline_matmul(self, memory_space):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(0))\n    x = jax.random.uniform(k1, (512, 512))\n    y = jax.random.uniform(k2, (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros(z_ref.shape, jnp.float32)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j, k: (i, k)), pl.BlockSpec((128, 128), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j, k: (i, j)))(x_ref, y_ref, z_ref)\n    z = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space))\n    jax.block_until_ready(z(x, y))\n    jax.block_until_ready(jnp.dot(x, y))\n    out = jax.block_until_ready(z(x, y))\n    expected_out = jax.block_until_ready(jnp.dot(x, y))\n    np.testing.assert_allclose(out, expected_out)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM), ('hbm', pltpu.TPUMemorySpace.ANY))\ndef test_double_pipeline_matmul(self, memory_space):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(0))\n    x = jax.random.uniform(k1, (512, 512))\n    y = jax.random.uniform(k2, (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros(z_ref.shape, jnp.float32)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref):\n\n        def emit_pipeline(should_accumulate_out):\n            pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j, k: (i, k)), pl.BlockSpec((128, 128), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j, k: (i, j)), should_accumulate_out=should_accumulate_out)(x_ref, y_ref, z_ref)\n        emit_pipeline(False)\n        emit_pipeline(True)\n    z = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space))(x, y)\n    np.testing.assert_allclose(z, jnp.dot(x, y) + jnp.dot(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.local_device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref, lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_ref, dst_ref=lhs_scratch_ref.at[1, working_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step - 1, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return pl.ds(pl.multiple_of(offset * sharded_k, sharded_k), sharded_k)\n        accum_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            prefetch_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            prefetch_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            prefetch_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[prefetch_phase, prefetch_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(prefetch_step, prefetch_phase)])\n            scheduler.prefetch(out_bref, out_ref, accum_schedule)\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref, allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=is_start, prefetch=prefetch, schedule=[None, None, accum_schedule])\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=pltpu.TPUCompilerParams(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9)))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_122', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_121', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = out_dtype\n    num_devices = jax.local_device_count()\n    tn = 128\n    tm = 128\n    tk = 128\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices\n    sharded_k = k // num_devices\n    half_m = m // 2\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(1, m // 2)], lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(0, m // 2)], lhs_scratch_ref.at[1, working_slot], bwd_send_sem)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return make_ds(offset, sharded_k)\n\n        def get_half(phase):\n            return make_ds(jnp.where(phase, 0, 1), m // 2)\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[next_phase, next_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(next_step, next_phase)])\n            scheduler.prefetch(out_bref, out_ref.at[get_half(next_phase)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref.at[get_half(phase)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch)\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, half_m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.parameters((1024, 1024, 1024, 256, 512, 256), (768, 1024, 1024, 256, 512, 256), (1024, 1024, 768, 256, 512, 256), (768, 1024, 768, 256, 512, 256))\ndef test_megacore_matmul(self, m, k, n, bm, bk, bn):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k))\n    y = jax.random.uniform(k2, (k, n))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros_like(z_ref)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref, *, bm, bk, bn):\n        m, k = x_ref.shape\n        _, n = y_ref.shape\n        assert k % bk == 0\n        pltpu.emit_pipeline(matmul_pipeline, grid=(pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk)), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_ref, y_ref, z_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(functools.partial(matmul_kernel, bm=bm, bk=bk, bn=bn), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x, y), x @ y, atol=7e-05)",
    "assertions": [
      "assert k % bk == 0"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(('float32', 'float32'), ('bfloat16', 'bfloat16'), ('int8', 'int8'))\n@hp.given(hps.integers(1, 1024), hps.integers(1, 1024), hps.integers(1, 1024), hps.sampled_from([8, 16, 32, 128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.integers(0, 4))\ndef test_padded_matmul(self, dtype, m, k, n, bm, bk, bn, seed):\n    if dtype == 'int8' and jtu.is_device_tpu_at_least(6):\n        self.skipTest('Not implemented for TPU v6.')\n\n    def align_up_to(x, y):\n        return (x + y - 1) // y * y\n    hp.assume(bm <= m)\n    hp.assume(bn <= n)\n    hp.assume(bk <= k)\n    if dtype == 'bfloat16':\n        hp.assume(bm >= 16)\n    if dtype == 'int8':\n        if not jtu.is_device_tpu_at_least(5):\n            self.skipTest('Only TPU v5+ allowed for int8.')\n        hp.assume(bm >= 32)\n    packing = 4 // jnp.dtype(dtype).itemsize\n    if packing != 1:\n        m = align_up_to(m, 8 * packing)\n        k = align_up_to(k, 8 * packing)\n    k1, k2 = jax.random.split(jax.random.key(seed))\n    x = jax.random.normal(k1, (m, k), jnp.float32).astype(dtype)\n    y = jax.random.normal(k2, (k, n), jnp.float32).astype(dtype)\n    out = matmul(x, y, bm=bm, bk=bk, bn=bn)\n    expected = x @ y\n    atol = rtol = 2.3e-05\n    if dtype == 'bfloat16':\n        out = out.astype('float32')\n        expected = expected.astype('float32')\n        atol = rtol = 0.01\n    np.testing.assert_allclose(out, expected, atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.local_device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref, lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_ref, dst_ref=lhs_scratch_ref.at[1, working_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step - 1, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return pl.ds(pl.multiple_of(offset * sharded_k, sharded_k), sharded_k)\n        accum_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            prefetch_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            prefetch_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            prefetch_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[prefetch_phase, prefetch_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(prefetch_step, prefetch_phase)])\n            scheduler.prefetch(out_bref, out_ref, accum_schedule)\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref, allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=is_start, prefetch=prefetch, schedule=[None, None, accum_schedule])\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=pltpu.TPUCompilerParams(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9)))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def test(x):\n    val, vec = jnp.linalg.eigh(x)\n    return jnp.real(jnp.sum(val))"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_122', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_121', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = out_dtype\n    num_devices = jax.local_device_count()\n    tn = 128\n    tm = 128\n    tk = 128\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices\n    sharded_k = k // num_devices\n    half_m = m // 2\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(1, m // 2)], lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(0, m // 2)], lhs_scratch_ref.at[1, working_slot], bwd_send_sem)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return make_ds(offset, sharded_k)\n\n        def get_half(phase):\n            return make_ds(jnp.where(phase, 0, 1), m // 2)\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[next_phase, next_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(next_step, next_phase)])\n            scheduler.prefetch(out_bref, out_ref.at[get_half(next_phase)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref.at[get_half(phase)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch)\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, half_m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def test(x):\n    val, vec = jnp.linalg.eigh(x)\n    return jnp.real(jnp.sum(val))"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def test(x):\n    val, vec = jnp.linalg.eigh(x)\n    return jnp.real(jnp.sum(val))"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def test(x):\n    val, vec = jnp.linalg.eigh(x)\n    return jnp.real(jnp.sum(val))"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.local_device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref, lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_ref, dst_ref=lhs_scratch_ref.at[1, working_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step - 1, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return pl.ds(pl.multiple_of(offset * sharded_k, sharded_k), sharded_k)\n        accum_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            prefetch_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            prefetch_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            prefetch_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[prefetch_phase, prefetch_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(prefetch_step, prefetch_phase)])\n            scheduler.prefetch(out_bref, out_ref, accum_schedule)\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref, allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=is_start, prefetch=prefetch, schedule=[None, None, accum_schedule])\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=pltpu.TPUCompilerParams(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9)))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def test(x):\n    return jax.lax.psum(x, axis_name='batch')"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_122', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_121', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = out_dtype\n    num_devices = jax.local_device_count()\n    tn = 128\n    tm = 128\n    tk = 128\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices\n    sharded_k = k // num_devices\n    half_m = m // 2\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(1, m // 2)], lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(0, m // 2)], lhs_scratch_ref.at[1, working_slot], bwd_send_sem)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return make_ds(offset, sharded_k)\n\n        def get_half(phase):\n            return make_ds(jnp.where(phase, 0, 1), m // 2)\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[next_phase, next_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(next_step, next_phase)])\n            scheduler.prefetch(out_bref, out_ref.at[get_half(next_phase)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref.at[get_half(phase)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch)\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, half_m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def test(x):\n    return jax.lax.psum(x, axis_name='batch')"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def test(x):\n    return jax.lax.psum(x, axis_name='batch')"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def test(x):\n    return jax.lax.psum(x, axis_name='batch')"
  },
  {
    "test_code": "def test_can_partition_nondivisible_grid_with_dynamic_dimensions(self):\n    self.skipTest('Broken test.')\n\n    def mul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def mul_kernel(iters_ref, x_ref, y_ref):\n        pltpu.emit_pipeline(mul_pipeline, grid=(iters_ref[0], 5), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(mul_kernel, out_shape=jax.ShapeDtypeStruct((640, 640), jnp.float32), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,)), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    x = jax.random.uniform(jax.random.key(0), (640, 640))\n    np.testing.assert_allclose(func(jnp.array([5]), x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@partial(self.pmap, axis_name='i')\ndef func(_):\n    return jax.lax.psum(dtype(0), axis_name='i')"
  },
  {
    "test_code": "def test_megacore_mul(self):\n    self.skipTest('Broken test.')\n    x = jax.random.uniform(jax.random.key(0), (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def matmul_kernel(x_ref, y_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.ARBITRARY, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@partial(self.pmap, axis_name='i')\ndef func(_):\n    return jax.lax.psum(dtype(0), axis_name='i')"
  },
  {
    "test_code": "@parameterized.parameters((1024, 1024, 1024, 256, 512, 256), (768, 1024, 1024, 256, 512, 256), (1024, 1024, 768, 256, 512, 256), (768, 1024, 768, 256, 512, 256))\ndef test_megacore_matmul(self, m, k, n, bm, bk, bn):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k))\n    y = jax.random.uniform(k2, (k, n))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros_like(z_ref)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref, *, bm, bk, bn):\n        m, k = x_ref.shape\n        _, n = y_ref.shape\n        assert k % bk == 0\n        pltpu.emit_pipeline(matmul_pipeline, grid=(pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk)), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_ref, y_ref, z_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(functools.partial(matmul_kernel, bm=bm, bk=bk, bn=bn), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x, y), x @ y, atol=7e-05)",
    "assertions": [
      "assert k % bk == 0"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@partial(self.pmap, axis_name='i')\ndef func(_):\n    return jax.lax.psum(dtype(0), axis_name='i')"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.local_device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref, lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_ref, dst_ref=lhs_scratch_ref.at[1, working_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step - 1, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return pl.ds(pl.multiple_of(offset * sharded_k, sharded_k), sharded_k)\n        accum_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            prefetch_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            prefetch_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            prefetch_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[prefetch_phase, prefetch_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(prefetch_step, prefetch_phase)])\n            scheduler.prefetch(out_bref, out_ref, accum_schedule)\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref, allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=is_start, prefetch=prefetch, schedule=[None, None, accum_schedule])\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=pltpu.TPUCompilerParams(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9)))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def reference(x, split_axis, concat_axis, vmap_axis):\n    pmap_axis = 0\n    vmap_axis = adj(vmap_axis, [pmap_axis])\n    ref = x\n    split_axis = adj(split_axis, [pmap_axis, vmap_axis])\n    ref = jnp.moveaxis(ref, split_axis, pmap_axis + 1)\n    vmap_axis = vmap_axis + (0 if split_axis < vmap_axis else 1)\n    split_axis = pmap_axis + 1\n    concat_axis = adj(concat_axis, [pmap_axis, split_axis, vmap_axis]) - 1\n    ref = jnp.moveaxis(ref, pmap_axis, concat_axis)\n    pmap_axis = 0\n    vmap_axis = vmap_axis - (1 if concat_axis >= vmap_axis else 0)\n    del split_axis, concat_axis\n    ref = jnp.moveaxis(ref, vmap_axis, 1)\n    return ref"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_122', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_121', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = out_dtype\n    num_devices = jax.local_device_count()\n    tn = 128\n    tm = 128\n    tk = 128\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices\n    sharded_k = k // num_devices\n    half_m = m // 2\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(1, m // 2)], lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(0, m // 2)], lhs_scratch_ref.at[1, working_slot], bwd_send_sem)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return make_ds(offset, sharded_k)\n\n        def get_half(phase):\n            return make_ds(jnp.where(phase, 0, 1), m // 2)\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[next_phase, next_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(next_step, next_phase)])\n            scheduler.prefetch(out_bref, out_ref.at[get_half(next_phase)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref.at[get_half(phase)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch)\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, half_m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def reference(x, split_axis, concat_axis, vmap_axis):\n    pmap_axis = 0\n    vmap_axis = adj(vmap_axis, [pmap_axis])\n    ref = x\n    split_axis = adj(split_axis, [pmap_axis, vmap_axis])\n    ref = jnp.moveaxis(ref, split_axis, pmap_axis + 1)\n    vmap_axis = vmap_axis + (0 if split_axis < vmap_axis else 1)\n    split_axis = pmap_axis + 1\n    concat_axis = adj(concat_axis, [pmap_axis, split_axis, vmap_axis]) - 1\n    ref = jnp.moveaxis(ref, pmap_axis, concat_axis)\n    pmap_axis = 0\n    vmap_axis = vmap_axis - (1 if concat_axis >= vmap_axis else 0)\n    del split_axis, concat_axis\n    ref = jnp.moveaxis(ref, vmap_axis, 1)\n    return ref"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def reference(x, split_axis, concat_axis, vmap_axis):\n    pmap_axis = 0\n    vmap_axis = adj(vmap_axis, [pmap_axis])\n    ref = x\n    split_axis = adj(split_axis, [pmap_axis, vmap_axis])\n    ref = jnp.moveaxis(ref, split_axis, pmap_axis + 1)\n    vmap_axis = vmap_axis + (0 if split_axis < vmap_axis else 1)\n    split_axis = pmap_axis + 1\n    concat_axis = adj(concat_axis, [pmap_axis, split_axis, vmap_axis]) - 1\n    ref = jnp.moveaxis(ref, pmap_axis, concat_axis)\n    pmap_axis = 0\n    vmap_axis = vmap_axis - (1 if concat_axis >= vmap_axis else 0)\n    del split_axis, concat_axis\n    ref = jnp.moveaxis(ref, vmap_axis, 1)\n    return ref"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def reference(x, split_axis, concat_axis, vmap_axis):\n    pmap_axis = 0\n    vmap_axis = adj(vmap_axis, [pmap_axis])\n    ref = x\n    split_axis = adj(split_axis, [pmap_axis, vmap_axis])\n    ref = jnp.moveaxis(ref, split_axis, pmap_axis + 1)\n    vmap_axis = vmap_axis + (0 if split_axis < vmap_axis else 1)\n    split_axis = pmap_axis + 1\n    concat_axis = adj(concat_axis, [pmap_axis, split_axis, vmap_axis]) - 1\n    ref = jnp.moveaxis(ref, pmap_axis, concat_axis)\n    pmap_axis = 0\n    vmap_axis = vmap_axis - (1 if concat_axis >= vmap_axis else 0)\n    del split_axis, concat_axis\n    ref = jnp.moveaxis(ref, vmap_axis, 1)\n    return ref"
  },
  {
    "test_code": "def test_can_partition_nondivisible_grid_with_dynamic_dimensions(self):\n    self.skipTest('Broken test.')\n\n    def mul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def mul_kernel(iters_ref, x_ref, y_ref):\n        pltpu.emit_pipeline(mul_pipeline, grid=(iters_ref[0], 5), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(mul_kernel, out_shape=jax.ShapeDtypeStruct((640, 640), jnp.float32), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,)), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    x = jax.random.uniform(jax.random.key(0), (640, 640))\n    np.testing.assert_allclose(func(jnp.array([5]), x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@self.sparsify\ndef func(x):\n    return jit(lambda x: jnp.sum(x, 1))(x)"
  },
  {
    "test_code": "def test_megacore_mul(self):\n    self.skipTest('Broken test.')\n    x = jax.random.uniform(jax.random.key(0), (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def matmul_kernel(x_ref, y_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.ARBITRARY, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@self.sparsify\ndef func(x):\n    return jit(lambda x: jnp.sum(x, 1))(x)"
  },
  {
    "test_code": "@parameterized.parameters((1024, 1024, 1024, 256, 512, 256), (768, 1024, 1024, 256, 512, 256), (1024, 1024, 768, 256, 512, 256), (768, 1024, 768, 256, 512, 256))\ndef test_megacore_matmul(self, m, k, n, bm, bk, bn):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k))\n    y = jax.random.uniform(k2, (k, n))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros_like(z_ref)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref, *, bm, bk, bn):\n        m, k = x_ref.shape\n        _, n = y_ref.shape\n        assert k % bk == 0\n        pltpu.emit_pipeline(matmul_pipeline, grid=(pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk)), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_ref, y_ref, z_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(functools.partial(matmul_kernel, bm=bm, bk=bk, bn=bn), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x, y), x @ y, atol=7e-05)",
    "assertions": [
      "assert k % bk == 0"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@self.sparsify\ndef func(x):\n    return jit(lambda x: jnp.sum(x, 1))(x)"
  },
  {
    "test_code": "def test_can_partition_nondivisible_grid_with_dynamic_dimensions(self):\n    self.skipTest('Broken test.')\n\n    def mul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def mul_kernel(iters_ref, x_ref, y_ref):\n        pltpu.emit_pipeline(mul_pipeline, grid=(iters_ref[0], 5), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(mul_kernel, out_shape=jax.ShapeDtypeStruct((640, 640), jnp.float32), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,)), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    x = jax.random.uniform(jax.random.key(0), (640, 640))\n    np.testing.assert_allclose(func(jnp.array([5]), x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(D0):\n\n    def shift(R, dR, **unused_kwargs):\n        return R + dR\n\n    def apply_fn(R):\n        return D0 * R\n    Rinit = jax.random.uniform(split, (n, 3), minval=0.0, maxval=5.0, dtype=jnp.float32)\n\n    def move(R, i):\n        F = apply_fn(R)\n        return (shift(R, 0.001 * F), jnp.array([0.0]))\n    move = remat(move)\n    R, temp = lax.scan(move, Rinit, jnp.arange(2))\n    return R[0, 0]"
  },
  {
    "test_code": "def test_megacore_mul(self):\n    self.skipTest('Broken test.')\n    x = jax.random.uniform(jax.random.key(0), (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def matmul_kernel(x_ref, y_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.ARBITRARY, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(D0):\n\n    def shift(R, dR, **unused_kwargs):\n        return R + dR\n\n    def apply_fn(R):\n        return D0 * R\n    Rinit = jax.random.uniform(split, (n, 3), minval=0.0, maxval=5.0, dtype=jnp.float32)\n\n    def move(R, i):\n        F = apply_fn(R)\n        return (shift(R, 0.001 * F), jnp.array([0.0]))\n    move = remat(move)\n    R, temp = lax.scan(move, Rinit, jnp.arange(2))\n    return R[0, 0]"
  },
  {
    "test_code": "@parameterized.parameters((1024, 1024, 1024, 256, 512, 256), (768, 1024, 1024, 256, 512, 256), (1024, 1024, 768, 256, 512, 256), (768, 1024, 768, 256, 512, 256))\ndef test_megacore_matmul(self, m, k, n, bm, bk, bn):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k))\n    y = jax.random.uniform(k2, (k, n))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros_like(z_ref)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref, *, bm, bk, bn):\n        m, k = x_ref.shape\n        _, n = y_ref.shape\n        assert k % bk == 0\n        pltpu.emit_pipeline(matmul_pipeline, grid=(pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk)), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_ref, y_ref, z_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(functools.partial(matmul_kernel, bm=bm, bk=bk, bn=bn), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x, y), x @ y, atol=7e-05)",
    "assertions": [
      "assert k % bk == 0"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func(D0):\n\n    def shift(R, dR, **unused_kwargs):\n        return R + dR\n\n    def apply_fn(R):\n        return D0 * R\n    Rinit = jax.random.uniform(split, (n, 3), minval=0.0, maxval=5.0, dtype=jnp.float32)\n\n    def move(R, i):\n        F = apply_fn(R)\n        return (shift(R, 0.001 * F), jnp.array([0.0]))\n    move = remat(move)\n    R, temp = lax.scan(move, Rinit, jnp.arange(2))\n    return R[0, 0]"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM), ('hbm', pltpu.TPUMemorySpace.ANY))\ndef test_pipeline_matmul(self, memory_space):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(0))\n    x = jax.random.uniform(k1, (512, 512))\n    y = jax.random.uniform(k2, (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros(z_ref.shape, jnp.float32)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j, k: (i, k)), pl.BlockSpec((128, 128), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j, k: (i, j)))(x_ref, y_ref, z_ref)\n    z = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space))\n    jax.block_until_ready(z(x, y))\n    jax.block_until_ready(jnp.dot(x, y))\n    out = jax.block_until_ready(z(x, y))\n    expected_out = jax.block_until_ready(jnp.dot(x, y))\n    np.testing.assert_allclose(out, expected_out)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM), ('hbm', pltpu.TPUMemorySpace.ANY))\ndef test_double_pipeline_matmul(self, memory_space):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(0))\n    x = jax.random.uniform(k1, (512, 512))\n    y = jax.random.uniform(k2, (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros(z_ref.shape, jnp.float32)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref):\n\n        def emit_pipeline(should_accumulate_out):\n            pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j, k: (i, k)), pl.BlockSpec((128, 128), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j, k: (i, j)), should_accumulate_out=should_accumulate_out)(x_ref, y_ref, z_ref)\n        emit_pipeline(False)\n        emit_pipeline(True)\n    z = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space))(x, y)\n    np.testing.assert_allclose(z, jnp.dot(x, y) + jnp.dot(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.local_device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref, lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_ref, dst_ref=lhs_scratch_ref.at[1, working_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step - 1, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return pl.ds(pl.multiple_of(offset * sharded_k, sharded_k), sharded_k)\n        accum_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            prefetch_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            prefetch_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            prefetch_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[prefetch_phase, prefetch_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(prefetch_step, prefetch_phase)])\n            scheduler.prefetch(out_bref, out_ref, accum_schedule)\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref, allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=is_start, prefetch=prefetch, schedule=[None, None, accum_schedule])\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=pltpu.TPUCompilerParams(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9)))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_122', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_121', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = out_dtype\n    num_devices = jax.local_device_count()\n    tn = 128\n    tm = 128\n    tk = 128\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices\n    sharded_k = k // num_devices\n    half_m = m // 2\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(1, m // 2)], lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(0, m // 2)], lhs_scratch_ref.at[1, working_slot], bwd_send_sem)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return make_ds(offset, sharded_k)\n\n        def get_half(phase):\n            return make_ds(jnp.where(phase, 0, 1), m // 2)\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[next_phase, next_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(next_step, next_phase)])\n            scheduler.prefetch(out_bref, out_ref.at[get_half(next_phase)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref.at[get_half(phase)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch)\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, half_m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.local_device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref, lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_ref, dst_ref=lhs_scratch_ref.at[1, working_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step - 1, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return pl.ds(pl.multiple_of(offset * sharded_k, sharded_k), sharded_k)\n        accum_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            prefetch_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            prefetch_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            prefetch_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[prefetch_phase, prefetch_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(prefetch_step, prefetch_phase)])\n            scheduler.prefetch(out_bref, out_ref, accum_schedule)\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref, allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=is_start, prefetch=prefetch, schedule=[None, None, accum_schedule])\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=pltpu.TPUCompilerParams(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9)))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@jax.jit\ndef test(x: jax.Array) -> jax.Array:\n    return pl.pallas_call(test_kernel, out_shape=jax.ShapeDtypeStruct([x.shape[0], x.shape[1] * 2], x.dtype))(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_122', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_121', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = out_dtype\n    num_devices = jax.local_device_count()\n    tn = 128\n    tm = 128\n    tk = 128\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices\n    sharded_k = k // num_devices\n    half_m = m // 2\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(1, m // 2)], lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(0, m // 2)], lhs_scratch_ref.at[1, working_slot], bwd_send_sem)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return make_ds(offset, sharded_k)\n\n        def get_half(phase):\n            return make_ds(jnp.where(phase, 0, 1), m // 2)\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[next_phase, next_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(next_step, next_phase)])\n            scheduler.prefetch(out_bref, out_ref.at[get_half(next_phase)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref.at[get_half(phase)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch)\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, half_m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@jax.jit\ndef test(x: jax.Array) -> jax.Array:\n    return pl.pallas_call(test_kernel, out_shape=jax.ShapeDtypeStruct([x.shape[0], x.shape[1] * 2], x.dtype))(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@jax.jit\ndef test(x: jax.Array) -> jax.Array:\n    return pl.pallas_call(test_kernel, out_shape=jax.ShapeDtypeStruct([x.shape[0], x.shape[1] * 2], x.dtype))(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@jax.jit\ndef test(x: jax.Array) -> jax.Array:\n    return pl.pallas_call(test_kernel, out_shape=jax.ShapeDtypeStruct([x.shape[0], x.shape[1] * 2], x.dtype))(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(('float32', 'float32'), ('bfloat16', 'bfloat16'), ('int8', 'int8'))\n@hp.given(hps.integers(1, 1024), hps.integers(1, 1024), hps.integers(1, 1024), hps.sampled_from([8, 16, 32, 128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.integers(0, 4))\ndef test_padded_matmul(self, dtype, m, k, n, bm, bk, bn, seed):\n    if dtype == 'int8' and jtu.is_device_tpu_at_least(6):\n        self.skipTest('Not implemented for TPU v6.')\n\n    def align_up_to(x, y):\n        return (x + y - 1) // y * y\n    hp.assume(bm <= m)\n    hp.assume(bn <= n)\n    hp.assume(bk <= k)\n    if dtype == 'bfloat16':\n        hp.assume(bm >= 16)\n    if dtype == 'int8':\n        if not jtu.is_device_tpu_at_least(5):\n            self.skipTest('Only TPU v5+ allowed for int8.')\n        hp.assume(bm >= 32)\n    packing = 4 // jnp.dtype(dtype).itemsize\n    if packing != 1:\n        m = align_up_to(m, 8 * packing)\n        k = align_up_to(k, 8 * packing)\n    k1, k2 = jax.random.split(jax.random.key(seed))\n    x = jax.random.normal(k1, (m, k), jnp.float32).astype(dtype)\n    y = jax.random.normal(k2, (k, n), jnp.float32).astype(dtype)\n    out = matmul(x, y, bm=bm, bk=bk, bn=bn)\n    expected = x @ y\n    atol = rtol = 2.3e-05\n    if dtype == 'bfloat16':\n        out = out.astype('float32')\n        expected = expected.astype('float32')\n        atol = rtol = 0.01\n    np.testing.assert_allclose(out, expected, atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def matmul(impl, x, y):\n    z = impl(x, y)\n    return jnp.exp(jnp.tanh(z)).astype(x.dtype)"
  },
  {
    "test_code": "@parameterized.named_parameters(('float32', 'float32'), ('bfloat16', 'bfloat16'), ('int8', 'int8'))\n@hp.given(hps.integers(1, 1024), hps.integers(1, 1024), hps.integers(1, 1024), hps.sampled_from([8, 16, 32, 128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.integers(0, 4))\ndef test_padded_matmul(self, dtype, m, k, n, bm, bk, bn, seed):\n    if dtype == 'int8' and jtu.is_device_tpu_at_least(6):\n        self.skipTest('Not implemented for TPU v6.')\n\n    def align_up_to(x, y):\n        return (x + y - 1) // y * y\n    hp.assume(bm <= m)\n    hp.assume(bn <= n)\n    hp.assume(bk <= k)\n    if dtype == 'bfloat16':\n        hp.assume(bm >= 16)\n    if dtype == 'int8':\n        if not jtu.is_device_tpu_at_least(5):\n            self.skipTest('Only TPU v5+ allowed for int8.')\n        hp.assume(bm >= 32)\n    packing = 4 // jnp.dtype(dtype).itemsize\n    if packing != 1:\n        m = align_up_to(m, 8 * packing)\n        k = align_up_to(k, 8 * packing)\n    k1, k2 = jax.random.split(jax.random.key(seed))\n    x = jax.random.normal(k1, (m, k), jnp.float32).astype(dtype)\n    y = jax.random.normal(k2, (k, n), jnp.float32).astype(dtype)\n    out = matmul(x, y, bm=bm, bk=bk, bn=bn)\n    expected = x @ y\n    atol = rtol = 2.3e-05\n    if dtype == 'bfloat16':\n        out = out.astype('float32')\n        expected = expected.astype('float32')\n        atol = rtol = 0.01\n    np.testing.assert_allclose(out, expected, atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@jax.jit\ndef matmul(x: jax.Array, y: jax.Array):\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype), grid=(2, 2), in_specs=[pl.BlockSpec((x.shape[0] // 2, x.shape[1]), lambda i, j: (i, 0)), pl.BlockSpec((y.shape[0], y.shape[1] // 2), lambda i, j: (0, j))], out_specs=pl.BlockSpec((x.shape[0] // 2, y.shape[1] // 2), lambda i, j: (i, j)), interpret=mosaic_interpret.TPUInterpretParams())(x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM), ('hbm', pltpu.TPUMemorySpace.ANY))\ndef test_pipeline_matmul(self, memory_space):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(0))\n    x = jax.random.uniform(k1, (512, 512))\n    y = jax.random.uniform(k2, (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros(z_ref.shape, jnp.float32)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j, k: (i, k)), pl.BlockSpec((128, 128), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j, k: (i, j)))(x_ref, y_ref, z_ref)\n    z = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space))\n    jax.block_until_ready(z(x, y))\n    jax.block_until_ready(jnp.dot(x, y))\n    out = jax.block_until_ready(z(x, y))\n    expected_out = jax.block_until_ready(jnp.dot(x, y))\n    np.testing.assert_allclose(out, expected_out)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM), ('hbm', pltpu.TPUMemorySpace.ANY))\ndef test_double_pipeline_matmul(self, memory_space):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(0))\n    x = jax.random.uniform(k1, (512, 512))\n    y = jax.random.uniform(k2, (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros(z_ref.shape, jnp.float32)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref):\n\n        def emit_pipeline(should_accumulate_out):\n            pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j, k: (i, k)), pl.BlockSpec((128, 128), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j, k: (i, j)), should_accumulate_out=should_accumulate_out)(x_ref, y_ref, z_ref)\n        emit_pipeline(False)\n        emit_pipeline(True)\n    z = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space))(x, y)\n    np.testing.assert_allclose(z, jnp.dot(x, y) + jnp.dot(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.local_device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref, lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_ref, dst_ref=lhs_scratch_ref.at[1, working_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step - 1, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return pl.ds(pl.multiple_of(offset * sharded_k, sharded_k), sharded_k)\n        accum_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            prefetch_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            prefetch_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            prefetch_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[prefetch_phase, prefetch_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(prefetch_step, prefetch_phase)])\n            scheduler.prefetch(out_bref, out_ref, accum_schedule)\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref, allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=is_start, prefetch=prefetch, schedule=[None, None, accum_schedule])\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=pltpu.TPUCompilerParams(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9)))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_122', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_121', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = out_dtype\n    num_devices = jax.local_device_count()\n    tn = 128\n    tm = 128\n    tk = 128\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices\n    sharded_k = k // num_devices\n    half_m = m // 2\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(1, m // 2)], lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(0, m // 2)], lhs_scratch_ref.at[1, working_slot], bwd_send_sem)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return make_ds(offset, sharded_k)\n\n        def get_half(phase):\n            return make_ds(jnp.where(phase, 0, 1), m // 2)\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[next_phase, next_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(next_step, next_phase)])\n            scheduler.prefetch(out_bref, out_ref.at[get_half(next_phase)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref.at[get_half(phase)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch)\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, half_m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM), ('hbm', pltpu.TPUMemorySpace.ANY))\ndef test_pipeline_matmul(self, memory_space):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(0))\n    x = jax.random.uniform(k1, (512, 512))\n    y = jax.random.uniform(k2, (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros(z_ref.shape, jnp.float32)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j, k: (i, k)), pl.BlockSpec((128, 128), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j, k: (i, j)))(x_ref, y_ref, z_ref)\n    z = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space))\n    jax.block_until_ready(z(x, y))\n    jax.block_until_ready(jnp.dot(x, y))\n    out = jax.block_until_ready(z(x, y))\n    expected_out = jax.block_until_ready(jnp.dot(x, y))\n    np.testing.assert_allclose(out, expected_out)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM), ('hbm', pltpu.TPUMemorySpace.ANY))\ndef test_double_pipeline_matmul(self, memory_space):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(0))\n    x = jax.random.uniform(k1, (512, 512))\n    y = jax.random.uniform(k2, (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros(z_ref.shape, jnp.float32)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref):\n\n        def emit_pipeline(should_accumulate_out):\n            pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j, k: (i, k)), pl.BlockSpec((128, 128), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j, k: (i, j)), should_accumulate_out=should_accumulate_out)(x_ref, y_ref, z_ref)\n        emit_pipeline(False)\n        emit_pipeline(True)\n    z = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space))(x, y)\n    np.testing.assert_allclose(z, jnp.dot(x, y) + jnp.dot(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.local_device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref, lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_ref, dst_ref=lhs_scratch_ref.at[1, working_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step - 1, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return pl.ds(pl.multiple_of(offset * sharded_k, sharded_k), sharded_k)\n        accum_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            prefetch_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            prefetch_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            prefetch_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[prefetch_phase, prefetch_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(prefetch_step, prefetch_phase)])\n            scheduler.prefetch(out_bref, out_ref, accum_schedule)\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref, allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=is_start, prefetch=prefetch, schedule=[None, None, accum_schedule])\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=pltpu.TPUCompilerParams(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9)))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_122', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_121', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = out_dtype\n    num_devices = jax.local_device_count()\n    tn = 128\n    tm = 128\n    tk = 128\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices\n    sharded_k = k // num_devices\n    half_m = m // 2\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(1, m // 2)], lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(0, m // 2)], lhs_scratch_ref.at[1, working_slot], bwd_send_sem)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return make_ds(offset, sharded_k)\n\n        def get_half(phase):\n            return make_ds(jnp.where(phase, 0, 1), m // 2)\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[next_phase, next_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(next_step, next_phase)])\n            scheduler.prefetch(out_bref, out_ref.at[get_half(next_phase)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref.at[get_half(phase)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch)\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, half_m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_can_partition_nondivisible_grid_with_dynamic_dimensions(self):\n    self.skipTest('Broken test.')\n\n    def mul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def mul_kernel(iters_ref, x_ref, y_ref):\n        pltpu.emit_pipeline(mul_pipeline, grid=(iters_ref[0], 5), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(mul_kernel, out_shape=jax.ShapeDtypeStruct((640, 640), jnp.float32), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,)), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    x = jax.random.uniform(jax.random.key(0), (640, 640))\n    np.testing.assert_allclose(func(jnp.array([5]), x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_megacore_mul(self):\n    self.skipTest('Broken test.')\n    x = jax.random.uniform(jax.random.key(0), (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def matmul_kernel(x_ref, y_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.ARBITRARY, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters((1024, 1024, 1024, 256, 512, 256), (768, 1024, 1024, 256, 512, 256), (1024, 1024, 768, 256, 512, 256), (768, 1024, 768, 256, 512, 256))\ndef test_megacore_matmul(self, m, k, n, bm, bk, bn):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k))\n    y = jax.random.uniform(k2, (k, n))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros_like(z_ref)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref, *, bm, bk, bn):\n        m, k = x_ref.shape\n        _, n = y_ref.shape\n        assert k % bk == 0\n        pltpu.emit_pipeline(matmul_pipeline, grid=(pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk)), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_ref, y_ref, z_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(functools.partial(matmul_kernel, bm=bm, bk=bk, bn=bn), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x, y), x @ y, atol=7e-05)",
    "assertions": [
      "assert k % bk == 0"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(('float32', 'float32'), ('bfloat16', 'bfloat16'), ('int8', 'int8'))\n@hp.given(hps.integers(1, 1024), hps.integers(1, 1024), hps.integers(1, 1024), hps.sampled_from([8, 16, 32, 128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.integers(0, 4))\ndef test_padded_matmul(self, dtype, m, k, n, bm, bk, bn, seed):\n    if dtype == 'int8' and jtu.is_device_tpu_at_least(6):\n        self.skipTest('Not implemented for TPU v6.')\n\n    def align_up_to(x, y):\n        return (x + y - 1) // y * y\n    hp.assume(bm <= m)\n    hp.assume(bn <= n)\n    hp.assume(bk <= k)\n    if dtype == 'bfloat16':\n        hp.assume(bm >= 16)\n    if dtype == 'int8':\n        if not jtu.is_device_tpu_at_least(5):\n            self.skipTest('Only TPU v5+ allowed for int8.')\n        hp.assume(bm >= 32)\n    packing = 4 // jnp.dtype(dtype).itemsize\n    if packing != 1:\n        m = align_up_to(m, 8 * packing)\n        k = align_up_to(k, 8 * packing)\n    k1, k2 = jax.random.split(jax.random.key(seed))\n    x = jax.random.normal(k1, (m, k), jnp.float32).astype(dtype)\n    y = jax.random.normal(k2, (k, n), jnp.float32).astype(dtype)\n    out = matmul(x, y, bm=bm, bk=bk, bn=bn)\n    expected = x @ y\n    atol = rtol = 2.3e-05\n    if dtype == 'bfloat16':\n        out = out.astype('float32')\n        expected = expected.astype('float32')\n        atol = rtol = 0.01\n    np.testing.assert_allclose(out, expected, atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(('float32', 'float32'), ('bfloat16', 'bfloat16'), ('int8', 'int8'))\n@hp.given(hps.integers(1, 1024), hps.integers(1, 1024), hps.integers(1, 1024), hps.sampled_from([8, 16, 32, 128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.integers(0, 4))\ndef test_padded_matmul(self, dtype, m, k, n, bm, bk, bn, seed):\n    if dtype == 'int8' and jtu.is_device_tpu_at_least(6):\n        self.skipTest('Not implemented for TPU v6.')\n\n    def align_up_to(x, y):\n        return (x + y - 1) // y * y\n    hp.assume(bm <= m)\n    hp.assume(bn <= n)\n    hp.assume(bk <= k)\n    if dtype == 'bfloat16':\n        hp.assume(bm >= 16)\n    if dtype == 'int8':\n        if not jtu.is_device_tpu_at_least(5):\n            self.skipTest('Only TPU v5+ allowed for int8.')\n        hp.assume(bm >= 32)\n    packing = 4 // jnp.dtype(dtype).itemsize\n    if packing != 1:\n        m = align_up_to(m, 8 * packing)\n        k = align_up_to(k, 8 * packing)\n    k1, k2 = jax.random.split(jax.random.key(seed))\n    x = jax.random.normal(k1, (m, k), jnp.float32).astype(dtype)\n    y = jax.random.normal(k2, (k, n), jnp.float32).astype(dtype)\n    out = matmul(x, y, bm=bm, bk=bk, bn=bn)\n    expected = x @ y\n    atol = rtol = 2.3e-05\n    if dtype == 'bfloat16':\n        out = out.astype('float32')\n        expected = expected.astype('float32')\n        atol = rtol = 0.01\n    np.testing.assert_allclose(out, expected, atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def matmul(x, y):\n\n    def run_matmul(refs):\n        x_ref, y_ref, o_ref = refs\n\n        def matmul_pipeline_kernel(acc_ref):\n            pltpu.emit_pipeline(functools.partial(matmul_kernel, acc_ref), grid=(m // bm, n // bn, k // bk), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)))(x_ref, y_ref, o_ref)\n        pl.pallas_call(matmul_pipeline_kernel, out_shape=[], scratch_shapes=[pltpu.VMEM((bm, bn), jnp.float32)])()\n    _, _, o = pl.run_state(run_matmul)((x, y, jnp.ones((m, n), dtype=x.dtype)))\n    return o"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "def test_can_partition_nondivisible_grid_with_dynamic_dimensions(self):\n    self.skipTest('Broken test.')\n\n    def mul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def mul_kernel(iters_ref, x_ref, y_ref):\n        pltpu.emit_pipeline(mul_pipeline, grid=(iters_ref[0], 5), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(mul_kernel, out_shape=jax.ShapeDtypeStruct((640, 640), jnp.float32), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,)), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    x = jax.random.uniform(jax.random.key(0), (640, 640))\n    np.testing.assert_allclose(func(jnp.array([5]), x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func():\n\n    def dma_kernel(x, y):\n\n        def body(dma_sem, sem):\n            pltpu.async_copy(x, y, dma_sem).wait()\n            pltpu.semaphore_signal(sem)\n            pltpu.semaphore_wait(sem)\n        pl.run_scoped(body, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.REGULAR)\n    x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n    y = pl.pallas_call(dma_kernel, out_shape=x)(x)\n    return jnp.array_equal(x, y).astype(jnp.float32)"
  },
  {
    "test_code": "def test_megacore_mul(self):\n    self.skipTest('Broken test.')\n    x = jax.random.uniform(jax.random.key(0), (512, 512))\n\n    def matmul_pipeline(x_ref, y_ref):\n        y_ref[...] = x_ref[...] * 2\n\n    def matmul_kernel(x_ref, y_ref):\n        pltpu.emit_pipeline(matmul_pipeline, grid=(4, 4), in_specs=[pl.BlockSpec((128, 128), lambda i, j: (i, j))], out_specs=pl.BlockSpec((128, 128), lambda i, j: (i, j)), core_axis=0, dimension_semantics=(pltpu.ARBITRARY, pltpu.PARALLEL))(x_ref, y_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((512, 512), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x), x * 2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func():\n\n    def dma_kernel(x, y):\n\n        def body(dma_sem, sem):\n            pltpu.async_copy(x, y, dma_sem).wait()\n            pltpu.semaphore_signal(sem)\n            pltpu.semaphore_wait(sem)\n        pl.run_scoped(body, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.REGULAR)\n    x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n    y = pl.pallas_call(dma_kernel, out_shape=x)(x)\n    return jnp.array_equal(x, y).astype(jnp.float32)"
  },
  {
    "test_code": "@parameterized.parameters((1024, 1024, 1024, 256, 512, 256), (768, 1024, 1024, 256, 512, 256), (1024, 1024, 768, 256, 512, 256), (768, 1024, 768, 256, 512, 256))\ndef test_megacore_matmul(self, m, k, n, bm, bk, bn):\n    self.skipTest('Broken test.')\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k))\n    y = jax.random.uniform(k2, (k, n))\n\n    def matmul_pipeline(x_ref, y_ref, z_ref):\n\n        @pl.when(pl.program_id(2) == 0)\n        def _():\n            z_ref[...] = jnp.zeros_like(z_ref)\n        z_ref[...] += x_ref[...] @ y_ref[...]\n\n    def matmul_kernel(x_ref, y_ref, z_ref, *, bm, bk, bn):\n        m, k = x_ref.shape\n        _, n = y_ref.shape\n        assert k % bk == 0\n        pltpu.emit_pipeline(matmul_pipeline, grid=(pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk)), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_ref, y_ref, z_ref)\n    num_cores = jax.devices()[0].num_cores\n    func = pl.pallas_call(functools.partial(matmul_kernel, bm=bm, bk=bk, bn=bn), out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,), compiler_params=dict(mosaic=dict(dimension_semantics=('parallel',))))\n    np.testing.assert_allclose(func(x, y), x @ y, atol=7e-05)",
    "assertions": [
      "assert k % bk == 0"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def func():\n\n    def dma_kernel(x, y):\n\n        def body(dma_sem, sem):\n            pltpu.async_copy(x, y, dma_sem).wait()\n            pltpu.semaphore_signal(sem)\n            pltpu.semaphore_wait(sem)\n        pl.run_scoped(body, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.REGULAR)\n    x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n    y = pl.pallas_call(dma_kernel, out_shape=x)(x)\n    return jnp.array_equal(x, y).astype(jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.named_parameters(('float32', 'float32'), ('bfloat16', 'bfloat16'), ('int8', 'int8'))\n@hp.given(hps.integers(1, 1024), hps.integers(1, 1024), hps.integers(1, 1024), hps.sampled_from([8, 16, 32, 128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.integers(0, 4))\ndef test_padded_matmul(self, dtype, m, k, n, bm, bk, bn, seed):\n    if dtype == 'int8' and jtu.is_device_tpu_at_least(6):\n        self.skipTest('Not implemented for TPU v6.')\n\n    def align_up_to(x, y):\n        return (x + y - 1) // y * y\n    hp.assume(bm <= m)\n    hp.assume(bn <= n)\n    hp.assume(bk <= k)\n    if dtype == 'bfloat16':\n        hp.assume(bm >= 16)\n    if dtype == 'int8':\n        if not jtu.is_device_tpu_at_least(5):\n            self.skipTest('Only TPU v5+ allowed for int8.')\n        hp.assume(bm >= 32)\n    packing = 4 // jnp.dtype(dtype).itemsize\n    if packing != 1:\n        m = align_up_to(m, 8 * packing)\n        k = align_up_to(k, 8 * packing)\n    k1, k2 = jax.random.split(jax.random.key(seed))\n    x = jax.random.normal(k1, (m, k), jnp.float32).astype(dtype)\n    y = jax.random.normal(k2, (k, n), jnp.float32).astype(dtype)\n    out = matmul(x, y, bm=bm, bk=bk, bn=bn)\n    expected = x @ y\n    atol = rtol = 2.3e-05\n    if dtype == 'bfloat16':\n        out = out.astype('float32')\n        expected = expected.astype('float32')\n        atol = rtol = 0.01\n    np.testing.assert_allclose(out, expected, atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def matmul(a, b):\n    return jnp.matmul(a, b)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.local_device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref, lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_ref, dst_ref=lhs_scratch_ref.at[1, working_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step - 1, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return pl.ds(pl.multiple_of(offset * sharded_k, sharded_k), sharded_k)\n        accum_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            prefetch_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            prefetch_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            prefetch_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[prefetch_phase, prefetch_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(prefetch_step, prefetch_phase)])\n            scheduler.prefetch(out_bref, out_ref, accum_schedule)\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref, allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=is_start, prefetch=prefetch, schedule=[None, None, accum_schedule])\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=pltpu.TPUCompilerParams(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9)))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def make_async_copy(target_memory_space=None):\n    if target_memory_space is None:\n        target_memory_space = pltpu.ANY\n\n    @jax.named_call\n    def copy_start(x: jax.Array) -> tuple[jax.Array, Future]:\n\n        def copy_start_kernel(x_ref, aliased_x_ref, o_ref, sem):\n            del aliased_x_ref\n            pltpu.make_async_copy(x_ref, o_ref, sem).start()\n        x, out, sem = pl.pallas_call(copy_start_kernel, out_shape=(jax.ShapeDtypeStruct(x.shape, x.dtype), target_memory_space(x.shape, x.dtype), pltpu.SemaphoreType.DMA(())), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=(pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)), input_output_aliases={0: 0})(x)\n        return (x, (out, sem))\n\n    @jax.named_call\n    def copy_done(x: jax.Array, future: Future) -> jax.Array:\n        out, sem = future\n\n        def copy_done_kernel(x_ref, o_ref, sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, o_ref, sem).wait()\n        out = pl.pallas_call(copy_done_kernel, out_shape=target_memory_space(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=target_memory_space), input_output_aliases={1: 0})(x, out, sem)\n        return out\n    return (copy_start, copy_done)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_122', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_121', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = out_dtype\n    num_devices = jax.local_device_count()\n    tn = 128\n    tm = 128\n    tk = 128\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices\n    sharded_k = k // num_devices\n    half_m = m // 2\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(1, m // 2)], lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(0, m // 2)], lhs_scratch_ref.at[1, working_slot], bwd_send_sem)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return make_ds(offset, sharded_k)\n\n        def get_half(phase):\n            return make_ds(jnp.where(phase, 0, 1), m // 2)\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[next_phase, next_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(next_step, next_phase)])\n            scheduler.prefetch(out_bref, out_ref.at[get_half(next_phase)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref.at[get_half(phase)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch)\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, half_m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def make_async_copy(target_memory_space=None):\n    if target_memory_space is None:\n        target_memory_space = pltpu.ANY\n\n    @jax.named_call\n    def copy_start(x: jax.Array) -> tuple[jax.Array, Future]:\n\n        def copy_start_kernel(x_ref, aliased_x_ref, o_ref, sem):\n            del aliased_x_ref\n            pltpu.make_async_copy(x_ref, o_ref, sem).start()\n        x, out, sem = pl.pallas_call(copy_start_kernel, out_shape=(jax.ShapeDtypeStruct(x.shape, x.dtype), target_memory_space(x.shape, x.dtype), pltpu.SemaphoreType.DMA(())), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=(pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)), input_output_aliases={0: 0})(x)\n        return (x, (out, sem))\n\n    @jax.named_call\n    def copy_done(x: jax.Array, future: Future) -> jax.Array:\n        out, sem = future\n\n        def copy_done_kernel(x_ref, o_ref, sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, o_ref, sem).wait()\n        out = pl.pallas_call(copy_done_kernel, out_shape=target_memory_space(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=target_memory_space), input_output_aliases={1: 0})(x, out, sem)\n        return out\n    return (copy_start, copy_done)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.local_device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref, lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_ref, dst_ref=lhs_scratch_ref.at[1, working_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step - 1, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return pl.ds(pl.multiple_of(offset * sharded_k, sharded_k), sharded_k)\n        accum_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            prefetch_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            prefetch_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            prefetch_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[prefetch_phase, prefetch_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(prefetch_step, prefetch_phase)])\n            scheduler.prefetch(out_bref, out_ref, accum_schedule)\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref, allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=is_start, prefetch=prefetch, schedule=[None, None, accum_schedule])\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=pltpu.TPUCompilerParams(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9)))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def make_async_remote_copy(axis_name: str, direction: str='right', target_memory_space=None):\n    if target_memory_space is None:\n        target_memory_space = pltpu.ANY\n\n    @jax.named_call\n    def copy_start(x: jax.Array) -> tuple[jax.Array, Future]:\n\n        def copy_start_kernel(x_ref, aliased_x_ref, o_ref, send_sem, recv_sem):\n            del aliased_x_ref\n            axis_size = jax.lax.psum(1, axis_name)\n            left_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) - 1 + axis_size, axis_size)\n            right_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) + 1, axis_size)\n            if direction == 'right':\n                src_neighbor = left_neighbor\n                dst_neighbor = right_neighbor\n            else:\n                src_neighbor = right_neighbor\n                dst_neighbor = left_neighbor\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=src_neighbor, core_index=0)\n            pltpu.semaphore_wait(barrier_sem, 1)\n            pltpu.make_async_remote_copy(x_ref, o_ref, send_sem, recv_sem, device_id=dst_neighbor).start()\n        x, out, send_sem, recv_sem = pl.pallas_call(copy_start_kernel, out_shape=(jax.ShapeDtypeStruct(x.shape, x.dtype), target_memory_space(x.shape, x.dtype), pltpu.SemaphoreType.DMA(()), pltpu.SemaphoreType.DMA(())), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=(pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)), input_output_aliases={0: 0}, compiler_params=pltpu.TPUCompilerParams(collective_id=0, has_side_effects=True))(x)\n        return (x, (out, send_sem, recv_sem))\n\n    @jax.named_call\n    def send_done(x: jax.Array, future: Future) -> jax.Array:\n        _, send_sem, _ = future\n\n        def send_done_kernel(x_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, x_ref, send_sem).wait()\n        x = pl.pallas_call(send_done_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), input_output_aliases={0: 0})(x, send_sem)\n        return x\n\n    @jax.named_call\n    def recv_done(x: jax.Array, future: Future) -> jax.Array:\n        out, _, recv_sem = future\n\n        def send_done_kernel(x_ref, o_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, o_ref, send_sem).wait()\n        out = pl.pallas_call(send_done_kernel, out_shape=target_memory_space(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=target_memory_space), input_output_aliases={1: 0})(x, out, recv_sem)\n        return out\n    return (copy_start, send_done, recv_done)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 2, 2), ('hbm_float32_122', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_121', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_allgather_matmul(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = out_dtype\n    num_devices = jax.local_device_count()\n    tn = 128\n    tm = 128\n    tk = 128\n    n = tn * n_tiles\n    m = tm * m_tiles\n    k = tk * k_tiles * num_devices\n    outer_steps = num_devices\n    sharded_k = k // num_devices\n    half_m = m // 2\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def all_gather_lhs_matmul_kernel(lhs_ref, rhs_ref, out_ref, lhs_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        initial_bwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(1, m // 2)], lhs_scratch_ref.at[0, working_slot], bwd_send_sem)\n        initial_fwd_copy = pltpu.make_async_copy(lhs_ref.at[make_ds(0, m // 2)], lhs_scratch_ref.at[1, working_slot], bwd_send_sem)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[0, working_slot], dst_ref=lhs_scratch_ref.at[0, buffering_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=lhs_scratch_ref.at[1, working_slot], dst_ref=lhs_scratch_ref.at[1, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_rhs_slice(outer_step, phase):\n            bwd_rhs_offset = mod(my_id + outer_step, num_devices)\n            fwd_rhs_offset = mod(my_id - outer_step, num_devices)\n            offset = jnp.where(phase, fwd_rhs_offset, bwd_rhs_offset)\n            return make_ds(offset, sharded_k)\n\n        def get_half(phase):\n            return make_ds(jnp.where(phase, 0, 1), m // 2)\n\n        @pl.when(is_start)\n        @jax.named_scope('sync_and_bwd_init')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n            initial_bwd_copy.start()\n            initial_fwd_copy.start()\n            initial_bwd_copy.wait()\n\n        @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 0))\n        @jax.named_scope('send_next_dma')\n        def _send_next_dma():\n            bwd_copy.start()\n\n            @pl.when(jnp.logical_not(is_start))\n            def _send_next_fwd_dma():\n                fwd_copy.start()\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(is_start)\n            @jax.named_scope('fwd_init')\n            def _fwd_init():\n                initial_fwd_copy.wait()\n                fwd_copy.start()\n\n            @pl.when(jnp.logical_and(outer_step != outer_steps - 1, phase == 1))\n            @jax.named_scope('wait_on_prev_dma')\n            def _wait_on_prev_dma():\n                bwd_copy.wait()\n                fwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_scratch_ref.at[next_phase, next_working_slot])\n            scheduler.prefetch(rhs_bref, rhs_ref.at[get_rhs_slice(next_step, next_phase)])\n            scheduler.prefetch(out_bref, out_ref.at[get_half(next_phase)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_scratch_ref.at[phase, working_slot], rhs_ref.at[get_rhs_slice(outer_step, phase)], out_ref.at[get_half(phase)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch)\n    kernel = pl.pallas_call(all_gather_lhs_matmul_kernel, out_shape=[jax.ShapeDtypeStruct((m, n), out_dtype), jax.ShapeDtypeStruct((2, 2, half_m, sharded_k), input_dtype)], grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()), ['x']), in_specs=(P(None, 'x'), P(None, None)), out_specs=P(None, None), check_rep=False)\n    test = jax.jit(shard(kernel))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        x = jax.lax.all_gather(x, 'x', axis=1, tiled=True)\n        return jnp.dot(x, y, preferred_element_type=out_dtype)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y)[0])\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def make_async_remote_copy(axis_name: str, direction: str='right', target_memory_space=None):\n    if target_memory_space is None:\n        target_memory_space = pltpu.ANY\n\n    @jax.named_call\n    def copy_start(x: jax.Array) -> tuple[jax.Array, Future]:\n\n        def copy_start_kernel(x_ref, aliased_x_ref, o_ref, send_sem, recv_sem):\n            del aliased_x_ref\n            axis_size = jax.lax.psum(1, axis_name)\n            left_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) - 1 + axis_size, axis_size)\n            right_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) + 1, axis_size)\n            if direction == 'right':\n                src_neighbor = left_neighbor\n                dst_neighbor = right_neighbor\n            else:\n                src_neighbor = right_neighbor\n                dst_neighbor = left_neighbor\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=src_neighbor, core_index=0)\n            pltpu.semaphore_wait(barrier_sem, 1)\n            pltpu.make_async_remote_copy(x_ref, o_ref, send_sem, recv_sem, device_id=dst_neighbor).start()\n        x, out, send_sem, recv_sem = pl.pallas_call(copy_start_kernel, out_shape=(jax.ShapeDtypeStruct(x.shape, x.dtype), target_memory_space(x.shape, x.dtype), pltpu.SemaphoreType.DMA(()), pltpu.SemaphoreType.DMA(())), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=(pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)), input_output_aliases={0: 0}, compiler_params=pltpu.TPUCompilerParams(collective_id=0, has_side_effects=True))(x)\n        return (x, (out, send_sem, recv_sem))\n\n    @jax.named_call\n    def send_done(x: jax.Array, future: Future) -> jax.Array:\n        _, send_sem, _ = future\n\n        def send_done_kernel(x_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, x_ref, send_sem).wait()\n        x = pl.pallas_call(send_done_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), input_output_aliases={0: 0})(x, send_sem)\n        return x\n\n    @jax.named_call\n    def recv_done(x: jax.Array, future: Future) -> jax.Array:\n        out, _, recv_sem = future\n\n        def send_done_kernel(x_ref, o_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, o_ref, send_sem).wait()\n        out = pl.pallas_call(send_done_kernel, out_shape=target_memory_space(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=target_memory_space), input_output_aliases={1: 0})(x, out, recv_sem)\n        return out\n    return (copy_start, send_done, recv_done)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def make_async_remote_copy(axis_name: str, direction: str='right', target_memory_space=None):\n    if target_memory_space is None:\n        target_memory_space = pltpu.ANY\n\n    @jax.named_call\n    def copy_start(x: jax.Array) -> tuple[jax.Array, Future]:\n\n        def copy_start_kernel(x_ref, aliased_x_ref, o_ref, send_sem, recv_sem):\n            del aliased_x_ref\n            axis_size = jax.lax.psum(1, axis_name)\n            left_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) - 1 + axis_size, axis_size)\n            right_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) + 1, axis_size)\n            if direction == 'right':\n                src_neighbor = left_neighbor\n                dst_neighbor = right_neighbor\n            else:\n                src_neighbor = right_neighbor\n                dst_neighbor = left_neighbor\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=src_neighbor, core_index=0)\n            pltpu.semaphore_wait(barrier_sem, 1)\n            pltpu.make_async_remote_copy(x_ref, o_ref, send_sem, recv_sem, device_id=dst_neighbor).start()\n        x, out, send_sem, recv_sem = pl.pallas_call(copy_start_kernel, out_shape=(jax.ShapeDtypeStruct(x.shape, x.dtype), target_memory_space(x.shape, x.dtype), pltpu.SemaphoreType.DMA(()), pltpu.SemaphoreType.DMA(())), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=(pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)), input_output_aliases={0: 0}, compiler_params=pltpu.TPUCompilerParams(collective_id=0, has_side_effects=True))(x)\n        return (x, (out, send_sem, recv_sem))\n\n    @jax.named_call\n    def send_done(x: jax.Array, future: Future) -> jax.Array:\n        _, send_sem, _ = future\n\n        def send_done_kernel(x_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, x_ref, send_sem).wait()\n        x = pl.pallas_call(send_done_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), input_output_aliases={0: 0})(x, send_sem)\n        return x\n\n    @jax.named_call\n    def recv_done(x: jax.Array, future: Future) -> jax.Array:\n        out, _, recv_sem = future\n\n        def send_done_kernel(x_ref, o_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, o_ref, send_sem).wait()\n        out = pl.pallas_call(send_done_kernel, out_shape=target_memory_space(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=target_memory_space), input_output_aliases={1: 0})(x, out, recv_sem)\n        return out\n    return (copy_start, send_done, recv_done)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def make_async_remote_copy(axis_name: str, direction: str='right', target_memory_space=None):\n    if target_memory_space is None:\n        target_memory_space = pltpu.ANY\n\n    @jax.named_call\n    def copy_start(x: jax.Array) -> tuple[jax.Array, Future]:\n\n        def copy_start_kernel(x_ref, aliased_x_ref, o_ref, send_sem, recv_sem):\n            del aliased_x_ref\n            axis_size = jax.lax.psum(1, axis_name)\n            left_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) - 1 + axis_size, axis_size)\n            right_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) + 1, axis_size)\n            if direction == 'right':\n                src_neighbor = left_neighbor\n                dst_neighbor = right_neighbor\n            else:\n                src_neighbor = right_neighbor\n                dst_neighbor = left_neighbor\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=src_neighbor, core_index=0)\n            pltpu.semaphore_wait(barrier_sem, 1)\n            pltpu.make_async_remote_copy(x_ref, o_ref, send_sem, recv_sem, device_id=dst_neighbor).start()\n        x, out, send_sem, recv_sem = pl.pallas_call(copy_start_kernel, out_shape=(jax.ShapeDtypeStruct(x.shape, x.dtype), target_memory_space(x.shape, x.dtype), pltpu.SemaphoreType.DMA(()), pltpu.SemaphoreType.DMA(())), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=(pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)), input_output_aliases={0: 0}, compiler_params=pltpu.TPUCompilerParams(collective_id=0, has_side_effects=True))(x)\n        return (x, (out, send_sem, recv_sem))\n\n    @jax.named_call\n    def send_done(x: jax.Array, future: Future) -> jax.Array:\n        _, send_sem, _ = future\n\n        def send_done_kernel(x_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, x_ref, send_sem).wait()\n        x = pl.pallas_call(send_done_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), input_output_aliases={0: 0})(x, send_sem)\n        return x\n\n    @jax.named_call\n    def recv_done(x: jax.Array, future: Future) -> jax.Array:\n        out, _, recv_sem = future\n\n        def send_done_kernel(x_ref, o_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, o_ref, send_sem).wait()\n        out = pl.pallas_call(send_done_kernel, out_shape=target_memory_space(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=target_memory_space), input_output_aliases={1: 0})(x, out, recv_sem)\n        return out\n    return (copy_start, send_done, recv_done)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(('float32', 'float32'), ('bfloat16', 'bfloat16'), ('int8', 'int8'))\n@hp.given(hps.integers(1, 1024), hps.integers(1, 1024), hps.integers(1, 1024), hps.sampled_from([8, 16, 32, 128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.integers(0, 4))\ndef test_padded_matmul(self, dtype, m, k, n, bm, bk, bn, seed):\n    if dtype == 'int8' and jtu.is_device_tpu_at_least(6):\n        self.skipTest('Not implemented for TPU v6.')\n\n    def align_up_to(x, y):\n        return (x + y - 1) // y * y\n    hp.assume(bm <= m)\n    hp.assume(bn <= n)\n    hp.assume(bk <= k)\n    if dtype == 'bfloat16':\n        hp.assume(bm >= 16)\n    if dtype == 'int8':\n        if not jtu.is_device_tpu_at_least(5):\n            self.skipTest('Only TPU v5+ allowed for int8.')\n        hp.assume(bm >= 32)\n    packing = 4 // jnp.dtype(dtype).itemsize\n    if packing != 1:\n        m = align_up_to(m, 8 * packing)\n        k = align_up_to(k, 8 * packing)\n    k1, k2 = jax.random.split(jax.random.key(seed))\n    x = jax.random.normal(k1, (m, k), jnp.float32).astype(dtype)\n    y = jax.random.normal(k2, (k, n), jnp.float32).astype(dtype)\n    out = matmul(x, y, bm=bm, bk=bk, bn=bn)\n    expected = x @ y\n    atol = rtol = 2.3e-05\n    if dtype == 'bfloat16':\n        out = out.astype('float32')\n        expected = expected.astype('float32')\n        atol = rtol = 0.01\n    np.testing.assert_allclose(out, expected, atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def matmul(x: jax.Array, y: jax.Array, x_sentinel: jax.Array, *, bm: int=128, bk: int=128, bn: int=640):\n    grid = (n // bn, k // bk)\n    return pl.pallas_call(matmul_kernel, out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32), in_specs=[pl.BlockSpec((bm, bk), lambda j, k: (0, k)), pl.BlockSpec((bk, bn), lambda j, k: (k, j)), pl.BlockSpec((bm, bn), lambda j, k: (0, j))], out_specs=pl.BlockSpec((bm, bn), lambda j, k: (0, j)), grid=grid, input_output_aliases={2: 0}, interpret=self.INTERPRET)(x, y, x_sentinel)"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 1, 1))\ndef test_pipeline_latency_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, sharded_m // tm, sharded_k // tk)\n    outer_steps = num_devices // 2\n    reduce_grid = (sharded_m // tm,)\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n\n    def reduce_kernel(out_ref, rs_accum_scratch_ref):\n        rs_accum_scratch_ref[...] = out_ref[...]\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, n), lambda m: (m, 0)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, accumulator_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref, reduce_in_bref, reduce_out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[1, buffering_slot], dst_ref=accumulator_ref.at[1, working_slot], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=accumulator_ref.at[0, working_slot], dst_ref=accumulator_ref.at[0, buffering_slot], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = mod(my_id + step + num_devices // 2 + 1, num_devices)\n            fwd_lhs_offset = mod(my_id - step - num_devices // 2, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * sharded_m, sharded_m), sharded_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_barrier():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start)\n                def _rdmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('send_prev_fwd_dma')\n                    def _send_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('send_prev_bwd_dma')\n                    def _send_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_slot])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accum():\n                scheduler.prefetch(out_bref, accumulator_ref.at[next_phase, next_working_slot])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, accumulator_ref.at[phase, working_slot], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n\n        @pl.when(is_end)\n        def _loop_epilogue():\n            pltpu.emit_pipeline(reduce_kernel, grid=reduce_grid)(accumulator_ref.at[1, 1], accumulator_ref.at[0, 0], allocations=[reduce_in_bref, reduce_out_bref], scratches=[], first_cycle=True, last_cycle=True, init_accumulators=False)\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, 2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[0, 0]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)\n    np.mean(np.abs(out - expected_out))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def _grid_size(grid):\n    size = jnp.array(1, jnp.int32)\n    for dim in grid:\n        size *= dim\n    return size"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM, jnp.bfloat16, 2, 2, 2), ('hbm', pltpu.TPUMemorySpace.ANY, jnp.bfloat16, 2, 2, 2), ('hbm_float32', pltpu.TPUMemorySpace.ANY, jnp.float32, 2, 4, 2), ('hbm_float32_112', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 2), ('hbm_float32_111', pltpu.TPUMemorySpace.ANY, jnp.float32, 1, 2, 1))\ndef test_pipeline_throughput_optimized_matmul_reducescatter(self, memory_space, out_dtype, n_tiles, m_tiles, k_tiles):\n    self.skipTest('Broken test.')\n    input_dtype = jnp.float32\n    num_devices = jax.device_count()\n    tn = 128 * 1\n    tm = 128 * 1\n    tk = 128 * 1\n    n = tn * n_tiles\n    m = tm * m_tiles * num_devices\n    k = tk * k_tiles * num_devices\n    sharded_m = m // num_devices\n    half_m = sharded_m // 2\n    sharded_k = k // num_devices\n    inner_grid = (n // tn, half_m // tm, sharded_k // tk)\n    outer_steps = num_devices\n    inner_kernel = partial(basic_matmul_kernel, k=sharded_k)\n    inner_allocs = [pltpu.BufferedRef.input(pl.BlockSpec((tm, tk), lambda n, m, k: (m, k)), input_dtype), pltpu.BufferedRef.input(pl.BlockSpec((tk, tn), lambda n, m, k: (k, n)), input_dtype), pltpu.BufferedRef.accumulator(pl.BlockSpec((tm, tn), lambda n, m, k: (m, n)), out_dtype)]\n\n    def reduce_scatter_lhs_matmul_kernel(lhs_ref, rhs_ref, rs_accum_scratch_ref, acc_scratch_ref, bwd_recv_sem, bwd_send_sem, fwd_recv_sem, fwd_send_sem, lhs_bref, rhs_bref, out_bref):\n        outer_step = pl.program_id(0)\n        phase = pl.program_id(1)\n        num_inner_steps = _grid_size(inner_grid)\n        trivial_loop = num_inner_steps == 1\n        is_start = jnp.logical_and(outer_step == 0, phase == 0)\n        is_end = jnp.logical_and(outer_step == outer_steps - 1, phase == 1)\n        working_slot = lax.rem(outer_step, 2)\n        buffering_slot = 1 - working_slot\n        my_id = lax.axis_index('x')\n        right_neighbor = mod(my_id + 1, num_devices)\n        left_neighbor = mod(my_id - 1, num_devices)\n        bwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(1, half_m)], dst_ref=rs_accum_scratch_ref.at[working_slot, make_ds(1, half_m)], send_sem=bwd_send_sem, recv_sem=bwd_recv_sem, device_id=left_neighbor)\n        fwd_copy = pltpu.make_async_remote_copy(src_ref=rs_accum_scratch_ref.at[working_slot, make_ds(0, half_m)], dst_ref=rs_accum_scratch_ref.at[buffering_slot, make_ds(0, half_m)], send_sem=fwd_send_sem, recv_sem=fwd_recv_sem, device_id=right_neighbor)\n\n        def get_lhs_slice(step, phase):\n            bwd_lhs_offset = 2 * mod(my_id + step + 1, num_devices) + 1\n            fwd_lhs_offset = 2 * mod(my_id - step - 1, num_devices)\n            offset = jnp.where(phase, bwd_lhs_offset, fwd_lhs_offset)\n            return (pl.ds(pl.multiple_of(offset * half_m, half_m), half_m), pl.ds(pl.multiple_of(0, sharded_k), sharded_k))\n\n        def get_accum_slice(phase, slot):\n            return (slot, make_ds(phase, half_m))\n        rhs_schedule = pltpu.get_pipeline_schedule('fixed')\n\n        @pl.when(is_start)\n        @jax.named_scope('sync')\n        def _sync_and_bwd_init():\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=left_neighbor)\n            pltpu.semaphore_signal(barrier_sem, device_id=right_neighbor)\n            pltpu.semaphore_wait(barrier_sem, 2)\n\n        def postyeet(lhs_bref, rhs_bref, out_bref, scheduler):\n            del lhs_bref, rhs_bref\n\n            @pl.when(~is_start & ~is_end)\n            def _rdmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('send_prev_fwd_dma')\n                def _send_prev_fwd_dma():\n                    fwd_copy.start()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('send_prev_bwd_dma')\n                def _send_prev_bwd_dma():\n                    bwd_copy.start()\n\n            @pl.when(trivial_loop)\n            def _prefetch_accumulator_late():\n\n                @pl.when(~is_start & ~is_end)\n                def _wait_dmas():\n\n                    @pl.when(phase == 1)\n                    @jax.named_scope('wait_prev_fwd_dma')\n                    def _wait_prev_fwd_dma():\n                        fwd_copy.wait()\n\n                    @pl.when(phase == 0)\n                    @jax.named_scope('wait_prev_bwd_dma')\n                    def _wait_prev_bwd_dma():\n                        bwd_copy.wait()\n\n                @pl.when(~is_start)\n                def _prefetch():\n                    next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n                    next_phase = 1 - phase\n                    scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n\n        def prefetch(lhs_bref, rhs_bref, out_bref, scheduler):\n\n            @pl.when(~is_start & ~is_end & ~trivial_loop)\n            def _wait_dmas():\n\n                @pl.when(phase == 1)\n                @jax.named_scope('wait_prev_fwd_dma')\n                def _wait_prev_fwd_dma():\n                    fwd_copy.wait()\n\n                @pl.when(phase == 0)\n                @jax.named_scope('wait_prev_bwd_dma')\n                def _wait_prev_bwd_dma():\n                    bwd_copy.wait()\n            next_working_slot = jnp.where(phase == 0, working_slot, buffering_slot)\n            next_step = jnp.where(phase == 0, outer_step, outer_step + 1)\n            next_phase = lax.rem(phase + 1, 2)\n            scheduler.prefetch(lhs_bref, lhs_ref.at[get_lhs_slice(next_step, next_phase)])\n            scheduler.prefetch(rhs_bref, rhs_ref, rhs_schedule)\n\n            @pl.when(~trivial_loop & ~is_start)\n            def _prefetch_accumulator():\n                scheduler.prefetch(out_bref, rs_accum_scratch_ref.at[get_accum_slice(next_phase, next_working_slot)])\n        pltpu.emit_pipeline(inner_kernel, grid=inner_grid)(lhs_ref.at[get_lhs_slice(outer_step, phase)], rhs_ref, rs_accum_scratch_ref.at[get_accum_slice(phase, working_slot)], allocations=[lhs_bref, rhs_bref, out_bref], scratches=[acc_scratch_ref], first_cycle=is_start, last_cycle=is_end, init_accumulators=outer_step == 0, prefetch=prefetch, postyeet=postyeet, schedule=[None, rhs_schedule, None])\n    kernel = pl.pallas_call(reduce_scatter_lhs_matmul_kernel, out_shape=jax.ShapeDtypeStruct((2, sharded_m, n), out_dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=memory_space), pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space), grid=(outer_steps, 2), scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)] + [pltpu.SemaphoreType.DMA] * 4 + inner_allocs), compiler_params=dict(mosaic=dict(collective_id=0, vmem_limit_bytes=int(134217728 * 0.9))))\n    shard = partial(shard_map.shard_map, mesh=jax.sharding.Mesh(mesh_utils.create_device_mesh((num_devices,), jax.devices()[:num_devices]), ['x']), in_specs=(P(None, 'x'), P('x', None)), out_specs=P('x', None), check_rep=False)\n    test = jax.jit(shard(lambda x, y: kernel(x, y)[1]))\n\n    @jax.jit\n    @shard\n    def reference(x, y):\n        unreduced = jnp.dot(x, y, preferred_element_type=out_dtype)\n        return lax.psum_scatter(unreduced, 'x', scatter_dimension=0, tiled=True)\n    k1, k2 = jax.random.split(jax.random.key(42))\n    x = jax.random.uniform(k1, (m, k), dtype=input_dtype, minval=-1, maxval=1)\n    y = jax.random.uniform(k2, (k, n), dtype=input_dtype, minval=-1, maxval=1)\n    out = jax.block_until_ready(test(x, y))\n    expected_out = jax.block_until_ready(reference(x, y))\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=1 if out_dtype == jnp.float32 else 5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "def _grid_size(grid):\n    size = jnp.array(1, jnp.int32)\n    for dim in grid:\n        size *= dim\n    return size"
  },
  {
    "test_code": "@parameterized.named_parameters(('float32', 'float32'), ('bfloat16', 'bfloat16'), ('int8', 'int8'))\n@hp.given(hps.integers(1, 1024), hps.integers(1, 1024), hps.integers(1, 1024), hps.sampled_from([8, 16, 32, 128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.sampled_from([128, 256, 512]), hps.integers(0, 4))\ndef test_padded_matmul(self, dtype, m, k, n, bm, bk, bn, seed):\n    if dtype == 'int8' and jtu.is_device_tpu_at_least(6):\n        self.skipTest('Not implemented for TPU v6.')\n\n    def align_up_to(x, y):\n        return (x + y - 1) // y * y\n    hp.assume(bm <= m)\n    hp.assume(bn <= n)\n    hp.assume(bk <= k)\n    if dtype == 'bfloat16':\n        hp.assume(bm >= 16)\n    if dtype == 'int8':\n        if not jtu.is_device_tpu_at_least(5):\n            self.skipTest('Only TPU v5+ allowed for int8.')\n        hp.assume(bm >= 32)\n    packing = 4 // jnp.dtype(dtype).itemsize\n    if packing != 1:\n        m = align_up_to(m, 8 * packing)\n        k = align_up_to(k, 8 * packing)\n    k1, k2 = jax.random.split(jax.random.key(seed))\n    x = jax.random.normal(k1, (m, k), jnp.float32).astype(dtype)\n    y = jax.random.normal(k2, (k, n), jnp.float32).astype(dtype)\n    out = matmul(x, y, bm=bm, bk=bk, bn=bn)\n    expected = x @ y\n    atol = rtol = 2.3e-05\n    if dtype == 'bfloat16':\n        out = out.astype('float32')\n        expected = expected.astype('float32')\n        atol = rtol = 0.01\n    np.testing.assert_allclose(out, expected, atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_pipeline_test.py",
    "function": "@partial(jax.jit, static_argnames=['bm', 'bk', 'bn'])\ndef matmul(x: jax.Array, y: jax.Array, *, bm: int, bk: int, bn: int):\n    m, k = x.shape\n    _, n = y.shape\n\n    def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n        grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n        def run(acc_scratch_ref):\n            pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n        accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n        pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))\n    num_cores = jax.devices()[0].num_cores\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((m, n), x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), grid=(num_cores,))(x, y)"
  }
]