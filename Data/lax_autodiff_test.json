[
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@jtu.sample_product([dict(base_shape=base_shape, axis=axis) for base_shape in [(4,), (3, 4), (2, 3, 4)] for axis in range(len(base_shape))], num_pieces=range(3), dtype=float_dtypes)\ndef testSplitGrad(self, axis, base_shape, dtype, num_pieces):\n    sizes = jtu.rand_int(self.rng(), 5)((num_pieces + 1,), np.int64)\n    shape = list(base_shape)\n    shape[axis] = np.sum(sizes)\n    rng = jtu.rand_default(self.rng())\n    operands = (rng(shape, dtype),)\n    split = lambda x: lax.split(x, sizes, axis)\n    check_grads(split, operands, 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, axis=axis) for shape in [(3,), (5, 3), (4, 9, 3)] for axis in [len(shape) - 1]], key_dtype=[np.float32], val_dtype=[np.float32], is_stable=[False, True])\ndef testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, is_stable):\n    rng = jtu.rand_default(self.rng())\n\n    def args_maker():\n        flat_keys = np.arange(math.prod(shape), dtype=key_dtype)\n        keys = self.rng().permutation(flat_keys).reshape(shape)\n        values = rng(shape, val_dtype)\n        return (keys, values)\n    keys, values = args_maker()\n    fun = lambda keys, values: lax.sort_key_val(keys, values, axis, is_stable)\n    check_grads(fun, (keys, values), 2, ['fwd', 'rev'], 0.01, 0.01, 0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, axis=axis) for shape in [(3,), (5, 3), (4, 9, 3)] for axis in [len(shape) - 1]], key_dtype=[np.float32], val_dtype=[np.float32], is_stable=[False, True])\ndef testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, is_stable):\n    rng = jtu.rand_default(self.rng())\n\n    def args_maker():\n        flat_keys = np.arange(math.prod(shape), dtype=key_dtype)\n        keys = self.rng().permutation(flat_keys).reshape(shape)\n        values = rng(shape, val_dtype)\n        return (keys, values)\n    keys, values = args_maker()\n    fun = lambda keys, values: lax.sort_key_val(keys, values, axis, is_stable)\n    check_grads(fun, (keys, values), 2, ['fwd', 'rev'], 0.01, 0.01, 0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def testHigherOrderGradientOfReciprocal(self):\n\n    def inv(x):\n        return 1 / x\n    grad_fn = jax.grad(jax.grad(jax.grad(jax.grad(jax.grad(jax.grad(inv))))))\n    self.assertAllClose(np.float32(0.0439453125), grad_fn(np.float32(4.0)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def grad_fn(batch):\n    return jax.value_and_grad(loss_fn)(params, batch)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, axis=axis) for shape in [(3,), (5, 3), (4, 9, 3)] for axis in [len(shape) - 1]], key_dtype=[np.float32], val_dtype=[np.float32], is_stable=[False, True])\ndef testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, is_stable):\n    rng = jtu.rand_default(self.rng())\n\n    def args_maker():\n        flat_keys = np.arange(math.prod(shape), dtype=key_dtype)\n        keys = self.rng().permutation(flat_keys).reshape(shape)\n        values = rng(shape, val_dtype)\n        return (keys, values)\n    keys, values = args_maker()\n    fun = lambda keys, values: lax.sort_key_val(keys, values, axis, is_stable)\n    check_grads(fun, (keys, values), 2, ['fwd', 'rev'], 0.01, 0.01, 0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def testStopGradient(self):\n\n    def f(x):\n        return lax.sin(x) * lax.cos(lax.stop_gradient(x))\n\n    def f2(x, y):\n        return lax.sin(x) * lax.cos(y)\n    x = 3.14\n    ans = jax.grad(f)(x)\n    expected = jax.grad(f2)(x, x)\n    self.assertAllClose(ans, expected)\n    ans = jax.grad(jax.grad(f))(x)\n    expected = jax.grad(jax.grad(f2))(x, x)\n    self.assertAllClose(ans, expected)\n    ans = jax.grad(lambda x: lax.stop_gradient({'foo': x})['foo'])(3.0)\n    expected = np.array(0.0)\n    self.assertAllClose(ans, expected, check_dtypes=False)\n    with jax.enable_checks(False):\n        with self.assertRaises(TypeError):\n            lax.stop_gradient(lambda x: x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, axis=axis) for shape in [(3,), (5, 3), (4, 9, 3)] for axis in [len(shape) - 1]], key_dtype=[np.float32], val_dtype=[np.float32], is_stable=[False, True])\ndef testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, is_stable):\n    rng = jtu.rand_default(self.rng())\n\n    def args_maker():\n        flat_keys = np.arange(math.prod(shape), dtype=key_dtype)\n        keys = self.rng().permutation(flat_keys).reshape(shape)\n        values = rng(shape, val_dtype)\n        return (keys, values)\n    keys, values = args_maker()\n    fun = lambda keys, values: lax.sort_key_val(keys, values, axis, is_stable)\n    check_grads(fun, (keys, values), 2, ['fwd', 'rev'], 0.01, 0.01, 0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def args_maker():\n    factor_shape = shape[:-1] + (2 * shape[-1],)\n    a = rng(factor_shape, dtype)\n    return [np.matmul(a, jnp.conj(T(a)))]"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.parameters(itertools.chain.from_iterable((jtu.sample_product_testcases([dict(op=rec.op, rng_factory=rec.rng_factory, order=rec.order, tol=rec.tol)], shapes=[shapes for shape_group in compatible_shapes for shapes in itertools.combinations_with_replacement(shape_group, rec.nargs)], dtype=rec.dtypes) for rec in LAX_GRAD_OPS)))\ndef testOpGrad(self, op, rng_factory, shapes, dtype, order, tol):\n    rng = rng_factory(self.rng())\n    if jtu.test_device_matches(['cpu']):\n        if op is lax.cosh and dtype == np.complex64:\n            tol = 0.3\n    if jtu.test_device_matches(['tpu']):\n        if op is lax.pow:\n            raise SkipTest('pow grad imprecise on tpu')\n        if op is lax.cos:\n            order = 1\n        if op is lax.log:\n            order = 1\n    tol = jtu.join_tolerance(0.15, tol) if jtu.num_float_bits(dtype) == 32 else tol\n    args = tuple((rng(shape, dtype) for shape in shapes))\n    check_grads(op, args, order, ['fwd', 'rev'], tol, tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@parameterized.parameters(itertools.chain.from_iterable((jtu.sample_product_testcases([dict(op=rec.op, tol=rec.tol)], special_value=rec.values) for rec in LAX_GRAD_SPECIAL_VALUE_TESTS)))\ndef testOpGradSpecialValue(self, op, special_value, tol):\n    if op in (lax.sinh, lax.cosh) and jtu.test_device_matches(['tpu']):\n        tol = {np.float32: 0.01}\n    check_grads(op, (special_value,), 2, ['fwd', 'rev'], rtol=tol, atol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product(from_dtype=inexact_dtypes, to_dtype=inexact_dtypes)\ndef testConvertElementTypeGrad(self, from_dtype, to_dtype):\n    rng = jtu.rand_default(self.rng())\n    tol = max(jtu.tolerance(to_dtype, jtu.default_gradient_tolerance), jtu.tolerance(from_dtype, jtu.default_gradient_tolerance))\n    args = (rng((2, 3), from_dtype),)\n    convert_element_type = lambda x: lax.convert_element_type(x, to_dtype)\n    convert_element_type = jtu.ignore_warning(category=NumpyComplexWarning)(convert_element_type)\n    check_grads(convert_element_type, args, 2, ['fwd', 'rev'], tol, tol, eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(), (2, 3)], dtype=grad_float_dtypes)\ndef testClampGrad(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    operand = rng(shape, dtype)\n    low = operand - dtype(10)\n    high = operand + dtype(10)\n    check_grads(lax.clamp, (operand, low, high), 2, ['fwd', 'rev'], eps=0.01)\n    check_grads(lax.clamp, (low, operand, high), 2, ['fwd', 'rev'], eps=0.01)\n    check_grads(lax.clamp, (low, high, operand), 2, ['fwd', 'rev'], eps=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(base_shape=base_shape, dim=dim) for base_shape in [(4,), (3, 4), (2, 3, 4)] for dim in range(len(base_shape))], num_arrs=[3], dtype=float_dtypes)\ndef testConcatenateGrad(self, dim, base_shape, dtype, num_arrs):\n    rng = jtu.rand_default(self.rng())\n    shapes = [base_shape[:dim] + (size,) + base_shape[dim + 1:] for size, _ in zip(itertools.cycle([3, 1, 4]), range(num_arrs))]\n    operands = tuple((rng(shape, dtype) for shape in shapes))\n    concatenate = lambda *args: lax.concatenate(args, dim)\n    check_grads(concatenate, operands, 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(base_shape=base_shape, axis=axis) for base_shape in [(4,), (3, 4), (2, 3, 4)] for axis in range(len(base_shape))], num_pieces=range(3), dtype=float_dtypes)\ndef testSplitGrad(self, axis, base_shape, dtype, num_pieces):\n    sizes = jtu.rand_int(self.rng(), 5)((num_pieces + 1,), np.int64)\n    shape = list(base_shape)\n    shape[axis] = np.sum(sizes)\n    rng = jtu.rand_default(self.rng())\n    operands = (rng(shape, dtype),)\n    split = lambda x: lax.split(x, sizes, axis)\n    check_grads(split, operands, 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(), (2, 3)], dtype=float_dtypes, broadcast_sizes=[(), (2,), (1, 2)])\ndef testBroadcastGrad(self, shape, dtype, broadcast_sizes):\n    rng = jtu.rand_default(self.rng())\n    args = (rng(shape, dtype),)\n    broadcast = lambda x: lax.broadcast(x, broadcast_sizes)\n    check_grads(broadcast, args, 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(inshape=inshape, outshape=outshape, dimensions=broadcast_dimensions) for inshape, outshape, broadcast_dimensions in [([2], [2, 2], [0]), ([2], [2, 2], [1]), ([2], [2, 3], [0]), ([], [2, 3], [])]], dtype=float_dtypes)\ndef testBroadcastInDimGrad(self, inshape, dtype, outshape, dimensions):\n    rng = jtu.rand_default(self.rng())\n    operand = rng(inshape, dtype)\n    broadcast_in_dim = lambda x: lax.broadcast_in_dim(x, outshape, dimensions)\n    check_grads(broadcast_in_dim, (operand,), 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(arg_shape=arg_shape, out_shape=out_shape, permutation=permutation) for arg_shape, out_shape, permutation in [[(3, 4), (12,), None], [(2, 1, 4), (8,), None], [(2, 2, 4), (2, 8), None], [(3, 4), (12,), (0, 1)], [(3, 4), (12,), (1, 0)], [(2, 1, 4), (8,), (0, 2, 1)], [(2, 1, 4), (8,), (2, 0, 1)], [(2, 2, 4), (2, 8), (0, 2, 1)], [(2, 2, 4), (2, 8), (2, 0, 1)]]], dtype=float_dtypes)\ndef testReshapeGrad(self, arg_shape, out_shape, permutation, dtype):\n    rng = jtu.rand_default(self.rng())\n    operand = rng(arg_shape, dtype)\n    reshape = lambda x: lax.reshape(x, out_shape, permutation)\n    check_grads(reshape, (operand,), 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, pads=pads) for shape, paddings in [[(), [()]], ((2, 3), [[(1, 2, 1), (0, 1, 0)], [(-1, 0, 0), (-1, 0, 2)]])] for pads in paddings], dtype=float_dtypes)\ndef testPadGrad(self, shape, dtype, pads):\n    rng = jtu.rand_small(self.rng())\n    operand = rng(shape, dtype)\n    pad = lambda operand: lax.pad(operand, np.array(0, dtype), pads)\n    check_grads(pad, (operand,), 2, ['fwd', 'rev'], eps=1.0)\n    operand = rng(shape, dtype)\n    padding_value = np.array(0.0, dtype)\n    pad = lambda operand, padding_value: lax.pad(operand, padding_value, pads)\n    check_grads(pad, (operand, padding_value), 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "def testReverseGrad(self):\n    rev = lambda operand: lax.rev(operand, dimensions)\n    dimensions = [0]\n    check_grads(rev, (np.array([3.0, 2.0, 1.0]),), 2)\n    dimensions = [0, 1]\n    check_grads(rev, (np.array([[6.0, 5.0, 4.0], [3.0, 2.0, 1.0]]),), 2, rtol={np.float32: 0.003})",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(arg_shape=arg_shape, pred_shape=pred_shape) for arg_shape in [(), (3,), (2, 3)] for pred_shape in ([(), arg_shape] if arg_shape else [()])], dtype=float_dtypes)\ndef testSelectGrad(self, pred_shape, arg_shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    pred = rng(pred_shape, np.bool_)\n    on_true = rng(arg_shape, dtype)\n    on_false = rng(arg_shape, dtype)\n    select = lambda on_true, on_false: lax.select(pred, on_true, on_false)\n    check_grads(select, (on_true, on_false), 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, starts=start_indices, limits=limit_indices, strides=strides) for shape, start_indices, limit_indices, strides in [[(3,), (1,), (2,), None], [(7,), (4,), (7,), None], [(5,), (1,), (5,), (2,)], [(8,), (1,), (6,), (2,)], [(5, 3), (1, 1), (3, 2), None], [(5, 3), (1, 1), (3, 1), None], [(7, 5, 3), (4, 0, 1), (7, 1, 3), None], [(5, 3), (1, 1), (2, 1), (1, 1)], [(5, 3), (1, 1), (5, 3), (2, 1)], [(3, 3, 5), (0, 2, 0), (3, 2, 5), (1, 2, 1)]]], dtype=float_dtypes)\ndef testSliceGrad(self, shape, dtype, starts, limits, strides):\n    rng = jtu.rand_default(self.rng())\n    operand = rng(shape, dtype)\n    slice = lambda x: lax.slice(x, starts, limits, strides)\n    check_grads(slice, (operand,), 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, start_indices=start_indices, size_indices=size_indices) for shape, start_indices, size_indices in [[(3,), (1,), (1,)], [(5, 3), (1, 1), (3, 1)], [(7, 5, 3), (4, 1, 0), (2, 0, 1)]]], dtype=float_dtypes)\ndef testDynamicSliceGrad(self, shape, dtype, start_indices, size_indices):\n    rng = jtu.rand_default(self.rng())\n    operand = rng(shape, dtype)\n    dynamic_slice = lambda x: lax.dynamic_slice(x, start_indices, size_indices)\n    check_grads(dynamic_slice, (operand,), 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, start_indices=start_indices, update_shape=update_shape) for shape, start_indices, update_shape in [[(3,), (1,), (1,)], [(5, 3), (1, 1), (3, 1)], [(7, 5, 3), (4, 1, 0), (2, 0, 1)]]], dtype=float_dtypes)\ndef testDynamicUpdateSliceGrad(self, shape, dtype, start_indices, update_shape):\n    rng = jtu.rand_default(self.rng())\n    operand = rng(shape, dtype)\n    update = rng(update_shape, dtype)\n    start_indices = np.array(start_indices)\n    dus = lambda x, y: lax.dynamic_update_slice(x, y, start_indices)\n    check_grads(dus, (operand, update), 2, ['fwd', 'rev'], eps=1.0)\n    dus = lambda x: lax.dynamic_update_slice(x, update, start_indices)\n    check_grads(dus, (operand,), 2, ['fwd', 'rev'], eps=1.0)\n    dus = lambda y: lax.dynamic_update_slice(operand, y, start_indices)\n    check_grads(dus, (update,), 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, perm=perm) for shape, perm in [[(3, 4), (1, 0)], [(3, 4), (0, 1)], [(3, 4, 5), (2, 1, 0)], [(3, 4, 5), (1, 0, 2)]]], dtype=float_dtypes)\ndef testTransposeGrad(self, shape, dtype, perm):\n    rng = jtu.rand_default(self.rng())\n    operand = rng(shape, dtype)\n    transpose = lambda x: lax.transpose(x, perm)\n    check_grads(transpose, (operand,), 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(init_val=init_val, op=op, dtype=dtype, rng_factory=rng_factory) for init_val, op, dtypes, rng_factory in [(0, lax.add, float_dtypes + jtu.dtypes.complex, jtu.rand_default), (-np.inf, lax.max, grad_inexact_dtypes, jtu.rand_unique_int), (np.inf, lax.min, grad_inexact_dtypes, jtu.rand_unique_int), (1, lax.mul, grad_float_dtypes, partial(jtu.rand_default, scale=1))] for dtype in dtypes], [dict(shape=shape, dims=dims) for shape, dims in [[(), ()], [(3, 4, 5), ()], [(3, 4, 5), (0,)], [(3, 4, 5), (1, 2)], [(3, 4, 5), (0, 2)], [(3, 4, 5), (0, 1, 2)], [(3, 1), (1,)], [(3, 0, 5), (1,)]]])\ndef testReduceGrad(self, op, init_val, shape, dtype, dims, rng_factory):\n    rng = rng_factory(self.rng())\n    if jtu.test_device_matches(['tpu']) and op is lax.mul:\n        raise SkipTest('unimplemented case')\n    tol = {dtypes.bfloat16: 0.2, np.float16: 0.1, np.float32: 0.1, np.float64: 0.001, np.complex64: 0.1}\n    operand = rng(shape, dtype)\n    init_val = np.asarray(init_val, dtype=dtype)\n    reduce = lambda operand: lax.reduce(operand, init_val, op, dims)\n    eps = 1.0 if dtypes.finfo(dtype).bits == 16 and op is lax.add else 0.1 if dtype == dtypes.bfloat16 else 0.01 if dtypes.finfo(dtype).bits == 32 else None\n    if op not in (lax.max, lax.min) or all((d > 0 for d in shape)):\n        check_grads(reduce, (operand,), 2, ['fwd', 'rev'], tol, tol, eps)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, dims=dims) for shape, dims in [[(3, 4, 5), ()], [(3, 4, 5), (0,)], [(3, 4, 5), (1, 2)], [(3, 4, 5), (0, 2)], [(3, 4, 5), (0, 1, 2)], [(3, 1), (1,)], [(3, 0, 5), (1,)]]], dtype=grad_float_dtypes)\ndef testReducePairGrad(self, shape, dtype, dims):\n    rng = jtu.rand_default(self.rng(), scale=1)\n    tol = {np.float32: 0.01, np.float64: 0.0001}\n    operands = (rng(shape, dtype), rng(shape, dtype))\n    init_vals = (np.array(0, dtype), np.array(1, dtype))\n\n    def op(xs, ys):\n        return (xs[0] + ys[0], xs[1] * ys[1])\n    reduce = lambda xs, ys: lax.reduce((xs, ys), init_vals, op, dims)\n    check_grads(reduce, operands, 2, ['fwd', 'rev'], tol, tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(init_val=init_val, op=op, dtype=dtype, rng_factory=rng_factory, shape=shape, dims=dims, strides=strides, padding=padding, base_dilation=base_dilation, window_dilation=window_dilation) for init_val, op, dtypes, rng_factory in [(0, lax.add, grad_float_dtypes, jtu.rand_small), (-np.inf, lax.max, grad_float_dtypes, jtu.rand_unique_int), (np.inf, lax.min, grad_float_dtypes, jtu.rand_unique_int)] for dtype in dtypes for shape, dims, strides, padding, base_dilation, window_dilation in itertools.chain(itertools.product([(4, 6)], [(2, 1), (1, 2)], [(1, 1), (2, 1), (1, 2)], ['VALID', 'SAME', [(0, 3), (1, 2)]], [(1, 1)] + [(2, 3)], [(1, 1)] + ([(1, 2)] if op is lax.add else [])), itertools.product([(3, 2, 4, 6)], [(1, 1, 2, 1), (2, 1, 2, 1)], [(1, 2, 2, 1), (1, 1, 1, 1)], ['VALID', 'SAME', [(0, 1), (1, 0), (2, 3), (0, 2)]], [(1, 1, 1, 1)] + [(2, 1, 3, 2)], [(1, 1, 1, 1)] + ([(1, 2, 2, 1)] if op is lax.add else [])))])\n@jtu.ignore_warning(category=UserWarning, message='Using reduced precision for gradient.*')\ndef testReduceWindowGrad(self, op, init_val, dtype, shape, dims, strides, padding, base_dilation, window_dilation, rng_factory):\n    rng = rng_factory(self.rng())\n    init_val = np.asarray(init_val, dtype=dtype)\n    gradient_order = 3\n    if jtu.test_device_matches(['tpu']) and op is not lax.add:\n        if len(shape) != 4 or dims != (1, 1, 2, 1) or (not isinstance(padding, str)):\n            raise SkipTest('Only R4 SelectAndScatter implemented on TPU')\n\n    def fun(operand):\n        return lax.reduce_window(operand, init_val, op, dims, strides, padding, base_dilation, window_dilation)\n    operand = rng(shape, dtype)\n    if op is lax.add:\n        eps = 1.0\n        tol = None\n    else:\n        self.assertEqual(np.unique(operand).size, operand.size, msg='test requires operand elements to be unique.')\n        eps = 0.01\n        tol = {np.float16: 0.1, np.float32: 0.06, np.float64: 0.06}\n    check_grads(fun, (operand,), gradient_order, ['fwd', 'rev'], tol, tol, eps)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(op=op, dtype=dtype) for op, types in [(lax.cumsum, [np.float32, np.float64]), (lax.cumprod, [np.float32, np.float64])] for dtype in types], [dict(shape=shape, axis=axis) for shape in [[10], [3, 4, 5]] for axis in range(len(shape))], reverse=[False, True])\ndef testCumulativeReduceGrad(self, op, shape, dtype, axis, reverse):\n    rng_factory = jtu.rand_default if dtypes.issubdtype(dtype, np.integer) else jtu.rand_small\n    rng = rng_factory(self.rng())\n    check_grads(partial(op, axis=axis, reverse=reverse), (rng(shape, dtype),), order=2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, axis=axis) for shape in [(5,), (5, 7), (4, 9, 3)] for axis in [len(shape) - 1]], dtype=[np.float32], is_stable=[False, True])\ndef testSortGrad(self, shape, dtype, axis, is_stable):\n    rng = jtu.rand_unique_int(self.rng())\n    operand = rng(shape, dtype)\n    sort = lambda x: lax.sort(x, dimension=axis, is_stable=is_stable)\n    check_grads(sort, (operand,), 2, ['fwd', 'rev'], eps=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, axis=axis) for shape in [(3,), (5, 3), (4, 9, 3)] for axis in [len(shape) - 1]], key_dtype=[np.float32], val_dtype=[np.float32], is_stable=[False, True])\ndef testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, is_stable):\n    rng = jtu.rand_default(self.rng())\n\n    def args_maker():\n        flat_keys = np.arange(math.prod(shape), dtype=key_dtype)\n        keys = self.rng().permutation(flat_keys).reshape(shape)\n        values = rng(shape, val_dtype)\n        return (keys, values)\n    keys, values = args_maker()\n    fun = lambda keys, values: lax.sort_key_val(keys, values, axis, is_stable)\n    check_grads(fun, (keys, values), 2, ['fwd', 'rev'], 0.01, 0.01, 0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product(dtype=[np.float32], shape=[(4,), (5, 5), (2, 1, 4)], k=[1, 3])\ndef testTopKGrad(self, shape, dtype, k):\n    flat_values = np.arange(math.prod(shape), dtype=dtype)\n    values = self.rng().permutation(flat_values).reshape(shape)\n    fun = lambda vs: lax.top_k(vs, k=k)[0]\n    check_grads(fun, (values,), 2, ['fwd', 'rev'], eps=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, idxs=idxs, axes=axes) for shape, idxs, axes in [[(3, 4, 5), (np.array([0, 2, 1]),), (0,)], [(3, 4, 5), (np.array([-1, -2]),), (0,)], [(3, 4, 5), (np.array([0, 2]), np.array([1, 3])), (0, 1)], [(3, 4, 5), (np.array([0, 2]), np.array([1, 3])), (0, 2)]]], dtype=float_dtypes)\n@jax.numpy_rank_promotion('allow')\ndef testIndexTakeGrad(self, shape, dtype, idxs, axes):\n    rng = jtu.rand_default(self.rng())\n    src = rng(shape, dtype)\n    index_take = lambda src: lax.index_take(src, idxs, axes)\n    check_grads(index_take, (src,), 2, ['fwd', 'rev'], eps=1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, idxs_shape=idxs.shape, idxs_dtype=idxs.dtype, dnums=dnums, slice_sizes=slice_sizes, max_idx=max_idx) for shape, idxs, dnums, slice_sizes, max_idx in [((5,), np.array([[0], [2]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,), 5), ((10,), np.array([[0], [0], [0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,), 9), ((10, 5), np.array([[0], [2], [1]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3), 3)]], dtype=grad_float_dtypes, mode=['clip', 'fill', 'promise_in_bounds'], iteration=range(5))\ndef testGatherGrad(self, shape, dtype, idxs_shape, idxs_dtype, dnums, slice_sizes, mode, max_idx, iteration):\n    rng = jtu.rand_default(self.rng())\n    if mode == 'promise_in_bounds':\n        rng_idx = jtu.rand_int(self.rng(), high=max_idx)\n    else:\n        rng_idx = jtu.rand_int(self.rng(), low=-max_idx, high=2 * max_idx)\n    idxs = rng_idx(idxs_shape, idxs_dtype)\n    gather = lambda x: lax.gather(x, idxs, dimension_numbers=dnums, slice_sizes=slice_sizes, mode=mode, fill_value=-1)\n    x = rng(shape, dtype)\n    check_grads(gather, (x,), 2, ['fwd', 'rev'], 0.01, 0.01, 1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(arg_shape=arg_shape, idxs_shape=idxs.shape, idxs_dtype=idxs.dtype, dnums=dnums, update_shape=update_shape, max_idx=max_idx) for arg_shape, idxs, update_shape, dnums, max_idx in [((5,), np.array([[0], [2]]), (2,), lax.ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)), 4), ((10,), np.array([[0], [0], [0]]), (3, 2), lax.ScatterDimensionNumbers(update_window_dims=(1,), inserted_window_dims=(), scatter_dims_to_operand_dims=(0,)), 9), ((10, 5), np.array([[0], [2], [1]]), (3, 3), lax.ScatterDimensionNumbers(update_window_dims=(1,), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)), 3)]], dtype=grad_float_dtypes, mode=['clip', 'fill', 'promise_in_bounds'], iteration=range(5))\ndef testScatterAddGrad(self, arg_shape, dtype, idxs_shape, idxs_dtype, update_shape, dnums, max_idx, mode, iteration):\n    rng = jtu.rand_default(self.rng())\n    if mode == 'promise_in_bounds':\n        rng_idx = jtu.rand_int(self.rng(), high=max_idx)\n    else:\n        rng_idx = jtu.rand_int(self.rng(), low=-max_idx, high=2 * max_idx)\n    idxs = rng_idx(idxs_shape, idxs_dtype)\n    scatter_add = lambda x, y: lax.scatter_add(x, idxs, y, dimension_numbers=dnums, mode=mode)\n    x = rng(arg_shape, dtype)\n    y = rng(update_shape, dtype)\n    check_grads(scatter_add, (x, y), 2, ['fwd', 'rev'], 0.01, 0.01, 1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(arg_shape=arg_shape, idxs=idxs, dnums=dnums, update_shape=update_shape, max_idx=max_idx, multiplier=multiplier) for arg_shape, idxs, update_shape, dnums, max_idx, multiplier in [((5,), np.array([[0], [2]]), (2,), lax.ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)), 4, 1), ((10,), np.array([[0], [0], [0]]), (3, 2), lax.ScatterDimensionNumbers(update_window_dims=(1,), inserted_window_dims=(), scatter_dims_to_operand_dims=(0,)), 4, 2), ((10, 5), np.array([[0], [2], [1]]), (3, 3), lax.ScatterDimensionNumbers(update_window_dims=(1,), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)), 9, 1)]], dtype=grad_float_dtypes)\ndef testScatterGrad(self, arg_shape, dtype, idxs, update_shape, dnums, max_idx, multiplier):\n    rng_idx = jtu.rand_unique_int(self.rng(), high=max_idx)\n    rng = jtu.rand_default(self.rng())\n    idxs = rng_idx(idxs.shape, idxs.dtype) * multiplier\n    scatter = lambda x, y: lax.scatter(x, idxs, y, dimension_numbers=dnums)\n    x = rng(arg_shape, dtype)\n    y = rng(update_shape, dtype)\n    check_grads(scatter, (x, y), 2, ['fwd', 'rev'], 0.01, 0.01, 1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "def testScatterGradSymbolicZeroUpdate(self):\n\n    def f(x):\n        n = x.shape[0]\n        y = np.arange(n, dtype=x.dtype)\n        return jax.device_put(x).at[np.diag_indices(n)].set(y)\n    rng = jtu.rand_default(self.rng())\n    check_grads(f, (rng((5, 5), np.float32),), 2, ['fwd', 'rev'], 0.01, 0.01, 1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(arg_shape=arg_shape, idxs=idxs, dnums=dnums, update_shape=update_shape) for arg_shape, idxs, update_shape, dnums in [((5,), np.array([[0], [2]]), (2,), lax.ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,))), ((10,), np.array([[0], [0], [0]]), (3, 2), lax.ScatterDimensionNumbers(update_window_dims=(1,), inserted_window_dims=(), scatter_dims_to_operand_dims=(0,))), ((10, 5), np.array([[0], [2], [1]]), (3, 3), lax.ScatterDimensionNumbers(update_window_dims=(1,), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)))]], dtype=grad_float_dtypes)\ndef testScatterMax(self, arg_shape, dtype, idxs, update_shape, dnums):\n    rng = jtu.rand_default(self.rng())\n    rng_idx = jtu.rand_int(self.rng(), high=max(arg_shape))\n    idxs = rng_idx(idxs.shape, idxs.dtype)\n    scatter_max = lambda x, y: lax.scatter_max(x, idxs, y, dnums)\n    x = rng(arg_shape, dtype)\n    y = rng(update_shape, dtype)\n    check_grads(scatter_max, (x, y), 2, ['fwd', 'rev'], 0.01, 0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(arg_shape=arg_shape, idxs=idxs, dnums=dnums, update_shape=update_shape) for arg_shape, idxs, update_shape, dnums in [((5,), np.array([[0], [2]]), (2,), lax.ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,))), ((10,), np.array([[0], [0], [0]]), (3, 2), lax.ScatterDimensionNumbers(update_window_dims=(1,), inserted_window_dims=(), scatter_dims_to_operand_dims=(0,))), ((10, 5), np.array([[0], [2], [1]]), (3, 3), lax.ScatterDimensionNumbers(update_window_dims=(1,), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)))]], dtype=grad_float_dtypes)\ndef testScatterMin(self, arg_shape, dtype, idxs, update_shape, dnums):\n    rng = jtu.rand_default(self.rng())\n    rng_idx = jtu.rand_int(self.rng(), high=max(arg_shape))\n    idxs = rng_idx(idxs.shape, idxs.dtype)\n    scatter_min = lambda x, y: lax.scatter_min(x, idxs, y, dnums)\n    x = rng(arg_shape, dtype)\n    y = rng(update_shape, dtype)\n    check_grads(scatter_min, (x, y), 2, ['fwd', 'rev'], 0.01, 0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "def testRemainder(self):\n\n    def gen_x(rng, size):\n        return rng.uniform(-9, 9, size=size)\n\n    def gen_y(rng, size):\n        return rng.uniform(0.1, 5, size=size) * rng.choice([-1, 1], size=size)\n    rng = self.rng()\n    x = gen_x(rng, (5, 8))\n    y = gen_y(rng, (1, 8))\n    assert not set(np.unique(x)) & set(np.unique(y))\n    check_grads(lax.rem, (x, y), 2, ['fwd', 'rev'])\n    rng = self.rng()\n    x = gen_x(rng, (1, 8))\n    y = gen_y(rng, (5, 8))\n    assert not set(np.unique(x)) & set(np.unique(y))\n    check_grads(lax.rem, (x, y), 2, ['fwd', 'rev'])",
    "assertions": [
      "assert not set(np.unique(x)) & set(np.unique(y))",
      "assert not set(np.unique(x)) & set(np.unique(y))"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, axis=axis) for shape in [(3,), (5, 3), (4, 9, 3)] for axis in [len(shape) - 1]], key_dtype=[np.float32], val_dtype=[np.float32], is_stable=[False, True])\ndef testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, is_stable):\n    rng = jtu.rand_default(self.rng())\n\n    def args_maker():\n        flat_keys = np.arange(math.prod(shape), dtype=key_dtype)\n        keys = self.rng().permutation(flat_keys).reshape(shape)\n        values = rng(shape, val_dtype)\n        return (keys, values)\n    keys, values = args_maker()\n    fun = lambda keys, values: lax.sort_key_val(keys, values, axis, is_stable)\n    check_grads(fun, (keys, values), 2, ['fwd', 'rev'], 0.01, 0.01, 0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def args_maker():\n    p = rng((length,), dtype)\n    return [jnp.concatenate([jnp.zeros(leading, p.dtype), p, jnp.zeros(trailing, p.dtype)])]"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, axis=axis) for shape in [(3,), (5, 3), (4, 9, 3)] for axis in [len(shape) - 1]], key_dtype=[np.float32], val_dtype=[np.float32], is_stable=[False, True])\ndef testSortKeyValGrad(self, shape, key_dtype, val_dtype, axis, is_stable):\n    rng = jtu.rand_default(self.rng())\n\n    def args_maker():\n        flat_keys = np.arange(math.prod(shape), dtype=key_dtype)\n        keys = self.rng().permutation(flat_keys).reshape(shape)\n        values = rng(shape, val_dtype)\n        return (keys, values)\n    keys, values = args_maker()\n    fun = lambda keys, values: lax.sort_key_val(keys, values, axis, is_stable)\n    check_grads(fun, (keys, values), 2, ['fwd', 'rev'], 0.01, 0.01, 0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def args_maker():\n    M = sprng(shape, dtype)\n    new_indices = jnp.concatenate([M.indices, M.indices], axis=n_batch)\n    new_data = jnp.concatenate([M.data, M.data], axis=n_batch)\n    return [sparse.BCOO((new_data, new_indices), shape=M.shape)]"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def testStopGradient(self):\n\n    def f(x):\n        return lax.sin(x) * lax.cos(lax.stop_gradient(x))\n\n    def f2(x, y):\n        return lax.sin(x) * lax.cos(y)\n    x = 3.14\n    ans = jax.grad(f)(x)\n    expected = jax.grad(f2)(x, x)\n    self.assertAllClose(ans, expected)\n    ans = jax.grad(jax.grad(f))(x)\n    expected = jax.grad(jax.grad(f2))(x, x)\n    self.assertAllClose(ans, expected)\n    ans = jax.grad(lambda x: lax.stop_gradient({'foo': x})['foo'])(3.0)\n    expected = np.array(0.0)\n    self.assertAllClose(ans, expected, check_dtypes=False)\n    with jax.enable_checks(False):\n        with self.assertRaises(TypeError):\n            lax.stop_gradient(lambda x: x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def testDynamicSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x):\n        return lax.dynamic_index_in_dim(x, index, axis).sum()\n    x = rng(shape, np.float32)\n    result1 = f(x)\n    result2, _ = jax.value_and_grad(f, 0)(x)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def testDynamicUpdateSliceValueAndGrad(self):\n    rng = jtu.rand_default(self.rng())\n    shape = (5, 5)\n    axis = 0\n    index = -(shape[axis] + 3)\n\n    def f(x, y):\n        return lax.dynamic_update_index_in_dim(x, y, index, axis).sum()\n    x = rng(shape, np.float32)\n    y = rng([1 for s in shape], np.float32)\n    result1 = f(x, y)\n    result2, _ = jax.value_and_grad(f, 0)(x, y)\n    self.assertAllClose(result1, result2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/lax_autodiff_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  }
]