[
  {
    "test_code": "@parameterized.named_parameters(*[(f'batch_size={batch_size!r}_seq_len={seq_len!r}_num_heads={num_heads!r}_head_dim={head_dim!r}_kwargs={kwargs!r}_start_idx={start_idx!r}_kv_seq_len={kv_seq_len!r}_return_residuals={return_residuals!r}', batch_size, seq_len, num_heads, head_dim, kwargs, start_idx, kv_seq_len, return_residuals) for batch_size, seq_len, num_heads, head_dim, kwargs in [(1, 1024, 1, 64, {}), (2, 1024, 2, 64, {}), (1, 1024, 8, 64, {})] for start_idx in [None, 123] for kv_seq_len in [None, 250] for return_residuals in [False, True]])\n@jax.numpy_dtype_promotion('standard')\ndef test_mqa(self, batch_size, seq_len, num_heads, head_dim, kwargs, start_idx, kv_seq_len, return_residuals):\n    del kwargs\n    normalize_output = not return_residuals\n    k1, k2, k3 = random.split(random.key(0), 3)\n    q = random.normal(k1, (batch_size, num_heads, head_dim), dtype=jnp.float16)\n    k = random.normal(k2, (batch_size, seq_len, head_dim), dtype=jnp.float16)\n    v = random.normal(k3, (batch_size, seq_len, head_dim), dtype=jnp.float16)\n    o, *res = decode_attention.mqa(q, k, v, start_idx=start_idx, kv_seq_len=kv_seq_len, return_residuals=return_residuals, normalize_output=normalize_output, interpret=self.INTERPRET)\n    o_ref, *res_ref = decode_attention.mqa_reference(q, k, v, start_idx=start_idx, kv_seq_len=kv_seq_len, return_residuals=return_residuals, normalize_output=normalize_output)\n    np.testing.assert_allclose(o, o_ref, atol=0.05)\n    if return_residuals:\n        l, m = res[0]\n        l_ref, m_ref = res_ref[0]\n        np.testing.assert_allclose(l, l_ref, atol=0.05)\n        np.testing.assert_allclose(m, m_ref, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_attention_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'batch_size={batch_size!r}_seq_len={seq_len!r}_num_q_heads={num_q_heads!r}_num_kv_heads={num_kv_heads!r}_head_dim={head_dim!r}_kwargs={kwargs!r}_start_idx={start_idx!r}_kv_seq_len={kv_seq_len!r}_return_residuals={return_residuals!r}', batch_size, seq_len, num_q_heads, num_kv_heads, head_dim, kwargs, start_idx, kv_seq_len, return_residuals) for batch_size, seq_len, num_q_heads, num_kv_heads, head_dim, kwargs in [(1, 1024, 16, 4, 64, {}), (1, 1024, 16, 16, 64, {}), (1, 1024, 32, 32, 64, {})] for start_idx in [None, 123] for kv_seq_len in [None, 250] for return_residuals in [False, True]])\n@jax.numpy_dtype_promotion('standard')\ndef test_gqa(self, batch_size, seq_len, num_q_heads, num_kv_heads, head_dim, kwargs, start_idx, kv_seq_len, return_residuals):\n    del kwargs\n    normalize_output = not return_residuals\n    k1, k2, k3 = random.split(random.key(0), 3)\n    q = random.normal(k1, (batch_size, num_q_heads, head_dim), dtype=jnp.float16)\n    k = random.normal(k2, (batch_size, seq_len, num_kv_heads, head_dim), dtype=jnp.float16)\n    v = random.normal(k3, (batch_size, seq_len, num_kv_heads, head_dim), dtype=jnp.float16)\n    o, *res = decode_attention.gqa(q, k, v, start_idx=start_idx, kv_seq_len=kv_seq_len, return_residuals=return_residuals, normalize_output=normalize_output, interpret=self.INTERPRET)\n    o_ref, *res_ref = decode_attention.gqa_reference(q, k, v, start_idx=start_idx, kv_seq_len=kv_seq_len, return_residuals=return_residuals, normalize_output=normalize_output)\n    np.testing.assert_allclose(o, o_ref, atol=0.05)\n    if return_residuals:\n        l, m = res[0]\n        l_ref, m_ref = res_ref[0]\n        np.testing.assert_allclose(l, l_ref, atol=0.05)\n        np.testing.assert_allclose(m, m_ref, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_attention_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'batch_size={batch_size!r}_seq_len={seq_len!r}_num_heads={num_heads!r}_head_dim={head_dim!r}_kwargs={kwargs!r}_start_idx={start_idx!r}_kv_seq_len={kv_seq_len!r}_return_residuals={return_residuals!r}', batch_size, seq_len, num_heads, head_dim, kwargs, start_idx, kv_seq_len, return_residuals) for batch_size, seq_len, num_heads, head_dim, kwargs in [(1, 1024, 1, 64, {}), (2, 1024, 2, 64, {}), (1, 1024, 8, 64, {})] for start_idx in [None, 123] for kv_seq_len in [None, 250] for return_residuals in [False, True]])\n@jax.numpy_dtype_promotion('standard')\ndef test_mqa(self, batch_size, seq_len, num_heads, head_dim, kwargs, start_idx, kv_seq_len, return_residuals):\n    del kwargs\n    normalize_output = not return_residuals\n    k1, k2, k3 = random.split(random.key(0), 3)\n    q = random.normal(k1, (batch_size, num_heads, head_dim), dtype=jnp.float16)\n    k = random.normal(k2, (batch_size, seq_len, head_dim), dtype=jnp.float16)\n    v = random.normal(k3, (batch_size, seq_len, head_dim), dtype=jnp.float16)\n    o, *res = decode_attention.mqa(q, k, v, start_idx=start_idx, kv_seq_len=kv_seq_len, return_residuals=return_residuals, normalize_output=normalize_output, interpret=self.INTERPRET)\n    o_ref, *res_ref = decode_attention.mqa_reference(q, k, v, start_idx=start_idx, kv_seq_len=kv_seq_len, return_residuals=return_residuals, normalize_output=normalize_output)\n    np.testing.assert_allclose(o, o_ref, atol=0.05)\n    if return_residuals:\n        l, m = res[0]\n        l_ref, m_ref = res_ref[0]\n        np.testing.assert_allclose(l, l_ref, atol=0.05)\n        np.testing.assert_allclose(m, m_ref, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_attention_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'batch_size={batch_size!r}_seq_len={seq_len!r}_num_q_heads={num_q_heads!r}_num_kv_heads={num_kv_heads!r}_head_dim={head_dim!r}_kwargs={kwargs!r}_start_idx={start_idx!r}_kv_seq_len={kv_seq_len!r}_return_residuals={return_residuals!r}', batch_size, seq_len, num_q_heads, num_kv_heads, head_dim, kwargs, start_idx, kv_seq_len, return_residuals) for batch_size, seq_len, num_q_heads, num_kv_heads, head_dim, kwargs in [(1, 1024, 16, 4, 64, {}), (1, 1024, 16, 16, 64, {}), (1, 1024, 32, 32, 64, {})] for start_idx in [None, 123] for kv_seq_len in [None, 250] for return_residuals in [False, True]])\n@jax.numpy_dtype_promotion('standard')\ndef test_gqa(self, batch_size, seq_len, num_q_heads, num_kv_heads, head_dim, kwargs, start_idx, kv_seq_len, return_residuals):\n    del kwargs\n    normalize_output = not return_residuals\n    k1, k2, k3 = random.split(random.key(0), 3)\n    q = random.normal(k1, (batch_size, num_q_heads, head_dim), dtype=jnp.float16)\n    k = random.normal(k2, (batch_size, seq_len, num_kv_heads, head_dim), dtype=jnp.float16)\n    v = random.normal(k3, (batch_size, seq_len, num_kv_heads, head_dim), dtype=jnp.float16)\n    o, *res = decode_attention.gqa(q, k, v, start_idx=start_idx, kv_seq_len=kv_seq_len, return_residuals=return_residuals, normalize_output=normalize_output, interpret=self.INTERPRET)\n    o_ref, *res_ref = decode_attention.gqa_reference(q, k, v, start_idx=start_idx, kv_seq_len=kv_seq_len, return_residuals=return_residuals, normalize_output=normalize_output)\n    np.testing.assert_allclose(o, o_ref, atol=0.05)\n    if return_residuals:\n        l, m = res[0]\n        l_ref, m_ref = res_ref[0]\n        np.testing.assert_allclose(l, l_ref, atol=0.05)\n        np.testing.assert_allclose(m, m_ref, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_attention_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  }
]