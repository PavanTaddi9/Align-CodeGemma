[
  {
    "test_code": "def testCustomJVPLeak(self):\n\n    @jax.jit\n    def fwd():\n        a = jnp.array(1.0)\n\n        def f(hx, _):\n            hx = jax.nn.sigmoid(hx + a)\n            return (hx, None)\n        hx = jnp.array(0.0)\n        jax.lax.scan(f, hx, None, length=2)\n    with jax.checking_leaks():\n        fwd()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.jit\n@partial(shard_map, mesh=mesh, in_specs=(P(axis_name, None), P(axis_name, None), P(axis_name, None), P(axis_name, None), P(axis_name, None), P(axis_name, None)), out_specs=P(axis_name), check_rep=False)\ndef fwd(operand, output, input_offsets, send_sizes, output_offsets, recv_sizes):\n    operand = operand.reshape(operand.shape[1:])\n    output = output.reshape(output.shape[1:])\n    input_offsets = input_offsets.reshape(input_offsets.shape[1:])\n    send_sizes = send_sizes.reshape(send_sizes.shape[1:])\n    output_offsets = output_offsets.reshape(output_offsets.shape[1:])\n    recv_sizes = recv_sizes.reshape(recv_sizes.shape[1:])\n    return lax.ragged_all_to_all(operand, output, input_offsets, send_sizes, output_offsets, recv_sizes, axis_name=axis_name, axis_index_groups=axis_index_groups)"
  },
  {
    "test_code": "def testCustomJVPLeak2(self):\n\n    @jax.custom_jvp\n    def sigmoid(x):\n        one = jnp.float32(1)\n        return jax.lax.div(one, jax.lax.add(one, jax.lax.exp(jax.lax.neg(x))))\n    sigmoid.defjvps(lambda g, ans, x: g * ans * (jnp.float32(1) - ans))\n\n    @jax.jit\n    def fwd():\n        a = jnp.array(1.0, 'float32')\n\n        def f(hx, _):\n            hx = sigmoid(hx + a)\n            return (hx, None)\n        hx = jnp.array(0.0, 'float32')\n        jax.lax.scan(f, hx, None, length=2)\n    with jax.checking_leaks():\n        fwd()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.jit\n@partial(shard_map, mesh=mesh, in_specs=(P(axis_name, None), P(axis_name, None), P(axis_name, None), P(axis_name, None), P(axis_name, None), P(axis_name, None)), out_specs=P(axis_name), check_rep=False)\ndef fwd(operand, output, input_offsets, send_sizes, output_offsets, recv_sizes):\n    operand = operand.reshape(operand.shape[1:])\n    output = output.reshape(output.shape[1:])\n    input_offsets = input_offsets.reshape(input_offsets.shape[1:])\n    send_sizes = send_sizes.reshape(send_sizes.shape[1:])\n    output_offsets = output_offsets.reshape(output_offsets.shape[1:])\n    recv_sizes = recv_sizes.reshape(recv_sizes.shape[1:])\n    return lax.ragged_all_to_all(operand, output, input_offsets, send_sizes, output_offsets, recv_sizes, axis_name=axis_name, axis_index_groups=axis_index_groups)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product((jnp.float32, jnp.bfloat16, jnp.float16), (partial(nn.gelu, approximate=False), partial(nn.gelu, approximate=True), nn.relu, nn.softplus, nn.sparse_plus, nn.sigmoid, nn.squareplus, nn.mish)))\ndef testDtypeMatchesInput(self, dtype, fn):\n    x = jnp.zeros((), dtype=dtype)\n    out = fn(x)\n    self.assertEqual(out.dtype, dtype)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@parameterized.product(is_training=[True, False], output_type=[jnp.float16, jnp.bfloat16, jnp.float32], impl=['cudnn'])\ndef testScaledDotGeneral(self, is_training, output_type, impl):\n    if impl == 'cudnn' and (not _is_required_cudnn_version_satisfied('10.0', 90700)):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible')\n    configs = create_mxfp8_configs_if_available()\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=jnp.float32)\n    k1, k2 = jax.random.split(jax.random.key(0), 2)\n    a_shape = [2, 256, 96]\n    b_shape = [2, 96, 160]\n    dimension_numbers = (([2], [1]), ([0], [0]))\n    a = cast_to_representable(jax.random.uniform(k1, a_shape, minval=-1.0, dtype=output_type), configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, b_shape, minval=-1.0, dtype=output_type), configs[1].data_type)\n    scaled_dot_general_fn = partial(nn.scaled_dot_general, configs=configs)\n\n    def fwd(a, b, is_ref=False):\n        fn = jax.lax.dot_general if is_ref else scaled_dot_general_fn\n        y = fn(a, b, dimension_numbers, preferred_element_type=output_type)\n        return jnp.sum(y)\n    if is_training:\n        j_train = jax.jit(jax.value_and_grad(fwd, argnums=[0, 1]))\n        j_train_ref = jax.jit(jax.value_and_grad(partial(fwd, is_ref=True), argnums=[0, 1]))\n        out, (x_grad, w_grad) = j_train(a, b)\n        out_ref, (x_grad_ref, w_grad_ref) = j_train_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(x_grad, x_grad_ref, rtol=0.01, atol=10.0)\n        self.assertArraysAllClose(w_grad, w_grad_ref, rtol=0.01, atol=10.0)\n    else:\n        j_inference = jax.jit(fwd)\n        j_inference_ref = jax.jit(partial(fwd, is_ref=True))\n        out = j_inference(a, b)\n        out_ref = j_inference_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product((jnp.float32, jnp.bfloat16, jnp.float16), (partial(nn.gelu, approximate=False), partial(nn.gelu, approximate=True), nn.relu, nn.softplus, nn.sparse_plus, nn.sigmoid, nn.squareplus, nn.mish)))\ndef testDtypeMatchesInput(self, dtype, fn):\n    x = jnp.zeros((), dtype=dtype)\n    out = fn(x)\n    self.assertEqual(out.dtype, dtype)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.parameters([nn.softmax, nn.log_softmax])\ndef testSoftmaxEmptyArray(self, fn):\n    x = jnp.array([], dtype=float)\n    self.assertArraysEqual(fn(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.parameters([nn.softmax, nn.log_softmax])\ndef testSoftmaxEmptyMask(self, fn):\n    x = jnp.array([5.5, 1.3, -4.2, 0.9])\n    m = jnp.zeros_like(x, dtype=bool)\n    expected = jnp.full_like(x, 0.0 if fn is nn.softmax else -jnp.inf)\n    self.assertArraysEqual(fn(x, where=m), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.parameters([nn.softmax, nn.log_softmax])\ndef testSoftmaxWhereMask(self, fn):\n    x = jnp.array([5.5, 1.3, -4.2, 0.9])\n    m = jnp.array([True, False, True, True])\n    out = fn(x, where=m)\n    self.assertAllClose(out[m], fn(x[m]))\n    probs = out if fn is nn.softmax else jnp.exp(out)\n    self.assertAllClose(probs.sum(), 1.0)\n    if fn is nn.softmax and config.softmax_custom_jvp.value:\n        g_fun = lambda x: jnp.take(fn(x, where=m, initial=-jnp.inf), jnp.array([0, 2, 3]))\n        jtu.check_grads(g_fun, (x,), order=2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.parameters([nn.softmax, nn.log_softmax])\ndef testSoftmaxWhereGrad(self, fn):\n    x = jnp.array([36.0, 10000.0])\n    mask = x < 1000\n    f = lambda x, mask: fn(x, where=mask)[0]\n    self.assertAllClose(jax.grad(f)(x, mask), jnp.zeros_like(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.product(is_training=[True, False], output_type=[jnp.float16, jnp.bfloat16, jnp.float32], impl=['cudnn'])\ndef testScaledDotGeneral(self, is_training, output_type, impl):\n    if impl == 'cudnn' and (not _is_required_cudnn_version_satisfied('10.0', 90700)):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible')\n    configs = create_mxfp8_configs_if_available()\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=jnp.float32)\n    k1, k2 = jax.random.split(jax.random.key(0), 2)\n    a_shape = [2, 256, 96]\n    b_shape = [2, 96, 160]\n    dimension_numbers = (([2], [1]), ([0], [0]))\n    a = cast_to_representable(jax.random.uniform(k1, a_shape, minval=-1.0, dtype=output_type), configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, b_shape, minval=-1.0, dtype=output_type), configs[1].data_type)\n    scaled_dot_general_fn = partial(nn.scaled_dot_general, configs=configs)\n\n    def fwd(a, b, is_ref=False):\n        fn = jax.lax.dot_general if is_ref else scaled_dot_general_fn\n        y = fn(a, b, dimension_numbers, preferred_element_type=output_type)\n        return jnp.sum(y)\n    if is_training:\n        j_train = jax.jit(jax.value_and_grad(fwd, argnums=[0, 1]))\n        j_train_ref = jax.jit(jax.value_and_grad(partial(fwd, is_ref=True), argnums=[0, 1]))\n        out, (x_grad, w_grad) = j_train(a, b)\n        out_ref, (x_grad_ref, w_grad_ref) = j_train_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(x_grad, x_grad_ref, rtol=0.01, atol=10.0)\n        self.assertArraysAllClose(w_grad, w_grad_ref, rtol=0.01, atol=10.0)\n    else:\n        j_inference = jax.jit(fwd)\n        j_inference_ref = jax.jit(partial(fwd, is_ref=True))\n        out = j_inference(a, b)\n        out_ref = j_inference_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product((jnp.float32, jnp.bfloat16, jnp.float16), (partial(nn.gelu, approximate=False), partial(nn.gelu, approximate=True), nn.relu, nn.softplus, nn.sparse_plus, nn.sigmoid, nn.squareplus, nn.mish)))\ndef testDtypeMatchesInput(self, dtype, fn):\n    x = jnp.zeros((), dtype=dtype)\n    out = fn(x)\n    self.assertEqual(out.dtype, dtype)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@parameterized.parameters([nn.softmax, nn.log_softmax])\ndef testSoftmaxEmptyArray(self, fn):\n    x = jnp.array([], dtype=float)\n    self.assertArraysEqual(fn(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@parameterized.parameters([nn.softmax, nn.log_softmax])\ndef testSoftmaxEmptyMask(self, fn):\n    x = jnp.array([5.5, 1.3, -4.2, 0.9])\n    m = jnp.zeros_like(x, dtype=bool)\n    expected = jnp.full_like(x, 0.0 if fn is nn.softmax else -jnp.inf)\n    self.assertArraysEqual(fn(x, where=m), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@parameterized.parameters([nn.softmax, nn.log_softmax])\ndef testSoftmaxWhereMask(self, fn):\n    x = jnp.array([5.5, 1.3, -4.2, 0.9])\n    m = jnp.array([True, False, True, True])\n    out = fn(x, where=m)\n    self.assertAllClose(out[m], fn(x[m]))\n    probs = out if fn is nn.softmax else jnp.exp(out)\n    self.assertAllClose(probs.sum(), 1.0)\n    if fn is nn.softmax and config.softmax_custom_jvp.value:\n        g_fun = lambda x: jnp.take(fn(x, where=m, initial=-jnp.inf), jnp.array([0, 2, 3]))\n        jtu.check_grads(g_fun, (x,), order=2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@parameterized.parameters([nn.softmax, nn.log_softmax])\ndef testSoftmaxWhereGrad(self, fn):\n    x = jnp.array([36.0, 10000.0])\n    mask = x < 1000\n    f = lambda x, mask: fn(x, where=mask)[0]\n    self.assertAllClose(jax.grad(f)(x, mask), jnp.zeros_like(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "def testCustomJVPLeak(self):\n\n    @jax.jit\n    def fwd():\n        a = jnp.array(1.0)\n\n        def f(hx, _):\n            hx = jax.nn.sigmoid(hx + a)\n            return (hx, None)\n        hx = jnp.array(0.0)\n        jax.lax.scan(f, hx, None, length=2)\n    with jax.checking_leaks():\n        fwd()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def scan(y):\n\n    def body(carry, x):\n        return (carry, jnp.dot(x, x))\n    return jax.lax.scan(body, 1.0, y, unroll=False)"
  },
  {
    "test_code": "def testCustomJVPLeak2(self):\n\n    @jax.custom_jvp\n    def sigmoid(x):\n        one = jnp.float32(1)\n        return jax.lax.div(one, jax.lax.add(one, jax.lax.exp(jax.lax.neg(x))))\n    sigmoid.defjvps(lambda g, ans, x: g * ans * (jnp.float32(1) - ans))\n\n    @jax.jit\n    def fwd():\n        a = jnp.array(1.0, 'float32')\n\n        def f(hx, _):\n            hx = sigmoid(hx + a)\n            return (hx, None)\n        hx = jnp.array(0.0, 'float32')\n        jax.lax.scan(f, hx, None, length=2)\n    with jax.checking_leaks():\n        fwd()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def scan(y):\n\n    def body(carry, x):\n        return (carry, jnp.dot(x, x))\n    return jax.lax.scan(body, 1.0, y, unroll=False)"
  },
  {
    "test_code": "def testCustomJVPLeak(self):\n\n    @jax.jit\n    def fwd():\n        a = jnp.array(1.0)\n\n        def f(hx, _):\n            hx = jax.nn.sigmoid(hx + a)\n            return (hx, None)\n        hx = jnp.array(0.0)\n        jax.lax.scan(f, hx, None, length=2)\n    with jax.checking_leaks():\n        fwd()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def fwd(a, b, is_ref=False):\n    fn = jax.vmap(jax.lax.dot_general if is_ref else scaled_dot_general, in_axes=(a_axis, b_axis, None), out_axes=o_axis)\n    y = fn(a, b, dimension_numbers)\n    return jnp.sum(y)"
  },
  {
    "test_code": "def testCustomJVPLeak2(self):\n\n    @jax.custom_jvp\n    def sigmoid(x):\n        one = jnp.float32(1)\n        return jax.lax.div(one, jax.lax.add(one, jax.lax.exp(jax.lax.neg(x))))\n    sigmoid.defjvps(lambda g, ans, x: g * ans * (jnp.float32(1) - ans))\n\n    @jax.jit\n    def fwd():\n        a = jnp.array(1.0, 'float32')\n\n        def f(hx, _):\n            hx = sigmoid(hx + a)\n            return (hx, None)\n        hx = jnp.array(0.0, 'float32')\n        jax.lax.scan(f, hx, None, length=2)\n    with jax.checking_leaks():\n        fwd()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def fwd(a, b, is_ref=False):\n    fn = jax.vmap(jax.lax.dot_general if is_ref else scaled_dot_general, in_axes=(a_axis, b_axis, None), out_axes=o_axis)\n    y = fn(a, b, dimension_numbers)\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.product(is_training=[True, False], output_type=[jnp.float16, jnp.bfloat16, jnp.float32], impl=['cudnn'])\ndef testScaledDotGeneral(self, is_training, output_type, impl):\n    if impl == 'cudnn' and (not _is_required_cudnn_version_satisfied('10.0', 90700)):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible')\n    configs = create_mxfp8_configs_if_available()\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=jnp.float32)\n    k1, k2 = jax.random.split(jax.random.key(0), 2)\n    a_shape = [2, 256, 96]\n    b_shape = [2, 96, 160]\n    dimension_numbers = (([2], [1]), ([0], [0]))\n    a = cast_to_representable(jax.random.uniform(k1, a_shape, minval=-1.0, dtype=output_type), configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, b_shape, minval=-1.0, dtype=output_type), configs[1].data_type)\n    scaled_dot_general_fn = partial(nn.scaled_dot_general, configs=configs)\n\n    def fwd(a, b, is_ref=False):\n        fn = jax.lax.dot_general if is_ref else scaled_dot_general_fn\n        y = fn(a, b, dimension_numbers, preferred_element_type=output_type)\n        return jnp.sum(y)\n    if is_training:\n        j_train = jax.jit(jax.value_and_grad(fwd, argnums=[0, 1]))\n        j_train_ref = jax.jit(jax.value_and_grad(partial(fwd, is_ref=True), argnums=[0, 1]))\n        out, (x_grad, w_grad) = j_train(a, b)\n        out_ref, (x_grad_ref, w_grad_ref) = j_train_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(x_grad, x_grad_ref, rtol=0.01, atol=10.0)\n        self.assertArraysAllClose(w_grad, w_grad_ref, rtol=0.01, atol=10.0)\n    else:\n        j_inference = jax.jit(fwd)\n        j_inference_ref = jax.jit(partial(fwd, is_ref=True))\n        out = j_inference(a, b)\n        out_ref = j_inference_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.bfloat16, jnp.float16], group_num=[1, 2, 4], use_vmap=[False, True], impl=['cudnn', 'xla'])\ndef testDotProductAttention(self, dtype, group_num, use_vmap, impl):\n    if impl == 'cudnn' and (not _is_required_cudnn_version_satisfied('8.0', 8904)):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible.')\n    if impl == 'cudnn' and dtype == jnp.float32:\n        raise unittest.SkipTest('cuDNN only supports fp16 or bf16.')\n    B, S, T, N, H, G = (2, 128, 128, 4, 32, group_num)\n    keys = random.split(random.PRNGKey(0), 5)\n    Q = random.normal(keys[0], (B, T, N, H), dtype)\n    K = random.normal(keys[1], (B, S, N // G, H), dtype)\n    V = random.normal(keys[2], (B, S, N // G, H), dtype)\n    grad = random.normal(keys[3], (B, T, N, H), dtype)\n    bias, mask = (None, None)\n    sdpa = nn.dot_product_attention\n    sdpa_ref = partial(sdpa, implementation=None)\n    sdpa_ans = partial(sdpa, implementation=impl)\n    if use_vmap:\n        sdpa_ans = jax.vmap(sdpa_ans, in_axes=(0, 0, 0, None, None), out_axes=0)\n    K_ref = jnp.repeat(K, G, axis=2)\n    V_ref = jnp.repeat(V, G, axis=2)\n    out_ref, sdpa_vjp_ref = jax.vjp(sdpa_ref, Q, K_ref, V_ref, bias, mask)\n    out_ans, sdpa_vjp_ans = jax.vjp(sdpa_ans, Q, K, V, bias, mask)\n    dQ_ref, dK_ref, dV_ref = sdpa_vjp_ref(grad)[:3]\n    dQ_ans, dK_ans, dV_ans = sdpa_vjp_ans(grad)[:3]\n    dK_ref = dK_ref.reshape(B, S, N // G, G, H).sum(axis=3)\n    dV_ref = dV_ref.reshape(B, S, N // G, G, H).sum(axis=3)\n    if impl == 'cudnn':\n        self.assertTrue(_check_cudnn_backend(sdpa_ans, Q, K, V, bias, mask))\n        self.assertTrue(_check_cudnn_backend(sdpa_vjp_ans, grad))\n    self.assertAllClose(out_ref, out_ans, atol=0.01, rtol=0.01)\n    self.assertAllClose(dQ_ref, dQ_ans, rtol=0.01, atol=0.01)\n    self.assertAllClose(dK_ref, dK_ans, rtol=0.01, atol=0.01)\n    self.assertAllClose(dV_ref, dV_ans, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.product(mask_mode=['bias', 'causal', 'padding', 'custom', ('causal', 'padding'), ('custom', 'padding'), ('bias', 'causal'), ('causal', 'sliding_window')])\ndef testDotProductAttentionMask(self, mask_mode):\n    if isinstance(mask_mode, str):\n        mask_mode = (mask_mode,)\n    min_cudnn_version = 90200 if 'sliding_window' in mask_mode else 8904\n    if not _is_required_cudnn_version_satisfied('8.0', min_cudnn_version):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible.')\n    dtype = jnp.bfloat16\n    B, S, T, N, H = (2, 128, 128, 4, 32)\n    keys = random.split(random.PRNGKey(0), 4)\n    Q = random.normal(keys[0], (B, T, N, H), dtype)\n    K = random.normal(keys[1], (B, S, N, H), dtype)\n    V = random.normal(keys[2], (B, S, N, H), dtype)\n    grad = random.normal(keys[3], (B, T, N, H), dtype)\n    bias, mask = (None, None)\n    q_seqlen, kv_seqlen = (None, None)\n    window_size = None\n    is_causal = 'causal' in mask_mode\n    if 'padding' in mask_mode:\n        q_seqlen = jnp.array([T // 2, T // 4], dtype=jnp.int32)\n        kv_seqlen = jnp.array([S // 4, S // 2], dtype=jnp.int32)\n    if 'custom' in mask_mode:\n        custom_mask = jnp.tril(jnp.ones((T, S), dtype=jnp.bool_))\n        mask = custom_mask[None, None, :, :]\n    if 'bias' in mask_mode:\n        bias = random.normal(keys[4], (1, N, T, S), dtype)\n    if 'sliding_window' in mask_mode:\n        window_size = (3, 2) if is_causal else (3, 0)\n    sdpa = nn.dot_product_attention\n    sdpa_ref = partial(sdpa, is_causal=is_causal, implementation=None)\n    sdpa_ans = partial(sdpa, is_causal=is_causal, implementation='cudnn')\n    args = (Q, K, V, bias, mask)\n    kwargs = {'query_seq_lengths': q_seqlen, 'key_value_seq_lengths': kv_seqlen}\n    fn_ref = lambda q, k, v, b, m, qs, kvs: sdpa_ref(q, k, v, b, m, query_seq_lengths=qs, key_value_seq_lengths=kvs, local_window_size=window_size)\n    fn_ans = lambda q, k, v, b, m, qs, kvs: sdpa_ans(q, k, v, b, m, query_seq_lengths=qs, key_value_seq_lengths=kvs, local_window_size=window_size)\n    out_ref, sdpa_vjp_ref = jax.vjp(fn_ref, *args, q_seqlen, kv_seqlen)\n    out_ans, sdpa_vjp_ans = jax.vjp(fn_ans, *args, q_seqlen, kv_seqlen)\n    dQ_ref, dK_ref, dV_ref, dbias_ref = sdpa_vjp_ref(grad)[:4]\n    dQ_ans, dK_ans, dV_ans, dbias_ans = sdpa_vjp_ans(grad)[:4]\n    self.assertTrue(_check_cudnn_backend(sdpa_ans, *args, **kwargs))\n    self.assertTrue(_check_cudnn_backend(sdpa_vjp_ans, grad))\n    self.assertAllClose(out_ref, out_ans, atol=0.01, rtol=0.01)\n    self.assertAllClose(dQ_ref, dQ_ans, rtol=0.02, atol=0.02)\n    self.assertAllClose(dK_ref, dK_ans, rtol=0.02, atol=0.02)\n    self.assertAllClose(dV_ref, dV_ans, rtol=0.01, atol=0.01)\n    self.assertAllClose(dbias_ref, dbias_ans, rtol=0.02, atol=0.02)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.product(batch_size=[1, 16], use_vmap=[False, True])\ndef testDotProductAttentionBiasGradient(self, batch_size, use_vmap):\n    if not _is_required_cudnn_version_satisfied('8.0', 8904):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible.')\n    dtype = jnp.bfloat16\n    B, S, N, H = (batch_size, 128, 4, 32)\n    keys = random.split(random.PRNGKey(0), 2)\n    x = random.normal(keys[0], (B, S, N, H), dtype)\n    bias = random.normal(keys[1], (B, N, S, S), dtype=dtype)\n    mask = jnp.ones((1, 1, S), dtype=jnp.bool_)\n\n    def attention(x, bias, mask, impl):\n        return jax.nn.dot_product_attention(query=x, key=x, value=x, bias=bias, mask=mask, is_causal=False, implementation=impl)\n    attn_ref = partial(attention, impl=None)\n    attn_ans = partial(attention, impl='cudnn')\n    if use_vmap:\n        attn_batched_ref = jax.vmap(attn_ref, in_axes=(0, 0, None))\n        attn_batched_ans = jax.vmap(attn_ans, in_axes=(0, 0, None))\n    else:\n        attn_batched_ref = attn_ref\n        attn_batched_ans = attn_ans\n    fwd_ref = jax.jit(attn_batched_ref)\n    fwd_ans = jax.jit(attn_batched_ans)\n    y_ref = fwd_ref(x, bias, mask)\n    y_ans = fwd_ans(x, bias, mask)\n    self.assertAllClose(y_ref, y_ans)\n\n    @jax.jit\n    def bwd_ref(x, bias, mask):\n        _, f_vjp = jax.vjp(attn_ref, x, bias, mask)\n        return f_vjp(x)\n\n    @jax.jit\n    def bwd_ans(x, bias, mask):\n        _, f_vjp = jax.vjp(attn_ans, x, bias, mask)\n        return f_vjp(x)\n    if batch_size != 1:\n        with self.assertRaisesRegex(ValueError, _cudnn_dbias_error):\n            _, dbias_ans, _ = bwd_ans(x, bias, mask)\n    else:\n        _, dbias_ref, _ = bwd_ref(x, bias, mask)\n        _, dbias_ans, _ = bwd_ans(x, bias, mask)\n        self.assertAllClose(dbias_ans, dbias_ref, rtol=0.1, atol=0.1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "def testAccidentalUpcasting(self):\n    rng = random.PRNGKey(0)\n    shape = (4, 4)\n    scalar_param = jnp.array(1.0, dtype=jnp.float32)\n    for init_fn in (nn.initializers.uniform(scalar_param, jnp.bfloat16), nn.initializers.normal(scalar_param, jnp.bfloat16), nn.initializers.truncated_normal(scalar_param, jnp.bfloat16)):\n        sub_rng, rng = random.split(rng)\n        val = init_fn(sub_rng, shape)\n        self.assertEqual(val.dtype, jnp.bfloat16)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.product(is_training=[True, False], output_type=[jnp.float16, jnp.bfloat16, jnp.float32], impl=['cudnn'])\ndef testScaledDotGeneral(self, is_training, output_type, impl):\n    if impl == 'cudnn' and (not _is_required_cudnn_version_satisfied('10.0', 90700)):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible')\n    configs = create_mxfp8_configs_if_available()\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=jnp.float32)\n    k1, k2 = jax.random.split(jax.random.key(0), 2)\n    a_shape = [2, 256, 96]\n    b_shape = [2, 96, 160]\n    dimension_numbers = (([2], [1]), ([0], [0]))\n    a = cast_to_representable(jax.random.uniform(k1, a_shape, minval=-1.0, dtype=output_type), configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, b_shape, minval=-1.0, dtype=output_type), configs[1].data_type)\n    scaled_dot_general_fn = partial(nn.scaled_dot_general, configs=configs)\n\n    def fwd(a, b, is_ref=False):\n        fn = jax.lax.dot_general if is_ref else scaled_dot_general_fn\n        y = fn(a, b, dimension_numbers, preferred_element_type=output_type)\n        return jnp.sum(y)\n    if is_training:\n        j_train = jax.jit(jax.value_and_grad(fwd, argnums=[0, 1]))\n        j_train_ref = jax.jit(jax.value_and_grad(partial(fwd, is_ref=True), argnums=[0, 1]))\n        out, (x_grad, w_grad) = j_train(a, b)\n        out_ref, (x_grad_ref, w_grad_ref) = j_train_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(x_grad, x_grad_ref, rtol=0.01, atol=10.0)\n        self.assertArraysAllClose(w_grad, w_grad_ref, rtol=0.01, atol=10.0)\n    else:\n        j_inference = jax.jit(fwd)\n        j_inference_ref = jax.jit(partial(fwd, is_ref=True))\n        out = j_inference(a, b)\n        out_ref = j_inference_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "@parameterized.parameters(*itertools.product((jnp.float32, jnp.bfloat16, jnp.float16), (partial(nn.gelu, approximate=False), partial(nn.gelu, approximate=True), nn.relu, nn.softplus, nn.sparse_plus, nn.sigmoid, nn.squareplus, nn.mish)))\ndef testDtypeMatchesInput(self, dtype, fn):\n    x = jnp.zeros((), dtype=dtype)\n    out = fn(x)\n    self.assertEqual(out.dtype, dtype)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "@parameterized.parameters([nn.softmax, nn.log_softmax])\ndef testSoftmaxEmptyArray(self, fn):\n    x = jnp.array([], dtype=float)\n    self.assertArraysEqual(fn(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "@parameterized.parameters([nn.softmax, nn.log_softmax])\ndef testSoftmaxEmptyMask(self, fn):\n    x = jnp.array([5.5, 1.3, -4.2, 0.9])\n    m = jnp.zeros_like(x, dtype=bool)\n    expected = jnp.full_like(x, 0.0 if fn is nn.softmax else -jnp.inf)\n    self.assertArraysEqual(fn(x, where=m), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "@parameterized.parameters([nn.softmax, nn.log_softmax])\ndef testSoftmaxWhereMask(self, fn):\n    x = jnp.array([5.5, 1.3, -4.2, 0.9])\n    m = jnp.array([True, False, True, True])\n    out = fn(x, where=m)\n    self.assertAllClose(out[m], fn(x[m]))\n    probs = out if fn is nn.softmax else jnp.exp(out)\n    self.assertAllClose(probs.sum(), 1.0)\n    if fn is nn.softmax and config.softmax_custom_jvp.value:\n        g_fun = lambda x: jnp.take(fn(x, where=m, initial=-jnp.inf), jnp.array([0, 2, 3]))\n        jtu.check_grads(g_fun, (x,), order=2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "@parameterized.parameters([nn.softmax, nn.log_softmax])\ndef testSoftmaxWhereGrad(self, fn):\n    x = jnp.array([36.0, 10000.0])\n    mask = x < 1000\n    f = lambda x, mask: fn(x, where=mask)[0]\n    self.assertAllClose(jax.grad(f)(x, mask), jnp.zeros_like(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "def testCustomJVPLeak2(self):\n\n    @jax.custom_jvp\n    def sigmoid(x):\n        one = jnp.float32(1)\n        return jax.lax.div(one, jax.lax.add(one, jax.lax.exp(jax.lax.neg(x))))\n    sigmoid.defjvps(lambda g, ans, x: g * ans * (jnp.float32(1) - ans))\n\n    @jax.jit\n    def fwd():\n        a = jnp.array(1.0, 'float32')\n\n        def f(hx, _):\n            hx = sigmoid(hx + a)\n            return (hx, None)\n        hx = jnp.array(0.0, 'float32')\n        jax.lax.scan(f, hx, None, length=2)\n    with jax.checking_leaks():\n        fwd()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@staticmethod\ndef add(dt, x, y):\n    fromscale = partial(jax.lax.convert_element_type, new_dtype=dt.float_dtype)\n    toscale = partial(jax.lax.convert_element_type, new_dtype=dt)\n    return toscale(jax.lax.max(fromscale(x), fromscale(y)))"
  },
  {
    "test_code": "@parameterized.product(mask_mode=['bias', 'causal', 'padding', 'custom', ('causal', 'padding'), ('custom', 'padding'), ('bias', 'causal'), ('causal', 'sliding_window')])\ndef testDotProductAttentionMask(self, mask_mode):\n    if isinstance(mask_mode, str):\n        mask_mode = (mask_mode,)\n    min_cudnn_version = 90200 if 'sliding_window' in mask_mode else 8904\n    if not _is_required_cudnn_version_satisfied('8.0', min_cudnn_version):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible.')\n    dtype = jnp.bfloat16\n    B, S, T, N, H = (2, 128, 128, 4, 32)\n    keys = random.split(random.PRNGKey(0), 4)\n    Q = random.normal(keys[0], (B, T, N, H), dtype)\n    K = random.normal(keys[1], (B, S, N, H), dtype)\n    V = random.normal(keys[2], (B, S, N, H), dtype)\n    grad = random.normal(keys[3], (B, T, N, H), dtype)\n    bias, mask = (None, None)\n    q_seqlen, kv_seqlen = (None, None)\n    window_size = None\n    is_causal = 'causal' in mask_mode\n    if 'padding' in mask_mode:\n        q_seqlen = jnp.array([T // 2, T // 4], dtype=jnp.int32)\n        kv_seqlen = jnp.array([S // 4, S // 2], dtype=jnp.int32)\n    if 'custom' in mask_mode:\n        custom_mask = jnp.tril(jnp.ones((T, S), dtype=jnp.bool_))\n        mask = custom_mask[None, None, :, :]\n    if 'bias' in mask_mode:\n        bias = random.normal(keys[4], (1, N, T, S), dtype)\n    if 'sliding_window' in mask_mode:\n        window_size = (3, 2) if is_causal else (3, 0)\n    sdpa = nn.dot_product_attention\n    sdpa_ref = partial(sdpa, is_causal=is_causal, implementation=None)\n    sdpa_ans = partial(sdpa, is_causal=is_causal, implementation='cudnn')\n    args = (Q, K, V, bias, mask)\n    kwargs = {'query_seq_lengths': q_seqlen, 'key_value_seq_lengths': kv_seqlen}\n    fn_ref = lambda q, k, v, b, m, qs, kvs: sdpa_ref(q, k, v, b, m, query_seq_lengths=qs, key_value_seq_lengths=kvs, local_window_size=window_size)\n    fn_ans = lambda q, k, v, b, m, qs, kvs: sdpa_ans(q, k, v, b, m, query_seq_lengths=qs, key_value_seq_lengths=kvs, local_window_size=window_size)\n    out_ref, sdpa_vjp_ref = jax.vjp(fn_ref, *args, q_seqlen, kv_seqlen)\n    out_ans, sdpa_vjp_ans = jax.vjp(fn_ans, *args, q_seqlen, kv_seqlen)\n    dQ_ref, dK_ref, dV_ref, dbias_ref = sdpa_vjp_ref(grad)[:4]\n    dQ_ans, dK_ans, dV_ans, dbias_ans = sdpa_vjp_ans(grad)[:4]\n    self.assertTrue(_check_cudnn_backend(sdpa_ans, *args, **kwargs))\n    self.assertTrue(_check_cudnn_backend(sdpa_vjp_ans, grad))\n    self.assertAllClose(out_ref, out_ans, atol=0.01, rtol=0.01)\n    self.assertAllClose(dQ_ref, dQ_ans, rtol=0.02, atol=0.02)\n    self.assertAllClose(dK_ref, dK_ans, rtol=0.02, atol=0.02)\n    self.assertAllClose(dV_ref, dV_ans, rtol=0.01, atol=0.01)\n    self.assertAllClose(dbias_ref, dbias_ans, rtol=0.02, atol=0.02)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def sdpa_ref(query: Array, key: Array, value: Array, bias: Array | None=None, mask: Array | None=None, scale: float=0.5, mask_type: MaskType=MaskType.NO_MASK, dropout_rate: float=0.1, sliding_window_length: int | None=None) -> Array:\n\n    def get_causal_mask(logits):\n        large_negative_number = get_large_negative_number(logits.dtype)\n        t = logits.shape[-2]\n        col_idx = jax.lax.broadcasted_iota(np.int32, (t, t), 1)\n        row_idx = jax.lax.broadcasted_iota(np.int32, (t, t), 0)\n        mask = (row_idx < col_idx).astype(logits.dtype) * large_negative_number\n        return mask[*[jnp.newaxis] * (len(logits.shape) - 2), ...]\n\n    def get_padding_mask(logits):\n        S, T = logits.shape[-2:]\n        large_negative_number = get_large_negative_number(logits.dtype)\n        q_padding = (jax.lax.iota(np.int32, S) >= S // 2).reshape((S, 1))\n        kv_padding = (jax.lax.iota(np.int32, T) >= T // 2).reshape((1, T))\n        combined_padding = (q_padding + kv_padding).astype(logits.dtype) * large_negative_number\n        return jax.lax.broadcast(combined_padding, logits.shape[:-2])\n\n    def get_encoded_padding_mask(encoded):\n        S = encoded.shape[1]\n        encoded_padding = (jax.lax.iota(np.int32, S) < S // 2).astype(encoded.dtype)\n        return jax.lax.broadcast_in_dim(encoded_padding, encoded.shape, broadcast_dimensions=[1])\n\n    def get_sliding_window_mask(logits, window_length):\n        large_negative_number = get_large_negative_number(logits.dtype)\n        T = logits.shape[-2]\n        col_idx = jax.lax.broadcasted_iota(np.int32, (T, T), 1)\n        row_idx = jax.lax.broadcasted_iota(np.int32, (T, T), 0)\n        mask = jnp.logical_or(row_idx < col_idx, col_idx <= row_idx - window_length).astype(logits.dtype) * large_negative_number\n        return mask[*[jnp.newaxis] * (len(logits.shape) - 2), ...]\n    B, T, qN, H = query.shape\n    _, _, kN, _ = key.shape\n    logits = jnp.einsum('bqhd,bkhd->bhqk', query, key, preferred_element_type=jnp.float32)\n    if scale != 1.0:\n        logits = logits * scale\n    if mask_type == MaskType.CAUSAL:\n        bias = get_causal_mask(logits)\n    elif mask_type == MaskType.PADDING:\n        bias = get_padding_mask(logits)\n    elif sliding_window_length is not None:\n        if sliding_window_length <= 0:\n            raise ValueError(f'Expect sliding_window_length > 0, got {sliding_window_length}.')\n        bias = get_sliding_window_mask(logits, sliding_window_length)\n    if mask is not None:\n        large_negative_number = get_large_negative_number(logits.dtype)\n        mask = jnp.where(mask, 0, large_negative_number)\n    if bias is None:\n        bias = mask\n    elif mask is not None:\n        bias = bias.astype(logits.dtype)\n        bias += mask\n    if bias is not None:\n        if bias.shape != logits.shape:\n            bias = jnp.broadcast_to(bias, logits.shape)\n        logits = logits + bias.astype(logits.dtype)\n    probs = jax.nn.softmax(logits, axis=-1).astype(query.dtype)\n    if dropout_rate > 0.0:\n        keep_prob = 1.0 - dropout_rate\n        dropout_rng = jax.random.key(0)\n        keep = jax.random.bernoulli(dropout_rng, keep_prob, probs.shape)\n        probs = jax.lax.select(keep, probs / keep_prob, jnp.zeros_like(probs))\n    encoded = jnp.einsum('bhqk,bkhd->bqhd', probs, value, preferred_element_type=jnp.float32)\n    if mask_type == MaskType.PADDING:\n        encoded_mask = get_encoded_padding_mask(encoded)\n        encoded = encoded * encoded_mask\n    return encoded.astype(query.dtype)"
  },
  {
    "test_code": "def testCustomJVPLeak(self):\n\n    @jax.jit\n    def fwd():\n        a = jnp.array(1.0)\n\n        def f(hx, _):\n            hx = jax.nn.sigmoid(hx + a)\n            return (hx, None)\n        hx = jnp.array(0.0)\n        jax.lax.scan(f, hx, None, length=2)\n    with jax.checking_leaks():\n        fwd()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.jit\n@partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\ndef fwd(a):\n    axis_size = lax.psum(1, 'x')\n    perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n    return lax.ppermute(a, 'x', perm=perm)"
  },
  {
    "test_code": "def testCustomJVPLeak2(self):\n\n    @jax.custom_jvp\n    def sigmoid(x):\n        one = jnp.float32(1)\n        return jax.lax.div(one, jax.lax.add(one, jax.lax.exp(jax.lax.neg(x))))\n    sigmoid.defjvps(lambda g, ans, x: g * ans * (jnp.float32(1) - ans))\n\n    @jax.jit\n    def fwd():\n        a = jnp.array(1.0, 'float32')\n\n        def f(hx, _):\n            hx = sigmoid(hx + a)\n            return (hx, None)\n        hx = jnp.array(0.0, 'float32')\n        jax.lax.scan(f, hx, None, length=2)\n    with jax.checking_leaks():\n        fwd()",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "@jax.jit\n@partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\ndef fwd(a):\n    axis_size = lax.psum(1, 'x')\n    perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n    return lax.ppermute(a, 'x', perm=perm)"
  },
  {
    "test_code": "@parameterized.parameters(False, True)\ndef testGelu(self, approximate):\n\n    def gelu_reference(x):\n        return x * scipy.stats.norm.cdf(x)\n    args_maker = lambda: [jnp.linspace(-12, 5, 10000, dtype=jnp.float32)]\n    rtol = 2e-05\n    atol = 0.001 if approximate else 0\n    self._CheckAgainstNumpy(gelu_reference, partial(nn.gelu, approximate=approximate), args_maker, check_dtypes=False, tol=0, rtol=rtol, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def cdf(x):\n    result = jax.vmap(partial(kde.integrate_box_1d, -np.inf))(x)\n    return np.array(result)"
  },
  {
    "test_code": "@parameterized.product(dtype=[jnp.bfloat16, jnp.float16], group_num=[1, 2, 4], use_vmap=[False, True], impl=['cudnn', 'xla'])\ndef testDotProductAttention(self, dtype, group_num, use_vmap, impl):\n    if impl == 'cudnn' and (not _is_required_cudnn_version_satisfied('8.0', 8904)):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible.')\n    if impl == 'cudnn' and dtype == jnp.float32:\n        raise unittest.SkipTest('cuDNN only supports fp16 or bf16.')\n    B, S, T, N, H, G = (2, 128, 128, 4, 32, group_num)\n    keys = random.split(random.PRNGKey(0), 5)\n    Q = random.normal(keys[0], (B, T, N, H), dtype)\n    K = random.normal(keys[1], (B, S, N // G, H), dtype)\n    V = random.normal(keys[2], (B, S, N // G, H), dtype)\n    grad = random.normal(keys[3], (B, T, N, H), dtype)\n    bias, mask = (None, None)\n    sdpa = nn.dot_product_attention\n    sdpa_ref = partial(sdpa, implementation=None)\n    sdpa_ans = partial(sdpa, implementation=impl)\n    if use_vmap:\n        sdpa_ans = jax.vmap(sdpa_ans, in_axes=(0, 0, 0, None, None), out_axes=0)\n    K_ref = jnp.repeat(K, G, axis=2)\n    V_ref = jnp.repeat(V, G, axis=2)\n    out_ref, sdpa_vjp_ref = jax.vjp(sdpa_ref, Q, K_ref, V_ref, bias, mask)\n    out_ans, sdpa_vjp_ans = jax.vjp(sdpa_ans, Q, K, V, bias, mask)\n    dQ_ref, dK_ref, dV_ref = sdpa_vjp_ref(grad)[:3]\n    dQ_ans, dK_ans, dV_ans = sdpa_vjp_ans(grad)[:3]\n    dK_ref = dK_ref.reshape(B, S, N // G, G, H).sum(axis=3)\n    dV_ref = dV_ref.reshape(B, S, N // G, G, H).sum(axis=3)\n    if impl == 'cudnn':\n        self.assertTrue(_check_cudnn_backend(sdpa_ans, Q, K, V, bias, mask))\n        self.assertTrue(_check_cudnn_backend(sdpa_vjp_ans, grad))\n    self.assertAllClose(out_ref, out_ans, atol=0.01, rtol=0.01)\n    self.assertAllClose(dQ_ref, dQ_ans, rtol=0.01, atol=0.01)\n    self.assertAllClose(dK_ref, dK_ans, rtol=0.01, atol=0.01)\n    self.assertAllClose(dV_ref, dV_ans, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def _check_cudnn_backend(fn, *args, **kwargs):\n    lowered = jax.jit(fn).lower(*args, **kwargs)\n    hlo = lowered.as_text('stablehlo', debug_info=True)\n    return '__cudnn$fmha' in hlo"
  },
  {
    "test_code": "@parameterized.product(mask_mode=['bias', 'causal', 'padding', 'custom', ('causal', 'padding'), ('custom', 'padding'), ('bias', 'causal'), ('causal', 'sliding_window')])\ndef testDotProductAttentionMask(self, mask_mode):\n    if isinstance(mask_mode, str):\n        mask_mode = (mask_mode,)\n    min_cudnn_version = 90200 if 'sliding_window' in mask_mode else 8904\n    if not _is_required_cudnn_version_satisfied('8.0', min_cudnn_version):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible.')\n    dtype = jnp.bfloat16\n    B, S, T, N, H = (2, 128, 128, 4, 32)\n    keys = random.split(random.PRNGKey(0), 4)\n    Q = random.normal(keys[0], (B, T, N, H), dtype)\n    K = random.normal(keys[1], (B, S, N, H), dtype)\n    V = random.normal(keys[2], (B, S, N, H), dtype)\n    grad = random.normal(keys[3], (B, T, N, H), dtype)\n    bias, mask = (None, None)\n    q_seqlen, kv_seqlen = (None, None)\n    window_size = None\n    is_causal = 'causal' in mask_mode\n    if 'padding' in mask_mode:\n        q_seqlen = jnp.array([T // 2, T // 4], dtype=jnp.int32)\n        kv_seqlen = jnp.array([S // 4, S // 2], dtype=jnp.int32)\n    if 'custom' in mask_mode:\n        custom_mask = jnp.tril(jnp.ones((T, S), dtype=jnp.bool_))\n        mask = custom_mask[None, None, :, :]\n    if 'bias' in mask_mode:\n        bias = random.normal(keys[4], (1, N, T, S), dtype)\n    if 'sliding_window' in mask_mode:\n        window_size = (3, 2) if is_causal else (3, 0)\n    sdpa = nn.dot_product_attention\n    sdpa_ref = partial(sdpa, is_causal=is_causal, implementation=None)\n    sdpa_ans = partial(sdpa, is_causal=is_causal, implementation='cudnn')\n    args = (Q, K, V, bias, mask)\n    kwargs = {'query_seq_lengths': q_seqlen, 'key_value_seq_lengths': kv_seqlen}\n    fn_ref = lambda q, k, v, b, m, qs, kvs: sdpa_ref(q, k, v, b, m, query_seq_lengths=qs, key_value_seq_lengths=kvs, local_window_size=window_size)\n    fn_ans = lambda q, k, v, b, m, qs, kvs: sdpa_ans(q, k, v, b, m, query_seq_lengths=qs, key_value_seq_lengths=kvs, local_window_size=window_size)\n    out_ref, sdpa_vjp_ref = jax.vjp(fn_ref, *args, q_seqlen, kv_seqlen)\n    out_ans, sdpa_vjp_ans = jax.vjp(fn_ans, *args, q_seqlen, kv_seqlen)\n    dQ_ref, dK_ref, dV_ref, dbias_ref = sdpa_vjp_ref(grad)[:4]\n    dQ_ans, dK_ans, dV_ans, dbias_ans = sdpa_vjp_ans(grad)[:4]\n    self.assertTrue(_check_cudnn_backend(sdpa_ans, *args, **kwargs))\n    self.assertTrue(_check_cudnn_backend(sdpa_vjp_ans, grad))\n    self.assertAllClose(out_ref, out_ans, atol=0.01, rtol=0.01)\n    self.assertAllClose(dQ_ref, dQ_ans, rtol=0.02, atol=0.02)\n    self.assertAllClose(dK_ref, dK_ans, rtol=0.02, atol=0.02)\n    self.assertAllClose(dV_ref, dV_ans, rtol=0.01, atol=0.01)\n    self.assertAllClose(dbias_ref, dbias_ans, rtol=0.02, atol=0.02)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def _check_cudnn_backend(fn, *args, **kwargs):\n    lowered = jax.jit(fn).lower(*args, **kwargs)\n    hlo = lowered.as_text('stablehlo', debug_info=True)\n    return '__cudnn$fmha' in hlo"
  },
  {
    "test_code": "@parameterized.product(contract=[160, 96], lhs_non_contract=[240, 100], dtype=[jnp.float16, jnp.bfloat16, jnp.float32], impl=['cudnn'])\ndef testScaledMatmul(self, contract, lhs_non_contract, dtype, impl):\n    if impl == 'cudnn' and (not _is_required_cudnn_version_satisfied('10.0', 90700)):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible')\n    configs = create_mxfp8_configs_if_available()\n    batch, rhs_non_contract = (4, 256)\n    a, b, a_q, b_q, a_scales, b_scales = _generate_quantized_tensors(batch, lhs_non_contract, contract, rhs_non_contract, configs, dtype=dtype)\n    out = nn.scaled_matmul(a_q, b_q, a_scales, b_scales, preferred_element_type=dtype)\n    out_ref = jnp.matmul(a.astype(jnp.float32), jnp.transpose(b, (0, 2, 1)).astype(jnp.float32))\n    self.assertArraysAllClose(out, out_ref.astype(dtype), rtol=0.001, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def _generate_quantized_tensors(batch, lhs_non_contract, contract, rhs_non_contract, configs, dtype=jnp.float32):\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=dtype)\n    k1, k2 = jax.random.split(jax.random.key(123), 2)\n    a = cast_to_representable(jax.random.uniform(k1, (batch, lhs_non_contract, contract), minval=-1.0, dtype=dtype), configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, (batch, rhs_non_contract, contract), minval=-1.0, dtype=dtype), configs[1].data_type)\n    dn = ((2,), (0,))\n    a_3d = shape_normalization(a, dn)\n    b_3d = shape_normalization(b, dn)\n    a_q, a_scales = quantize(a, configs[0])\n    b_q, b_scales = quantize(b, configs[1])\n    return (a, b, a_q, b_q, a_scales, b_scales)"
  },
  {
    "test_code": "@parameterized.product(contract=[160, 96], lhs_non_contract=[240, 100], dtype=[jnp.float16, jnp.bfloat16, jnp.float32], impl=['cudnn'])\ndef testScaledMatmul(self, contract, lhs_non_contract, dtype, impl):\n    if impl == 'cudnn' and (not _is_required_cudnn_version_satisfied('10.0', 90700)):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible')\n    configs = create_mxfp8_configs_if_available()\n    batch, rhs_non_contract = (4, 256)\n    a, b, a_q, b_q, a_scales, b_scales = _generate_quantized_tensors(batch, lhs_non_contract, contract, rhs_non_contract, configs, dtype=dtype)\n    out = nn.scaled_matmul(a_q, b_q, a_scales, b_scales, preferred_element_type=dtype)\n    out_ref = jnp.matmul(a.astype(jnp.float32), jnp.transpose(b, (0, 2, 1)).astype(jnp.float32))\n    self.assertArraysAllClose(out, out_ref.astype(dtype), rtol=0.001, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def create_mxfp8_configs_if_available():\n    if _dtypes.float8_e8m0fnu is None:\n        raise unittest.SkipTest('float8_e8m0fnu is not available.')\n\n    def _create_mxfp8_config():\n        return BlockScaleConfig(mode='mxfp8', block_size=32, data_type=jnp.float8_e4m3fn, scale_type=jnp.float8_e8m0fnu, global_scale=None, infer_only=False)\n    return [_create_mxfp8_config() for _ in range(3)]"
  },
  {
    "test_code": "@parameterized.product(is_training=[True, False], output_type=[jnp.float16, jnp.bfloat16, jnp.float32], impl=['cudnn'])\ndef testScaledDotGeneral(self, is_training, output_type, impl):\n    if impl == 'cudnn' and (not _is_required_cudnn_version_satisfied('10.0', 90700)):\n        raise unittest.SkipTest('CUDA or cuDNN versions are not compatible')\n    configs = create_mxfp8_configs_if_available()\n    cast_to_representable = partial(quantize_dequantize, scale=jnp.ones((1,)), compute_dtype=jnp.float32)\n    k1, k2 = jax.random.split(jax.random.key(0), 2)\n    a_shape = [2, 256, 96]\n    b_shape = [2, 96, 160]\n    dimension_numbers = (([2], [1]), ([0], [0]))\n    a = cast_to_representable(jax.random.uniform(k1, a_shape, minval=-1.0, dtype=output_type), configs[0].data_type)\n    b = cast_to_representable(jax.random.uniform(k2, b_shape, minval=-1.0, dtype=output_type), configs[1].data_type)\n    scaled_dot_general_fn = partial(nn.scaled_dot_general, configs=configs)\n\n    def fwd(a, b, is_ref=False):\n        fn = jax.lax.dot_general if is_ref else scaled_dot_general_fn\n        y = fn(a, b, dimension_numbers, preferred_element_type=output_type)\n        return jnp.sum(y)\n    if is_training:\n        j_train = jax.jit(jax.value_and_grad(fwd, argnums=[0, 1]))\n        j_train_ref = jax.jit(jax.value_and_grad(partial(fwd, is_ref=True), argnums=[0, 1]))\n        out, (x_grad, w_grad) = j_train(a, b)\n        out_ref, (x_grad_ref, w_grad_ref) = j_train_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(x_grad, x_grad_ref, rtol=0.01, atol=10.0)\n        self.assertArraysAllClose(w_grad, w_grad_ref, rtol=0.01, atol=10.0)\n    else:\n        j_inference = jax.jit(fwd)\n        j_inference_ref = jax.jit(partial(fwd, is_ref=True))\n        out = j_inference(a, b)\n        out_ref = j_inference_ref(a, b)\n        self.assertArraysAllClose(out, out_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/nn_test.py",
    "function": "def create_mxfp8_configs_if_available():\n    if _dtypes.float8_e8m0fnu is None:\n        raise unittest.SkipTest('float8_e8m0fnu is not available.')\n\n    def _create_mxfp8_config():\n        return BlockScaleConfig(mode='mxfp8', block_size=32, data_type=jnp.float8_e4m3fn, scale_type=jnp.float8_e8m0fnu, global_scale=None, infer_only=False)\n    return [_create_mxfp8_config() for _ in range(3)]"
  }
]