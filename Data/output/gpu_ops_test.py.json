[
  {
    "test_code": "@jtu.sample_product(batch_size=(1, 2), seq_len=(128, 384), num_heads=(1, 2, 8), head_dim=(32, 64, 128), block_sizes=((('block_q', 128), ('block_k', 128)), (('block_q', 64), ('block_k', 64)), (('block_q', 64), ('block_k', 128))), causal=(True, False), use_fwd=(True, False), use_segment_ids=(True, False))\ndef test_fused_attention_fwd(self, *, batch_size, seq_len, num_heads, head_dim, block_sizes, causal, use_fwd, use_segment_ids):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    q = random.normal(k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    k = random.normal(k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    v = random.normal(k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n    else:\n        segment_ids = None\n    if use_fwd:\n\n        @jax.jit\n        def impl(q, k, v):\n            v, _ = jax.vjp(functools.partial(attention.mha, block_sizes=BlockSizes(**dict(block_sizes)), causal=causal, segment_ids=segment_ids, interpret=self.INTERPRET), q, k, v)\n            return v\n    else:\n        impl = functools.partial(attention.mha, block_sizes=BlockSizes(**dict(block_sizes)), causal=causal, segment_ids=segment_ids, interpret=self.INTERPRET)\n    o = impl(q, k, v)\n    o_ref = attention.mha_reference(q, k, v, segment_ids, causal=causal)\n    np.testing.assert_allclose(o, o_ref, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=(1, 2), seq_len=(128, 384), num_heads=(1, 2), head_dim=(32, 64, 128), block_sizes=((('block_q', 128), ('block_k', 128), ('block_q_dkv', 128), ('block_kv_dkv', 128), ('block_q_dq', 128), ('block_kv_dq', 128)), (('block_q', 64), ('block_k', 64), ('block_q_dkv', 64), ('block_kv_dkv', 64), ('block_q_dq', 64), ('block_kv_dq', 64)), (('block_q', 64), ('block_k', 128), ('block_q_dkv', 64), ('block_kv_dkv', 128), ('block_q_dq', 128), ('block_kv_dq', 64))), causal=(True, False), use_segment_ids=(True, False))\ndef test_fused_attention_bwd(self, *, batch_size, seq_len, num_heads, head_dim, block_sizes, causal, use_segment_ids):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    q = random.normal(k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    k = random.normal(k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    v = random.normal(k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n    else:\n        segment_ids = None\n\n    def f(q, k, v):\n        return attention.mha(q, k, v, block_sizes=BlockSizes(**dict(block_sizes)), causal=causal, segment_ids=segment_ids, interpret=self.INTERPRET).sum()\n\n    def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n    dq_ref, dk_ref, dv_ref = jax.grad(f_ref, argnums=(0, 1, 2))(q, k, v)\n    self.assertAllClose(dq, dq_ref, atol=0.05)\n    self.assertAllClose(dk, dk_ref, atol=0.05)\n    self.assertAllClose(dv, dv_ref, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=(1, 2), seq_len=(128, 384), num_heads=(1, 2, 8), head_dim=(32, 64, 128), block_sizes=((('block_q', 128), ('block_k', 128)), (('block_q', 64), ('block_k', 64)), (('block_q', 64), ('block_k', 128))), causal=(True, False), use_fwd=(True, False), use_segment_ids=(True, False))\ndef test_fused_attention_fwd(self, *, batch_size, seq_len, num_heads, head_dim, block_sizes, causal, use_fwd, use_segment_ids):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    q = random.normal(k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    k = random.normal(k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    v = random.normal(k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n    else:\n        segment_ids = None\n    if use_fwd:\n\n        @jax.jit\n        def impl(q, k, v):\n            v, _ = jax.vjp(functools.partial(attention.mha, block_sizes=BlockSizes(**dict(block_sizes)), causal=causal, segment_ids=segment_ids, interpret=self.INTERPRET), q, k, v)\n            return v\n    else:\n        impl = functools.partial(attention.mha, block_sizes=BlockSizes(**dict(block_sizes)), causal=causal, segment_ids=segment_ids, interpret=self.INTERPRET)\n    o = impl(q, k, v)\n    o_ref = attention.mha_reference(q, k, v, segment_ids, causal=causal)\n    np.testing.assert_allclose(o, o_ref, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=(1, 2), seq_len=(128, 384), num_heads=(1, 2), head_dim=(32, 64, 128), block_sizes=((('block_q', 128), ('block_k', 128), ('block_q_dkv', 128), ('block_kv_dkv', 128), ('block_q_dq', 128), ('block_kv_dq', 128)), (('block_q', 64), ('block_k', 64), ('block_q_dkv', 64), ('block_kv_dkv', 64), ('block_q_dq', 64), ('block_kv_dq', 64)), (('block_q', 64), ('block_k', 128), ('block_q_dkv', 64), ('block_kv_dkv', 128), ('block_q_dq', 128), ('block_kv_dq', 64))), causal=(True, False), use_segment_ids=(True, False))\ndef test_fused_attention_bwd(self, *, batch_size, seq_len, num_heads, head_dim, block_sizes, causal, use_segment_ids):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    q = random.normal(k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    k = random.normal(k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    v = random.normal(k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n    else:\n        segment_ids = None\n\n    def f(q, k, v):\n        return attention.mha(q, k, v, block_sizes=BlockSizes(**dict(block_sizes)), causal=causal, segment_ids=segment_ids, interpret=self.INTERPRET).sum()\n\n    def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n    dq_ref, dk_ref, dv_ref = jax.grad(f_ref, argnums=(0, 1, 2))(q, k, v)\n    self.assertAllClose(dq, dq_ref, atol=0.05)\n    self.assertAllClose(dk, dk_ref, atol=0.05)\n    self.assertAllClose(dv, dv_ref, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.parameters(*[(1, 384, 192), (2, 384, 192)])\ndef test_fused_layernorm_fwd(self, batch_size, seq_len, embed_dim):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    x = random.normal(k1, (batch_size, seq_len, embed_dim), dtype=jnp.float32)\n    w = jax.random.normal(k2, (embed_dim,), dtype=jnp.float32)\n    b = jax.random.normal(k3, (embed_dim,), dtype=jnp.float32)\n    o = layer_norm.layer_norm(x, w, b)\n    o_ref = layer_norm.layer_norm_reference(x, w, b)\n    np.testing.assert_allclose(o, o_ref, atol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.parameters(*[(1, 384, 192), (2, 384, 192)])\ndef test_fused_layernorm_bwd(self, batch_size, seq_len, embed_dim):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    x = random.normal(k1, (batch_size, seq_len, embed_dim), dtype=jnp.float32)\n    w = jax.random.normal(k2, (embed_dim,), dtype=jnp.float32)\n    b = jax.random.normal(k3, (embed_dim,), dtype=jnp.float32)\n\n    def f(x, w, b):\n        return layer_norm.layer_norm(x, w, b).sum()\n\n    def f_ref(x, w, b):\n        return layer_norm.layer_norm_reference(x, w, b).sum()\n    dx, dw, db = jax.grad(f, argnums=(0, 1, 2))(x, w, b)\n    dx_ref, dw_ref, db_ref = jax.grad(f_ref, argnums=(0, 1, 2))(x, w, b)\n    np.testing.assert_allclose(dx, dx_ref, rtol=1e-06, atol=1e-06)\n    np.testing.assert_allclose(dw, dw_ref, rtol=0.01, atol=0.01)\n    np.testing.assert_allclose(db, db_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.parameters(*[(1, 384, 192), (2, 384, 192)])\ndef test_rms_fwd(self, batch_size, seq_len, embed_dim):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    x = random.normal(k1, (batch_size, seq_len, embed_dim), dtype=jnp.float32)\n    w = jax.random.normal(k2, (embed_dim,), dtype=jnp.float32)\n    b = jax.random.normal(k3, (embed_dim,), dtype=jnp.float32)\n    o = rms_norm.rms_norm(x, w, b)\n    o_ref = rms_norm.rms_norm_reference(x, w, b)\n    np.testing.assert_allclose(o, o_ref, atol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.parameters(*[(1, 384, 192), (2, 384, 192)])\ndef test_rms_norm_bwd(self, batch_size, seq_len, embed_dim):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    x = random.normal(k1, (batch_size, seq_len, embed_dim), dtype=jnp.float32)\n    w = jax.random.normal(k2, (embed_dim,), dtype=jnp.float32)\n    b = jax.random.normal(k3, (embed_dim,), dtype=jnp.float32)\n\n    def f(x, w, b):\n        return rms_norm.rms_norm(x, w, b).sum()\n\n    def f_ref(x, w, b):\n        return rms_norm.rms_norm_reference(x, w, b).sum()\n    dx, dw, db = jax.grad(f, argnums=(0, 1, 2))(x, w, b)\n    dx_ref, dw_ref, db_ref = jax.grad(f_ref, argnums=(0, 1, 2))(x, w, b)\n    np.testing.assert_allclose(dx, dx_ref, rtol=1e-06, atol=1e-06)\n    np.testing.assert_allclose(dw, dw_ref, rtol=0.01, atol=0.01)\n    np.testing.assert_allclose(db, db_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=(1, 2), seq_len=(128, 384), num_heads=(1, 2, 8), head_dim=(32, 64, 128), block_sizes=((('block_q', 128), ('block_k', 128)), (('block_q', 64), ('block_k', 64)), (('block_q', 64), ('block_k', 128))), causal=(True, False), use_fwd=(True, False), use_segment_ids=(True, False))\ndef test_fused_attention_fwd(self, *, batch_size, seq_len, num_heads, head_dim, block_sizes, causal, use_fwd, use_segment_ids):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    q = random.normal(k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    k = random.normal(k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    v = random.normal(k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n    else:\n        segment_ids = None\n    if use_fwd:\n\n        @jax.jit\n        def impl(q, k, v):\n            v, _ = jax.vjp(functools.partial(attention.mha, block_sizes=BlockSizes(**dict(block_sizes)), causal=causal, segment_ids=segment_ids, interpret=self.INTERPRET), q, k, v)\n            return v\n    else:\n        impl = functools.partial(attention.mha, block_sizes=BlockSizes(**dict(block_sizes)), causal=causal, segment_ids=segment_ids, interpret=self.INTERPRET)\n    o = impl(q, k, v)\n    o_ref = attention.mha_reference(q, k, v, segment_ids, causal=causal)\n    np.testing.assert_allclose(o, o_ref, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def impl(x):\n    return spec.call((x, jnp.zeros_like(x)))[1]"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=(1, 2), seq_len=(128, 384), num_heads=(1, 2), head_dim=(32, 64, 128), block_sizes=((('block_q', 128), ('block_k', 128), ('block_q_dkv', 128), ('block_kv_dkv', 128), ('block_q_dq', 128), ('block_kv_dq', 128)), (('block_q', 64), ('block_k', 64), ('block_q_dkv', 64), ('block_kv_dkv', 64), ('block_q_dq', 64), ('block_kv_dq', 64)), (('block_q', 64), ('block_k', 128), ('block_q_dkv', 64), ('block_kv_dkv', 128), ('block_q_dq', 128), ('block_kv_dq', 64))), causal=(True, False), use_segment_ids=(True, False))\ndef test_fused_attention_bwd(self, *, batch_size, seq_len, num_heads, head_dim, block_sizes, causal, use_segment_ids):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    q = random.normal(k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    k = random.normal(k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    v = random.normal(k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n    else:\n        segment_ids = None\n\n    def f(q, k, v):\n        return attention.mha(q, k, v, block_sizes=BlockSizes(**dict(block_sizes)), causal=causal, segment_ids=segment_ids, interpret=self.INTERPRET).sum()\n\n    def f_ref(q, k, v):\n        return attention.mha_reference(q, k, v, segment_ids, causal=causal).sum()\n    dq, dk, dv = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n    dq_ref, dk_ref, dv_ref = jax.grad(f_ref, argnums=(0, 1, 2))(q, k, v)\n    self.assertAllClose(dq, dq_ref, atol=0.05)\n    self.assertAllClose(dk, dk_ref, atol=0.05)\n    self.assertAllClose(dv, dv_ref, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@parameterized.parameters(*[(1, 384, 192), (2, 384, 192)])\ndef test_fused_layernorm_bwd(self, batch_size, seq_len, embed_dim):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    x = random.normal(k1, (batch_size, seq_len, embed_dim), dtype=jnp.float32)\n    w = jax.random.normal(k2, (embed_dim,), dtype=jnp.float32)\n    b = jax.random.normal(k3, (embed_dim,), dtype=jnp.float32)\n\n    def f(x, w, b):\n        return layer_norm.layer_norm(x, w, b).sum()\n\n    def f_ref(x, w, b):\n        return layer_norm.layer_norm_reference(x, w, b).sum()\n    dx, dw, db = jax.grad(f, argnums=(0, 1, 2))(x, w, b)\n    dx_ref, dw_ref, db_ref = jax.grad(f_ref, argnums=(0, 1, 2))(x, w, b)\n    np.testing.assert_allclose(dx, dx_ref, rtol=1e-06, atol=1e-06)\n    np.testing.assert_allclose(dw, dw_ref, rtol=0.01, atol=0.01)\n    np.testing.assert_allclose(db, db_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@parameterized.parameters(*[(1, 384, 192), (2, 384, 192)])\ndef test_rms_norm_bwd(self, batch_size, seq_len, embed_dim):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    x = random.normal(k1, (batch_size, seq_len, embed_dim), dtype=jnp.float32)\n    w = jax.random.normal(k2, (embed_dim,), dtype=jnp.float32)\n    b = jax.random.normal(k3, (embed_dim,), dtype=jnp.float32)\n\n    def f(x, w, b):\n        return rms_norm.rms_norm(x, w, b).sum()\n\n    def f_ref(x, w, b):\n        return rms_norm.rms_norm_reference(x, w, b).sum()\n    dx, dw, db = jax.grad(f, argnums=(0, 1, 2))(x, w, b)\n    dx_ref, dw_ref, db_ref = jax.grad(f_ref, argnums=(0, 1, 2))(x, w, b)\n    np.testing.assert_allclose(dx, dx_ref, rtol=1e-06, atol=1e-06)\n    np.testing.assert_allclose(dw, dw_ref, rtol=0.01, atol=0.01)\n    np.testing.assert_allclose(db, db_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=(1, 2), seq_len=(128, 384), num_heads=(1, 2, 8), head_dim=(32, 64, 128), block_sizes=((('block_q', 128), ('block_k', 128)), (('block_q', 64), ('block_k', 64)), (('block_q', 64), ('block_k', 128))), causal=(True, False), use_fwd=(True, False), use_segment_ids=(True, False))\ndef test_fused_attention_fwd(self, *, batch_size, seq_len, num_heads, head_dim, block_sizes, causal, use_fwd, use_segment_ids):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    q = random.normal(k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    k = random.normal(k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    v = random.normal(k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16)\n    if use_segment_ids:\n        segment_ids_1 = jnp.zeros((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids_2 = jnp.ones((batch_size, seq_len // 2), dtype=jnp.int32)\n        segment_ids = jnp.concatenate((segment_ids_1, segment_ids_2), axis=-1)\n    else:\n        segment_ids = None\n    if use_fwd:\n\n        @jax.jit\n        def impl(q, k, v):\n            v, _ = jax.vjp(functools.partial(attention.mha, block_sizes=BlockSizes(**dict(block_sizes)), causal=causal, segment_ids=segment_ids, interpret=self.INTERPRET), q, k, v)\n            return v\n    else:\n        impl = functools.partial(attention.mha, block_sizes=BlockSizes(**dict(block_sizes)), causal=causal, segment_ids=segment_ids, interpret=self.INTERPRET)\n    o = impl(q, k, v)\n    o_ref = attention.mha_reference(q, k, v, segment_ids, causal=causal)\n    np.testing.assert_allclose(o, o_ref, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(*[(1, 384, 192), (2, 384, 192)])\ndef test_fused_layernorm_fwd(self, batch_size, seq_len, embed_dim):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    x = random.normal(k1, (batch_size, seq_len, embed_dim), dtype=jnp.float32)\n    w = jax.random.normal(k2, (embed_dim,), dtype=jnp.float32)\n    b = jax.random.normal(k3, (embed_dim,), dtype=jnp.float32)\n    o = layer_norm.layer_norm(x, w, b)\n    o_ref = layer_norm.layer_norm_reference(x, w, b)\n    np.testing.assert_allclose(o, o_ref, atol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(*[(1, 384, 192), (2, 384, 192)])\ndef test_fused_layernorm_bwd(self, batch_size, seq_len, embed_dim):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    x = random.normal(k1, (batch_size, seq_len, embed_dim), dtype=jnp.float32)\n    w = jax.random.normal(k2, (embed_dim,), dtype=jnp.float32)\n    b = jax.random.normal(k3, (embed_dim,), dtype=jnp.float32)\n\n    def f(x, w, b):\n        return layer_norm.layer_norm(x, w, b).sum()\n\n    def f_ref(x, w, b):\n        return layer_norm.layer_norm_reference(x, w, b).sum()\n    dx, dw, db = jax.grad(f, argnums=(0, 1, 2))(x, w, b)\n    dx_ref, dw_ref, db_ref = jax.grad(f_ref, argnums=(0, 1, 2))(x, w, b)\n    np.testing.assert_allclose(dx, dx_ref, rtol=1e-06, atol=1e-06)\n    np.testing.assert_allclose(dw, dw_ref, rtol=0.01, atol=0.01)\n    np.testing.assert_allclose(db, db_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(*[(1, 384, 192), (2, 384, 192)])\ndef test_rms_fwd(self, batch_size, seq_len, embed_dim):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    x = random.normal(k1, (batch_size, seq_len, embed_dim), dtype=jnp.float32)\n    w = jax.random.normal(k2, (embed_dim,), dtype=jnp.float32)\n    b = jax.random.normal(k3, (embed_dim,), dtype=jnp.float32)\n    o = rms_norm.rms_norm(x, w, b)\n    o_ref = rms_norm.rms_norm_reference(x, w, b)\n    np.testing.assert_allclose(o, o_ref, atol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(*[(1, 384, 192), (2, 384, 192)])\ndef test_rms_norm_bwd(self, batch_size, seq_len, embed_dim):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    x = random.normal(k1, (batch_size, seq_len, embed_dim), dtype=jnp.float32)\n    w = jax.random.normal(k2, (embed_dim,), dtype=jnp.float32)\n    b = jax.random.normal(k3, (embed_dim,), dtype=jnp.float32)\n\n    def f(x, w, b):\n        return rms_norm.rms_norm(x, w, b).sum()\n\n    def f_ref(x, w, b):\n        return rms_norm.rms_norm_reference(x, w, b).sum()\n    dx, dw, db = jax.grad(f, argnums=(0, 1, 2))(x, w, b)\n    dx_ref, dw_ref, db_ref = jax.grad(f_ref, argnums=(0, 1, 2))(x, w, b)\n    np.testing.assert_allclose(dx, dx_ref, rtol=1e-06, atol=1e-06)\n    np.testing.assert_allclose(dw, dw_ref, rtol=0.01, atol=0.01)\n    np.testing.assert_allclose(db, db_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.product(shape=[(1024, 125), (4, 1024, 125)], dtype=[jnp.bfloat16, jnp.float16, jnp.float32])\ndef test_softmax(self, shape, dtype):\n    x = jax.random.normal(random.key(0), shape, dtype=dtype)\n    atol, rtol = {jnp.bfloat16: (0.01, 0.0001), jnp.float16: (0.01, 0.0001), jnp.float32: (1e-07, 1e-06)}[dtype]\n    np.testing.assert_allclose(softmax.softmax(x, axis=-1).astype(jnp.float32), jax.nn.softmax(x, axis=-1).astype(jnp.float32), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(*[(1, 384, 192), (2, 384, 192)])\ndef test_fused_layernorm_fwd(self, batch_size, seq_len, embed_dim):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    x = random.normal(k1, (batch_size, seq_len, embed_dim), dtype=jnp.float32)\n    w = jax.random.normal(k2, (embed_dim,), dtype=jnp.float32)\n    b = jax.random.normal(k3, (embed_dim,), dtype=jnp.float32)\n    o = layer_norm.layer_norm(x, w, b)\n    o_ref = layer_norm.layer_norm_reference(x, w, b)\n    np.testing.assert_allclose(o, o_ref, atol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct([256], jnp.float32))\ndef layer_norm(x_ref, o_ref):\n    x_mean = jnp.mean(x_ref[...])\n    x_centered = x_ref[...] - x_mean\n    o_ref[...] = x_centered * jax.lax.rsqrt(jnp.mean(x_centered ** 2) + eps) * gamma + beta"
  },
  {
    "test_code": "@parameterized.parameters(*[(1, 384, 192), (2, 384, 192)])\ndef test_fused_layernorm_bwd(self, batch_size, seq_len, embed_dim):\n    k1, k2, k3 = random.split(random.key(0), 3)\n    x = random.normal(k1, (batch_size, seq_len, embed_dim), dtype=jnp.float32)\n    w = jax.random.normal(k2, (embed_dim,), dtype=jnp.float32)\n    b = jax.random.normal(k3, (embed_dim,), dtype=jnp.float32)\n\n    def f(x, w, b):\n        return layer_norm.layer_norm(x, w, b).sum()\n\n    def f_ref(x, w, b):\n        return layer_norm.layer_norm_reference(x, w, b).sum()\n    dx, dw, db = jax.grad(f, argnums=(0, 1, 2))(x, w, b)\n    dx_ref, dw_ref, db_ref = jax.grad(f_ref, argnums=(0, 1, 2))(x, w, b)\n    np.testing.assert_allclose(dx, dx_ref, rtol=1e-06, atol=1e-06)\n    np.testing.assert_allclose(dw, dw_ref, rtol=0.01, atol=0.01)\n    np.testing.assert_allclose(db, db_ref, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=jax.ShapeDtypeStruct([256], jnp.float32))\ndef layer_norm(x_ref, o_ref):\n    x_mean = jnp.mean(x_ref[...])\n    x_centered = x_ref[...] - x_mean\n    o_ref[...] = x_centered * jax.lax.rsqrt(jnp.mean(x_centered ** 2) + eps) * gamma + beta"
  },
  {
    "test_code": "@parameterized.product(shape=[(1024, 125), (4, 1024, 125)], dtype=[jnp.bfloat16, jnp.float16, jnp.float32])\ndef test_softmax(self, shape, dtype):\n    x = jax.random.normal(random.key(0), shape, dtype=dtype)\n    atol, rtol = {jnp.bfloat16: (0.01, 0.0001), jnp.float16: (0.01, 0.0001), jnp.float32: (1e-07, 1e-06)}[dtype]\n    np.testing.assert_allclose(softmax.softmax(x, axis=-1).astype(jnp.float32), jax.nn.softmax(x, axis=-1).astype(jnp.float32), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/gpu_ops_test.py",
    "function": "@functools.partial(jax.custom_jvp, nondiff_argnums=(1,))\ndef softmax(x, axis=-1):\n    unnormalized = jnp.exp(x - jnp.max(x, axis, keepdims=True))\n    return unnormalized / jnp.sum(unnormalized, axis, keepdims=True)"
  }
]