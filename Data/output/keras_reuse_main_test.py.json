[
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{model}', model=model) for model in ['mnist_pure_jax', 'mnist_flax']))\n@jtu.ignore_warning(message='the imp module is deprecated')\ndef test_keras_reuse(self, model='mnist_pure_jax'):\n    FLAGS.model = model\n    keras_reuse_main.main(None)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/examples/keras_reuse_main_test.py",
    "function": "def main(_):\n    print_ir(np.empty([7], np.bool_))(lax.bitwise_not)\n    print_ir(np.int8(0))(lax.neg)\n    print_ir(np.empty([0], np.int16))(lax.neg)\n    print_ir(np.empty([2, 3], np.int32))(lax.neg)\n    print_ir(np.empty([2, 3, 4], np.int64))(lax.neg)\n    print_ir(np.empty([4, 0, 1], np.uint8), np.empty([4, 0, 1], np.uint8))(lax.add)\n    print_ir(np.uint16(0), np.uint16(0))(lax.add)\n    print_ir(np.uint32(0), np.uint32(0))(lax.add)\n    print_ir(np.uint64(0), np.uint64(0))(lax.add)\n    print_ir(np.float16(0))(lax.sin)\n    print_ir(jnp.bfloat16(0))(lax.sin)\n    print_ir(np.float32(0))(lax.sin)\n    print_ir(np.float64(0))(lax.sin)\n    print_ir(np.complex64(0))(lax.cos)\n    print_ir(np.complex128(0))(lax.cos)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{model}', model=model) for model in ['mnist_pure_jax', 'mnist_flax']))\n@jtu.ignore_warning(message='the imp module is deprecated')\ndef test_keras_reuse(self, model='mnist_pure_jax'):\n    FLAGS.model = model\n    keras_reuse_main.main(None)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/examples/keras_reuse_main_test.py",
    "function": "def main(_):\n    print_ir(np.empty([7], np.int32))(lax.neg)\n\n    @print_ir(np.empty([7], np.int32))\n    @jax.jit\n    def foo(x):\n        return x + 2"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{model}', model=model) for model in ['mnist_pure_jax', 'mnist_flax']))\n@jtu.ignore_warning(message='the imp module is deprecated')\ndef test_keras_reuse(self, model='mnist_pure_jax'):\n    FLAGS.model = model\n    keras_reuse_main.main(None)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/examples/keras_reuse_main_test.py",
    "function": "def main(_):\n\n    @print_ir(np.empty([2, 7], np.int32), np.empty([2, 7], np.int32))\n    def cumsum_only_once(x, y):\n        return jax.lax.cumsum(x) + jax.lax.cumsum(y)\n\n    def make_module(c):\n\n        @jax.jit\n        def f(x):\n            return x + c\n\n        @jax.jit\n        def g(x):\n            return f(x * 2)\n        return g.lower(7).compiler_ir()\n    m1 = make_module(10)\n    m2 = str(make_module(20))\n    with m1.context:\n        m2_copy = ir.Module.parse(m2)\n        mlir.merge_mlir_modules(m1, 'm2_main_renamed', m2_copy)\n    print('\\nTEST: merge_modules')\n    print(str(m1))\n    with mlir.make_ir_context():\n        m_str = '\\nmodule @jit_f {\\n  func.func public @main(%arg0: tensor<i64>) -> tensor<i64> {\\n    %0 = call @f(%arg0) : (tensor<i64>) -> tensor<i64>\\n    return %0 : tensor<i64>\\n  }\\n  func.func private @f(%arg0: tensor<i64>) -> tensor<i64> {\\n    return %arg0 : tensor<i64>\\n  }\\n}'\n        m1 = ir.Module.parse(m_str)\n        m2 = ir.Module.parse(m_str)\n        mlir.merge_mlir_modules(m1, 'f', m2)\n    print('\\nTEST: merge_modules_2')\n    print(str(m1))"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{model}', model=model) for model in ['mnist_pure_jax', 'mnist_flax']))\n@jtu.ignore_warning(message='the imp module is deprecated')\ndef test_keras_reuse(self, model='mnist_pure_jax'):\n    FLAGS.model = model\n    keras_reuse_main.main(None)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/examples/keras_reuse_main_test.py",
    "function": "def main(_):\n    print_ir(np.int32(0))(lax.abs)\n    print_ir(np.float32(1), np.float32(2))(lax.add)\n    print_ir(np.float32(1))(lax.acos)\n    print_ir(np.float32(0))(lax.acosh)\n    print_ir(np.float32(1))(lax.asin)\n    print_ir(np.float32(0))(lax.asinh)\n    print_ir(np.float32(1))(lax.atan)\n    print_ir(np.float32(0))(lax.atanh)\n    print_ir(np.float64(1), np.float64(2))(lax.atan2)\n    print_ir(np.float32(0))(lax.bessel_i1e)\n    print_ir(np.empty((7,), np.uint32))(partial(lax.bitcast_convert_type, new_dtype=np.float32))\n    print_ir(np.int32(1), np.int32(2))(lax.bitwise_and)\n    print_ir(np.bool_(0), np.bool_(0))(lax.bitwise_and)\n    print_ir(np.int32(1), np.int32(2))(lax.bitwise_or)\n    print_ir(np.bool_(0), np.bool_(0))(lax.bitwise_or)\n    print_ir(np.int32(1), np.int32(2))(lax.bitwise_xor)\n    print_ir(np.bool_(0), np.bool_(0))(lax.bitwise_xor)\n    print_ir(jnp.bfloat16(0))(lax.cbrt)\n    print_ir(jnp.bfloat16(0), jnp.bfloat16(0), jnp.bfloat16(0))(lax.clamp)\n    print_ir(np.empty((7,), np.float16))(lax.ceil)\n    print_ir(np.empty((7,), np.float16))(partial(lax.convert_element_type, new_dtype=np.float32))\n    print_ir(np.empty((7,), np.complex64))(partial(lax.convert_element_type, new_dtype=np.float32))\n    print_ir(np.empty((7,), np.float32))(partial(lax.convert_element_type, new_dtype=np.bool_))\n    print_ir(np.uint32(0))(lax.clz)\n    print_ir(np.complex64(0))(lax.conj)\n    print_ir(np.float32(0))(lax.cos)\n    print_ir(np.float32(0))(lax.cosh)\n    print_ir(np.float32(0))(lax.digamma)\n    print_ir(np.float32(1), np.float32(2))(lax.div)\n    print_ir(np.float32(1), np.float32(2))(lax.eq)\n    print_ir(np.complex128(1), np.complex128(2))(lax.eq)\n    print_ir(np.int64(1), np.int64(2))(lax.eq)\n    print_ir(np.uint16(1), np.uint16(2))(lax.eq)\n    print_ir(np.float32(0))(lax.erf)\n    print_ir(np.float32(0))(lax.erfc)\n    print_ir(np.float32(0))(lax.erf_inv)\n    print_ir(np.float16(0))(lax.exp)\n    print_ir(jnp.bfloat16(0))(lax.expm1)\n    print_ir(np.empty((2, 3), jnp.bfloat16))(lax.floor)\n    print_ir(np.float32(1), np.float32(2))(lax.ge)\n    print_ir(np.float32(1), np.float32(2))(lax.gt)\n    print_ir(np.complex64(0))(lax.imag)\n\n    @print_ir(np.float32(1))\n    def integer_pow(x):\n        return lax.integer_pow(x, 3)\n    print_ir(np.float64(0))(lax.is_finite)\n    print_ir(np.float32(1), np.float32(2))(lax.le)\n    print_ir(np.float32(0))(lax.lgamma)\n    print_ir(np.float32(0))(lax.log)\n    print_ir(np.float32(0))(lax.log1p)\n    print_ir(np.float32(1), np.float32(2))(lax.lt)\n    print_ir(np.float32(1), np.float32(2))(lax.max)\n    print_ir(np.float32(1), np.float32(2))(lax.min)\n    print_ir(np.float32(1), np.float32(2))(lax.mul)\n    print_ir(np.float32(1), np.float32(2))(lax.ne)\n    print_ir(np.int64(0))(lax.neg)\n    print_ir(np.float32(0), np.float32(0))(lax.nextafter)\n    print_ir(np.int64(0))(lax.bitwise_not)\n    print_ir(np.bool_(0))(lax.bitwise_not)\n    print_ir(np.uint32(0))(lax.population_count)\n    print_ir(np.float32(1), np.float32(2))(lax.pow)\n    print_ir(np.complex128(0))(lax.real)\n    print_ir(jnp.bfloat16(0))(partial(lax.reduce_precision, exponent_bits=2, mantissa_bits=2))\n    print_ir(np.float32(1), np.float32(2))(lax.rem)\n    print_ir(np.empty((7, 1), np.float64))(partial(lax.round, rounding_method=lax.RoundingMethod.AWAY_FROM_ZERO))\n    print_ir(jnp.complex64(0))(lax.rsqrt)\n    print_ir(np.uint32(0), np.uint32(0))(lax.shift_left)\n    print_ir(np.uint8(0), np.uint8(0))(lax.shift_right_arithmetic)\n    print_ir(np.uint16(0), np.uint16(0))(lax.shift_right_logical)\n    print_ir(np.int64(0))(lax.sign)\n    print_ir(np.uint32(0))(lax.sign)\n    print_ir(np.float32(0))(lax.sin)\n    print_ir(np.float32(0))(lax.sinh)\n    print_ir(np.float32(1), np.float32(2))(lax.sub)\n    print_ir(jnp.bfloat16(0))(lax.sqrt)\n    print_ir(np.float16(0))(lax.tan)\n    print_ir(np.float32(0))(lax.tanh)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{model}', model=model) for model in ['mnist_pure_jax', 'mnist_flax']))\n@jtu.ignore_warning(message='the imp module is deprecated')\ndef test_keras_reuse(self, model='mnist_pure_jax'):\n    FLAGS.model = model\n    keras_reuse_main.main(None)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/examples/keras_reuse_main_test.py",
    "function": "def main(unused_argv):\n    numpts = 7\n    key = random.key(0)\n    eye = jnp.eye(numpts)\n\n    def cov_map(cov_func, xs, xs2=None):\n        \"\"\"Compute a covariance matrix from a covariance function and data points.\n\n    Args:\n      cov_func: callable function, maps pairs of data points to scalars.\n      xs: array of data points, stacked along the leading dimension.\n    Returns:\n      A 2d array `a` such that `a[i, j] = cov_func(xs[i], xs[j])`.\n    \"\"\"\n        if xs2 is None:\n            return vmap(lambda x: vmap(lambda y: cov_func(x, y))(xs))(xs)\n        else:\n            return vmap(lambda x: vmap(lambda y: cov_func(x, y))(xs))(xs2).T\n\n    def softplus(x):\n        return jnp.logaddexp(x, 0.0)\n\n    def exp_quadratic(x1, x2):\n        return jnp.exp(-jnp.sum((x1 - x2) ** 2))\n\n    def gp(params, x, y, xtest=None, compute_marginal_likelihood=False):\n        noise = softplus(params['noise'])\n        amp = softplus(params['amplitude'])\n        ls = softplus(params['lengthscale'])\n        ymean = jnp.mean(y)\n        y = y - ymean\n        x = x / ls\n        train_cov = amp * cov_map(exp_quadratic, x) + eye * (noise + 1e-06)\n        chol = scipy.linalg.cholesky(train_cov, lower=True)\n        kinvy = scipy.linalg.solve_triangular(chol.T, scipy.linalg.solve_triangular(chol, y, lower=True))\n        if compute_marginal_likelihood:\n            log2pi = jnp.log(2.0 * 3.1415)\n            ml = jnp.sum(-0.5 * jnp.dot(y.T, kinvy) - jnp.sum(jnp.log(jnp.diag(chol))) - numpts / 2.0 * log2pi)\n            ml -= jnp.sum(-0.5 * jnp.log(2 * 3.1415) - jnp.log(amp) - 0.5 * jnp.log(amp) ** 2)\n            return -ml\n        if xtest is not None:\n            xtest = xtest / ls\n        cross_cov = amp * cov_map(exp_quadratic, x, xtest)\n        mu = jnp.dot(cross_cov.T, kinvy) + ymean\n        v = scipy.linalg.solve_triangular(chol, cross_cov, lower=True)\n        var = amp * cov_map(exp_quadratic, xtest) - jnp.dot(v.T, v)\n        return (mu, var)\n    marginal_likelihood = partial(gp, compute_marginal_likelihood=True)\n    predict = partial(gp, compute_marginal_likelihood=False)\n    grad_fun = jit(grad(marginal_likelihood))\n    params = {'amplitude': jnp.zeros((1, 1)), 'noise': jnp.zeros((1, 1)) - 5.0, 'lengthscale': jnp.zeros((1, 1))}\n    momentums = {k: p * 0.0 for k, p in params.items()}\n    scales = {k: p * 0.0 + 1.0 for k, p in params.items()}\n    lr = 0.01\n\n    def train_step(params, momentums, scales, x, y):\n        grads = grad_fun(params, x, y)\n        for k in params:\n            momentums[k] = 0.9 * momentums[k] + 0.1 * grads[k][0]\n            scales[k] = 0.9 * scales[k] + 0.1 * grads[k][0] ** 2\n            params[k] -= lr * momentums[k] / jnp.sqrt(scales[k] + 1e-05)\n        return (params, momentums, scales)\n    y_fun = lambda x: jnp.sin(x) + 0.1 * random.normal(key, shape=(x.shape[0], 1))\n    x = random.uniform(key, shape=(numpts, 1)) * 4.0 + 1\n    y = y_fun(x)\n    xtest = jnp.linspace(0, 6.0, 200)[:, None]\n    for i in range(1000):\n        params, momentums, scales = train_step(params, momentums, scales, x, y)\n        if i % 50 == 0:\n            ml = marginal_likelihood(params, x, y)\n            print('Step: %d, neg marginal likelihood: %f' % (i, ml))\n    print(params)\n    mu, var = predict(params, x, y, xtest)\n    std = jnp.sqrt(jnp.diag(var))\n    plt.plot(x, y, 'k.')\n    plt.plot(xtest, mu)\n    plt.fill_between(xtest.flatten(), mu.flatten() - std * 2, mu.flatten() + std * 2)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{model}', model=model) for model in ['mnist_pure_jax', 'mnist_flax']))\n@jtu.ignore_warning(message='the imp module is deprecated')\ndef test_keras_reuse(self, model='mnist_pure_jax'):\n    FLAGS.model = model\n    keras_reuse_main.main(None)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/examples/keras_reuse_main_test.py",
    "function": "def main():\n    args = parse_args()\n    python_versions = args.python_versions.split(',')\n    print('ROCM_VERSION=%s' % args.rocm_version)\n    print('PYTHON_VERSIONS=%r' % python_versions)\n    print('JAX_PATH=%s' % args.jax_path)\n    print('XLA_PATH=%s' % args.xla_path)\n    rocm_path = build_rocm_path(args.rocm_version)\n    update_rocm_targets(rocm_path, GPU_DEVICE_TARGETS)\n    for py in python_versions:\n        build_jaxlib_wheel(args.jax_path, rocm_path, py, args.xla_path, args.compiler)\n        wheel_paths = find_wheels(os.path.join(args.jax_path, 'dist'))\n        for wheel_path in wheel_paths:\n            if not os.path.basename(wheel_path).startswith('jax-'):\n                fix_wheel(wheel_path, args.jax_path)\n    build_jax_wheel(args.jax_path, python_versions[-1])\n    wheels = find_wheels(os.path.join(args.jax_path, 'dist'))\n    wheelhouse_dir = '/wheelhouse/'\n    for whl in wheels:\n        if os.path.basename(whl).startswith('jax-'):\n            LOG.info('Copying %s into %s' % (whl, wheelhouse_dir))\n            shutil.copy(whl, wheelhouse_dir)\n    logging.info('Deleting dist, egg-info and cache directory')\n    shutil.rmtree(os.path.join(args.jax_path, 'dist'))\n    shutil.rmtree(os.path.join(args.jax_path, 'jax.egg-info'))\n    shutil.rmtree(os.path.join(args.jax_path, 'jax', '__pycache__'))\n    whl_house = os.path.join(args.jax_path, 'wheelhouse')\n    logging.info('Changing permissions for %s' % whl_house)\n    mode = 436\n    for item in os.listdir(whl_house):\n        whl_path = os.path.join(whl_house, item)\n        if os.path.isfile(whl_path):\n            os.chmod(whl_path, mode)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{model}', model=model) for model in ['mnist_pure_jax', 'mnist_flax']))\n@jtu.ignore_warning(message='the imp module is deprecated')\ndef test_keras_reuse(self, model='mnist_pure_jax'):\n    FLAGS.model = model\n    keras_reuse_main.main(None)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/examples/keras_reuse_main_test.py",
    "function": "def main(unused_argv):\n    m, k, n = (8192, 4096, 8192)\n    ka, kb = jr.split(jr.key(0), 2)\n    a = jr.normal(key=ka, shape=(m, k), dtype=jnp.float16)\n    b = jr.normal(key=kb, shape=(n, k), dtype=jnp.float16)\n    tile_m = (128,)\n    tile_n = (128, 256, 512)\n    max_concurrent_steps = (2, 4, 5, 6)\n    grid_tile_m = (1, 2, 4, 8, 16)\n    collective = (False, True)\n    configs = itertools.product(collective, tile_m, tile_n, grid_tile_m, max_concurrent_steps)\n    names = ('collective', 'tile_m', 'tile_n', 'grid_tile_m', 'max_concurrent_steps')\n    best_runtime = float('inf')\n    best_kwargs = {}\n    for config in configs:\n        kwargs = dict(zip(names, config))\n        tile_m = kwargs['tile_m']\n        tile_n = kwargs['tile_n']\n        if kwargs['collective']:\n            tile_m *= 2\n            tile_n *= 2\n        if m < tile_m or n < tile_n:\n            continue\n        if kwargs['collective'] and tile_n >= 512:\n            continue\n        if m // tile_m % kwargs['grid_tile_m']:\n            continue\n        try:\n            with mlir.make_ir_context(), ir.Location.unknown():\n                f = build_kernel(m, n, k, **kwargs)\n                _, runtime = profiler.measure(f)(a, b)\n        except ValueError as e:\n            if 'Mosaic GPU kernel exceeds available shared memory' not in str(e):\n                raise\n            runtime = float('inf')\n        else:\n            print(' '.join((f'{k}={v}' for k, v in kwargs.items())), int(runtime * 1000))\n        if runtime < best_runtime:\n            best_runtime = runtime\n            best_kwargs = kwargs\n    if not best_kwargs:\n        raise ValueError('No valid configuration found')\n    with mlir.make_ir_context(), ir.Location.unknown():\n        d, runtime = profiler.measure(build_kernel(m, n, k, **best_kwargs))(a, b)\n    d_ref, ref_runtime = profiler.measure(jax.jit(lambda a, b: a @ b.T))(a, b)\n    tflops = float(2 * k * m * n) / (runtime / 1000.0) / 1000000000000.0\n    ref_tflops = float(2 * k * m * n) / (ref_runtime / 1000.0) / 1000000000000.0\n    print('Best parameters: ', ' '.join((f'{k}={v}' for k, v in best_kwargs.items())))\n    print(f'Kernel:    {runtime * 1000:.1f} us = {tflops:.1f} TFLOPS')\n    print(f'Reference: {ref_runtime * 1000:.1f} us = {ref_tflops:.1f} TFLOPS')\n    np.testing.assert_allclose(d, d_ref, atol=0.001, rtol=0.001)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{model}', model=model) for model in ['mnist_pure_jax', 'mnist_flax']))\n@jtu.ignore_warning(message='the imp module is deprecated')\ndef test_keras_reuse(self, model='mnist_pure_jax'):\n    FLAGS.model = model\n    keras_reuse_main.main(None)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/examples/keras_reuse_main_test.py",
    "function": "def main(args):\n    xs = map(parse_arr, range(len(args)), args)\n    assert all((len(x.shape) == 2 for x in xs))\n    slab = slab_make(1024)\n    x, y, *_ = xs\n    test_binop(add, jax.lax.add, slab, x, x)\n    test_binop(mul, jax.lax.mul, slab, x, x)\n    test_binop(matmul, lambda a, b: a @ b, slab, x, y)\n\n    def put(slab, x):\n        slab, v = slab_upload(slab, x)\n        print_seg('slab_read result')\n        print(slab_download(slab, v))\n        return (slab, v)\n    slab, vals = chain(slab, put, *xs, unary=True)\n    if len(vals) >= 2:\n        x, y, *_ = vals\n        slab, z = mul(slab, x, x)\n        print_seg('mul')\n        print(slab_download(slab, z))\n        slab, w = add(slab, x, z)\n        print_seg('add')\n        print(slab_download(slab, w))"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{model}', model=model) for model in ['mnist_pure_jax', 'mnist_flax']))\n@jtu.ignore_warning(message='the imp module is deprecated')\ndef test_keras_reuse(self, model='mnist_pure_jax'):\n    FLAGS.model = model\n    keras_reuse_main.main(None)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/examples/keras_reuse_main_test.py",
    "function": "def main(unused_argv):\n    num_q_heads = 16\n    num_kv_heads = 16\n    use_pipeline_emitter = False\n    if use_pipeline_emitter:\n        attention_impl = attention_with_pipeline_emitter\n        schedule_barrier_opts = (True, False)\n    else:\n        attention_impl = attention\n        schedule_barrier_opts = (True,)\n    problem_it = itertools.product((1,), (4096, 32768), (64, 128, 256), schedule_barrier_opts)\n    for batch_size, seq_len, head_dim, use_schedule_barrier in problem_it:\n        q_seq_len = kv_seq_len = seq_len\n        print(f'==== batch_size={batch_size:<6} kv_seq_len={kv_seq_len:<6} q_seq_len={q_seq_len:<6}num_q_heads={num_q_heads:<4} head_dim={head_dim:<6} use_schedule_barrier={use_schedule_barrier:} ====')\n        k1, k2, k3 = jax.random.split(jax.random.key(42), 3)\n        q = jax.random.normal(k1, (batch_size, q_seq_len, num_q_heads, head_dim), jnp.float16)\n        k = jax.random.normal(k2, (batch_size, kv_seq_len, num_kv_heads, head_dim), jnp.float16)\n        v = jax.random.normal(k3, (batch_size, kv_seq_len, num_kv_heads, head_dim), jnp.float16)\n        block_q = 64\n        best = None\n        for block_kv in (256, 128, 64):\n            config = TuningConfig(block_q=block_q, block_kv=block_kv, max_concurrent_steps=2, use_schedule_barrier=use_schedule_barrier)\n            try:\n                out, runtime_ms = profiler.measure(functools.partial(attention_impl, config=config))(q, k, v)\n                if seq_len < 32768:\n                    out_ref = attention_reference(q, k, v)\n                    np.testing.assert_allclose(out, out_ref, atol=0.002, rtol=0.001)\n            except ValueError as e:\n                if 'exceeds available shared memory' in e.args[0]:\n                    continue\n                raise\n            runtime_us = runtime_ms * 1000.0\n            matmul_flops = 4 * q_seq_len * kv_seq_len * head_dim * num_q_heads * batch_size\n            peak_flops = 1000000000000000.0\n            optimal_time = matmul_flops / peak_flops * 1000000.0\n            achieved_tc_util = optimal_time / runtime_us * 100\n            print(f'block_q={block_q:<4}block_kv={block_kv:<4}:  {runtime_us:<7.1f}us = {achieved_tc_util:4.1f}% TC utilization')\n            if best is None or runtime_us < best[0]:\n                best = (runtime_us, achieved_tc_util)\n            break\n        if best is not None:\n            print(f'Best: {best[0]:<7.1f}us = {best[1]:4.1f}% TC utilization')"
  }
]