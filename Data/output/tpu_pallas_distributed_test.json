[
  {
    "test_code": "@parameterized.parameters(('left',), ('right',))\ndef test_interpret_remote_dma_ppermute(self, permutation):\n    if jax.device_count() <= 1:\n        self.skipTest('Test requires multiple devices.')\n    num_devices = jax.device_count()\n    if permutation == 'left':\n        permute_fn = lambda x: lax.rem(x + num_devices - 1, num_devices)\n    else:\n        permute_fn = lambda x: lax.rem(x + num_devices + 1, num_devices)\n\n    def test_kernel(x_ref, o_ref, copy_send_sem, copy_recv_sem):\n        o_ref[...] = jnp.zeros_like(o_ref[...])\n        my_id = lax.axis_index('x')\n        dst_device = permute_fn(my_id)\n        input_to_output_copy = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=copy_send_sem, recv_sem=copy_recv_sem, device_id=dst_device, device_id_type=pltpu.DeviceIdType.LOGICAL)\n        input_to_output_copy.start()\n        input_to_output_copy.wait()\n    out_shape = jax.ShapeDtypeStruct((8, 128), jnp.float32)\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], scratch_shapes=[pltpu.SemaphoreType.DMA] * 2)\n    devices = mesh_utils.create_device_mesh((num_devices,))\n    mesh = jax.sharding.Mesh(devices, 'x')\n    sharding = jax.sharding.NamedSharding(mesh, P(None, 'x'))\n    unsharded_arr = jax.random.normal(jax.random.key(0), shape=(8, 128 * num_devices))\n    sharded_arr = jax.device_put(unsharded_arr, sharding)\n    kernel = pl.pallas_call(test_kernel, out_shape=out_shape, grid_spec=grid_spec, interpret=True)\n    compiled_func = jax.jit(shard_map.shard_map(kernel, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x'), check_rep=False))\n    result = compiled_func(sharded_arr)\n    perm = tuple(((src, permute_fn(src)) for src in range(num_devices)))\n    perm = jax.tree_util.tree_map(int, perm)\n\n    def lax_permute(x):\n        return lax.ppermute(x, 'x', perm)\n    expected = jax.jit(shard_map.shard_map(lax_permute, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x')))(sharded_arr)\n    np.testing.assert_array_equal(result, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_distributed_test.py",
    "function": "def ppermute(input):\n    return jax.lax.ppermute(input, axis_name='i', perm=[[0, 1], [1, 0]])"
  },
  {
    "test_code": "@parameterized.named_parameters(('vmem', pltpu.TPUMemorySpace.VMEM), ('hbm', pltpu.TPUMemorySpace.ANY))\ndef test_basic_remote_vmem_dma(self, mem):\n\n    def kernel(x_ref, y_ref):\n\n        def body(ready_sem, send_sem, recv_sem):\n            dev_id = pltpu.device_id()\n            other_dev_id = 1 - dev_id\n            pltpu.semaphore_signal(ready_sem, device_id=other_dev_id, device_id_type=pltpu.DeviceIdType.LOGICAL)\n            pltpu.semaphore_wait(ready_sem)\n            copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, other_dev_id, device_id_type=pltpu.DeviceIdType.LOGICAL)\n            copy_done.wait_send()\n            copy_done.wait_recv()\n        pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)\n    x = jnp.arange(2 * 8 * 128.0).reshape((2 * 8, 128))\n\n    def body(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=mem)], out_specs=pl.BlockSpec(memory_space=mem), out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32))(x)\n    devices = jax.devices()[:2]\n    mesh = jax.sharding.Mesh(devices, ['x'])\n    y = jax.jit(shard_map.shard_map(body, mesh, in_specs=P('x'), out_specs=P('x'), check_rep=False))(x)\n    expected = jnp.concatenate([x[8:], x[:8]])\n    np.testing.assert_allclose(y, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_distributed_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(('left', 'left'), ('right', 'right'))\ndef test_pallas_call_axis_index(self, direction):\n\n    def kernel(x_ref, y_ref):\n\n        def body(ready_sem, send_sem, recv_sem):\n            my_id = lax.axis_index('x')\n            num_devices = lax.psum(1, 'x')\n            if direction == 'right':\n                neighbor = lax.rem(my_id + 1, num_devices)\n            else:\n                neighbor = lax.rem(my_id - 1, num_devices)\n                neighbor = jnp.where(neighbor < 0, neighbor + num_devices, neighbor)\n            pltpu.semaphore_signal(ready_sem, device_id=neighbor)\n            pltpu.semaphore_wait(ready_sem)\n            copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=neighbor)\n            copy_done.wait_send()\n            copy_done.wait_recv()\n        pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)\n    num_devices = jax.local_device_count()\n    x = jnp.arange(num_devices * 8 * 128).reshape((num_devices * 8, 128))\n\n    def body(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM), out_shape=x)(x)\n    device_mesh = mesh_utils.create_device_mesh((jax.device_count(),), jax.devices())\n    mesh = jax.sharding.Mesh(device_mesh, ['x'])\n    y = jax.jit(shard_map.shard_map(body, mesh, in_specs=P('x'), out_specs=P('x'), check_rep=False))(x)\n    if direction == 'right':\n        expected = jnp.concatenate([x[-8:], x[:-8]])\n    else:\n        expected = jnp.concatenate([x[8:], x[:8]])\n    np.testing.assert_allclose(y, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_distributed_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(('left', 'left'), ('right', 'right'))\ndef test_pallas_call_axis_index_2d_mesh(self, direction):\n\n    def kernel(x_ref, y_ref):\n\n        def body(ready_sem, send_sem, recv_sem):\n            my_id = lax.axis_index('x')\n            my_other_id = lax.axis_index('y')\n            axis_size = lax.psum(1, 'x')\n            if direction == 'right':\n                neighbor = lax.rem(my_id + 1, axis_size)\n            else:\n                neighbor = lax.rem(my_id - 1, axis_size)\n                neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n            pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n            pltpu.semaphore_wait(ready_sem)\n            copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n            copy_done.wait_send()\n            copy_done.wait_recv()\n        pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)\n    axis_size = jax.device_count() // 2\n    x = jnp.arange(axis_size * 8 * 128).reshape((axis_size * 8, 128))\n\n    def body(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM), out_shape=x)(x)\n    device_mesh = mesh_utils.create_device_mesh((2, axis_size), jax.devices())\n    mesh = jax.sharding.Mesh(device_mesh, ['y', 'x'])\n    y = jax.jit(shard_map.shard_map(body, mesh, in_specs=P('x', None), out_specs=P('x', None), check_rep=False))(x)\n    if direction == 'right':\n        expected = jnp.concatenate([x[-8:], x[:-8]])\n    else:\n        expected = jnp.concatenate([x[8:], x[:8]])\n    np.testing.assert_allclose(y, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_distributed_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_barrier_semaphore(self):\n\n    def kernel(x_ref, y_ref):\n\n        def body(ready_sem, send_sem, recv_sem):\n            my_id = lax.axis_index('x')\n            num_devices = lax.psum(1, 'x')\n            neighbor = lax.rem(my_id + 1, num_devices)\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=neighbor)\n            pltpu.semaphore_wait(barrier_sem)\n            pltpu.semaphore_signal(ready_sem, device_id=neighbor)\n            pltpu.semaphore_wait(ready_sem)\n            pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=neighbor).wait()\n        pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)\n    num_devices = jax.local_device_count()\n    x = jnp.arange(num_devices * 8 * 128).reshape((num_devices * 8, 128))\n\n    def body(x):\n        return pl.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM), out_shape=x, compiler_params=dict(mosaic=dict(collective_id=0)))(x)\n    device_mesh = mesh_utils.create_device_mesh((jax.device_count(),), jax.devices())\n    mesh = jax.sharding.Mesh(device_mesh, ['x'])\n    y = jax.jit(shard_map.shard_map(body, mesh, in_specs=P('x'), out_specs=P('x'), check_rep=False))(x)\n    expected = jnp.concatenate([x[-8:], x[:-8]])\n    np.testing.assert_allclose(y, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_distributed_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_interpret_remote_dma_asymmetrical_indexer(self):\n    if jax.local_device_count() <= 1:\n        self.skipTest('Test requires multiple devices.')\n    if not jtu.is_device_tpu(5, 'e'):\n        self.skipTest('Only works with TPU v5e.')\n    num_devices = jax.local_device_count()\n\n    def test_kernel(x_ref, output_ref, send_sem, recv_sem):\n        output_ref[...] = jnp.zeros_like(output_ref[...])\n        my_id = lax.axis_index('x')\n        even_device = lax.rem(my_id, 2)\n        odd_device = 1 - even_device\n        neighbor = lax.rem(my_id + 1, num_devices)\n\n        @pl.when(even_device)\n        def _():\n            remote_dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=output_ref.at[1], send_sem=send_sem, recv_sem=recv_sem, device_id=neighbor)\n            remote_dma.start()\n            remote_dma.wait()\n\n        @pl.when(odd_device)\n        def _():\n            remote_dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=output_ref.at[0], send_sem=send_sem, recv_sem=recv_sem, device_id=neighbor)\n            remote_dma.start()\n            remote_dma.wait()\n    out_shape = jax.ShapeDtypeStruct((2, 8, 128), jnp.float32)\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM), scratch_shapes=[pltpu.SemaphoreType.DMA] * 2)\n    devices = mesh_utils.create_device_mesh((num_devices,))\n    mesh = jax.sharding.Mesh(devices, P('x'))\n    sharding = jax.sharding.NamedSharding(mesh, P(None, 'x'))\n    unsharded_arr = jax.random.normal(jax.random.key(0), shape=(8, 128 * num_devices))\n    sharded_arr = jax.device_put(unsharded_arr, sharding)\n    kernel = pl.pallas_call(test_kernel, out_shape=out_shape, grid_spec=grid_spec, interpret=True)\n    compiled_func = jax.jit(shard_map.shard_map(kernel, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x'), check_rep=False))\n    result_interpret = compiled_func(sharded_arr)\n    kernel = pl.pallas_call(test_kernel, out_shape=out_shape, grid_spec=grid_spec)\n    compiled_func = jax.jit(shard_map.shard_map(kernel, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x'), check_rep=False))\n    result_noninterpret = compiled_func(sharded_arr)\n    np.testing.assert_allclose(result_interpret, result_noninterpret, atol=1e-05, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_distributed_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_interpret_remote_dma_asymmetrical_refs(self):\n    self.skipTest('Known failure.')\n    num_devices = jax.local_device_count()\n\n    def test_kernel(x_ref, even_output, odd_output, send_sem, recv_sem):\n        even_output[...] = jnp.zeros_like(even_output[...])\n        odd_output[...] = jnp.zeros_like(odd_output[...])\n        my_id = lax.axis_index('x')\n        even_device = lax.rem(my_id, 2)\n        odd_device = 1 - even_device\n        neighbor = lax.rem(my_id + 1, num_devices)\n\n        @pl.when(even_device)\n        def _():\n            remote_dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=even_output, send_sem=send_sem, recv_sem=recv_sem, device_id=neighbor, device_id_type=pltpu.DeviceIdType.LOGICAL)\n            remote_dma.start()\n            remote_dma.wait()\n\n        @pl.when(odd_device)\n        def _():\n            remote_dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=odd_output, send_sem=send_sem, recv_sem=recv_sem, device_id=neighbor, device_id_type=pltpu.DeviceIdType.LOGICAL)\n            remote_dma.start()\n            remote_dma.wait()\n    out_shape = jax.ShapeDtypeStruct((8, 128), jnp.float32)\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM)], out_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM)], scratch_shapes=[pltpu.SemaphoreType.DMA] * 2)\n    devices = mesh_utils.create_device_mesh((1, num_devices))\n    mesh = jax.sharding.Mesh(devices, P(None, 'x'))\n    sharding = jax.sharding.NamedSharding(mesh, P(None, 'x'))\n    unsharded_arr = jax.random.normal(jax.random.key(0), shape=(8, 128 * num_devices))\n    sharded_arr = jax.device_put(unsharded_arr, sharding)\n    kernel = pl.pallas_call(test_kernel, out_shape=(out_shape, out_shape), grid_spec=grid_spec, interpret=True)\n    compiled_func = jax.jit(shard_map.shard_map(kernel, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x'), check_rep=False))\n    result_interpret = compiled_func(sharded_arr)\n    kernel = pl.pallas_call(test_kernel, out_shape=(out_shape, out_shape), grid_spec=grid_spec)\n    compiled_func = jax.jit(shard_map.shard_map(kernel, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x'), check_rep=False))\n    result_noninterpret = compiled_func(sharded_arr)\n    np.testing.assert_allclose(result_interpret, result_noninterpret, atol=1e-05, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_distributed_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(('left',), ('right',))\ndef test_interpret_remote_dma_ppermute(self, permutation):\n    if jax.device_count() <= 1:\n        self.skipTest('Test requires multiple devices.')\n    num_devices = jax.device_count()\n    if permutation == 'left':\n        permute_fn = lambda x: lax.rem(x + num_devices - 1, num_devices)\n    else:\n        permute_fn = lambda x: lax.rem(x + num_devices + 1, num_devices)\n\n    def test_kernel(x_ref, o_ref, copy_send_sem, copy_recv_sem):\n        o_ref[...] = jnp.zeros_like(o_ref[...])\n        my_id = lax.axis_index('x')\n        dst_device = permute_fn(my_id)\n        input_to_output_copy = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=copy_send_sem, recv_sem=copy_recv_sem, device_id=dst_device, device_id_type=pltpu.DeviceIdType.LOGICAL)\n        input_to_output_copy.start()\n        input_to_output_copy.wait()\n    out_shape = jax.ShapeDtypeStruct((8, 128), jnp.float32)\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], scratch_shapes=[pltpu.SemaphoreType.DMA] * 2)\n    devices = mesh_utils.create_device_mesh((num_devices,))\n    mesh = jax.sharding.Mesh(devices, 'x')\n    sharding = jax.sharding.NamedSharding(mesh, P(None, 'x'))\n    unsharded_arr = jax.random.normal(jax.random.key(0), shape=(8, 128 * num_devices))\n    sharded_arr = jax.device_put(unsharded_arr, sharding)\n    kernel = pl.pallas_call(test_kernel, out_shape=out_shape, grid_spec=grid_spec, interpret=True)\n    compiled_func = jax.jit(shard_map.shard_map(kernel, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x'), check_rep=False))\n    result = compiled_func(sharded_arr)\n    perm = tuple(((src, permute_fn(src)) for src in range(num_devices)))\n    perm = jax.tree_util.tree_map(int, perm)\n\n    def lax_permute(x):\n        return lax.ppermute(x, 'x', perm)\n    expected = jax.jit(shard_map.shard_map(lax_permute, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x')))(sharded_arr)\n    np.testing.assert_array_equal(result, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_distributed_test.py",
    "function": "def make_async_remote_copy(axis_name: str, direction: str='right', target_memory_space=None):\n    if target_memory_space is None:\n        target_memory_space = pltpu.ANY\n\n    @jax.named_call\n    def copy_start(x: jax.Array) -> tuple[jax.Array, Future]:\n\n        def copy_start_kernel(x_ref, aliased_x_ref, o_ref, send_sem, recv_sem):\n            del aliased_x_ref\n            axis_size = jax.lax.psum(1, axis_name)\n            left_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) - 1 + axis_size, axis_size)\n            right_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) + 1, axis_size)\n            if direction == 'right':\n                src_neighbor = left_neighbor\n                dst_neighbor = right_neighbor\n            else:\n                src_neighbor = right_neighbor\n                dst_neighbor = left_neighbor\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=src_neighbor, core_index=0)\n            pltpu.semaphore_wait(barrier_sem, 1)\n            pltpu.make_async_remote_copy(x_ref, o_ref, send_sem, recv_sem, device_id=dst_neighbor).start()\n        x, out, send_sem, recv_sem = pl.pallas_call(copy_start_kernel, out_shape=(jax.ShapeDtypeStruct(x.shape, x.dtype), target_memory_space(x.shape, x.dtype), pltpu.SemaphoreType.DMA(()), pltpu.SemaphoreType.DMA(())), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=(pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)), input_output_aliases={0: 0}, compiler_params=pltpu.TPUCompilerParams(collective_id=0, has_side_effects=True))(x)\n        return (x, (out, send_sem, recv_sem))\n\n    @jax.named_call\n    def send_done(x: jax.Array, future: Future) -> jax.Array:\n        _, send_sem, _ = future\n\n        def send_done_kernel(x_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, x_ref, send_sem).wait()\n        x = pl.pallas_call(send_done_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), input_output_aliases={0: 0})(x, send_sem)\n        return x\n\n    @jax.named_call\n    def recv_done(x: jax.Array, future: Future) -> jax.Array:\n        out, _, recv_sem = future\n\n        def send_done_kernel(x_ref, o_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, o_ref, send_sem).wait()\n        out = pl.pallas_call(send_done_kernel, out_shape=target_memory_space(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=target_memory_space), input_output_aliases={1: 0})(x, out, recv_sem)\n        return out\n    return (copy_start, send_done, recv_done)"
  },
  {
    "test_code": "def test_interpret_remote_dma_asymmetrical_indexer(self):\n    if jax.local_device_count() <= 1:\n        self.skipTest('Test requires multiple devices.')\n    if not jtu.is_device_tpu(5, 'e'):\n        self.skipTest('Only works with TPU v5e.')\n    num_devices = jax.local_device_count()\n\n    def test_kernel(x_ref, output_ref, send_sem, recv_sem):\n        output_ref[...] = jnp.zeros_like(output_ref[...])\n        my_id = lax.axis_index('x')\n        even_device = lax.rem(my_id, 2)\n        odd_device = 1 - even_device\n        neighbor = lax.rem(my_id + 1, num_devices)\n\n        @pl.when(even_device)\n        def _():\n            remote_dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=output_ref.at[1], send_sem=send_sem, recv_sem=recv_sem, device_id=neighbor)\n            remote_dma.start()\n            remote_dma.wait()\n\n        @pl.when(odd_device)\n        def _():\n            remote_dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=output_ref.at[0], send_sem=send_sem, recv_sem=recv_sem, device_id=neighbor)\n            remote_dma.start()\n            remote_dma.wait()\n    out_shape = jax.ShapeDtypeStruct((2, 8, 128), jnp.float32)\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM), scratch_shapes=[pltpu.SemaphoreType.DMA] * 2)\n    devices = mesh_utils.create_device_mesh((num_devices,))\n    mesh = jax.sharding.Mesh(devices, P('x'))\n    sharding = jax.sharding.NamedSharding(mesh, P(None, 'x'))\n    unsharded_arr = jax.random.normal(jax.random.key(0), shape=(8, 128 * num_devices))\n    sharded_arr = jax.device_put(unsharded_arr, sharding)\n    kernel = pl.pallas_call(test_kernel, out_shape=out_shape, grid_spec=grid_spec, interpret=True)\n    compiled_func = jax.jit(shard_map.shard_map(kernel, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x'), check_rep=False))\n    result_interpret = compiled_func(sharded_arr)\n    kernel = pl.pallas_call(test_kernel, out_shape=out_shape, grid_spec=grid_spec)\n    compiled_func = jax.jit(shard_map.shard_map(kernel, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x'), check_rep=False))\n    result_noninterpret = compiled_func(sharded_arr)\n    np.testing.assert_allclose(result_interpret, result_noninterpret, atol=1e-05, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_distributed_test.py",
    "function": "def make_async_remote_copy(axis_name: str, direction: str='right', target_memory_space=None):\n    if target_memory_space is None:\n        target_memory_space = pltpu.ANY\n\n    @jax.named_call\n    def copy_start(x: jax.Array) -> tuple[jax.Array, Future]:\n\n        def copy_start_kernel(x_ref, aliased_x_ref, o_ref, send_sem, recv_sem):\n            del aliased_x_ref\n            axis_size = jax.lax.psum(1, axis_name)\n            left_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) - 1 + axis_size, axis_size)\n            right_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) + 1, axis_size)\n            if direction == 'right':\n                src_neighbor = left_neighbor\n                dst_neighbor = right_neighbor\n            else:\n                src_neighbor = right_neighbor\n                dst_neighbor = left_neighbor\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=src_neighbor, core_index=0)\n            pltpu.semaphore_wait(barrier_sem, 1)\n            pltpu.make_async_remote_copy(x_ref, o_ref, send_sem, recv_sem, device_id=dst_neighbor).start()\n        x, out, send_sem, recv_sem = pl.pallas_call(copy_start_kernel, out_shape=(jax.ShapeDtypeStruct(x.shape, x.dtype), target_memory_space(x.shape, x.dtype), pltpu.SemaphoreType.DMA(()), pltpu.SemaphoreType.DMA(())), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=(pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)), input_output_aliases={0: 0}, compiler_params=pltpu.TPUCompilerParams(collective_id=0, has_side_effects=True))(x)\n        return (x, (out, send_sem, recv_sem))\n\n    @jax.named_call\n    def send_done(x: jax.Array, future: Future) -> jax.Array:\n        _, send_sem, _ = future\n\n        def send_done_kernel(x_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, x_ref, send_sem).wait()\n        x = pl.pallas_call(send_done_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), input_output_aliases={0: 0})(x, send_sem)\n        return x\n\n    @jax.named_call\n    def recv_done(x: jax.Array, future: Future) -> jax.Array:\n        out, _, recv_sem = future\n\n        def send_done_kernel(x_ref, o_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, o_ref, send_sem).wait()\n        out = pl.pallas_call(send_done_kernel, out_shape=target_memory_space(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=target_memory_space), input_output_aliases={1: 0})(x, out, recv_sem)\n        return out\n    return (copy_start, send_done, recv_done)"
  },
  {
    "test_code": "def test_interpret_remote_dma_asymmetrical_refs(self):\n    self.skipTest('Known failure.')\n    num_devices = jax.local_device_count()\n\n    def test_kernel(x_ref, even_output, odd_output, send_sem, recv_sem):\n        even_output[...] = jnp.zeros_like(even_output[...])\n        odd_output[...] = jnp.zeros_like(odd_output[...])\n        my_id = lax.axis_index('x')\n        even_device = lax.rem(my_id, 2)\n        odd_device = 1 - even_device\n        neighbor = lax.rem(my_id + 1, num_devices)\n\n        @pl.when(even_device)\n        def _():\n            remote_dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=even_output, send_sem=send_sem, recv_sem=recv_sem, device_id=neighbor, device_id_type=pltpu.DeviceIdType.LOGICAL)\n            remote_dma.start()\n            remote_dma.wait()\n\n        @pl.when(odd_device)\n        def _():\n            remote_dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=odd_output, send_sem=send_sem, recv_sem=recv_sem, device_id=neighbor, device_id_type=pltpu.DeviceIdType.LOGICAL)\n            remote_dma.start()\n            remote_dma.wait()\n    out_shape = jax.ShapeDtypeStruct((8, 128), jnp.float32)\n    grid_spec = pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM)], out_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM)], scratch_shapes=[pltpu.SemaphoreType.DMA] * 2)\n    devices = mesh_utils.create_device_mesh((1, num_devices))\n    mesh = jax.sharding.Mesh(devices, P(None, 'x'))\n    sharding = jax.sharding.NamedSharding(mesh, P(None, 'x'))\n    unsharded_arr = jax.random.normal(jax.random.key(0), shape=(8, 128 * num_devices))\n    sharded_arr = jax.device_put(unsharded_arr, sharding)\n    kernel = pl.pallas_call(test_kernel, out_shape=(out_shape, out_shape), grid_spec=grid_spec, interpret=True)\n    compiled_func = jax.jit(shard_map.shard_map(kernel, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x'), check_rep=False))\n    result_interpret = compiled_func(sharded_arr)\n    kernel = pl.pallas_call(test_kernel, out_shape=(out_shape, out_shape), grid_spec=grid_spec)\n    compiled_func = jax.jit(shard_map.shard_map(kernel, mesh=mesh, in_specs=P(None, 'x'), out_specs=P(None, 'x'), check_rep=False))\n    result_noninterpret = compiled_func(sharded_arr)\n    np.testing.assert_allclose(result_interpret, result_noninterpret, atol=1e-05, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_pallas_distributed_test.py",
    "function": "def make_async_remote_copy(axis_name: str, direction: str='right', target_memory_space=None):\n    if target_memory_space is None:\n        target_memory_space = pltpu.ANY\n\n    @jax.named_call\n    def copy_start(x: jax.Array) -> tuple[jax.Array, Future]:\n\n        def copy_start_kernel(x_ref, aliased_x_ref, o_ref, send_sem, recv_sem):\n            del aliased_x_ref\n            axis_size = jax.lax.psum(1, axis_name)\n            left_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) - 1 + axis_size, axis_size)\n            right_neighbor = jax.lax.rem(jax.lax.axis_index(axis_name) + 1, axis_size)\n            if direction == 'right':\n                src_neighbor = left_neighbor\n                dst_neighbor = right_neighbor\n            else:\n                src_neighbor = right_neighbor\n                dst_neighbor = left_neighbor\n            barrier_sem = pltpu.get_barrier_semaphore()\n            pltpu.semaphore_signal(barrier_sem, device_id=src_neighbor, core_index=0)\n            pltpu.semaphore_wait(barrier_sem, 1)\n            pltpu.make_async_remote_copy(x_ref, o_ref, send_sem, recv_sem, device_id=dst_neighbor).start()\n        x, out, send_sem, recv_sem = pl.pallas_call(copy_start_kernel, out_shape=(jax.ShapeDtypeStruct(x.shape, x.dtype), target_memory_space(x.shape, x.dtype), pltpu.SemaphoreType.DMA(()), pltpu.SemaphoreType.DMA(())), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY)], out_specs=(pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)), input_output_aliases={0: 0}, compiler_params=pltpu.TPUCompilerParams(collective_id=0, has_side_effects=True))(x)\n        return (x, (out, send_sem, recv_sem))\n\n    @jax.named_call\n    def send_done(x: jax.Array, future: Future) -> jax.Array:\n        _, send_sem, _ = future\n\n        def send_done_kernel(x_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, x_ref, send_sem).wait()\n        x = pl.pallas_call(send_done_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=pltpu.ANY), input_output_aliases={0: 0})(x, send_sem)\n        return x\n\n    @jax.named_call\n    def recv_done(x: jax.Array, future: Future) -> jax.Array:\n        out, _, recv_sem = future\n\n        def send_done_kernel(x_ref, o_ref, send_sem, aliased_o_ref):\n            del aliased_o_ref\n            pltpu.make_async_copy(x_ref, o_ref, send_sem).wait()\n        out = pl.pallas_call(send_done_kernel, out_shape=target_memory_space(x.shape, x.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.ANY), pl.BlockSpec(memory_space=target_memory_space), pl.BlockSpec(memory_space=pltpu.SEMAPHORE)], out_specs=pl.BlockSpec(memory_space=target_memory_space), input_output_aliases={1: 0})(x, out, recv_sem)\n        return out\n    return (copy_start, send_done, recv_done)"
  }
]