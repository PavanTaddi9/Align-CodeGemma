[
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\n@jtu.run_on_devices('cuda', 'rocm')\n@jax.default_matmul_precision('float32')\ndef test_lstm(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    if bidirectional and jtu.is_device_rocm():\n        self.skipTest('Bidirectional mode is not available for ROCm.')\n    num_directions = 2 if bidirectional else 1\n    seq_length_key, root_key = jax.random.split(jax.random.PRNGKey(0))\n    seq_lengths = jax.random.randint(seq_length_key, (batch_size,), 1, seq_len, dtype=jnp.int32)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    def f(weights, x, h_0, c_0):\n        if jtu.is_device_rocm():\n            weights = rnn.swap_lstm_gates(weights, input_size, hidden_size, num_layers, bidirectional)\n        y, h, c = rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        seq_length_mask = jnp.tile(jnp.arange(seq_len, dtype=jnp.int32)[None], [batch_size, 1]) < seq_lengths[:, None]\n        loss = jnp.sum(jnp.where(seq_length_mask[..., None], y, 0.0))\n        return (loss, (y, h, c))\n    jtu.check_grads(f, (weights, x, h_0, c_0), modes=['rev'], order=1, atol=0.005, rtol=0.005)\n    (loss, (y, h_n, c_n)), weights_grad = jax.value_and_grad(f, has_aux=True)(weights, x, h_0, c_0)\n\n    def g(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        seq_length_mask = jnp.tile(jnp.arange(seq_len, dtype=jnp.int32)[None], [batch_size, 1]) < seq_lengths[:, None]\n        loss = jnp.sum(jnp.where(seq_length_mask[..., None], y_ref, 0.0))\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), weights_grad_ref = jax.value_and_grad(g, has_aux=True)(weights, x, h_0, c_0)\n    self.assertAllClose(weights_grad_ref, weights_grad, rtol=1e-05, atol=1e-05)\n    np.testing.assert_allclose(loss_ref, loss, rtol=1e-05, atol=1e-05)\n    np.testing.assert_allclose(y_ref, y, rtol=1e-05, atol=1e-05)\n    np.testing.assert_allclose(h_n_ref, h_n, rtol=1e-05, atol=1e-05)\n    np.testing.assert_allclose(c_n_ref, c_n, rtol=1e-05, atol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.run_on_devices('cuda')\ndef test_struct_encoding_determinism(self):\n\n    def f(k1, k2, k3, k4):\n        batch_size = 1\n        seq_len = 1\n        input_size = 1\n        hidden_size = 1\n        bidirectional = False\n        num_directions = 2 if bidirectional else 1\n        num_layers = 1\n        x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n        h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n        c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n        seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n        weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n        return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n    k = jax.random.split(jax.random.PRNGKey(1), 4)\n    stablehlo = jax.jit(f).lower(*k).as_text('stablehlo')\n    self.assertIn('\"\\\\01\\\\00\\\\00\\\\00\\\\01\\\\00\\\\00\\\\00\\\\01\\\\00\\\\00\\\\00\\\\01\\\\00\\\\00\\\\00\\\\01\\\\00\\\\00\\\\00\\\\00\\\\00\\\\00\\\\00\\\\00\\\\00\\\\00\\\\00\\\\01\\\\00\\\\00\\\\00@\\\\03\\\\80\\\\00@\\\\01\\\\00\\\\00\"', stablehlo)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\n@jtu.run_on_devices('cuda', 'rocm')\n@jax.default_matmul_precision('float32')\ndef test_lstm(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    if bidirectional and jtu.is_device_rocm():\n        self.skipTest('Bidirectional mode is not available for ROCm.')\n    num_directions = 2 if bidirectional else 1\n    seq_length_key, root_key = jax.random.split(jax.random.PRNGKey(0))\n    seq_lengths = jax.random.randint(seq_length_key, (batch_size,), 1, seq_len, dtype=jnp.int32)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    def f(weights, x, h_0, c_0):\n        if jtu.is_device_rocm():\n            weights = rnn.swap_lstm_gates(weights, input_size, hidden_size, num_layers, bidirectional)\n        y, h, c = rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        seq_length_mask = jnp.tile(jnp.arange(seq_len, dtype=jnp.int32)[None], [batch_size, 1]) < seq_lengths[:, None]\n        loss = jnp.sum(jnp.where(seq_length_mask[..., None], y, 0.0))\n        return (loss, (y, h, c))\n    jtu.check_grads(f, (weights, x, h_0, c_0), modes=['rev'], order=1, atol=0.005, rtol=0.005)\n    (loss, (y, h_n, c_n)), weights_grad = jax.value_and_grad(f, has_aux=True)(weights, x, h_0, c_0)\n\n    def g(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        seq_length_mask = jnp.tile(jnp.arange(seq_len, dtype=jnp.int32)[None], [batch_size, 1]) < seq_lengths[:, None]\n        loss = jnp.sum(jnp.where(seq_length_mask[..., None], y_ref, 0.0))\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), weights_grad_ref = jax.value_and_grad(g, has_aux=True)(weights, x, h_0, c_0)\n    self.assertAllClose(weights_grad_ref, weights_grad, rtol=1e-05, atol=1e-05)\n    np.testing.assert_allclose(loss_ref, loss, rtol=1e-05, atol=1e-05)\n    np.testing.assert_allclose(y_ref, y, rtol=1e-05, atol=1e-05)\n    np.testing.assert_allclose(h_n_ref, h_n, rtol=1e-05, atol=1e-05)\n    np.testing.assert_allclose(c_n_ref, c_n, rtol=1e-05, atol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def check_grads(f, args, order, atol=None, rtol=None, eps=None):\n    default_tol = 1e-06 if config.enable_x64.value else 0.01\n    atol = atol or default_tol\n    rtol = rtol or default_tol\n    eps = eps or default_tol\n    jtu.check_jvp(f, partial(jax.jvp, f), args, atol, rtol, eps)\n    jtu.check_vjp(f, partial(jax.vjp, f), args, atol, rtol, eps)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@jtu.run_on_devices('cuda')\ndef test_struct_encoding_determinism(self):\n\n    def f(k1, k2, k3, k4):\n        batch_size = 1\n        seq_len = 1\n        input_size = 1\n        hidden_size = 1\n        bidirectional = False\n        num_directions = 2 if bidirectional else 1\n        num_layers = 1\n        x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n        h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n        c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n        seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n        weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n        return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n    k = jax.random.split(jax.random.PRNGKey(1), 4)\n    stablehlo = jax.jit(f).lower(*k).as_text('stablehlo')\n    self.assertIn('\"\\\\01\\\\00\\\\00\\\\00\\\\01\\\\00\\\\00\\\\00\\\\01\\\\00\\\\00\\\\00\\\\01\\\\00\\\\00\\\\00\\\\01\\\\00\\\\00\\\\00\\\\00\\\\00\\\\00\\\\00\\\\00\\\\00\\\\00\\\\00\\\\01\\\\00\\\\00\\\\00@\\\\03\\\\80\\\\00@\\\\01\\\\00\\\\00\"', stablehlo)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 4], seq_len=[1, 4], input_size=[1, 2], hidden_size=[1, 6], num_layers=[1, 4], bidirectional=[True, False])\ndef test_lstm_ref(self, batch_size: int, seq_len: int, input_size: int, hidden_size: int, num_layers: int, bidirectional: bool):\n    num_directions = 2 if bidirectional else 1\n    seq_lengths = jax.random.randint(jax.random.PRNGKey(0), (batch_size,), 0, seq_len, dtype=jnp.int32)\n    root_key = jax.random.PRNGKey(1)\n    k1, k2, k3, k4 = jax.random.split(root_key, 4)\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n\n    @partial(jax.value_and_grad, has_aux=True)\n    def f(weights, x, h_0, c_0):\n        W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n        y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n        loss = jnp.sum(y_ref)\n        return (loss, (y_ref, h_n_ref, c_n_ref))\n    (loss_ref, (y_ref, h_n_ref, c_n_ref)), grad_ref = f(weights, x, h_0, c_0)\n    self.assertFalse(np.isnan(loss_ref))\n    self.assertFalse(np.isnan(grad_ref).any())\n    self.assertEqual(y_ref.shape, (batch_size, seq_len, num_directions * hidden_size))\n    for i in range(batch_size):\n        y_padded = y_ref[i, seq_lengths[i]:]\n        np.testing.assert_allclose(y_padded, jnp.zeros_like(y_padded))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/experimental_rnn_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  }
]