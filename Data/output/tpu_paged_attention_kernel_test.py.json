[
  {
    "test_code": "@parameterized.product(dtype=(jnp.float32, jnp.bfloat16), page_size=(16, 32, 64), num_kv_heads=(1, 8), q_kv_head_ratio=(1, 4, 8), head_dim=(128, 256), megacore_mode=('batch', 'kv_head', None), attn_logits_soft_cap=(1.0, None), are_kv_quantized=(False, True))\ndef test_paged_attention(self, dtype, page_size, num_kv_heads, q_kv_head_ratio, head_dim, megacore_mode, attn_logits_soft_cap, are_kv_quantized):\n    if not jtu.is_device_tpu_at_least(4):\n        self.skipTest('Only supports TPU generation 4 or above')\n    if jtu.is_device_tpu(version=4) and are_kv_quantized:\n        self.skipTest('Quantization is not supported on TPU v4')\n    if jtu.is_device_tpu_at_least(6) and are_kv_quantized:\n        self.skipTest('Quantization is not supported on TPU v6')\n    if megacore_mode and (not _megacore_enabled()):\n        self.skipTest('Megacore is only available on TPU v4 or TPU v5p')\n    if num_kv_heads % 2 != 0 and megacore_mode == 'kv_head':\n        self.skipTest('Skip kv_head megacore mode when num_kv_heads is odd')\n    max_kv_len = 2048\n    block_size = 512\n    seq_lens = np.asarray([0, 3, 256, 513, 1023, 2048])\n    q, k_pages, v_pages, page_indices = _generate_qkv(seq_lens, page_size, max_kv_len, num_kv_heads, num_kv_heads * q_kv_head_ratio, head_dim, jax.random.key(0), dtype, are_kv_quantized=are_kv_quantized)\n    o = paged_attention.paged_attention(q, k_pages, v_pages, seq_lens, page_indices, pages_per_compute_block=block_size // page_size, megacore_mode=megacore_mode, attn_logits_soft_cap=attn_logits_soft_cap)\n    k = _reconstruct_kv(page_indices, k_pages)\n    v = _reconstruct_kv(page_indices, v_pages)\n    o_ref = _grouped_query_attention_reference(q, k, v, seq_lens, attn_logits_soft_cap)\n    if q_kv_head_ratio > 1:\n        atol, rtol = (0.01, 0.02)\n    else:\n        atol, rtol = (0.2, 0.1)\n    np.testing.assert_allclose(o[np.where(seq_lens > 0)].astype(jnp.float32), o_ref[np.where(seq_lens > 0)].astype(jnp.float32), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_paged_attention_kernel_test.py",
    "function": "def _generate_qkv(seq_lens, page_size, max_seq_len, num_kv_heads, num_heads, head_dim, prng_key, dtype=jnp.float32, are_kv_quantized=False):\n    assert max_seq_len % page_size == 0\n    pages_per_sequence = max_seq_len // page_size\n    batch_size = len(seq_lens)\n    total_pages = batch_size * pages_per_sequence\n    k1, k2, k3, k4 = jax.random.split(prng_key, 4)\n    k_pages = jax.random.normal(k1, (num_kv_heads, total_pages, page_size, head_dim), dtype=dtype)\n    v_pages = jax.random.normal(k2, (num_kv_heads, total_pages, page_size, head_dim), dtype=dtype)\n    if are_kv_quantized:\n        k_pages = quantization_utils.quantize_to_int8(k_pages)\n        v_pages = quantization_utils.quantize_to_int8(v_pages)\n    page_indices = jnp.arange(batch_size * pages_per_sequence, dtype=jnp.int32)\n    page_indices = jax.random.permutation(k3, page_indices, independent=True)\n    page_indices = page_indices.reshape(batch_size, pages_per_sequence)\n    q = jax.random.normal(k4, (batch_size, num_heads, head_dim), dtype=dtype)\n    return (q, k_pages, v_pages, page_indices)"
  },
  {
    "test_code": "@parameterized.product(dtype=(jnp.float32, jnp.bfloat16), page_size=(16, 32, 64), num_kv_heads=(1, 8), q_kv_head_ratio=(1, 4, 8), head_dim=(128, 256), megacore_mode=('batch', 'kv_head', None), attn_logits_soft_cap=(1.0, None), are_kv_quantized=(False, True))\ndef test_paged_attention(self, dtype, page_size, num_kv_heads, q_kv_head_ratio, head_dim, megacore_mode, attn_logits_soft_cap, are_kv_quantized):\n    if not jtu.is_device_tpu_at_least(4):\n        self.skipTest('Only supports TPU generation 4 or above')\n    if jtu.is_device_tpu(version=4) and are_kv_quantized:\n        self.skipTest('Quantization is not supported on TPU v4')\n    if jtu.is_device_tpu_at_least(6) and are_kv_quantized:\n        self.skipTest('Quantization is not supported on TPU v6')\n    if megacore_mode and (not _megacore_enabled()):\n        self.skipTest('Megacore is only available on TPU v4 or TPU v5p')\n    if num_kv_heads % 2 != 0 and megacore_mode == 'kv_head':\n        self.skipTest('Skip kv_head megacore mode when num_kv_heads is odd')\n    max_kv_len = 2048\n    block_size = 512\n    seq_lens = np.asarray([0, 3, 256, 513, 1023, 2048])\n    q, k_pages, v_pages, page_indices = _generate_qkv(seq_lens, page_size, max_kv_len, num_kv_heads, num_kv_heads * q_kv_head_ratio, head_dim, jax.random.key(0), dtype, are_kv_quantized=are_kv_quantized)\n    o = paged_attention.paged_attention(q, k_pages, v_pages, seq_lens, page_indices, pages_per_compute_block=block_size // page_size, megacore_mode=megacore_mode, attn_logits_soft_cap=attn_logits_soft_cap)\n    k = _reconstruct_kv(page_indices, k_pages)\n    v = _reconstruct_kv(page_indices, v_pages)\n    o_ref = _grouped_query_attention_reference(q, k, v, seq_lens, attn_logits_soft_cap)\n    if q_kv_head_ratio > 1:\n        atol, rtol = (0.01, 0.02)\n    else:\n        atol, rtol = (0.2, 0.1)\n    np.testing.assert_allclose(o[np.where(seq_lens > 0)].astype(jnp.float32), o_ref[np.where(seq_lens > 0)].astype(jnp.float32), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_paged_attention_kernel_test.py",
    "function": "def _reconstruct_kv(page_indices, pages):\n    if isinstance(pages, quantization_utils.QuantizedTensor):\n        pages = quantization_utils.unquantize_from_int8(pages, dtype=jnp.float32)\n    batch_size = page_indices.shape[0]\n    num_heads, _, _, head_dim = pages.shape\n\n    def per_sequence_page_gather(pages, page_indices):\n        return jnp.take(pages, page_indices, 1)\n    gathered = jax.vmap(per_sequence_page_gather, in_axes=(None, 0))(pages, page_indices)\n    return gathered.reshape(batch_size, num_heads, -1, head_dim)"
  },
  {
    "test_code": "@parameterized.product(dtype=(jnp.float32, jnp.bfloat16), page_size=(16, 32, 64), num_kv_heads=(1, 8), q_kv_head_ratio=(1, 4, 8), head_dim=(128, 256), megacore_mode=('batch', 'kv_head', None), attn_logits_soft_cap=(1.0, None), are_kv_quantized=(False, True))\ndef test_paged_attention(self, dtype, page_size, num_kv_heads, q_kv_head_ratio, head_dim, megacore_mode, attn_logits_soft_cap, are_kv_quantized):\n    if not jtu.is_device_tpu_at_least(4):\n        self.skipTest('Only supports TPU generation 4 or above')\n    if jtu.is_device_tpu(version=4) and are_kv_quantized:\n        self.skipTest('Quantization is not supported on TPU v4')\n    if jtu.is_device_tpu_at_least(6) and are_kv_quantized:\n        self.skipTest('Quantization is not supported on TPU v6')\n    if megacore_mode and (not _megacore_enabled()):\n        self.skipTest('Megacore is only available on TPU v4 or TPU v5p')\n    if num_kv_heads % 2 != 0 and megacore_mode == 'kv_head':\n        self.skipTest('Skip kv_head megacore mode when num_kv_heads is odd')\n    max_kv_len = 2048\n    block_size = 512\n    seq_lens = np.asarray([0, 3, 256, 513, 1023, 2048])\n    q, k_pages, v_pages, page_indices = _generate_qkv(seq_lens, page_size, max_kv_len, num_kv_heads, num_kv_heads * q_kv_head_ratio, head_dim, jax.random.key(0), dtype, are_kv_quantized=are_kv_quantized)\n    o = paged_attention.paged_attention(q, k_pages, v_pages, seq_lens, page_indices, pages_per_compute_block=block_size // page_size, megacore_mode=megacore_mode, attn_logits_soft_cap=attn_logits_soft_cap)\n    k = _reconstruct_kv(page_indices, k_pages)\n    v = _reconstruct_kv(page_indices, v_pages)\n    o_ref = _grouped_query_attention_reference(q, k, v, seq_lens, attn_logits_soft_cap)\n    if q_kv_head_ratio > 1:\n        atol, rtol = (0.01, 0.02)\n    else:\n        atol, rtol = (0.2, 0.1)\n    np.testing.assert_allclose(o[np.where(seq_lens > 0)].astype(jnp.float32), o_ref[np.where(seq_lens > 0)].astype(jnp.float32), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_paged_attention_kernel_test.py",
    "function": "def _grouped_query_attention_reference(q, k, v, lengths, attn_logits_soft_cap):\n    batch_size, num_heads, head_dim = q.shape\n    _, num_kv_heads, max_seq_len, _ = k.shape\n    assert k.shape == v.shape\n    assert num_heads % num_kv_heads == 0\n    q = q.reshape(batch_size, num_kv_heads, num_heads // num_kv_heads, head_dim)\n    if isinstance(k, quantization_utils.QuantizedTensor):\n        k = quantization_utils.unquantize_from_int8(k, dtype=jnp.float32)\n    if isinstance(v, quantization_utils.QuantizedTensor):\n        v = quantization_utils.unquantize_from_int8(v, dtype=jnp.float32)\n    logits = jnp.einsum('bhgd,bhtd->bhgt', q.astype(jnp.float32), k.astype(jnp.float32))\n    if attn_logits_soft_cap is not None:\n        logits = jnp.tanh(logits / attn_logits_soft_cap) * attn_logits_soft_cap\n    mask = jnp.arange(max_seq_len)[None] < lengths[:, None]\n    mask_value = -0.7 * float(np.finfo(np.dtype('float32')).max)\n    logits = logits + jnp.where(mask, 0.0, mask_value)[:, None, None, :]\n    weights = jax.nn.softmax(logits, axis=-1)\n    o = jnp.einsum('bhgt,bhtd->bhgd', weights.astype(v.dtype), v)\n    return o.reshape(batch_size, num_heads, head_dim)"
  },
  {
    "test_code": "@parameterized.product(dtype=(jnp.float32, jnp.bfloat16), page_size=(16, 32, 64), num_kv_heads=(1, 8), q_kv_head_ratio=(1, 4, 8), head_dim=(128, 256), megacore_mode=('batch', 'kv_head', None), attn_logits_soft_cap=(1.0, None), are_kv_quantized=(False, True))\ndef test_paged_attention(self, dtype, page_size, num_kv_heads, q_kv_head_ratio, head_dim, megacore_mode, attn_logits_soft_cap, are_kv_quantized):\n    if not jtu.is_device_tpu_at_least(4):\n        self.skipTest('Only supports TPU generation 4 or above')\n    if jtu.is_device_tpu(version=4) and are_kv_quantized:\n        self.skipTest('Quantization is not supported on TPU v4')\n    if jtu.is_device_tpu_at_least(6) and are_kv_quantized:\n        self.skipTest('Quantization is not supported on TPU v6')\n    if megacore_mode and (not _megacore_enabled()):\n        self.skipTest('Megacore is only available on TPU v4 or TPU v5p')\n    if num_kv_heads % 2 != 0 and megacore_mode == 'kv_head':\n        self.skipTest('Skip kv_head megacore mode when num_kv_heads is odd')\n    max_kv_len = 2048\n    block_size = 512\n    seq_lens = np.asarray([0, 3, 256, 513, 1023, 2048])\n    q, k_pages, v_pages, page_indices = _generate_qkv(seq_lens, page_size, max_kv_len, num_kv_heads, num_kv_heads * q_kv_head_ratio, head_dim, jax.random.key(0), dtype, are_kv_quantized=are_kv_quantized)\n    o = paged_attention.paged_attention(q, k_pages, v_pages, seq_lens, page_indices, pages_per_compute_block=block_size // page_size, megacore_mode=megacore_mode, attn_logits_soft_cap=attn_logits_soft_cap)\n    k = _reconstruct_kv(page_indices, k_pages)\n    v = _reconstruct_kv(page_indices, v_pages)\n    o_ref = _grouped_query_attention_reference(q, k, v, seq_lens, attn_logits_soft_cap)\n    if q_kv_head_ratio > 1:\n        atol, rtol = (0.01, 0.02)\n    else:\n        atol, rtol = (0.2, 0.1)\n    np.testing.assert_allclose(o[np.where(seq_lens > 0)].astype(jnp.float32), o_ref[np.where(seq_lens > 0)].astype(jnp.float32), atol=atol, rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/tpu_paged_attention_kernel_test.py",
    "function": "def _megacore_enabled():\n    return jax.devices()[0].device_kind == 'TPU v4' or jtu.is_device_tpu(version=5, variant='p')"
  }
]