[
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.named_parameters(((name, name, func, strategy) for name, func, strategy in UNARY_FUNCTIONS))\n@hp.given(hps.data())\ndef test_unary_primitives(self, name, func, shape_dtype_strategy, data):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('This hypothesis test is slow, even more so in interpret mode.')\n    tol = 0.0\n    if jtu.test_device_matches(['gpu']):\n        if func == jnp.round or func == jnp.rint:\n            self.skipTest('TODO: not implemented on GPU')\n        if name == 'tanh':\n            tol = 1e-06\n        elif name == 'exp2':\n            tol = 1e-06\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = func(x_ref[...])\n    x_shape_dtype = data.draw(shape_dtype_strategy)\n    key = random.key(0)\n    x = _random_value(key, x_shape_dtype)\n    out = self.pallas_call(kernel, out_shape=x_shape_dtype)(x)\n    self.assertAllClose(out, func(x), atol=tol, rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def func(x):\n    x, = promote_dtypes_complex(x)\n    return jnp.fft.irfft(jnp.concatenate([jnp.zeros_like(x, shape=1), x[:2] + 1j * x[2:]]))"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(('integer_1_1', (1, 1)), ('integer_1_16', (1, 16)), ('integer_16_1', (16, 1)), ('integer_-1_1', (-1, 1)), ('integer_1_-1', (1, -1)), ('float_1_1', (1.0, 1.0)), ('float_1_16', (1.0, 16.0)), ('float_16_1', (16.0, 1.0)), ('float_-1_1', (-1.0, 1.0)), ('float_1_-1', (1.0, -1.0)), ('float_1_inf', (1.0, float('inf'))), ('float_inf_1', (float('inf'), 1.0)), ('float_inf_inf', (float('inf'), float('inf'))), ('float_1_nan', (1.0, float('nan'))), ('float_nan_1', (float('nan'), 1.0)), ('float_nan_nan', (float('nan'), float('nan'))), ('float_inf_nan', (float('inf'), float('nan'))), ('float_nan_inf', (float('inf'), float('inf'))))\ndef test_vector_compare(self, params):\n    \"\"\"Test some vector compares.\n\n    We don't really expect that the results would be wrong, but rather we want\n    to exercise the lowering rules.\n    \"\"\"\n    self.skip_if_mosaic_gpu()\n\n    def kernel(x_ref, y_ref, o_ref):\n        x = x_ref[:]\n        y = y_ref[:]\n        one = jnp.ones([8, 128], dtype=jnp.int32)\n        zero = jnp.zeros([8, 128], dtype=jnp.int32)\n        o_ref[0] = jax.lax.select(x == y, one, zero)\n        o_ref[1] = jax.lax.select(x != y, one, zero)\n        o_ref[2] = jax.lax.select(x < y, one, zero)\n        o_ref[3] = jax.lax.select(x <= y, one, zero)\n        o_ref[4] = jax.lax.select(x > y, one, zero)\n        o_ref[5] = jax.lax.select(x >= y, one, zero)\n    x, y = params\n    x = jnp.full([8, 128], x)\n    y = jnp.full([8, 128], y)\n    r = [x == y, x != y, x < y, x <= y, x > y, x >= y]\n    result = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct([6, 8, 128], jnp.int32), in_specs=[pl.BlockSpec((8, 128), lambda *_: (0, 0)), pl.BlockSpec((8, 128), lambda *_: (0, 0))], out_specs=pl.BlockSpec((6, 8, 128), lambda *_: (0, 0, 0)), grid=(1,))(x, y)\n    np.testing.assert_array_equal(r[0], result[0])\n    np.testing.assert_array_equal(r[1], result[1])\n    np.testing.assert_array_equal(r[2], result[2])\n    np.testing.assert_array_equal(r[3], result[3])\n    np.testing.assert_array_equal(r[4], result[4])\n    np.testing.assert_array_equal(r[5], result[5])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@parameterized.named_parameters(('reduce_all_true', 'all_true', jnp.all, True), ('reduce_all_false', 'all_false', jnp.all, False), ('reduce_all_mixed', 'one_false', jnp.all, False), ('reduce_any_true', 'all_true', jnp.any, True), ('reduce_any_false', 'all_false', jnp.any, False), ('reduce_any_mixed', 'one_false', jnp.any, True))\ndef test_reduce_boolean(self, input_type, reduction_op, expected_result):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('TODO: error on GPU')\n\n    def kernel(x_ref, ones_ref, o_ref):\n        bool_x = x_ref[...] == ones_ref[...]\n        reduced_as_bool = reduction_op(bool_x, keepdims=True)\n        float_value = jnp.where(reduced_as_bool, 1.0, 0.0)\n        o_ref[0, 0] = float_value[0, 0]\n    if input_type == 'all_true':\n        x = jnp.ones((8, 128), dtype=jnp.float32)\n    elif input_type == 'all_false':\n        x = jnp.zeros((8, 128), dtype=jnp.float32)\n    elif input_type == 'one_false':\n        x = jnp.ones((8, 128), dtype=jnp.float32)\n        x = x.at[0, 0].set(0.0)\n    else:\n        raise ValueError(f'Unknown input type: {input_type}')\n    ones = jnp.ones_like(x)\n    result = self.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda *_: (0, 0)), pl.BlockSpec((8, 128), lambda *_: (0, 0))], out_specs=pl.BlockSpec(block_shape=(1, 1), memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct([1, 1], floatx), grid=(1,))(x, ones)\n    np.testing.assert_array_equal(result[0, 0], float(expected_result))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.bfloat16, jnp.int32)\ndef test_add_constant(self, dtype):\n    self.skip_if_mosaic_gpu()\n    shape = (256, 256)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...] + 1\n    np.testing.assert_array_equal(kernel(jnp.zeros(shape, dtype=dtype)), jnp.ones(shape, dtype=dtype))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@parameterized.parameters((0,), (1,))\ndef test_array_atomic_add(self, axis):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Unimplemented primitive: broadcast_to')\n    m, n = (32, 8)\n    if axis == 0:\n        grid = m\n    else:\n        grid = n\n    out_shape = jax.ShapeDtypeStruct((n if axis == 0 else m,), floatx)\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid, input_output_aliases={1: 0})\n    def reduce(x_ref, _, y_ref):\n        i = pl.program_id(axis=0)\n        if axis == 0:\n            idx = (i, jnp.arange(n))\n        else:\n            idx = (jnp.arange(m), i)\n        x = pl.load(x_ref, idx)\n        pl.atomic_add(y_ref, (jnp.arange(y.shape[0]),), x)\n    x = random.normal(random.key(0), (m, n))\n    y = jnp.zeros(out_shape.shape, out_shape.dtype)\n    y = reduce(x, y)\n    y_ref = np.sum(x, axis=axis)\n    np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "@parameterized.product(axis=[0, 1], dtype=['float16', 'float32', 'int32', 'uint32'])\ndef test_cumsum(self, dtype, axis):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    m, n = (32, 8)\n    out_dtype = dtype\n\n    def make_x(key):\n        if jnp.issubdtype(dtype, jnp.integer):\n            return random.permutation(key, jnp.arange(m * n, dtype=dtype), independent=True).reshape(m, n)\n        else:\n            return random.normal(key, (m, n), dtype=dtype)\n    out_shape = jax.ShapeDtypeStruct((m, n), out_dtype)\n    grid = ()\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\n    def reduce(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = jnp.cumsum(x, axis=axis)\n    for i, key in enumerate(random.split(random.key(0), 20)):\n        x = make_x(key)\n        y = reduce(x)\n        y_ref = jnp.cumsum(x, axis=axis)\n        np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01, err_msg=i)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def cumsum(x):\n\n    def body(i, _):\n        return (i + 1, jnp.sum(x[:i + 1]))\n    _, ans = lax.scan(body, 0, None, length=len(x))\n    return ans"
  },
  {
    "test_code": "@parameterized.named_parameters(((name, name, func, strategy) for name, func, strategy in UNARY_FUNCTIONS))\n@hp.given(hps.data())\ndef test_unary_primitives(self, name, func, shape_dtype_strategy, data):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('This hypothesis test is slow, even more so in interpret mode.')\n    tol = 0.0\n    if jtu.test_device_matches(['gpu']):\n        if func == jnp.round or func == jnp.rint:\n            self.skipTest('TODO: not implemented on GPU')\n        if name == 'tanh':\n            tol = 1e-06\n        elif name == 'exp2':\n            tol = 1e-06\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = func(x_ref[...])\n    x_shape_dtype = data.draw(shape_dtype_strategy)\n    key = random.key(0)\n    x = _random_value(key, x_shape_dtype)\n    out = self.pallas_call(kernel, out_shape=x_shape_dtype)(x)\n    self.assertAllClose(out, func(x), atol=tol, rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def func(size):\n    lhs_one_d = jnp.arange(size, dtype='int32') + 1\n    lhs_two_d = jax.lax.broadcast_in_dim(lhs_one_d, (size, 2), (0,))\n    rhs = jax.lax.broadcasted_iota('int32', (2, 4), 0) + 1\n    return jnp.dot(lhs_two_d, rhs)"
  },
  {
    "test_code": "@parameterized.named_parameters(((name, name, func, strategy) for name, func, strategy in UNARY_FUNCTIONS))\n@hp.given(hps.data())\ndef test_unary_primitives(self, name, func, shape_dtype_strategy, data):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('This hypothesis test is slow, even more so in interpret mode.')\n    tol = 0.0\n    if jtu.test_device_matches(['gpu']):\n        if func == jnp.round or func == jnp.rint:\n            self.skipTest('TODO: not implemented on GPU')\n        if name == 'tanh':\n            tol = 1e-06\n        elif name == 'exp2':\n            tol = 1e-06\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = func(x_ref[...])\n    x_shape_dtype = data.draw(shape_dtype_strategy)\n    key = random.key(0)\n    x = _random_value(key, x_shape_dtype)\n    out = self.pallas_call(kernel, out_shape=x_shape_dtype)(x)\n    self.assertAllClose(out, func(x), atol=tol, rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def func(x):\n    return jax.random.uniform(x, (2, 4), dtype=np.float32)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.product(axis=[0, 1], dtype=['float16', 'float32', 'int32', 'uint32'])\ndef test_cumsum(self, dtype, axis):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    m, n = (32, 8)\n    out_dtype = dtype\n\n    def make_x(key):\n        if jnp.issubdtype(dtype, jnp.integer):\n            return random.permutation(key, jnp.arange(m * n, dtype=dtype), independent=True).reshape(m, n)\n        else:\n            return random.normal(key, (m, n), dtype=dtype)\n    out_shape = jax.ShapeDtypeStruct((m, n), out_dtype)\n    grid = ()\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\n    def reduce(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = jnp.cumsum(x, axis=axis)\n    for i, key in enumerate(random.split(random.key(0), 20)):\n        x = make_x(key)\n        y = reduce(x)\n        y_ref = jnp.cumsum(x, axis=axis)\n        np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01, err_msg=i)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def cumsum(x):\n\n    def body(i, refs):\n        x_ref, accum_ref = refs\n        accum_ref[i + 1] = accum_ref[i] + x_ref[i]\n    accum = jnp.zeros(x.shape[0] + 1, x.dtype)\n    _, accum_out = for_impl(x.shape[0], body, (x, accum))\n    return accum_out[1:]"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.named_parameters(((name, name, func, strategy) for name, func, strategy in UNARY_FUNCTIONS))\n@hp.given(hps.data())\ndef test_unary_primitives(self, name, func, shape_dtype_strategy, data):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('This hypothesis test is slow, even more so in interpret mode.')\n    tol = 0.0\n    if jtu.test_device_matches(['gpu']):\n        if func == jnp.round or func == jnp.rint:\n            self.skipTest('TODO: not implemented on GPU')\n        if name == 'tanh':\n            tol = 1e-06\n        elif name == 'exp2':\n            tol = 1e-06\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = func(x_ref[...])\n    x_shape_dtype = data.draw(shape_dtype_strategy)\n    key = random.key(0)\n    x = _random_value(key, x_shape_dtype)\n    out = self.pallas_call(kernel, out_shape=x_shape_dtype)(x)\n    self.assertAllClose(out, func(x), atol=tol, rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def func(xs):\n    return jnp.array(list(xs))"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@parameterized.product(lhs_and_rhs_shape=[((16, 16), (16, 16)), ((32, 32), (32, 32)), ((64, 64), (64, 64)), ((128, 128), (128, 128)), ((256, 256), (256, 256)), ((8, 128), (128, 256)), ((8, 128), (256, 128)), ((8, 256), (256, 128)), ((16, 128), (128, 256)), ((16, 128), (256, 128)), ((16, 256), (256, 128)), ((24, 128), (128, 256)), ((24, 128), (256, 128)), ((24, 256), (256, 128)), ((128, 8), (128, 256)), ((128, 8), (256, 128)), ((256, 8), (256, 128)), ((128, 16), (128, 256)), ((128, 16), (256, 128)), ((256, 16), (256, 128)), ((128, 24), (128, 256)), ((128, 24), (256, 128)), ((256, 24), (256, 128))], dtype=[jnp.float32, jnp.float16, jnp.bfloat16], trans_x=[False, True], trans_y=[False, True])\ndef test_dot(self, lhs_and_rhs_shape, dtype, trans_x, trans_y):\n    self.skip_if_mosaic_gpu()\n    if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n        self.skipTest('Requires libtpu built after 2024-12-19')\n    lhs_shape, rhs_shape = lhs_and_rhs_shape\n    final_lhs_shape = lhs_shape[::-1] if trans_x else lhs_shape\n    final_rhs_shape = rhs_shape[::-1] if trans_y else rhs_shape\n    if final_lhs_shape[1] != final_rhs_shape[0]:\n        self.skipTest('Contraction dimensions do not match')\n    out_shape = (final_lhs_shape[0], final_rhs_shape[1])\n    if jtu.test_device_matches(['tpu']):\n        if dtype == jnp.float16:\n            self.skipTest('float16 type is not supported on TPU')\n        if dtype == jnp.bfloat16 and (not jtu.is_device_tpu_at_least(4)):\n            self.skipTest('bfloat16 matmul is supported on TPUv4+')\n        if trans_x:\n            self.skipTest('Not implemented: Transposed LHS')\n    if jtu.test_device_matches(['gpu']):\n        if dtype == jnp.bfloat16:\n            self.skipTest('bfloat16 type are not supported on GPU')\n        if math.prod(lhs_shape) + math.prod(rhs_shape) + math.prod(out_shape) > 256 * 256 * 2:\n            self.skipTest('Shared memory size limit exceeded')\n        if min(*lhs_shape, *rhs_shape) < 16:\n            self.skipTest('All dimensions of lhs and rhs must be >= 16')\n        if any((not is_power_of_two(x) for x in lhs_shape + rhs_shape)):\n            self.skipTest('All dimensions of lhs and rhs must be power of two')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, dtype))\n    def dot(x_ref, y_ref, o_ref):\n        x = x_ref[:, :]\n        y = y_ref[:, :]\n        o_ref[:, :] = pl.dot(x, y, trans_x, trans_y).astype(o_ref.dtype)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, lhs_shape, dtype=dtype)\n    y = random.normal(k2, rhs_shape, dtype=dtype)\n    out = dot(x, y)\n    expected = jnp.dot(x.T if trans_x else x, y.T if trans_y else y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected.astype(jnp.float32), atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'{op_name}_{dtype}_{axis}', op, dtype, axis) for op_name, op in [('add', jnp.sum), ('max', jnp.max), ('min', jnp.min), ('argmax', jnp.argmax), ('argmin', jnp.argmin)] for axis in [0, 1, (1,), (0, 1)] for dtype in ['float16', 'bfloat16', 'float32', 'float64', 'int32', 'int64', 'uint32', 'uint64']])\ndef test_array_reduce(self, op, dtype, axis):\n    self.skip_if_mosaic_gpu()\n    if not isinstance(axis, int):\n        self.skipTest('TODO: tuple axes are not yet supported')\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    if jax.config.x64_enabled and jtu.test_device_matches(['gpu']) and (op in (jnp.argmin, jnp.argmax)):\n        self.skipTest('Not supported on GPU in 64-bit mode')\n    m, n = (32, 8)\n\n    def make_x(key):\n        if jnp.issubdtype(dtype, jnp.integer):\n            return random.permutation(key, jnp.arange(m * n, dtype=dtype), independent=True).reshape(m, n)\n        else:\n            return random.normal(key, (m, n), dtype=dtype)\n    out_dtype = op(jnp.arange(1, dtype=dtype)).dtype\n    out_shape = jax.ShapeDtypeStruct(op(make_x(random.key(0)), axis=axis).shape, out_dtype)\n    if isinstance(axis, int):\n        grid = tuple((a for i, a in enumerate((m, n)) if i != axis))\n    else:\n        grid = tuple((a for i, a in enumerate((m, n)) if i not in axis))\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\n    def reduce(x_ref, y_ref):\n        x = pl.load(x_ref, (jnp.arange(m, dtype=jnp.int32)[:, None], jnp.arange(n, dtype=jnp.int32)[None]))\n        y = op(x, axis=axis)\n        pl.store(y_ref, tuple((jnp.arange(d, dtype=jnp.int32) for d in y.shape)), y)\n    for i, key in enumerate(random.split(random.key(0), 20)):\n        x = make_x(key)\n        y = reduce(x)\n        y_ref = op(x, axis=axis)\n        self.assertAllClose(y, y_ref, atol=0.01, rtol=0.01, err_msg=i)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.product(axis=[0, 1], dtype=['float16', 'float32', 'int32', 'uint32'])\ndef test_cumsum(self, dtype, axis):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    m, n = (32, 8)\n    out_dtype = dtype\n\n    def make_x(key):\n        if jnp.issubdtype(dtype, jnp.integer):\n            return random.permutation(key, jnp.arange(m * n, dtype=dtype), independent=True).reshape(m, n)\n        else:\n            return random.normal(key, (m, n), dtype=dtype)\n    out_shape = jax.ShapeDtypeStruct((m, n), out_dtype)\n    grid = ()\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\n    def reduce(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = jnp.cumsum(x, axis=axis)\n    for i, key in enumerate(random.split(random.key(0), 20)):\n        x = make_x(key)\n        y = reduce(x)\n        y_ref = jnp.cumsum(x, axis=axis)\n        np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01, err_msg=i)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__}_{value}', dtype, value) for dtypes, values in (((jnp.uint16, jnp.uint32, jnp.uint64), (0, 5)), ((jnp.int16, jnp.int32, jnp.int64), (-3, 0, 5)), ((jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64), (-3.2, -0.0, 0.0, 5.1, jnp.nan, jnp.inf, -jnp.inf))) for dtype in dtypes for value in values))\ndef test_sign(self, dtype, value):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sign(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=dtype)\n    out = kernel(x)\n    expected = jnp.sign(x)\n    np.testing.assert_array_equal(out.astype(jnp.float32), expected.astype(jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.bfloat16, jnp.int32)\ndef test_add_constant(self, dtype):\n    self.skip_if_mosaic_gpu()\n    shape = (256, 256)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...] + 1\n    np.testing.assert_array_equal(kernel(jnp.zeros(shape, dtype=dtype)), jnp.ones(shape, dtype=dtype))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(-3.2, -1.0, -0.999517, -0.4, 0.0, 0.72, 0.999517, 1.0, 2.4)\ndef test_erf_inv(self, value):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.erf_inv(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=floatx)\n    out = kernel(x)\n    expected = lax.erf_inv(x)\n    np.testing.assert_array_equal(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "def test_is_finite(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.is_finite(x_ref[...])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "def test_is_finite_scalar(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = jnp.isfinite(x_ref[i])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "def test_abs_weak_type(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.abs(x_ref[...])\n    x = jnp.broadcast_to(-3.2, (4, 4))\n    np.testing.assert_allclose(kernel(x), jnp.abs(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(('float32', 'int32'), ('float64', 'int32'), ('float32', 'float32'), ('float64', 'float64'))\ndef test_pow(self, x_dtype, y_dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), x_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = lax.pow(x_ref[...], y_ref[...])\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    x = jnp.array([1, 2, 3, 4]).astype(x_dtype)\n    y = jnp.array([1, 2, 3, 4]).astype(y_dtype)\n    np.testing.assert_allclose(kernel(x, y), lax.pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(0, 1, 2, 3, 4, 5, -1, -2, -3)\ndef test_integer_pow(self, y):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = lax.integer_pow(x_ref[...], y)\n    x = jnp.array([1, 2, 3, 4]).astype(jnp.float32) / 10\n    np.testing.assert_allclose(kernel(x), lax.integer_pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__} (x={x!r}, y={y!r})', dtype, x, y) for dtype, x, y in itertools.product((jnp.float32, jnp.float64), _NEXTAFTER_VALUES, _NEXTAFTER_VALUES)))\ndef test_nextafter(self, dtype, x, y):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.nextafter(x_ref[...], y_ref[...])\n    x = jnp.full((4,), x, dtype=dtype)\n    y = jnp.full((4,), y, dtype=dtype)\n    out = kernel(x, y)\n    expected = jnp.nextafter(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(('int32', 'float32'), ('float32', 'float32'), ('bfloat16', 'bfloat16'))\ndef test_true_divide(self, dtype, out_dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        if out_dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest('bfloat16 is not supported on older TPU generations')\n        if not jtu.if_cloud_tpu_at_least(2025, 1, 9):\n            self.skipTest('Requires libtpu built after 2025-01-09')\n    elif jtu.test_device_matches(['gpu']):\n        if dtype == 'bfloat16':\n            self.skipTest('bfloat16 not supported')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), out_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    x = jnp.repeat(x, 8, axis=0).reshape(8, 8)\n    y = jnp.tile(y, 8).reshape(8, 8)\n    rtol = 0.008 if dtype == 'bfloat16' else 1e-06\n    np.testing.assert_allclose(jnp.true_divide(x, y).astype(jnp.float32), kernel(x, y).astype(jnp.float32), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16')\ndef test_true_divide_unsupported(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('No lowering in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([2.4, 4.2]).astype(dtype)\n    y = jnp.array([4.2, 2.4]).astype(dtype)\n    with self.assertRaises(Exception):\n        kernel(x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16', 'float32')\ndef test_approx_tanh(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    if self.INTERPRET:\n        self.skipTest('approx_tanh is not supported in interpret mode')\n    if dtype == 'bfloat16' and (not jtu.is_cuda_compute_capability_at_least('9.0')):\n        self.skipTest('tanh.approx.bf16 requires a GPU with capability >= sm90')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = plgpu.approx_tanh(x_ref[...])\n    x = jnp.asarray([-1, 0.42, 0.24, 1]).astype(dtype)\n    np.testing.assert_allclose(kernel(x).astype(jnp.float32), jnp.tanh(x).astype(jnp.float32), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "def test_elementwise_inline_asm(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: elementwise_inline_asm_p')\n    if self.INTERPRET:\n        self.skipTest('elementwise_inline_asm is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((256,), jnp.float16))\n    def kernel(x_ref, o_ref):\n        [o_ref[...]] = plgpu.elementwise_inline_asm('tanh.approx.f16x2 $0, $1;', args=[x_ref[...]], constraints='=r,r', pack=2, result_shape_dtypes=[jax.ShapeDtypeStruct(x_ref.shape, x_ref.dtype)])\n    x = jnp.arange(256).astype(jnp.float16)\n    np.testing.assert_allclose(kernel(x), jnp.tanh(x), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "def test_debug_barrier(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: debug_barrier_p')\n    if self.INTERPRET:\n        self.skipTest('debug_barrier is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n        plgpu.debug_barrier()\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    np.testing.assert_array_equal(kernel(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('It works!')\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('It works!', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print_with_values(self):\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('x[0] =', x_ref[0])\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x[0] = 4.2', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "def test_num_programs(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((4,), intx), grid=4)\n    def kernel(o_ref):\n        o_ref[pl.program_id(0)] = pl.num_programs(0)\n    np.testing.assert_array_equal(kernel(), jnp.array([4, 4, 4, 4], dtype=intx))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.product(size=[1, 2, 64, 129, 1021], block_size=[1, 2, 32, 64, 128])\ndef test_masked_load_store(self, size, block_size):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), floatx), grid=pl.cdiv(size, block_size))\n    def kernel(x_ref, o_ref):\n        idx = pl.program_id(0) * block_size + jnp.arange(block_size, dtype=jnp.int32)\n        mask = idx < x_ref.shape[0]\n        x = pl.load(x_ref, (idx,), mask=mask)\n        pl.store(o_ref, (idx,), x + 1.0, mask=mask)\n    key = random.key(0)\n    x = random.normal(key, (size,))\n    np.testing.assert_allclose(kernel(x), x + 1.0, atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "def test_strided_load(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[::4]\n    x = jnp.arange(64, dtype=jnp.float32).reshape((16, 4))\n    np.testing.assert_array_equal(kernel(x), x[::4])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(((16, 32), (16,)), ((16, 32), (32,)), ((16, 32), (16, 16)))\ndef test_invalid_broadcasted_load(self, x_shape, mask_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    if self.INTERPRET:\n        self.skipTest('No broadcasting checks in pl.load in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def kernel(x_ref, mask_ref, o_ref):\n        del o_ref\n        pl.load(x_ref, slice(None), mask=mask_ref[:])\n    x = jnp.ones(x_shape, dtype=jnp.float32)\n    mask = jnp.ones(mask_shape, dtype=jnp.bool_)\n    try:\n        kernel(x, mask)\n    except Exception as e:\n        self.assertIn('Cannot broadcast', str(e.__cause__))\n    else:\n        self.fail('Expected exception due to invalid broadcasting')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "def test_debug_print(self):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        jax.debug.print('x = {}', x_ref)\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x = [4.2 2.4]', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n    tracer_spy.append(x_ref)\n    y_ref[...] = jnp.log(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@parameterized.parameters((jnp.float16, jnp.float16), (jnp.int16, jnp.bfloat16), (jnp.int16, jnp.float16), (jnp.uint16, jnp.float16), (jnp.float32, jnp.int32), (jnp.float32, jnp.uint32), (jnp.uint32, jnp.int32), (jnp.int32, jnp.uint32))\ndef test_bitcast_convert_type(self, in_dtype, out_dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    m, n = (4, 4)\n    out_shape = jax.ShapeDtypeStruct((m, n), out_dtype)\n    grid = ()\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\n    def convert(x_ref, y_ref):\n        y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)\n    x = jnp.arange(m * n, dtype=in_dtype).reshape((m, n))\n    y = convert(x)\n    y_ref = jax.lax.bitcast_convert_type(x, out_dtype)\n    np.testing.assert_array_equal(y, y_ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_bitcast_convert_type_scalar(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    x = jnp.int32(42)\n    out_dtype = jnp.float32\n    out_shape = jax.ShapeDtypeStruct(x.shape, out_dtype)\n    grid = ()\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\n    def convert(x_ref, y_ref):\n        y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)\n    y = convert(x)\n    y_ref = jax.lax.bitcast_convert_type(x, out_dtype)\n    np.testing.assert_array_equal(y, y_ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(data, segment_ids):\n    return jax.ops.segment_sum(data, segment_ids, num_segments).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(data, segment_ids):\n    return jax.ops.segment_sum(data, segment_ids, num_segments).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(data, segment_ids):\n    return jax.ops.segment_sum(data, segment_ids, num_segments).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(data, segment_ids):\n    return jax.ops.segment_sum(data, segment_ids, num_segments).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(data, segment_ids):\n    return jax.ops.segment_sum(data, segment_ids, num_segments).sum()"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@compute_on('device_host')\ndef fn():\n    k = jax.random.key(0)\n    return jax.nn.initializers.lecun_normal()(k, (2, 2), jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@compute_on('device_host')\ndef fn():\n    k = jax.random.key(0)\n    return jax.nn.initializers.lecun_normal()(k, (2, 2), jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@compute_on('device_host')\ndef fn():\n    k = jax.random.key(0)\n    return jax.nn.initializers.lecun_normal()(k, (2, 2), jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@compute_on('device_host')\ndef fn():\n    k = jax.random.key(0)\n    return jax.nn.initializers.lecun_normal()(k, (2, 2), jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@compute_on('device_host')\ndef fn():\n    k = jax.random.key(0)\n    return jax.nn.initializers.lecun_normal()(k, (2, 2), jnp.float32)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(indices):\n    return jnp.equal(indices, jnp.arange(3)).astype(jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(indices):\n    return jnp.equal(indices, jnp.arange(3)).astype(jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(indices):\n    return jnp.equal(indices, jnp.arange(3)).astype(jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(indices):\n    return jnp.equal(indices, jnp.arange(3)).astype(jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(indices):\n    return jnp.equal(indices, jnp.arange(3)).astype(jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(((name, name, func, strategy) for name, func, strategy in UNARY_FUNCTIONS))\n@hp.given(hps.data())\ndef test_unary_primitives(self, name, func, shape_dtype_strategy, data):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('This hypothesis test is slow, even more so in interpret mode.')\n    tol = 0.0\n    if jtu.test_device_matches(['gpu']):\n        if func == jnp.round or func == jnp.rint:\n            self.skipTest('TODO: not implemented on GPU')\n        if name == 'tanh':\n            tol = 1e-06\n        elif name == 'exp2':\n            tol = 1e-06\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = func(x_ref[...])\n    x_shape_dtype = data.draw(shape_dtype_strategy)\n    key = random.key(0)\n    x = _random_value(key, x_shape_dtype)\n    out = self.pallas_call(kernel, out_shape=x_shape_dtype)(x)\n    self.assertAllClose(out, func(x), atol=tol, rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(self.pmap, axis_name='i')\ndef func(_):\n    return jax.lax.psum(dtype(0), axis_name='i')"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(key):\n    x = jnp.arange(113003)\n    x = with_sharding_constraint(x, P('data'))\n    y = jnp.arange(65536)\n    y = with_sharding_constraint(y.reshape(-1), P('data'))\n    z = jnp.concatenate([x, y], axis=0)\n    z = with_sharding_constraint(z, P('data'))\n    return (x, y, z)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(key):\n    x = jnp.arange(113003)\n    x = with_sharding_constraint(x, P('data'))\n    y = jnp.arange(65536)\n    y = with_sharding_constraint(y.reshape(-1), P('data'))\n    z = jnp.concatenate([x, y], axis=0)\n    z = with_sharding_constraint(z, P('data'))\n    return (x, y, z)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(key):\n    x = jnp.arange(113003)\n    x = with_sharding_constraint(x, P('data'))\n    y = jnp.arange(65536)\n    y = with_sharding_constraint(y.reshape(-1), P('data'))\n    z = jnp.concatenate([x, y], axis=0)\n    z = with_sharding_constraint(z, P('data'))\n    return (x, y, z)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(key):\n    x = jnp.arange(113003)\n    x = with_sharding_constraint(x, P('data'))\n    y = jnp.arange(65536)\n    y = with_sharding_constraint(y.reshape(-1), P('data'))\n    z = jnp.concatenate([x, y], axis=0)\n    z = with_sharding_constraint(z, P('data'))\n    return (x, y, z)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(key):\n    x = jnp.arange(113003)\n    x = with_sharding_constraint(x, P('data'))\n    y = jnp.arange(65536)\n    y = with_sharding_constraint(y.reshape(-1), P('data'))\n    z = jnp.concatenate([x, y], axis=0)\n    z = with_sharding_constraint(z, P('data'))\n    return (x, y, z)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\ndef fn(x):\n    R1 = jnp.array([[x[0], 0, 0], [0, x[0], 0], [0, 0, x[0]]])\n    R2 = jnp.array([[x[0], 0, 0], [0, x[1], 0], [0, 0, x[2]]])\n    H = jnp.eye(4)\n    H = H.at[:3, :3].set(R2.T)\n    pos = H @ jnp.concatenate([x, jnp.array([1.0])])\n    return (pos, R1)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\ndef fn(x):\n    R1 = jnp.array([[x[0], 0, 0], [0, x[0], 0], [0, 0, x[0]]])\n    R2 = jnp.array([[x[0], 0, 0], [0, x[1], 0], [0, 0, x[2]]])\n    H = jnp.eye(4)\n    H = H.at[:3, :3].set(R2.T)\n    pos = H @ jnp.concatenate([x, jnp.array([1.0])])\n    return (pos, R1)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\ndef fn(x):\n    R1 = jnp.array([[x[0], 0, 0], [0, x[0], 0], [0, 0, x[0]]])\n    R2 = jnp.array([[x[0], 0, 0], [0, x[1], 0], [0, 0, x[2]]])\n    H = jnp.eye(4)\n    H = H.at[:3, :3].set(R2.T)\n    pos = H @ jnp.concatenate([x, jnp.array([1.0])])\n    return (pos, R1)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\ndef fn(x):\n    R1 = jnp.array([[x[0], 0, 0], [0, x[0], 0], [0, 0, x[0]]])\n    R2 = jnp.array([[x[0], 0, 0], [0, x[1], 0], [0, 0, x[2]]])\n    H = jnp.eye(4)\n    H = H.at[:3, :3].set(R2.T)\n    pos = H @ jnp.concatenate([x, jnp.array([1.0])])\n    return (pos, R1)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.vmap\ndef fn(x):\n    R1 = jnp.array([[x[0], 0, 0], [0, x[0], 0], [0, 0, x[0]]])\n    R2 = jnp.array([[x[0], 0, 0], [0, x[1], 0], [0, 0, x[2]]])\n    H = jnp.eye(4)\n    H = H.at[:3, :3].set(R2.T)\n    pos = H @ jnp.concatenate([x, jnp.array([1.0])])\n    return (pos, R1)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(x: jax.Array):\n    checkify.check(jnp.all(x > 0), 'x must be positive')\n    return x + 1"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(x: jax.Array):\n    checkify.check(jnp.all(x > 0), 'x must be positive')\n    return x + 1"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(x: jax.Array):\n    checkify.check(jnp.all(x > 0), 'x must be positive')\n    return x + 1"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(x: jax.Array):\n    checkify.check(jnp.all(x > 0), 'x must be positive')\n    return x + 1"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(x: jax.Array):\n    checkify.check(jnp.all(x > 0), 'x must be positive')\n    return x + 1"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@parameterized.named_parameters(((name, name, func, strategy) for name, func, strategy in UNARY_FUNCTIONS))\n@hp.given(hps.data())\ndef test_unary_primitives(self, name, func, shape_dtype_strategy, data):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('This hypothesis test is slow, even more so in interpret mode.')\n    tol = 0.0\n    if jtu.test_device_matches(['gpu']):\n        if func == jnp.round or func == jnp.rint:\n            self.skipTest('TODO: not implemented on GPU')\n        if name == 'tanh':\n            tol = 1e-06\n        elif name == 'exp2':\n            tol = 1e-06\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = func(x_ref[...])\n    x_shape_dtype = data.draw(shape_dtype_strategy)\n    key = random.key(0)\n    x = _random_value(key, x_shape_dtype)\n    out = self.pallas_call(kernel, out_shape=x_shape_dtype)(x)\n    self.assertAllClose(out, func(x), atol=tol, rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@self.sparsify\ndef func(x):\n    return jit(lambda x: jnp.sum(x, 1))(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(x):\n    return x * x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(x):\n    return x * x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(x):\n    return x * x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(x):\n    return x * x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(x):\n    return x * x"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_concat_constant(self):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    axis = 0\n    num_arrays = 16\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        num_arrays = 2\n        axis = -1\n\n    def kernel(out):\n        result = []\n        for i in range(num_arrays):\n            result.append(jnp.full((1, 128), i, jnp.float32))\n        out[:] = jnp.stack(result, axis=axis).reshape(num_arrays, 128)\n\n    def run(interpret=False):\n        return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((num_arrays, 128), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.VMEM), interpret=interpret)()\n    expected = run(True)\n    if not self.INTERPRET:\n        actual = run(False)\n        self.assertAllClose(actual, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def run(pos):\n    maxiter = 1000\n\n    def cond(v):\n        return v[0] < maxiter\n\n    def step(v):\n        i, pos = v\n        jax.debug.callback(print_it, i + 1, maxiter)\n        return (i + 1, pos + 1)\n    val = (jnp.array(0), pos)\n    val = jax.lax.while_loop(cond, step, val)\n    return val[1]"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@custom_transpose(jnp.ones(2))\ndef fn(r, x):\n    return x / r"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@custom_transpose(jnp.ones(2))\ndef fn(r, x):\n    return x / r"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@custom_transpose(jnp.ones(2))\ndef fn(r, x):\n    return x / r"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@custom_transpose(jnp.ones(2))\ndef fn(r, x):\n    return x / r"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@custom_transpose(jnp.ones(2))\ndef fn(r, x):\n    return x / r"
  },
  {
    "test_code": "def test_concat_constant(self):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    axis = 0\n    num_arrays = 16\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        num_arrays = 2\n        axis = -1\n\n    def kernel(out):\n        result = []\n        for i in range(num_arrays):\n            result.append(jnp.full((1, 128), i, jnp.float32))\n        out[:] = jnp.stack(result, axis=axis).reshape(num_arrays, 128)\n\n    def run(interpret=False):\n        return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((num_arrays, 128), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.VMEM), interpret=interpret)()\n    expected = run(True)\n    if not self.INTERPRET:\n        actual = run(False)\n        self.assertAllClose(actual, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def run(primal_ins, cotangent_outs):\n    primal_outs, vjp = jax.vjp(g, *primal_ins)\n    cotangent_ins = vjp(cotangent_outs)\n    return (primal_outs, cotangent_ins)"
  },
  {
    "test_code": "@parameterized.named_parameters(((name, name, func, strategy) for name, func, strategy in UNARY_FUNCTIONS))\n@hp.given(hps.data())\ndef test_unary_primitives(self, name, func, shape_dtype_strategy, data):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('This hypothesis test is slow, even more so in interpret mode.')\n    tol = 0.0\n    if jtu.test_device_matches(['gpu']):\n        if func == jnp.round or func == jnp.rint:\n            self.skipTest('TODO: not implemented on GPU')\n        if name == 'tanh':\n            tol = 1e-06\n        elif name == 'exp2':\n            tol = 1e-06\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = func(x_ref[...])\n    x_shape_dtype = data.draw(shape_dtype_strategy)\n    key = random.key(0)\n    x = _random_value(key, x_shape_dtype)\n    out = self.pallas_call(kernel, out_shape=x_shape_dtype)(x)\n    self.assertAllClose(out, func(x), atol=tol, rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def func(D0):\n\n    def shift(R, dR, **unused_kwargs):\n        return R + dR\n\n    def apply_fn(R):\n        return D0 * R\n    Rinit = jax.random.uniform(split, (n, 3), minval=0.0, maxval=5.0, dtype=jnp.float32)\n\n    def move(R, i):\n        F = apply_fn(R)\n        return (shift(R, 0.001 * F), jnp.array([0.0]))\n    move = remat(move)\n    R, temp = lax.scan(move, Rinit, jnp.arange(2))\n    return R[0, 0]"
  },
  {
    "test_code": "@parameterized.parameters((0,), (1,))\ndef test_array_atomic_add(self, axis):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Unimplemented primitive: broadcast_to')\n    m, n = (32, 8)\n    if axis == 0:\n        grid = m\n    else:\n        grid = n\n    out_shape = jax.ShapeDtypeStruct((n if axis == 0 else m,), floatx)\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid, input_output_aliases={1: 0})\n    def reduce(x_ref, _, y_ref):\n        i = pl.program_id(axis=0)\n        if axis == 0:\n            idx = (i, jnp.arange(n))\n        else:\n            idx = (jnp.arange(m), i)\n        x = pl.load(x_ref, idx)\n        pl.atomic_add(y_ref, (jnp.arange(y.shape[0]),), x)\n    x = random.normal(random.key(0), (m, n))\n    y = jnp.zeros(out_shape.shape, out_shape.dtype)\n    y = reduce(x, y)\n    y_ref = np.sum(x, axis=axis)\n    np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@parameterized.parameters(False, True)\ndef test_reduce_only_dim(self, use_store):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    m = 32\n    x = random.normal(random.key(0), (m,), dtype=jnp.float32)\n    out_shape = jax.ShapeDtypeStruct((), x.dtype)\n\n    @functools.partial(self.pallas_call, out_shape=out_shape)\n    def reduce(x_ref, y_ref):\n        x = pl.load(x_ref, (jnp.arange(m),))\n        y = jnp.sum(x, axis=-1)\n        if use_store:\n            pl.store(y_ref, (), y)\n        else:\n            y_ref[...] = y\n    y = reduce(x)\n    y_ref = jnp.sum(x, axis=-1)\n    np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@parameterized.product(lhs_and_rhs_shape=[((16, 16), (16, 16)), ((32, 32), (32, 32)), ((64, 64), (64, 64)), ((128, 128), (128, 128)), ((256, 256), (256, 256)), ((8, 128), (128, 256)), ((8, 128), (256, 128)), ((8, 256), (256, 128)), ((16, 128), (128, 256)), ((16, 128), (256, 128)), ((16, 256), (256, 128)), ((24, 128), (128, 256)), ((24, 128), (256, 128)), ((24, 256), (256, 128)), ((128, 8), (128, 256)), ((128, 8), (256, 128)), ((256, 8), (256, 128)), ((128, 16), (128, 256)), ((128, 16), (256, 128)), ((256, 16), (256, 128)), ((128, 24), (128, 256)), ((128, 24), (256, 128)), ((256, 24), (256, 128))], dtype=[jnp.float32, jnp.float16, jnp.bfloat16], trans_x=[False, True], trans_y=[False, True])\ndef test_dot(self, lhs_and_rhs_shape, dtype, trans_x, trans_y):\n    self.skip_if_mosaic_gpu()\n    if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n        self.skipTest('Requires libtpu built after 2024-12-19')\n    lhs_shape, rhs_shape = lhs_and_rhs_shape\n    final_lhs_shape = lhs_shape[::-1] if trans_x else lhs_shape\n    final_rhs_shape = rhs_shape[::-1] if trans_y else rhs_shape\n    if final_lhs_shape[1] != final_rhs_shape[0]:\n        self.skipTest('Contraction dimensions do not match')\n    out_shape = (final_lhs_shape[0], final_rhs_shape[1])\n    if jtu.test_device_matches(['tpu']):\n        if dtype == jnp.float16:\n            self.skipTest('float16 type is not supported on TPU')\n        if dtype == jnp.bfloat16 and (not jtu.is_device_tpu_at_least(4)):\n            self.skipTest('bfloat16 matmul is supported on TPUv4+')\n        if trans_x:\n            self.skipTest('Not implemented: Transposed LHS')\n    if jtu.test_device_matches(['gpu']):\n        if dtype == jnp.bfloat16:\n            self.skipTest('bfloat16 type are not supported on GPU')\n        if math.prod(lhs_shape) + math.prod(rhs_shape) + math.prod(out_shape) > 256 * 256 * 2:\n            self.skipTest('Shared memory size limit exceeded')\n        if min(*lhs_shape, *rhs_shape) < 16:\n            self.skipTest('All dimensions of lhs and rhs must be >= 16')\n        if any((not is_power_of_two(x) for x in lhs_shape + rhs_shape)):\n            self.skipTest('All dimensions of lhs and rhs must be power of two')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, dtype))\n    def dot(x_ref, y_ref, o_ref):\n        x = x_ref[:, :]\n        y = y_ref[:, :]\n        o_ref[:, :] = pl.dot(x, y, trans_x, trans_y).astype(o_ref.dtype)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, lhs_shape, dtype=dtype)\n    y = random.normal(k2, rhs_shape, dtype=dtype)\n    out = dot(x, y)\n    expected = jnp.dot(x.T if trans_x else x, y.T if trans_y else y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected.astype(jnp.float32), atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(*args, split_transpose=False):\n    v, fn_transpose = jax.vjp(partial(loss, split_transpose=split_transpose), *args)\n    grads = fn_transpose(1.0)\n    return (*grads, v)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(*args, split_transpose=False):\n    v, fn_transpose = jax.vjp(partial(loss, split_transpose=split_transpose), *args)\n    grads = fn_transpose(1.0)\n    return (*grads, v)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(*args, split_transpose=False):\n    v, fn_transpose = jax.vjp(partial(loss, split_transpose=split_transpose), *args)\n    grads = fn_transpose(1.0)\n    return (*grads, v)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(*args, split_transpose=False):\n    v, fn_transpose = jax.vjp(partial(loss, split_transpose=split_transpose), *args)\n    grads = fn_transpose(1.0)\n    return (*grads, v)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def fn(*args, split_transpose=False):\n    v, fn_transpose = jax.vjp(partial(loss, split_transpose=split_transpose), *args)\n    grads = fn_transpose(1.0)\n    return (*grads, v)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(x: int, static_y: BlackBox):\n    nonlocal num_called\n    num_called += 1\n    return x + static_y.value"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(x: int, static_y: BlackBox):\n    nonlocal num_called\n    num_called += 1\n    return x + static_y.value"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(x: int, static_y: BlackBox):\n    nonlocal num_called\n    num_called += 1\n    return x + static_y.value"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(x: int, static_y: BlackBox):\n    nonlocal num_called\n    num_called += 1\n    return x + static_y.value"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef fn(x: int, static_y: BlackBox):\n    nonlocal num_called\n    num_called += 1\n    return x + static_y.value"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__}_{value}', dtype, value) for dtypes, values in (((jnp.uint16, jnp.uint32, jnp.uint64), (0, 5)), ((jnp.int16, jnp.int32, jnp.int64), (-3, 0, 5)), ((jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64), (-3.2, -0.0, 0.0, 5.1, jnp.nan, jnp.inf, -jnp.inf))) for dtype in dtypes for value in values))\ndef test_sign(self, dtype, value):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sign(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=dtype)\n    out = kernel(x)\n    expected = jnp.sign(x)\n    np.testing.assert_array_equal(out.astype(jnp.float32), expected.astype(jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.bfloat16, jnp.int32)\ndef test_add_constant(self, dtype):\n    self.skip_if_mosaic_gpu()\n    shape = (256, 256)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...] + 1\n    np.testing.assert_array_equal(kernel(jnp.zeros(shape, dtype=dtype)), jnp.ones(shape, dtype=dtype))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.parameters(-3.2, -1.0, -0.999517, -0.4, 0.0, 0.72, 0.999517, 1.0, 2.4)\ndef test_erf_inv(self, value):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.erf_inv(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=floatx)\n    out = kernel(x)\n    expected = lax.erf_inv(x)\n    np.testing.assert_array_equal(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "def test_is_finite(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.is_finite(x_ref[...])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "def test_is_finite_scalar(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = jnp.isfinite(x_ref[i])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "def test_abs_weak_type(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.abs(x_ref[...])\n    x = jnp.broadcast_to(-3.2, (4, 4))\n    np.testing.assert_allclose(kernel(x), jnp.abs(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.parameters(('float32', 'int32'), ('float64', 'int32'), ('float32', 'float32'), ('float64', 'float64'))\ndef test_pow(self, x_dtype, y_dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), x_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = lax.pow(x_ref[...], y_ref[...])\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    x = jnp.array([1, 2, 3, 4]).astype(x_dtype)\n    y = jnp.array([1, 2, 3, 4]).astype(y_dtype)\n    np.testing.assert_allclose(kernel(x, y), lax.pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.parameters(0, 1, 2, 3, 4, 5, -1, -2, -3)\ndef test_integer_pow(self, y):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = lax.integer_pow(x_ref[...], y)\n    x = jnp.array([1, 2, 3, 4]).astype(jnp.float32) / 10\n    np.testing.assert_allclose(kernel(x), lax.integer_pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__} (x={x!r}, y={y!r})', dtype, x, y) for dtype, x, y in itertools.product((jnp.float32, jnp.float64), _NEXTAFTER_VALUES, _NEXTAFTER_VALUES)))\ndef test_nextafter(self, dtype, x, y):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.nextafter(x_ref[...], y_ref[...])\n    x = jnp.full((4,), x, dtype=dtype)\n    y = jnp.full((4,), y, dtype=dtype)\n    out = kernel(x, y)\n    expected = jnp.nextafter(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.parameters(('int32', 'float32'), ('float32', 'float32'), ('bfloat16', 'bfloat16'))\ndef test_true_divide(self, dtype, out_dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        if out_dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest('bfloat16 is not supported on older TPU generations')\n        if not jtu.if_cloud_tpu_at_least(2025, 1, 9):\n            self.skipTest('Requires libtpu built after 2025-01-09')\n    elif jtu.test_device_matches(['gpu']):\n        if dtype == 'bfloat16':\n            self.skipTest('bfloat16 not supported')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), out_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    x = jnp.repeat(x, 8, axis=0).reshape(8, 8)\n    y = jnp.tile(y, 8).reshape(8, 8)\n    rtol = 0.008 if dtype == 'bfloat16' else 1e-06\n    np.testing.assert_allclose(jnp.true_divide(x, y).astype(jnp.float32), kernel(x, y).astype(jnp.float32), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16')\ndef test_true_divide_unsupported(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('No lowering in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([2.4, 4.2]).astype(dtype)\n    y = jnp.array([4.2, 2.4]).astype(dtype)\n    with self.assertRaises(Exception):\n        kernel(x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16', 'float32')\ndef test_approx_tanh(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    if self.INTERPRET:\n        self.skipTest('approx_tanh is not supported in interpret mode')\n    if dtype == 'bfloat16' and (not jtu.is_cuda_compute_capability_at_least('9.0')):\n        self.skipTest('tanh.approx.bf16 requires a GPU with capability >= sm90')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = plgpu.approx_tanh(x_ref[...])\n    x = jnp.asarray([-1, 0.42, 0.24, 1]).astype(dtype)\n    np.testing.assert_allclose(kernel(x).astype(jnp.float32), jnp.tanh(x).astype(jnp.float32), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "def test_elementwise_inline_asm(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: elementwise_inline_asm_p')\n    if self.INTERPRET:\n        self.skipTest('elementwise_inline_asm is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((256,), jnp.float16))\n    def kernel(x_ref, o_ref):\n        [o_ref[...]] = plgpu.elementwise_inline_asm('tanh.approx.f16x2 $0, $1;', args=[x_ref[...]], constraints='=r,r', pack=2, result_shape_dtypes=[jax.ShapeDtypeStruct(x_ref.shape, x_ref.dtype)])\n    x = jnp.arange(256).astype(jnp.float16)\n    np.testing.assert_allclose(kernel(x), jnp.tanh(x), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "def test_debug_barrier(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: debug_barrier_p')\n    if self.INTERPRET:\n        self.skipTest('debug_barrier is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n        plgpu.debug_barrier()\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    np.testing.assert_array_equal(kernel(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('It works!')\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('It works!', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print_with_values(self):\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('x[0] =', x_ref[0])\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x[0] = 4.2', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "def test_num_programs(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((4,), intx), grid=4)\n    def kernel(o_ref):\n        o_ref[pl.program_id(0)] = pl.num_programs(0)\n    np.testing.assert_array_equal(kernel(), jnp.array([4, 4, 4, 4], dtype=intx))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.product(size=[1, 2, 64, 129, 1021], block_size=[1, 2, 32, 64, 128])\ndef test_masked_load_store(self, size, block_size):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), floatx), grid=pl.cdiv(size, block_size))\n    def kernel(x_ref, o_ref):\n        idx = pl.program_id(0) * block_size + jnp.arange(block_size, dtype=jnp.int32)\n        mask = idx < x_ref.shape[0]\n        x = pl.load(x_ref, (idx,), mask=mask)\n        pl.store(o_ref, (idx,), x + 1.0, mask=mask)\n    key = random.key(0)\n    x = random.normal(key, (size,))\n    np.testing.assert_allclose(kernel(x), x + 1.0, atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "def test_strided_load(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[::4]\n    x = jnp.arange(64, dtype=jnp.float32).reshape((16, 4))\n    np.testing.assert_array_equal(kernel(x), x[::4])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.parameters(((16, 32), (16,)), ((16, 32), (32,)), ((16, 32), (16, 16)))\ndef test_invalid_broadcasted_load(self, x_shape, mask_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    if self.INTERPRET:\n        self.skipTest('No broadcasting checks in pl.load in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def kernel(x_ref, mask_ref, o_ref):\n        del o_ref\n        pl.load(x_ref, slice(None), mask=mask_ref[:])\n    x = jnp.ones(x_shape, dtype=jnp.float32)\n    mask = jnp.ones(mask_shape, dtype=jnp.bool_)\n    try:\n        kernel(x, mask)\n    except Exception as e:\n        self.assertIn('Cannot broadcast', str(e.__cause__))\n    else:\n        self.fail('Expected exception due to invalid broadcasting')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "def test_debug_print(self):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        jax.debug.print('x = {}', x_ref)\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x = [4.2 2.4]', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(ctx, dst, _):\n    i8 = ir.IntegerType.get_signless(8)\n    iota = iota_tensor(m, n, jnp.uint8)\n    (iota > 10).astype(i8, is_signed=False).store_untiled(dst)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__}_{value}', dtype, value) for dtypes, values in (((jnp.uint16, jnp.uint32, jnp.uint64), (0, 5)), ((jnp.int16, jnp.int32, jnp.int64), (-3, 0, 5)), ((jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64), (-3.2, -0.0, 0.0, 5.1, jnp.nan, jnp.inf, -jnp.inf))) for dtype in dtypes for value in values))\ndef test_sign(self, dtype, value):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sign(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=dtype)\n    out = kernel(x)\n    expected = jnp.sign(x)\n    np.testing.assert_array_equal(out.astype(jnp.float32), expected.astype(jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.bfloat16, jnp.int32)\ndef test_add_constant(self, dtype):\n    self.skip_if_mosaic_gpu()\n    shape = (256, 256)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...] + 1\n    np.testing.assert_array_equal(kernel(jnp.zeros(shape, dtype=dtype)), jnp.ones(shape, dtype=dtype))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.parameters(-3.2, -1.0, -0.999517, -0.4, 0.0, 0.72, 0.999517, 1.0, 2.4)\ndef test_erf_inv(self, value):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.erf_inv(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=floatx)\n    out = kernel(x)\n    expected = lax.erf_inv(x)\n    np.testing.assert_array_equal(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "def test_is_finite(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.is_finite(x_ref[...])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "def test_is_finite_scalar(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = jnp.isfinite(x_ref[i])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "def test_abs_weak_type(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.abs(x_ref[...])\n    x = jnp.broadcast_to(-3.2, (4, 4))\n    np.testing.assert_allclose(kernel(x), jnp.abs(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.parameters(('float32', 'int32'), ('float64', 'int32'), ('float32', 'float32'), ('float64', 'float64'))\ndef test_pow(self, x_dtype, y_dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), x_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = lax.pow(x_ref[...], y_ref[...])\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    x = jnp.array([1, 2, 3, 4]).astype(x_dtype)\n    y = jnp.array([1, 2, 3, 4]).astype(y_dtype)\n    np.testing.assert_allclose(kernel(x, y), lax.pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.parameters(0, 1, 2, 3, 4, 5, -1, -2, -3)\ndef test_integer_pow(self, y):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = lax.integer_pow(x_ref[...], y)\n    x = jnp.array([1, 2, 3, 4]).astype(jnp.float32) / 10\n    np.testing.assert_allclose(kernel(x), lax.integer_pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__} (x={x!r}, y={y!r})', dtype, x, y) for dtype, x, y in itertools.product((jnp.float32, jnp.float64), _NEXTAFTER_VALUES, _NEXTAFTER_VALUES)))\ndef test_nextafter(self, dtype, x, y):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.nextafter(x_ref[...], y_ref[...])\n    x = jnp.full((4,), x, dtype=dtype)\n    y = jnp.full((4,), y, dtype=dtype)\n    out = kernel(x, y)\n    expected = jnp.nextafter(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.parameters(('int32', 'float32'), ('float32', 'float32'), ('bfloat16', 'bfloat16'))\ndef test_true_divide(self, dtype, out_dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        if out_dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest('bfloat16 is not supported on older TPU generations')\n        if not jtu.if_cloud_tpu_at_least(2025, 1, 9):\n            self.skipTest('Requires libtpu built after 2025-01-09')\n    elif jtu.test_device_matches(['gpu']):\n        if dtype == 'bfloat16':\n            self.skipTest('bfloat16 not supported')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), out_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    x = jnp.repeat(x, 8, axis=0).reshape(8, 8)\n    y = jnp.tile(y, 8).reshape(8, 8)\n    rtol = 0.008 if dtype == 'bfloat16' else 1e-06\n    np.testing.assert_allclose(jnp.true_divide(x, y).astype(jnp.float32), kernel(x, y).astype(jnp.float32), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16')\ndef test_true_divide_unsupported(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('No lowering in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([2.4, 4.2]).astype(dtype)\n    y = jnp.array([4.2, 2.4]).astype(dtype)\n    with self.assertRaises(Exception):\n        kernel(x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16', 'float32')\ndef test_approx_tanh(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    if self.INTERPRET:\n        self.skipTest('approx_tanh is not supported in interpret mode')\n    if dtype == 'bfloat16' and (not jtu.is_cuda_compute_capability_at_least('9.0')):\n        self.skipTest('tanh.approx.bf16 requires a GPU with capability >= sm90')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = plgpu.approx_tanh(x_ref[...])\n    x = jnp.asarray([-1, 0.42, 0.24, 1]).astype(dtype)\n    np.testing.assert_allclose(kernel(x).astype(jnp.float32), jnp.tanh(x).astype(jnp.float32), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "def test_elementwise_inline_asm(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: elementwise_inline_asm_p')\n    if self.INTERPRET:\n        self.skipTest('elementwise_inline_asm is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((256,), jnp.float16))\n    def kernel(x_ref, o_ref):\n        [o_ref[...]] = plgpu.elementwise_inline_asm('tanh.approx.f16x2 $0, $1;', args=[x_ref[...]], constraints='=r,r', pack=2, result_shape_dtypes=[jax.ShapeDtypeStruct(x_ref.shape, x_ref.dtype)])\n    x = jnp.arange(256).astype(jnp.float16)\n    np.testing.assert_allclose(kernel(x), jnp.tanh(x), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "def test_debug_barrier(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: debug_barrier_p')\n    if self.INTERPRET:\n        self.skipTest('debug_barrier is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n        plgpu.debug_barrier()\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    np.testing.assert_array_equal(kernel(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('It works!')\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('It works!', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print_with_values(self):\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('x[0] =', x_ref[0])\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x[0] = 4.2', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "def test_num_programs(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((4,), intx), grid=4)\n    def kernel(o_ref):\n        o_ref[pl.program_id(0)] = pl.num_programs(0)\n    np.testing.assert_array_equal(kernel(), jnp.array([4, 4, 4, 4], dtype=intx))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.product(size=[1, 2, 64, 129, 1021], block_size=[1, 2, 32, 64, 128])\ndef test_masked_load_store(self, size, block_size):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), floatx), grid=pl.cdiv(size, block_size))\n    def kernel(x_ref, o_ref):\n        idx = pl.program_id(0) * block_size + jnp.arange(block_size, dtype=jnp.int32)\n        mask = idx < x_ref.shape[0]\n        x = pl.load(x_ref, (idx,), mask=mask)\n        pl.store(o_ref, (idx,), x + 1.0, mask=mask)\n    key = random.key(0)\n    x = random.normal(key, (size,))\n    np.testing.assert_allclose(kernel(x), x + 1.0, atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "def test_strided_load(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[::4]\n    x = jnp.arange(64, dtype=jnp.float32).reshape((16, 4))\n    np.testing.assert_array_equal(kernel(x), x[::4])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.parameters(((16, 32), (16,)), ((16, 32), (32,)), ((16, 32), (16, 16)))\ndef test_invalid_broadcasted_load(self, x_shape, mask_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    if self.INTERPRET:\n        self.skipTest('No broadcasting checks in pl.load in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def kernel(x_ref, mask_ref, o_ref):\n        del o_ref\n        pl.load(x_ref, slice(None), mask=mask_ref[:])\n    x = jnp.ones(x_shape, dtype=jnp.float32)\n    mask = jnp.ones(mask_shape, dtype=jnp.bool_)\n    try:\n        kernel(x, mask)\n    except Exception as e:\n        self.assertIn('Cannot broadcast', str(e.__cause__))\n    else:\n        self.fail('Expected exception due to invalid broadcasting')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "def test_debug_print(self):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        jax.debug.print('x = {}', x_ref)\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x = [4.2 2.4]', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((64, 128), x.dtype), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=1, grid=(grid_size,), in_specs=[pl.BlockSpec((8, 128), lambda i, s_ref: (pl.load(s_ref[0], (i,)), 0)), pl.BlockSpec((1, 128), lambda i, s_ref: (0, 0))], out_specs=pl.BlockSpec((32, 128), lambda i, s_ref: (pl.load(s_ref[0], i), 0)), scratch_shapes=[pltpu.SemaphoreType.REGULAR((3,))] if scratch else []))\ndef kernel(s_refs, src, to_store, dst, *scratch_refs):\n    s_ref, s2, s3 = s_refs\n    assert s_ref.shape == (2,)\n    assert s2.shape == (3,)\n    assert s3 is None\n    store_idx = s_ref[pl.program_id(0)]\n    pl.store(dst, (pl.dslice(store_idx, 1), slice(None)), to_store[...])"
  },
  {
    "test_code": "@parameterized.parameters((0,), (1,))\ndef test_array_atomic_add(self, axis):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Unimplemented primitive: broadcast_to')\n    m, n = (32, 8)\n    if axis == 0:\n        grid = m\n    else:\n        grid = n\n    out_shape = jax.ShapeDtypeStruct((n if axis == 0 else m,), floatx)\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid, input_output_aliases={1: 0})\n    def reduce(x_ref, _, y_ref):\n        i = pl.program_id(axis=0)\n        if axis == 0:\n            idx = (i, jnp.arange(n))\n        else:\n            idx = (jnp.arange(m), i)\n        x = pl.load(x_ref, idx)\n        pl.atomic_add(y_ref, (jnp.arange(y.shape[0]),), x)\n    x = random.normal(random.key(0), (m, n))\n    y = jnp.zeros(out_shape.shape, out_shape.dtype)\n    y = reduce(x, y)\n    y_ref = np.sum(x, axis=axis)\n    np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef reduce(x):\n    return self.pallas_call(body, out_shape=jax.ShapeDtypeStruct(red_shape, dty), in_specs=[pl.BlockSpec(in_shape)], out_specs=pl.BlockSpec(red_shape), grid=1)(x)"
  },
  {
    "test_code": "@parameterized.parameters(False, True)\ndef test_reduce_only_dim(self, use_store):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    m = 32\n    x = random.normal(random.key(0), (m,), dtype=jnp.float32)\n    out_shape = jax.ShapeDtypeStruct((), x.dtype)\n\n    @functools.partial(self.pallas_call, out_shape=out_shape)\n    def reduce(x_ref, y_ref):\n        x = pl.load(x_ref, (jnp.arange(m),))\n        y = jnp.sum(x, axis=-1)\n        if use_store:\n            pl.store(y_ref, (), y)\n        else:\n            y_ref[...] = y\n    y = reduce(x)\n    y_ref = jnp.sum(x, axis=-1)\n    np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef reduce(x):\n    return self.pallas_call(body, out_shape=jax.ShapeDtypeStruct(red_shape, dty), in_specs=[pl.BlockSpec(in_shape)], out_specs=pl.BlockSpec(red_shape), grid=1)(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(*[(f'{op_name}_{dtype}_{axis}', op, dtype, axis) for op_name, op in [('add', jnp.sum), ('max', jnp.max), ('min', jnp.min), ('argmax', jnp.argmax), ('argmin', jnp.argmin)] for axis in [0, 1, (1,), (0, 1)] for dtype in ['float16', 'bfloat16', 'float32', 'float64', 'int32', 'int64', 'uint32', 'uint64']])\ndef test_array_reduce(self, op, dtype, axis):\n    self.skip_if_mosaic_gpu()\n    if not isinstance(axis, int):\n        self.skipTest('TODO: tuple axes are not yet supported')\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    if jax.config.x64_enabled and jtu.test_device_matches(['gpu']) and (op in (jnp.argmin, jnp.argmax)):\n        self.skipTest('Not supported on GPU in 64-bit mode')\n    m, n = (32, 8)\n\n    def make_x(key):\n        if jnp.issubdtype(dtype, jnp.integer):\n            return random.permutation(key, jnp.arange(m * n, dtype=dtype), independent=True).reshape(m, n)\n        else:\n            return random.normal(key, (m, n), dtype=dtype)\n    out_dtype = op(jnp.arange(1, dtype=dtype)).dtype\n    out_shape = jax.ShapeDtypeStruct(op(make_x(random.key(0)), axis=axis).shape, out_dtype)\n    if isinstance(axis, int):\n        grid = tuple((a for i, a in enumerate((m, n)) if i != axis))\n    else:\n        grid = tuple((a for i, a in enumerate((m, n)) if i not in axis))\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\n    def reduce(x_ref, y_ref):\n        x = pl.load(x_ref, (jnp.arange(m, dtype=jnp.int32)[:, None], jnp.arange(n, dtype=jnp.int32)[None]))\n        y = op(x, axis=axis)\n        pl.store(y_ref, tuple((jnp.arange(d, dtype=jnp.int32) for d in y.shape)), y)\n    for i, key in enumerate(random.split(random.key(0), 20)):\n        x = make_x(key)\n        y = reduce(x)\n        y_ref = op(x, axis=axis)\n        self.assertAllClose(y, y_ref, atol=0.01, rtol=0.01, err_msg=i)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef reduce(x):\n    return self.pallas_call(body, out_shape=jax.ShapeDtypeStruct(red_shape, dty), in_specs=[pl.BlockSpec(in_shape)], out_specs=pl.BlockSpec(red_shape), grid=1)(x)"
  },
  {
    "test_code": "@parameterized.product(axis=[0, 1], dtype=['float16', 'float32', 'int32', 'uint32'])\ndef test_cumsum(self, dtype, axis):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    m, n = (32, 8)\n    out_dtype = dtype\n\n    def make_x(key):\n        if jnp.issubdtype(dtype, jnp.integer):\n            return random.permutation(key, jnp.arange(m * n, dtype=dtype), independent=True).reshape(m, n)\n        else:\n            return random.normal(key, (m, n), dtype=dtype)\n    out_shape = jax.ShapeDtypeStruct((m, n), out_dtype)\n    grid = ()\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\n    def reduce(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = jnp.cumsum(x, axis=axis)\n    for i, key in enumerate(random.split(random.key(0), 20)):\n        x = make_x(key)\n        y = reduce(x)\n        y_ref = jnp.cumsum(x, axis=axis)\n        np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01, err_msg=i)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef reduce(x):\n    return self.pallas_call(body, out_shape=jax.ShapeDtypeStruct(red_shape, dty), in_specs=[pl.BlockSpec(in_shape)], out_specs=pl.BlockSpec(red_shape), grid=1)(x)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@parameterized.product(lhs_and_rhs_shape=[((16, 16), (16, 16)), ((32, 32), (32, 32)), ((64, 64), (64, 64)), ((128, 128), (128, 128)), ((256, 256), (256, 256)), ((8, 128), (128, 256)), ((8, 128), (256, 128)), ((8, 256), (256, 128)), ((16, 128), (128, 256)), ((16, 128), (256, 128)), ((16, 256), (256, 128)), ((24, 128), (128, 256)), ((24, 128), (256, 128)), ((24, 256), (256, 128)), ((128, 8), (128, 256)), ((128, 8), (256, 128)), ((256, 8), (256, 128)), ((128, 16), (128, 256)), ((128, 16), (256, 128)), ((256, 16), (256, 128)), ((128, 24), (128, 256)), ((128, 24), (256, 128)), ((256, 24), (256, 128))], dtype=[jnp.float32, jnp.float16, jnp.bfloat16], trans_x=[False, True], trans_y=[False, True])\ndef test_dot(self, lhs_and_rhs_shape, dtype, trans_x, trans_y):\n    self.skip_if_mosaic_gpu()\n    if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n        self.skipTest('Requires libtpu built after 2024-12-19')\n    lhs_shape, rhs_shape = lhs_and_rhs_shape\n    final_lhs_shape = lhs_shape[::-1] if trans_x else lhs_shape\n    final_rhs_shape = rhs_shape[::-1] if trans_y else rhs_shape\n    if final_lhs_shape[1] != final_rhs_shape[0]:\n        self.skipTest('Contraction dimensions do not match')\n    out_shape = (final_lhs_shape[0], final_rhs_shape[1])\n    if jtu.test_device_matches(['tpu']):\n        if dtype == jnp.float16:\n            self.skipTest('float16 type is not supported on TPU')\n        if dtype == jnp.bfloat16 and (not jtu.is_device_tpu_at_least(4)):\n            self.skipTest('bfloat16 matmul is supported on TPUv4+')\n        if trans_x:\n            self.skipTest('Not implemented: Transposed LHS')\n    if jtu.test_device_matches(['gpu']):\n        if dtype == jnp.bfloat16:\n            self.skipTest('bfloat16 type are not supported on GPU')\n        if math.prod(lhs_shape) + math.prod(rhs_shape) + math.prod(out_shape) > 256 * 256 * 2:\n            self.skipTest('Shared memory size limit exceeded')\n        if min(*lhs_shape, *rhs_shape) < 16:\n            self.skipTest('All dimensions of lhs and rhs must be >= 16')\n        if any((not is_power_of_two(x) for x in lhs_shape + rhs_shape)):\n            self.skipTest('All dimensions of lhs and rhs must be power of two')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, dtype))\n    def dot(x_ref, y_ref, o_ref):\n        x = x_ref[:, :]\n        y = y_ref[:, :]\n        o_ref[:, :] = pl.dot(x, y, trans_x, trans_y).astype(o_ref.dtype)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, lhs_shape, dtype=dtype)\n    y = random.normal(k2, rhs_shape, dtype=dtype)\n    out = dot(x, y)\n    expected = jnp.dot(x.T if trans_x else x, y.T if trans_y else y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected.astype(jnp.float32), atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(('sum', jnp.sum), ('max', jnp.max), ('min', jnp.min))\ndef test_reduce_float(self, reduction_op):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('TODO: error on GPU')\n\n    def kernel(x_ref, o_ref):\n        o_ref[0, 0] = reduction_op(x_ref[...])\n    x = jax.random.normal(jax.random.key(0), (8, 128))\n    result = self.pallas_call(kernel, in_specs=[pl.BlockSpec((8, 128), lambda *_: (0, 0))], out_specs=pl.BlockSpec((1, 1), memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct([1, 1], floatx), grid=(1,))(x)\n    np.testing.assert_allclose(result[0, 0], reduction_op(x), atol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@hp.given(select_n_strategy(max_cases=2, min_rank=2, max_rank=4, min_size_exp=1))\ndef test_select_n(self, args):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('TODO: error on GPU, lowering bug for select_n')\n    pred, *cases = args\n    scalar_pred = not pred.shape\n\n    def kernel(*refs):\n        if scalar_pred:\n            *case_refs, o_ref = refs\n            pred_ = pred\n        else:\n            pred_ref, *case_refs, o_ref = refs\n            pred_ = pred_ref[...]\n        vals = [case_ref[...] for case_ref in case_refs]\n        o_ref[...] = lax.select_n(pred_, *vals)\n    out_ref = lax.select_n(pred, *cases)\n    if scalar_pred:\n        args = cases\n    else:\n        args = [pred, *cases]\n    out = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(out_ref.shape, out_ref.dtype))(*args)\n    if out.dtype == jnp.bfloat16:\n        out, out_ref = (out.astype(jnp.float32), out_ref.astype(jnp.float32))\n    np.testing.assert_allclose(out, out_ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.product(from_dtype=_DTYPES_32BIT, to_dtype=_DTYPES)\n@hp.given(hps.data())\ndef test_cast_from_32bit(self, from_dtype, to_dtype, data):\n    self.skip_if_mosaic_gpu()\n    if to_dtype in {'float8_e4m3b11fnuz', 'float8_e5m2', 'float8_e4m3fn'}:\n        if not jtu.test_device_matches(['tpu']) or jtu.get_tpu_version() < 5:\n            self.skipTest('Not supported on this hardware')\n        if not jtu.if_cloud_tpu_at_least(2025, 3, 8):\n            self.skipTest('Test requires libtpu from 2025/3/8 or later')\n    if from_dtype == to_dtype:\n        self.skipTest('Unnecessary test')\n    if jtu.is_device_tpu(version=4):\n        if to_dtype in {'int8', 'uint8', 'int4', 'uint4'}:\n            self.skipTest('Not supported on this TPU generation')\n        if to_dtype in {'int16', 'uint16'} and (not jtu.if_cloud_tpu_at_least(2025, 1, 18)):\n            self.skipTest('Test requires libtpu from 2025/1/18 or later')\n    if jtu.test_device_matches(['tpu']) and jtu.get_tpu_version() < 4:\n        if to_dtype not in {'int32', 'uint32', 'float32', 'bfloat16'}:\n            self.skipTest('Not supported on this TPU generation')\n    if jtu.test_device_matches(['gpu']) and to_dtype in {'int4', 'uint4'}:\n        self.skipTest('int4/uint4 casts are buggy on GPU')\n    elements = dict(allow_nan=not jnp.issubdtype(to_dtype, jnp.integer))\n    x = data.draw(hnp.arrays(from_dtype, (8, 128), elements=elements))\n    x = jnp.asarray(x)\n\n    def kernel(x_ref, y_ref):\n        x = x_ref[...]\n        y = x.astype(to_dtype)\n        if to_dtype == jnp.bool:\n            y = y.astype(jnp.int32)\n        y_ref[...] = y\n    y_dtype = jnp.int32 if to_dtype == jnp.bool else to_dtype\n    try:\n        y = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, y_dtype))(x)\n    except Exception as e:\n        if 'Unsupported cast' in e.args[0]:\n            self.skipTest('Unsupported cast')\n        raise\n    if to_dtype == jnp.bool:\n        y = y.astype(jnp.bool)\n    y_ref = x.astype(to_dtype)\n    if jnp.dtype(to_dtype) in map(jnp.dtype, (jnp.bfloat16, jnp.int4, jnp.uint4)):\n        y, y_ref = (y.astype(np.float32), y_ref.astype(np.float32))\n    np.testing.assert_allclose(y, y_ref, atol=0.0, rtol=0.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.product(from_dtype=_DTYPES_SUB_32BIT, to_dtype=_DTYPES, randomize=(False, True))\ndef test_cast_from_sub_32bit(self, from_dtype, to_dtype, randomize):\n    self.skip_if_mosaic_gpu()\n    if from_dtype == to_dtype:\n        self.skipTest('Unnecessary test')\n    if jtu.is_device_tpu(version=4):\n        allowed_v4_cats = {('int16', 'int32'): (2025, 1, 18)}\n        if (from_dtype in {'int16', 'int8', 'uint16', 'uint8', 'int4', 'uint4'} or to_dtype in {'int8', 'uint8', 'int4', 'uint4'}) and (from_dtype, to_dtype) not in allowed_v4_cats:\n            self.skipTest('Not supported on this TPU generation')\n        if (minimum_libtpu_date := allowed_v4_cats.get((from_dtype, to_dtype), None)):\n            if not jtu.if_cloud_tpu_at_least(*minimum_libtpu_date):\n                self.skipTest('Test requires a newer libtpu')\n        if to_dtype in {'int16', 'uint16'} and (not jtu.if_cloud_tpu_at_least(2025, 1, 18)):\n            self.skipTest('Test requires libtpu from 2025/1/18 or later')\n    if jtu.test_device_matches(['tpu']) and jtu.get_tpu_version() < 4:\n        self.skipTest('Not supported on this TPU generation')\n    if jtu.test_device_matches(['gpu']) and to_dtype in {'int4', 'uint4'}:\n        self.skipTest('int4/uint4 casts are buggy on GPU')\n    if from_dtype in {'float8_e4m3b11fnuz', 'float8_e5m2', 'float8_e4m3fn'} or to_dtype in {'float8_e4m3b11fnuz', 'float8_e5m2', 'float8_e4m3fn'}:\n        if not jtu.test_device_matches(['tpu']) or jtu.get_tpu_version() < 5:\n            self.skipTest('Not supported on this hardware')\n        if not jtu.if_cloud_tpu_at_least(2025, 3, 9):\n            self.skipTest('Test requires libtpu from 2025/3/9 or later')\n    from_int = np.issubdtype(np.dtype(from_dtype), np.integer)\n    to_int = np.issubdtype(np.dtype(to_dtype), np.integer)\n    if from_int and to_int and (np.dtype(from_dtype).itemsize != 4) and (not jtu.if_cloud_tpu_at_least(2025, 1, 12)):\n        self.skipTest('trunc from non-32 bit only implemented recently')\n    if from_dtype == 'bool' and to_dtype in {'int16', 'int8', 'int4', 'uint16', 'uint8', 'uint4'}:\n        self.skipTest('Not supported: cannot extend to sub-32 bit types')\n\n    def bitwidth(dtype):\n        if jnp.issubdtype(dtype, jnp.integer):\n            return jnp.iinfo(dtype).bits\n        elif jnp.issubdtype(dtype, jnp.floating):\n            return jnp.finfo(dtype).bits\n        else:\n            raise ValueError(f'Unsupported dtype: {dtype}')\n    if from_dtype != 'bool':\n        from_bitwidth = bitwidth(from_dtype)\n        from_int_dtype = getattr(jnp, 'uint' + str(from_bitwidth))\n        if randomize:\n            shape = (128, 128)\n            rand_int_dtype = getattr(jnp, 'uint' + str(max(8, from_bitwidth)))\n            x = random.randint(random.key(1234), shape, 0, 1 << from_bitwidth, rand_int_dtype).astype(from_int_dtype)\n            x = lax.bitcast_convert_type(x, from_dtype)\n        else:\n            x = jax.lax.bitcast_convert_type(jnp.arange(1 << from_bitwidth, dtype=from_int_dtype), from_dtype).reshape(8, -1)\n    elif randomize:\n        x = random.randint(random.key(234), (16, 16), 0, 1, jnp.int32) != 0\n    else:\n        x = jnp.tile(jnp.asarray([[False, True], [True, False]], dtype='bool'), (8, 8))\n    assert x.dtype == jnp.dtype(from_dtype)\n    if jnp.issubdtype(from_dtype, jnp.floating):\n        x = x.at[jnp.isnan(x)].set(0)\n    if from_dtype == jnp.bool:\n        x = x.astype(jnp.int32)\n\n    def kernel(x_ref, y_ref):\n        x = x_ref[...]\n        if from_dtype == jnp.bool:\n            x = x.astype(jnp.bool)\n        y = x.astype(to_dtype)\n        if to_dtype == jnp.bool:\n            y = y.astype(jnp.int32)\n        y_ref[...] = y\n    y_dtype = jnp.int32 if to_dtype == jnp.bool else to_dtype\n    try:\n        y = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, y_dtype))(x)\n    except Exception as e:\n        if 'Unsupported cast' in e.args[0]:\n            self.skipTest('Unsupported cast')\n        raise\n    if to_dtype == jnp.bool:\n        y = y.astype(jnp.bool)\n    y_ref = x.astype(to_dtype)\n    if jnp.dtype(to_dtype) in map(jnp.dtype, (jnp.bfloat16, jnp.int4, jnp.uint4)):\n        y, y_ref = (y.astype(np.float32), y_ref.astype(np.float32))\n    np.testing.assert_allclose(y, y_ref, atol=0.0, rtol=0.0)",
    "assertions": [
      "assert x.dtype == jnp.dtype(from_dtype)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_abs_weak_type(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.abs(x_ref[...])\n    x = jnp.broadcast_to(-3.2, (4, 4))\n    np.testing.assert_allclose(kernel(x), jnp.abs(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(('float32', 'int32'), ('float64', 'int32'), ('float32', 'float32'), ('float64', 'float64'))\ndef test_pow(self, x_dtype, y_dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), x_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = lax.pow(x_ref[...], y_ref[...])\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    x = jnp.array([1, 2, 3, 4]).astype(x_dtype)\n    y = jnp.array([1, 2, 3, 4]).astype(y_dtype)\n    np.testing.assert_allclose(kernel(x, y), lax.pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(0, 1, 2, 3, 4, 5, -1, -2, -3)\ndef test_integer_pow(self, y):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = lax.integer_pow(x_ref[...], y)\n    x = jnp.array([1, 2, 3, 4]).astype(jnp.float32) / 10\n    np.testing.assert_allclose(kernel(x), lax.integer_pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_isnan(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def isnan(x_ref, o_ref):\n        o_ref[:] = jnp.isnan(x_ref[...])\n    x = jnp.arange(8.0)\n    x = x.at[3].set(jnp.nan)\n    np.testing.assert_allclose(isnan(x), jnp.isnan(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(('int32', 'float32'), ('float32', 'float32'), ('bfloat16', 'bfloat16'))\ndef test_true_divide(self, dtype, out_dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        if out_dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest('bfloat16 is not supported on older TPU generations')\n        if not jtu.if_cloud_tpu_at_least(2025, 1, 9):\n            self.skipTest('Requires libtpu built after 2025-01-09')\n    elif jtu.test_device_matches(['gpu']):\n        if dtype == 'bfloat16':\n            self.skipTest('bfloat16 not supported')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), out_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    x = jnp.repeat(x, 8, axis=0).reshape(8, 8)\n    y = jnp.tile(y, 8).reshape(8, 8)\n    rtol = 0.008 if dtype == 'bfloat16' else 1e-06\n    np.testing.assert_allclose(jnp.true_divide(x, y).astype(jnp.float32), kernel(x, y).astype(jnp.float32), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16', 'float32')\ndef test_approx_tanh(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    if self.INTERPRET:\n        self.skipTest('approx_tanh is not supported in interpret mode')\n    if dtype == 'bfloat16' and (not jtu.is_cuda_compute_capability_at_least('9.0')):\n        self.skipTest('tanh.approx.bf16 requires a GPU with capability >= sm90')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = plgpu.approx_tanh(x_ref[...])\n    x = jnp.asarray([-1, 0.42, 0.24, 1]).astype(dtype)\n    np.testing.assert_allclose(kernel(x).astype(jnp.float32), jnp.tanh(x).astype(jnp.float32), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_elementwise_inline_asm(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: elementwise_inline_asm_p')\n    if self.INTERPRET:\n        self.skipTest('elementwise_inline_asm is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((256,), jnp.float16))\n    def kernel(x_ref, o_ref):\n        [o_ref[...]] = plgpu.elementwise_inline_asm('tanh.approx.f16x2 $0, $1;', args=[x_ref[...]], constraints='=r,r', pack=2, result_shape_dtypes=[jax.ShapeDtypeStruct(x_ref.shape, x_ref.dtype)])\n    x = jnp.arange(256).astype(jnp.float16)\n    np.testing.assert_allclose(kernel(x), jnp.tanh(x), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_where_broadcasting(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 2, 2), floatx))\n    def copyitem(x_ref, in_idx_ref, out_idx_ref, o_ref):\n        mask = (jnp.arange(o_ref.shape[0]) == out_idx_ref[()])[:, None, None]\n        o_ref[...] = jnp.where(mask, x_ref[in_idx_ref[()]], 0)\n    x = jnp.arange(7 * 2 * 2.0).reshape(7, 2, 2)\n    for ii in range(7):\n        for oi in range(4):\n            out = copyitem(x, ii, oi)\n            self.assertEqual((4, 2, 2), out.shape)\n            np.testing.assert_allclose(out[:oi], jnp.zeros_like(out[:oi]))\n            np.testing.assert_allclose(out[oi], x[ii])\n            np.testing.assert_allclose(out[oi + 1:], jnp.zeros_like(out[oi + 1:]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.product(lhs_and_rhs_shape=[((16, 16), (16, 16)), ((32, 32), (32, 32)), ((64, 64), (64, 64)), ((128, 128), (128, 128)), ((256, 256), (256, 256)), ((8, 128), (128, 256)), ((8, 128), (256, 128)), ((8, 256), (256, 128)), ((16, 128), (128, 256)), ((16, 128), (256, 128)), ((16, 256), (256, 128)), ((24, 128), (128, 256)), ((24, 128), (256, 128)), ((24, 256), (256, 128)), ((128, 8), (128, 256)), ((128, 8), (256, 128)), ((256, 8), (256, 128)), ((128, 16), (128, 256)), ((128, 16), (256, 128)), ((256, 16), (256, 128)), ((128, 24), (128, 256)), ((128, 24), (256, 128)), ((256, 24), (256, 128))], dtype=[jnp.float32, jnp.float16, jnp.bfloat16], trans_x=[False, True], trans_y=[False, True])\ndef test_dot(self, lhs_and_rhs_shape, dtype, trans_x, trans_y):\n    self.skip_if_mosaic_gpu()\n    if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n        self.skipTest('Requires libtpu built after 2024-12-19')\n    lhs_shape, rhs_shape = lhs_and_rhs_shape\n    final_lhs_shape = lhs_shape[::-1] if trans_x else lhs_shape\n    final_rhs_shape = rhs_shape[::-1] if trans_y else rhs_shape\n    if final_lhs_shape[1] != final_rhs_shape[0]:\n        self.skipTest('Contraction dimensions do not match')\n    out_shape = (final_lhs_shape[0], final_rhs_shape[1])\n    if jtu.test_device_matches(['tpu']):\n        if dtype == jnp.float16:\n            self.skipTest('float16 type is not supported on TPU')\n        if dtype == jnp.bfloat16 and (not jtu.is_device_tpu_at_least(4)):\n            self.skipTest('bfloat16 matmul is supported on TPUv4+')\n        if trans_x:\n            self.skipTest('Not implemented: Transposed LHS')\n    if jtu.test_device_matches(['gpu']):\n        if dtype == jnp.bfloat16:\n            self.skipTest('bfloat16 type are not supported on GPU')\n        if math.prod(lhs_shape) + math.prod(rhs_shape) + math.prod(out_shape) > 256 * 256 * 2:\n            self.skipTest('Shared memory size limit exceeded')\n        if min(*lhs_shape, *rhs_shape) < 16:\n            self.skipTest('All dimensions of lhs and rhs must be >= 16')\n        if any((not is_power_of_two(x) for x in lhs_shape + rhs_shape)):\n            self.skipTest('All dimensions of lhs and rhs must be power of two')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, dtype))\n    def dot(x_ref, y_ref, o_ref):\n        x = x_ref[:, :]\n        y = y_ref[:, :]\n        o_ref[:, :] = pl.dot(x, y, trans_x, trans_y).astype(o_ref.dtype)\n    k1, k2 = random.split(random.key(0))\n    x = random.normal(k1, lhs_shape, dtype=dtype)\n    y = random.normal(k2, rhs_shape, dtype=dtype)\n    out = dot(x, y)\n    expected = jnp.dot(x.T if trans_x else x, y.T if trans_y else y, preferred_element_type=jnp.float32).astype(dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected.astype(jnp.float32), atol=0.05, rtol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.product(size=[1, 2, 64, 129, 1021], block_size=[1, 2, 32, 64, 128])\ndef test_masked_load_store(self, size, block_size):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), floatx), grid=pl.cdiv(size, block_size))\n    def kernel(x_ref, o_ref):\n        idx = pl.program_id(0) * block_size + jnp.arange(block_size, dtype=jnp.int32)\n        mask = idx < x_ref.shape[0]\n        x = pl.load(x_ref, (idx,), mask=mask)\n        pl.store(o_ref, (idx,), x + 1.0, mask=mask)\n    key = random.key(0)\n    x = random.normal(key, (size,))\n    np.testing.assert_allclose(kernel(x), x + 1.0, atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "def test_broadcasted_load_store(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Unimplemented primitive: broadcast_to')\n    m, n = (16, 32)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), floatx))\n    def load(x_ref, o_ref):\n        x = pl.load(x_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]))\n        pl.store(o_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]), x + 1.0)\n    key = random.key(0)\n    x = random.normal(key, (m, n))\n    np.testing.assert_allclose(load(x), x + 1.0, atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.named_parameters(('add_i32', pl.atomic_add, np.array([1, 2, 3, 4], np.int32), np.sum), ('max_i', pl.atomic_max, np.array([1, 2, 3, 4], np.int32), np.max), ('min_i32', pl.atomic_min, np.array([1, 2, 3, 4], np.int32), np.min), ('add_f16', pl.atomic_add, np.array([1, 2, 3, 4], np.float16), np.sum), ('add_f32', pl.atomic_add, np.array([1, 2, 3, 4], np.float32), np.sum), ('max_f32', pl.atomic_max, np.array([1, 2, 3, 4], np.float32), np.max), ('min_f32', pl.atomic_min, np.array([1, 2, 3, 4], np.float32), np.min))\ndef test_scalar_atomic(self, op, value, numpy_op):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), value.dtype), grid=value.shape[0], input_output_aliases={1: 0})\n    def atomic_kernel(x_ref, _, o_ref):\n        pid = pl.program_id(axis=0)\n        op(o_ref, (), x_ref[pid])\n    if op == pl.atomic_add:\n        neutral = np.array(0, dtype=value.dtype)\n    elif op == pl.atomic_max:\n        if np.issubdtype(value.dtype, np.integer):\n            neutral = np.array(np.iinfo(value.dtype).min, value.dtype)\n        else:\n            neutral = np.array(-float('inf'), value.dtype)\n    elif op == pl.atomic_min:\n        if np.issubdtype(value.dtype, np.integer):\n            neutral = np.array(np.iinfo(value.dtype).max, value.dtype)\n        else:\n            neutral = np.array(float('inf'), value.dtype)\n    elif op == pl.atomic_or:\n        neutral = np.array(False, value.dtype)\n    else:\n        raise NotImplementedError()\n    out = atomic_kernel(value, neutral)\n    np.testing.assert_allclose(out, numpy_op(value))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters((0,), (1,))\ndef test_array_atomic_add(self, axis):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Unimplemented primitive: broadcast_to')\n    m, n = (32, 8)\n    if axis == 0:\n        grid = m\n    else:\n        grid = n\n    out_shape = jax.ShapeDtypeStruct((n if axis == 0 else m,), floatx)\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid, input_output_aliases={1: 0})\n    def reduce(x_ref, _, y_ref):\n        i = pl.program_id(axis=0)\n        if axis == 0:\n            idx = (i, jnp.arange(n))\n        else:\n            idx = (jnp.arange(m), i)\n        x = pl.load(x_ref, idx)\n        pl.atomic_add(y_ref, (jnp.arange(y.shape[0]),), x)\n    x = random.normal(random.key(0), (m, n))\n    y = jnp.zeros(out_shape.shape, out_shape.dtype)\n    y = reduce(x, y)\n    y_ref = np.sum(x, axis=axis)\n    np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters((0, 0, 1), (0, 1, 1), (1, 0, 1), (1, 1, 1), (2, 1, 1), (2, 1, 1))\ndef test_atomic_cas(self, init_value, cmp, new_value):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    if jax.config.x64_enabled and jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU in 64-bit mode')\n\n    @functools.partial(self.pallas_call, out_shape=(jax.ShapeDtypeStruct((), intx), jax.ShapeDtypeStruct((), intx)), input_output_aliases={0: 0})\n    def swap(_, lock_ref, out_ref):\n        out_ref[()] = pl.atomic_cas(lock_ref, cmp, new_value)\n    lock, out = swap(init_value)\n    np.testing.assert_allclose(lock, new_value if cmp == init_value else init_value)\n    np.testing.assert_allclose(out, init_value)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(1, 2, 3, 4, 8)\ndef test_atomic_counter(self, num_threads):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    if self.INTERPRET:\n        self.skipTest('While loop not supported in interpret mode.')\n    if jax.config.x64_enabled and jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU in 64-bit mode')\n\n    @functools.partial(self.pallas_call, out_shape=(jax.ShapeDtypeStruct((), intx), jax.ShapeDtypeStruct((), intx)), input_output_aliases={0: 0, 1: 1}, grid=(num_threads,))\n    def increment(_, __, lock_ref, counter_ref):\n\n        def _cond(_):\n            return pl.atomic_cas(lock_ref, 0, 1) == 1\n        lax.while_loop(_cond, lambda a: a, 0)\n        counter_ref[...] += 1\n        pl.atomic_xchg(lock_ref, (), 0)\n    lock, count = increment(0, 0)\n    np.testing.assert_allclose(lock, 0)\n    np.testing.assert_allclose(count, num_threads)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.parameters(False, True)\ndef test_reduce_only_dim(self, use_store):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    m = 32\n    x = random.normal(random.key(0), (m,), dtype=jnp.float32)\n    out_shape = jax.ShapeDtypeStruct((), x.dtype)\n\n    @functools.partial(self.pallas_call, out_shape=out_shape)\n    def reduce(x_ref, y_ref):\n        x = pl.load(x_ref, (jnp.arange(m),))\n        y = jnp.sum(x, axis=-1)\n        if use_store:\n            pl.store(y_ref, (), y)\n        else:\n            y_ref[...] = y\n    y = reduce(x)\n    y_ref = jnp.sum(x, axis=-1)\n    np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.product(axis=[0, 1], dtype=['float16', 'float32', 'int32', 'uint32'])\ndef test_cumsum(self, dtype, axis):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    m, n = (32, 8)\n    out_dtype = dtype\n\n    def make_x(key):\n        if jnp.issubdtype(dtype, jnp.integer):\n            return random.permutation(key, jnp.arange(m * n, dtype=dtype), independent=True).reshape(m, n)\n        else:\n            return random.normal(key, (m, n), dtype=dtype)\n    out_shape = jax.ShapeDtypeStruct((m, n), out_dtype)\n    grid = ()\n\n    @functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\n    def reduce(x_ref, y_ref):\n        x = x_ref[...]\n        y_ref[...] = jnp.cumsum(x, axis=axis)\n    for i, key in enumerate(random.split(random.key(0), 20)):\n        x = make_x(key)\n        y = reduce(x)\n        y_ref = jnp.cumsum(x, axis=axis)\n        np.testing.assert_allclose(y, y_ref, atol=0.01, rtol=0.01, err_msg=i)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def assert_allclose(self, out: jnp.ndarray, expected_out: jnp.ndarray, *, atol: float=1e-05, rtol: float=1e-05):\n    self.assertEqual(out.dtype, expected_out.dtype)\n    np.testing.assert_allclose(out.astype(jnp.float32), expected_out.astype(jnp.float32), atol=atol, rtol=rtol)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__}_{value}', dtype, value) for dtypes, values in (((jnp.uint16, jnp.uint32, jnp.uint64), (0, 5)), ((jnp.int16, jnp.int32, jnp.int64), (-3, 0, 5)), ((jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64), (-3.2, -0.0, 0.0, 5.1, jnp.nan, jnp.inf, -jnp.inf))) for dtype in dtypes for value in values))\ndef test_sign(self, dtype, value):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sign(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=dtype)\n    out = kernel(x)\n    expected = jnp.sign(x)\n    np.testing.assert_array_equal(out.astype(jnp.float32), expected.astype(jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.bfloat16, jnp.int32)\ndef test_add_constant(self, dtype):\n    self.skip_if_mosaic_gpu()\n    shape = (256, 256)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...] + 1\n    np.testing.assert_array_equal(kernel(jnp.zeros(shape, dtype=dtype)), jnp.ones(shape, dtype=dtype))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.parameters(-3.2, -1.0, -0.999517, -0.4, 0.0, 0.72, 0.999517, 1.0, 2.4)\ndef test_erf_inv(self, value):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.erf_inv(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=floatx)\n    out = kernel(x)\n    expected = lax.erf_inv(x)\n    np.testing.assert_array_equal(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "def test_is_finite(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.is_finite(x_ref[...])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "def test_is_finite_scalar(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = jnp.isfinite(x_ref[i])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "def test_abs_weak_type(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.abs(x_ref[...])\n    x = jnp.broadcast_to(-3.2, (4, 4))\n    np.testing.assert_allclose(kernel(x), jnp.abs(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.parameters(('float32', 'int32'), ('float64', 'int32'), ('float32', 'float32'), ('float64', 'float64'))\ndef test_pow(self, x_dtype, y_dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), x_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = lax.pow(x_ref[...], y_ref[...])\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    x = jnp.array([1, 2, 3, 4]).astype(x_dtype)\n    y = jnp.array([1, 2, 3, 4]).astype(y_dtype)\n    np.testing.assert_allclose(kernel(x, y), lax.pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.parameters(0, 1, 2, 3, 4, 5, -1, -2, -3)\ndef test_integer_pow(self, y):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = lax.integer_pow(x_ref[...], y)\n    x = jnp.array([1, 2, 3, 4]).astype(jnp.float32) / 10\n    np.testing.assert_allclose(kernel(x), lax.integer_pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__} (x={x!r}, y={y!r})', dtype, x, y) for dtype, x, y in itertools.product((jnp.float32, jnp.float64), _NEXTAFTER_VALUES, _NEXTAFTER_VALUES)))\ndef test_nextafter(self, dtype, x, y):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.nextafter(x_ref[...], y_ref[...])\n    x = jnp.full((4,), x, dtype=dtype)\n    y = jnp.full((4,), y, dtype=dtype)\n    out = kernel(x, y)\n    expected = jnp.nextafter(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.parameters(('int32', 'float32'), ('float32', 'float32'), ('bfloat16', 'bfloat16'))\ndef test_true_divide(self, dtype, out_dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        if out_dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest('bfloat16 is not supported on older TPU generations')\n        if not jtu.if_cloud_tpu_at_least(2025, 1, 9):\n            self.skipTest('Requires libtpu built after 2025-01-09')\n    elif jtu.test_device_matches(['gpu']):\n        if dtype == 'bfloat16':\n            self.skipTest('bfloat16 not supported')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), out_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    x = jnp.repeat(x, 8, axis=0).reshape(8, 8)\n    y = jnp.tile(y, 8).reshape(8, 8)\n    rtol = 0.008 if dtype == 'bfloat16' else 1e-06\n    np.testing.assert_allclose(jnp.true_divide(x, y).astype(jnp.float32), kernel(x, y).astype(jnp.float32), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16')\ndef test_true_divide_unsupported(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('No lowering in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([2.4, 4.2]).astype(dtype)\n    y = jnp.array([4.2, 2.4]).astype(dtype)\n    with self.assertRaises(Exception):\n        kernel(x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16', 'float32')\ndef test_approx_tanh(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    if self.INTERPRET:\n        self.skipTest('approx_tanh is not supported in interpret mode')\n    if dtype == 'bfloat16' and (not jtu.is_cuda_compute_capability_at_least('9.0')):\n        self.skipTest('tanh.approx.bf16 requires a GPU with capability >= sm90')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = plgpu.approx_tanh(x_ref[...])\n    x = jnp.asarray([-1, 0.42, 0.24, 1]).astype(dtype)\n    np.testing.assert_allclose(kernel(x).astype(jnp.float32), jnp.tanh(x).astype(jnp.float32), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "def test_elementwise_inline_asm(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: elementwise_inline_asm_p')\n    if self.INTERPRET:\n        self.skipTest('elementwise_inline_asm is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((256,), jnp.float16))\n    def kernel(x_ref, o_ref):\n        [o_ref[...]] = plgpu.elementwise_inline_asm('tanh.approx.f16x2 $0, $1;', args=[x_ref[...]], constraints='=r,r', pack=2, result_shape_dtypes=[jax.ShapeDtypeStruct(x_ref.shape, x_ref.dtype)])\n    x = jnp.arange(256).astype(jnp.float16)\n    np.testing.assert_allclose(kernel(x), jnp.tanh(x), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "def test_debug_barrier(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: debug_barrier_p')\n    if self.INTERPRET:\n        self.skipTest('debug_barrier is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n        plgpu.debug_barrier()\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    np.testing.assert_array_equal(kernel(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('It works!')\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('It works!', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print_with_values(self):\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('x[0] =', x_ref[0])\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x[0] = 4.2', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "def test_num_programs(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((4,), intx), grid=4)\n    def kernel(o_ref):\n        o_ref[pl.program_id(0)] = pl.num_programs(0)\n    np.testing.assert_array_equal(kernel(), jnp.array([4, 4, 4, 4], dtype=intx))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.product(size=[1, 2, 64, 129, 1021], block_size=[1, 2, 32, 64, 128])\ndef test_masked_load_store(self, size, block_size):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), floatx), grid=pl.cdiv(size, block_size))\n    def kernel(x_ref, o_ref):\n        idx = pl.program_id(0) * block_size + jnp.arange(block_size, dtype=jnp.int32)\n        mask = idx < x_ref.shape[0]\n        x = pl.load(x_ref, (idx,), mask=mask)\n        pl.store(o_ref, (idx,), x + 1.0, mask=mask)\n    key = random.key(0)\n    x = random.normal(key, (size,))\n    np.testing.assert_allclose(kernel(x), x + 1.0, atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "def test_strided_load(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[::4]\n    x = jnp.arange(64, dtype=jnp.float32).reshape((16, 4))\n    np.testing.assert_array_equal(kernel(x), x[::4])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.parameters(((16, 32), (16,)), ((16, 32), (32,)), ((16, 32), (16, 16)))\ndef test_invalid_broadcasted_load(self, x_shape, mask_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    if self.INTERPRET:\n        self.skipTest('No broadcasting checks in pl.load in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def kernel(x_ref, mask_ref, o_ref):\n        del o_ref\n        pl.load(x_ref, slice(None), mask=mask_ref[:])\n    x = jnp.ones(x_shape, dtype=jnp.float32)\n    mask = jnp.ones(mask_shape, dtype=jnp.bool_)\n    try:\n        kernel(x, mask)\n    except Exception as e:\n        self.assertIn('Cannot broadcast', str(e.__cause__))\n    else:\n        self.fail('Expected exception due to invalid broadcasting')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "def test_debug_print(self):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        jax.debug.print('x = {}', x_ref)\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x = [4.2 2.4]', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x, out):\n    mask = x[...] != 0\n    concated_mask = jnp.concatenate([mask, mask], axis=0)\n    concated_x = jnp.concatenate([x[:], x[:]], axis=0)\n    out[:] = lax.select(concated_mask, concated_x, jnp.zeros_like(concated_x))"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((name, name, func, strategy) for name, func, strategy in UNARY_FUNCTIONS))\n@hp.given(hps.data())\ndef test_unary_primitives(self, name, func, shape_dtype_strategy, data):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('This hypothesis test is slow, even more so in interpret mode.')\n    tol = 0.0\n    if jtu.test_device_matches(['gpu']):\n        if func == jnp.round or func == jnp.rint:\n            self.skipTest('TODO: not implemented on GPU')\n        if name == 'tanh':\n            tol = 1e-06\n        elif name == 'exp2':\n            tol = 1e-06\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = func(x_ref[...])\n    x_shape_dtype = data.draw(shape_dtype_strategy)\n    key = random.key(0)\n    x = _random_value(key, x_shape_dtype)\n    out = self.pallas_call(kernel, out_shape=x_shape_dtype)(x)\n    self.assertAllClose(out, func(x), atol=tol, rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def func():\n\n    def dma_kernel(x, y):\n\n        def body(dma_sem, sem):\n            pltpu.async_copy(x, y, dma_sem).wait()\n            pltpu.semaphore_signal(sem)\n            pltpu.semaphore_wait(sem)\n        pl.run_scoped(body, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.REGULAR)\n    x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n    y = pl.pallas_call(dma_kernel, out_shape=x)(x)\n    return jnp.array_equal(x, y).astype(jnp.float32)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__}_{value}', dtype, value) for dtypes, values in (((jnp.uint16, jnp.uint32, jnp.uint64), (0, 5)), ((jnp.int16, jnp.int32, jnp.int64), (-3, 0, 5)), ((jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64), (-3.2, -0.0, 0.0, 5.1, jnp.nan, jnp.inf, -jnp.inf))) for dtype in dtypes for value in values))\ndef test_sign(self, dtype, value):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sign(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=dtype)\n    out = kernel(x)\n    expected = jnp.sign(x)\n    np.testing.assert_array_equal(out.astype(jnp.float32), expected.astype(jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.bfloat16, jnp.int32)\ndef test_add_constant(self, dtype):\n    self.skip_if_mosaic_gpu()\n    shape = (256, 256)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...] + 1\n    np.testing.assert_array_equal(kernel(jnp.zeros(shape, dtype=dtype)), jnp.ones(shape, dtype=dtype))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.parameters(-3.2, -1.0, -0.999517, -0.4, 0.0, 0.72, 0.999517, 1.0, 2.4)\ndef test_erf_inv(self, value):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.erf_inv(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=floatx)\n    out = kernel(x)\n    expected = lax.erf_inv(x)\n    np.testing.assert_array_equal(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "def test_is_finite(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.is_finite(x_ref[...])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "def test_is_finite_scalar(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = jnp.isfinite(x_ref[i])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "def test_abs_weak_type(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.abs(x_ref[...])\n    x = jnp.broadcast_to(-3.2, (4, 4))\n    np.testing.assert_allclose(kernel(x), jnp.abs(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.parameters(('float32', 'int32'), ('float64', 'int32'), ('float32', 'float32'), ('float64', 'float64'))\ndef test_pow(self, x_dtype, y_dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), x_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = lax.pow(x_ref[...], y_ref[...])\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    x = jnp.array([1, 2, 3, 4]).astype(x_dtype)\n    y = jnp.array([1, 2, 3, 4]).astype(y_dtype)\n    np.testing.assert_allclose(kernel(x, y), lax.pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.parameters(0, 1, 2, 3, 4, 5, -1, -2, -3)\ndef test_integer_pow(self, y):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = lax.integer_pow(x_ref[...], y)\n    x = jnp.array([1, 2, 3, 4]).astype(jnp.float32) / 10\n    np.testing.assert_allclose(kernel(x), lax.integer_pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__} (x={x!r}, y={y!r})', dtype, x, y) for dtype, x, y in itertools.product((jnp.float32, jnp.float64), _NEXTAFTER_VALUES, _NEXTAFTER_VALUES)))\ndef test_nextafter(self, dtype, x, y):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.nextafter(x_ref[...], y_ref[...])\n    x = jnp.full((4,), x, dtype=dtype)\n    y = jnp.full((4,), y, dtype=dtype)\n    out = kernel(x, y)\n    expected = jnp.nextafter(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.parameters(('int32', 'float32'), ('float32', 'float32'), ('bfloat16', 'bfloat16'))\ndef test_true_divide(self, dtype, out_dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        if out_dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest('bfloat16 is not supported on older TPU generations')\n        if not jtu.if_cloud_tpu_at_least(2025, 1, 9):\n            self.skipTest('Requires libtpu built after 2025-01-09')\n    elif jtu.test_device_matches(['gpu']):\n        if dtype == 'bfloat16':\n            self.skipTest('bfloat16 not supported')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), out_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    x = jnp.repeat(x, 8, axis=0).reshape(8, 8)\n    y = jnp.tile(y, 8).reshape(8, 8)\n    rtol = 0.008 if dtype == 'bfloat16' else 1e-06\n    np.testing.assert_allclose(jnp.true_divide(x, y).astype(jnp.float32), kernel(x, y).astype(jnp.float32), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16')\ndef test_true_divide_unsupported(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('No lowering in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([2.4, 4.2]).astype(dtype)\n    y = jnp.array([4.2, 2.4]).astype(dtype)\n    with self.assertRaises(Exception):\n        kernel(x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16', 'float32')\ndef test_approx_tanh(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    if self.INTERPRET:\n        self.skipTest('approx_tanh is not supported in interpret mode')\n    if dtype == 'bfloat16' and (not jtu.is_cuda_compute_capability_at_least('9.0')):\n        self.skipTest('tanh.approx.bf16 requires a GPU with capability >= sm90')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = plgpu.approx_tanh(x_ref[...])\n    x = jnp.asarray([-1, 0.42, 0.24, 1]).astype(dtype)\n    np.testing.assert_allclose(kernel(x).astype(jnp.float32), jnp.tanh(x).astype(jnp.float32), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "def test_elementwise_inline_asm(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: elementwise_inline_asm_p')\n    if self.INTERPRET:\n        self.skipTest('elementwise_inline_asm is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((256,), jnp.float16))\n    def kernel(x_ref, o_ref):\n        [o_ref[...]] = plgpu.elementwise_inline_asm('tanh.approx.f16x2 $0, $1;', args=[x_ref[...]], constraints='=r,r', pack=2, result_shape_dtypes=[jax.ShapeDtypeStruct(x_ref.shape, x_ref.dtype)])\n    x = jnp.arange(256).astype(jnp.float16)\n    np.testing.assert_allclose(kernel(x), jnp.tanh(x), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "def test_debug_barrier(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: debug_barrier_p')\n    if self.INTERPRET:\n        self.skipTest('debug_barrier is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n        plgpu.debug_barrier()\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    np.testing.assert_array_equal(kernel(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('It works!')\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('It works!', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print_with_values(self):\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('x[0] =', x_ref[0])\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x[0] = 4.2', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "def test_num_programs(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((4,), intx), grid=4)\n    def kernel(o_ref):\n        o_ref[pl.program_id(0)] = pl.num_programs(0)\n    np.testing.assert_array_equal(kernel(), jnp.array([4, 4, 4, 4], dtype=intx))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.product(size=[1, 2, 64, 129, 1021], block_size=[1, 2, 32, 64, 128])\ndef test_masked_load_store(self, size, block_size):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), floatx), grid=pl.cdiv(size, block_size))\n    def kernel(x_ref, o_ref):\n        idx = pl.program_id(0) * block_size + jnp.arange(block_size, dtype=jnp.int32)\n        mask = idx < x_ref.shape[0]\n        x = pl.load(x_ref, (idx,), mask=mask)\n        pl.store(o_ref, (idx,), x + 1.0, mask=mask)\n    key = random.key(0)\n    x = random.normal(key, (size,))\n    np.testing.assert_allclose(kernel(x), x + 1.0, atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "def test_strided_load(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[::4]\n    x = jnp.arange(64, dtype=jnp.float32).reshape((16, 4))\n    np.testing.assert_array_equal(kernel(x), x[::4])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "@parameterized.parameters(((16, 32), (16,)), ((16, 32), (32,)), ((16, 32), (16, 16)))\ndef test_invalid_broadcasted_load(self, x_shape, mask_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    if self.INTERPRET:\n        self.skipTest('No broadcasting checks in pl.load in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def kernel(x_ref, mask_ref, o_ref):\n        del o_ref\n        pl.load(x_ref, slice(None), mask=mask_ref[:])\n    x = jnp.ones(x_shape, dtype=jnp.float32)\n    mask = jnp.ones(mask_shape, dtype=jnp.bool_)\n    try:\n        kernel(x, mask)\n    except Exception as e:\n        self.assertIn('Cannot broadcast', str(e.__cause__))\n    else:\n        self.fail('Expected exception due to invalid broadcasting')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "def test_debug_print(self):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        jax.debug.print('x = {}', x_ref)\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x = [4.2 2.4]', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(src_dst_ids_ref, x_ref, o_ref, send_sem, recv_sem):\n    barrier_sem = pltpu.get_barrier_semaphore()\n\n    @functools.partial(jax.lax.fori_loop, 0, num_devices, init_val=None)\n    def _(i, _):\n        pltpu.semaphore_signal(barrier_sem, inc=1, device_id=(jnp.int32(i),), device_id_type=pltpu.DeviceIdType.MESH)\n        return None\n    pltpu.semaphore_wait(barrier_sem, num_devices)\n    my_id = lax.axis_index('x')\n    src_dst_ids = src_dst_ids_ref[:]\n    recv_count = 0\n    for i in range(src_dst_ids.shape[0]):\n        src_id = src_dst_ids[i, 0]\n        dst_id = src_dst_ids[i, 1]\n\n        @pl.when(src_id == my_id)\n        def _():\n            dma = pltpu.make_async_remote_copy(src_ref=x_ref, dst_ref=o_ref, send_sem=send_sem, recv_sem=recv_sem, device_id=(dst_id,), device_id_type=pltpu.DeviceIdType.MESH)\n            dma.start()\n            dma.wait_send()\n        recv_count += jnp.where(dst_id == my_id, 1, 0)\n\n    @pl.when(recv_count > 0)\n    def _():\n        fake_dma = pltpu.make_async_remote_copy(src_ref=x_ref.at[pl.ds(0, 8 * recv_count)], dst_ref=o_ref.at[pl.ds(0, 8 * recv_count)], send_sem=send_sem, recv_sem=recv_sem, device_id=(my_id,), device_id_type=pltpu.DeviceIdType.MESH)\n        fake_dma.wait_recv()"
  },
  {
    "test_code": "def test_concat_constant(self):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    axis = 0\n    num_arrays = 16\n    if jtu.test_device_matches(['gpu']) and (not self.INTERPRET):\n        num_arrays = 2\n        axis = -1\n\n    def kernel(out):\n        result = []\n        for i in range(num_arrays):\n            result.append(jnp.full((1, 128), i, jnp.float32))\n        out[:] = jnp.stack(result, axis=axis).reshape(num_arrays, 128)\n\n    def run(interpret=False):\n        return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((num_arrays, 128), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.VMEM), interpret=interpret)()\n    expected = run(True)\n    if not self.INTERPRET:\n        actual = run(False)\n        self.assertAllClose(actual, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef run(src_dst_ids):\n    return shard_map.shard_map(pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), input_arr.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY), scratch_shapes=[pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA], compiler_params=pltpu.TPUCompilerParams(collective_id=0), interpret=mosaic_interpret.TPUInterpretParams(dma_execution_mode='eager', detect_races=True)), mesh=mesh, in_specs=(P(None), P('x', None)), out_specs=P('x', None), check_rep=False)(src_dst_ids, input_arr)"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__}_{value}', dtype, value) for dtypes, values in (((jnp.uint16, jnp.uint32, jnp.uint64), (0, 5)), ((jnp.int16, jnp.int32, jnp.int64), (-3, 0, 5)), ((jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64), (-3.2, -0.0, 0.0, 5.1, jnp.nan, jnp.inf, -jnp.inf))) for dtype in dtypes for value in values))\ndef test_sign(self, dtype, value):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sign(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=dtype)\n    out = kernel(x)\n    expected = jnp.sign(x)\n    np.testing.assert_array_equal(out.astype(jnp.float32), expected.astype(jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.bfloat16, jnp.int32)\ndef test_add_constant(self, dtype):\n    self.skip_if_mosaic_gpu()\n    shape = (256, 256)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...] + 1\n    np.testing.assert_array_equal(kernel(jnp.zeros(shape, dtype=dtype)), jnp.ones(shape, dtype=dtype))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.parameters(-3.2, -1.0, -0.999517, -0.4, 0.0, 0.72, 0.999517, 1.0, 2.4)\ndef test_erf_inv(self, value):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.erf_inv(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=floatx)\n    out = kernel(x)\n    expected = lax.erf_inv(x)\n    np.testing.assert_array_equal(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "def test_is_finite(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.is_finite(x_ref[...])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "def test_is_finite_scalar(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = jnp.isfinite(x_ref[i])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "def test_abs_weak_type(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.abs(x_ref[...])\n    x = jnp.broadcast_to(-3.2, (4, 4))\n    np.testing.assert_allclose(kernel(x), jnp.abs(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.parameters(('float32', 'int32'), ('float64', 'int32'), ('float32', 'float32'), ('float64', 'float64'))\ndef test_pow(self, x_dtype, y_dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), x_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = lax.pow(x_ref[...], y_ref[...])\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    x = jnp.array([1, 2, 3, 4]).astype(x_dtype)\n    y = jnp.array([1, 2, 3, 4]).astype(y_dtype)\n    np.testing.assert_allclose(kernel(x, y), lax.pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.parameters(0, 1, 2, 3, 4, 5, -1, -2, -3)\ndef test_integer_pow(self, y):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = lax.integer_pow(x_ref[...], y)\n    x = jnp.array([1, 2, 3, 4]).astype(jnp.float32) / 10\n    np.testing.assert_allclose(kernel(x), lax.integer_pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__} (x={x!r}, y={y!r})', dtype, x, y) for dtype, x, y in itertools.product((jnp.float32, jnp.float64), _NEXTAFTER_VALUES, _NEXTAFTER_VALUES)))\ndef test_nextafter(self, dtype, x, y):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.nextafter(x_ref[...], y_ref[...])\n    x = jnp.full((4,), x, dtype=dtype)\n    y = jnp.full((4,), y, dtype=dtype)\n    out = kernel(x, y)\n    expected = jnp.nextafter(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.parameters(('int32', 'float32'), ('float32', 'float32'), ('bfloat16', 'bfloat16'))\ndef test_true_divide(self, dtype, out_dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        if out_dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest('bfloat16 is not supported on older TPU generations')\n        if not jtu.if_cloud_tpu_at_least(2025, 1, 9):\n            self.skipTest('Requires libtpu built after 2025-01-09')\n    elif jtu.test_device_matches(['gpu']):\n        if dtype == 'bfloat16':\n            self.skipTest('bfloat16 not supported')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), out_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    x = jnp.repeat(x, 8, axis=0).reshape(8, 8)\n    y = jnp.tile(y, 8).reshape(8, 8)\n    rtol = 0.008 if dtype == 'bfloat16' else 1e-06\n    np.testing.assert_allclose(jnp.true_divide(x, y).astype(jnp.float32), kernel(x, y).astype(jnp.float32), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16')\ndef test_true_divide_unsupported(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('No lowering in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([2.4, 4.2]).astype(dtype)\n    y = jnp.array([4.2, 2.4]).astype(dtype)\n    with self.assertRaises(Exception):\n        kernel(x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16', 'float32')\ndef test_approx_tanh(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    if self.INTERPRET:\n        self.skipTest('approx_tanh is not supported in interpret mode')\n    if dtype == 'bfloat16' and (not jtu.is_cuda_compute_capability_at_least('9.0')):\n        self.skipTest('tanh.approx.bf16 requires a GPU with capability >= sm90')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = plgpu.approx_tanh(x_ref[...])\n    x = jnp.asarray([-1, 0.42, 0.24, 1]).astype(dtype)\n    np.testing.assert_allclose(kernel(x).astype(jnp.float32), jnp.tanh(x).astype(jnp.float32), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "def test_elementwise_inline_asm(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: elementwise_inline_asm_p')\n    if self.INTERPRET:\n        self.skipTest('elementwise_inline_asm is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((256,), jnp.float16))\n    def kernel(x_ref, o_ref):\n        [o_ref[...]] = plgpu.elementwise_inline_asm('tanh.approx.f16x2 $0, $1;', args=[x_ref[...]], constraints='=r,r', pack=2, result_shape_dtypes=[jax.ShapeDtypeStruct(x_ref.shape, x_ref.dtype)])\n    x = jnp.arange(256).astype(jnp.float16)\n    np.testing.assert_allclose(kernel(x), jnp.tanh(x), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "def test_debug_barrier(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: debug_barrier_p')\n    if self.INTERPRET:\n        self.skipTest('debug_barrier is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n        plgpu.debug_barrier()\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    np.testing.assert_array_equal(kernel(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('It works!')\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('It works!', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print_with_values(self):\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('x[0] =', x_ref[0])\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x[0] = 4.2', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "def test_num_programs(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((4,), intx), grid=4)\n    def kernel(o_ref):\n        o_ref[pl.program_id(0)] = pl.num_programs(0)\n    np.testing.assert_array_equal(kernel(), jnp.array([4, 4, 4, 4], dtype=intx))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.product(size=[1, 2, 64, 129, 1021], block_size=[1, 2, 32, 64, 128])\ndef test_masked_load_store(self, size, block_size):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), floatx), grid=pl.cdiv(size, block_size))\n    def kernel(x_ref, o_ref):\n        idx = pl.program_id(0) * block_size + jnp.arange(block_size, dtype=jnp.int32)\n        mask = idx < x_ref.shape[0]\n        x = pl.load(x_ref, (idx,), mask=mask)\n        pl.store(o_ref, (idx,), x + 1.0, mask=mask)\n    key = random.key(0)\n    x = random.normal(key, (size,))\n    np.testing.assert_allclose(kernel(x), x + 1.0, atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "def test_strided_load(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[::4]\n    x = jnp.arange(64, dtype=jnp.float32).reshape((16, 4))\n    np.testing.assert_array_equal(kernel(x), x[::4])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.parameters(((16, 32), (16,)), ((16, 32), (32,)), ((16, 32), (16, 16)))\ndef test_invalid_broadcasted_load(self, x_shape, mask_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    if self.INTERPRET:\n        self.skipTest('No broadcasting checks in pl.load in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def kernel(x_ref, mask_ref, o_ref):\n        del o_ref\n        pl.load(x_ref, slice(None), mask=mask_ref[:])\n    x = jnp.ones(x_shape, dtype=jnp.float32)\n    mask = jnp.ones(mask_shape, dtype=jnp.bool_)\n    try:\n        kernel(x, mask)\n    except Exception as e:\n        self.assertIn('Cannot broadcast', str(e.__cause__))\n    else:\n        self.fail('Expected exception due to invalid broadcasting')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "def test_debug_print(self):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        jax.debug.print('x = {}', x_ref)\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x = [4.2 2.4]', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, y_ref):\n\n    def body(ready_sem, send_sem, recv_sem):\n        my_id = lax.axis_index('x')\n        my_other_id = lax.axis_index('y')\n        axis_size = lax.psum(1, 'x')\n        if direction == 'right':\n            neighbor = lax.rem(my_id + 1, axis_size)\n        else:\n            neighbor = lax.rem(my_id - 1, axis_size)\n            neighbor = jnp.where(neighbor < 0, neighbor + axis_size, neighbor)\n        pltpu.semaphore_signal(ready_sem, device_id=(my_other_id, neighbor))\n        pltpu.semaphore_wait(ready_sem)\n        copy_done = pltpu.async_remote_copy(x_ref, y_ref, send_sem, recv_sem, device_id=(my_other_id, neighbor))\n        copy_done.wait_send()\n        copy_done.wait_recv()\n    pl.run_scoped(body, pltpu.SemaphoreType.REGULAR, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__}_{value}', dtype, value) for dtypes, values in (((jnp.uint16, jnp.uint32, jnp.uint64), (0, 5)), ((jnp.int16, jnp.int32, jnp.int64), (-3, 0, 5)), ((jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64), (-3.2, -0.0, 0.0, 5.1, jnp.nan, jnp.inf, -jnp.inf))) for dtype in dtypes for value in values))\ndef test_sign(self, dtype, value):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sign(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=dtype)\n    out = kernel(x)\n    expected = jnp.sign(x)\n    np.testing.assert_array_equal(out.astype(jnp.float32), expected.astype(jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.bfloat16, jnp.int32)\ndef test_add_constant(self, dtype):\n    self.skip_if_mosaic_gpu()\n    shape = (256, 256)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...] + 1\n    np.testing.assert_array_equal(kernel(jnp.zeros(shape, dtype=dtype)), jnp.ones(shape, dtype=dtype))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(-3.2, -1.0, -0.999517, -0.4, 0.0, 0.72, 0.999517, 1.0, 2.4)\ndef test_erf_inv(self, value):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.erf_inv(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=floatx)\n    out = kernel(x)\n    expected = lax.erf_inv(x)\n    np.testing.assert_array_equal(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "def test_is_finite(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.is_finite(x_ref[...])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "def test_is_finite_scalar(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = jnp.isfinite(x_ref[i])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "def test_abs_weak_type(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.abs(x_ref[...])\n    x = jnp.broadcast_to(-3.2, (4, 4))\n    np.testing.assert_allclose(kernel(x), jnp.abs(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(('float32', 'int32'), ('float64', 'int32'), ('float32', 'float32'), ('float64', 'float64'))\ndef test_pow(self, x_dtype, y_dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), x_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = lax.pow(x_ref[...], y_ref[...])\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    x = jnp.array([1, 2, 3, 4]).astype(x_dtype)\n    y = jnp.array([1, 2, 3, 4]).astype(y_dtype)\n    np.testing.assert_allclose(kernel(x, y), lax.pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(0, 1, 2, 3, 4, 5, -1, -2, -3)\ndef test_integer_pow(self, y):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = lax.integer_pow(x_ref[...], y)\n    x = jnp.array([1, 2, 3, 4]).astype(jnp.float32) / 10\n    np.testing.assert_allclose(kernel(x), lax.integer_pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__} (x={x!r}, y={y!r})', dtype, x, y) for dtype, x, y in itertools.product((jnp.float32, jnp.float64), _NEXTAFTER_VALUES, _NEXTAFTER_VALUES)))\ndef test_nextafter(self, dtype, x, y):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.nextafter(x_ref[...], y_ref[...])\n    x = jnp.full((4,), x, dtype=dtype)\n    y = jnp.full((4,), y, dtype=dtype)\n    out = kernel(x, y)\n    expected = jnp.nextafter(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(('int32', 'float32'), ('float32', 'float32'), ('bfloat16', 'bfloat16'))\ndef test_true_divide(self, dtype, out_dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        if out_dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest('bfloat16 is not supported on older TPU generations')\n        if not jtu.if_cloud_tpu_at_least(2025, 1, 9):\n            self.skipTest('Requires libtpu built after 2025-01-09')\n    elif jtu.test_device_matches(['gpu']):\n        if dtype == 'bfloat16':\n            self.skipTest('bfloat16 not supported')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), out_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    x = jnp.repeat(x, 8, axis=0).reshape(8, 8)\n    y = jnp.tile(y, 8).reshape(8, 8)\n    rtol = 0.008 if dtype == 'bfloat16' else 1e-06\n    np.testing.assert_allclose(jnp.true_divide(x, y).astype(jnp.float32), kernel(x, y).astype(jnp.float32), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16')\ndef test_true_divide_unsupported(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('No lowering in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([2.4, 4.2]).astype(dtype)\n    y = jnp.array([4.2, 2.4]).astype(dtype)\n    with self.assertRaises(Exception):\n        kernel(x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16', 'float32')\ndef test_approx_tanh(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    if self.INTERPRET:\n        self.skipTest('approx_tanh is not supported in interpret mode')\n    if dtype == 'bfloat16' and (not jtu.is_cuda_compute_capability_at_least('9.0')):\n        self.skipTest('tanh.approx.bf16 requires a GPU with capability >= sm90')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = plgpu.approx_tanh(x_ref[...])\n    x = jnp.asarray([-1, 0.42, 0.24, 1]).astype(dtype)\n    np.testing.assert_allclose(kernel(x).astype(jnp.float32), jnp.tanh(x).astype(jnp.float32), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "def test_elementwise_inline_asm(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: elementwise_inline_asm_p')\n    if self.INTERPRET:\n        self.skipTest('elementwise_inline_asm is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((256,), jnp.float16))\n    def kernel(x_ref, o_ref):\n        [o_ref[...]] = plgpu.elementwise_inline_asm('tanh.approx.f16x2 $0, $1;', args=[x_ref[...]], constraints='=r,r', pack=2, result_shape_dtypes=[jax.ShapeDtypeStruct(x_ref.shape, x_ref.dtype)])\n    x = jnp.arange(256).astype(jnp.float16)\n    np.testing.assert_allclose(kernel(x), jnp.tanh(x), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "def test_debug_barrier(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: debug_barrier_p')\n    if self.INTERPRET:\n        self.skipTest('debug_barrier is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n        plgpu.debug_barrier()\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    np.testing.assert_array_equal(kernel(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('It works!')\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('It works!', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print_with_values(self):\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('x[0] =', x_ref[0])\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x[0] = 4.2', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "def test_num_programs(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((4,), intx), grid=4)\n    def kernel(o_ref):\n        o_ref[pl.program_id(0)] = pl.num_programs(0)\n    np.testing.assert_array_equal(kernel(), jnp.array([4, 4, 4, 4], dtype=intx))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.product(size=[1, 2, 64, 129, 1021], block_size=[1, 2, 32, 64, 128])\ndef test_masked_load_store(self, size, block_size):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), floatx), grid=pl.cdiv(size, block_size))\n    def kernel(x_ref, o_ref):\n        idx = pl.program_id(0) * block_size + jnp.arange(block_size, dtype=jnp.int32)\n        mask = idx < x_ref.shape[0]\n        x = pl.load(x_ref, (idx,), mask=mask)\n        pl.store(o_ref, (idx,), x + 1.0, mask=mask)\n    key = random.key(0)\n    x = random.normal(key, (size,))\n    np.testing.assert_allclose(kernel(x), x + 1.0, atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "def test_strided_load(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[::4]\n    x = jnp.arange(64, dtype=jnp.float32).reshape((16, 4))\n    np.testing.assert_array_equal(kernel(x), x[::4])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.parameters(((16, 32), (16,)), ((16, 32), (32,)), ((16, 32), (16, 16)))\ndef test_invalid_broadcasted_load(self, x_shape, mask_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    if self.INTERPRET:\n        self.skipTest('No broadcasting checks in pl.load in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def kernel(x_ref, mask_ref, o_ref):\n        del o_ref\n        pl.load(x_ref, slice(None), mask=mask_ref[:])\n    x = jnp.ones(x_shape, dtype=jnp.float32)\n    mask = jnp.ones(mask_shape, dtype=jnp.bool_)\n    try:\n        kernel(x, mask)\n    except Exception as e:\n        self.assertIn('Cannot broadcast', str(e.__cause__))\n    else:\n        self.fail('Expected exception due to invalid broadcasting')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "def test_debug_print(self):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        jax.debug.print('x = {}', x_ref)\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x = [4.2 2.4]', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_ref, o_ref):\n    o_ref[...] = jnp.sin(x_ref[...])"
  },
  {
    "test_code": "@parameterized.named_parameters(((fn.__name__, fn, dtype) for fn, dtype in [(lax.pow, jnp.float32), (lax.bitwise_and, jnp.int32), (lax.bitwise_or, jnp.int32), (lax.bitwise_xor, jnp.int32), (lax.shift_left, jnp.int32), (lax.shift_right_arithmetic, jnp.int32), (lax.shift_right_logical, jnp.int32)]))\ndef test_weak_dtype(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = fn(x_ref[...], y_ref[...])\n    x = jnp.full((8, 128), 4, dtype=dtype)\n    y = jnp.full((8, 128), 2 if jnp.issubdtype(dtype, jnp.integer) else 2.0, dtype=dtype)\n    np.testing.assert_allclose(kernel(x, y), fn(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__}_{value}', dtype, value) for dtypes, values in (((jnp.uint16, jnp.uint32, jnp.uint64), (0, 5)), ((jnp.int16, jnp.int32, jnp.int64), (-3, 0, 5)), ((jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64), (-3.2, -0.0, 0.0, 5.1, jnp.nan, jnp.inf, -jnp.inf))) for dtype in dtypes for value in values))\ndef test_sign(self, dtype, value):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.sign(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=dtype)\n    out = kernel(x)\n    expected = jnp.sign(x)\n    np.testing.assert_array_equal(out.astype(jnp.float32), expected.astype(jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.parameters(jnp.float32, jnp.bfloat16, jnp.int32)\ndef test_add_constant(self, dtype):\n    self.skip_if_mosaic_gpu()\n    shape = (256, 256)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...] + 1\n    np.testing.assert_array_equal(kernel(jnp.zeros(shape, dtype=dtype)), jnp.ones(shape, dtype=dtype))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.parameters(-3.2, -1.0, -0.999517, -0.4, 0.0, 0.72, 0.999517, 1.0, 2.4)\ndef test_erf_inv(self, value):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.erf_inv(x_ref[...])\n    x = jnp.full((8, 128), value, dtype=floatx)\n    out = kernel(x)\n    expected = lax.erf_inv(x)\n    np.testing.assert_array_equal(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "def test_is_finite(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = lax.is_finite(x_ref[...])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "def test_is_finite_scalar(self):\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('Not supported on GPU')\n    size = len(self.IS_FINITE_TEST_VALUES)\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((size,), jnp.bool_))\n    def kernel(x_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = jnp.isfinite(x_ref[i])\n    x = jnp.array(self.IS_FINITE_TEST_VALUES, dtype=jnp.float32)\n    out = kernel(x)\n    expected = lax.is_finite(x)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']):\n        if dtype in ('int16', 'float16'):\n            self.skipTest('int16 and float16 are not supported on TPU')\n        if fn in (jnp.ceil, jnp.floor, jnp.negative, jnp.exp, jnp.exp2, jnp.log, jnp.sqrt, lax.rsqrt) and dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest(f'bfloat16 {fn.__name__} is only supported on TPU v6+')\n        if fn in (jnp.sin, jnp.cos, jnp.tan, jnp.tanh, jnp.log1p) and dtype == 'bfloat16':\n            self.skipTest(f'bfloat16 {fn.__name__} is not supported on TPU')\n        if fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n            self.skipTest(f'{fn.__name__} not implemented on TPU')\n        if not jtu.if_cloud_tpu_at_least(2024, 12, 19):\n            self.skipTest('Requires libtpu built at least on 2024-12-19')\n        if fn == jnp.exp2 and dtype == 'bfloat16' and (not jtu.if_cloud_tpu_at_least(2025, 1, 31)):\n            self.skipTest('Test requires newer libtpu')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 128), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = fn(x_ref[...])\n    if fn in (jnp.exp, jnp.exp2) and dtype == 'bfloat16':\n        x = jnp.array([0.42, 1.26] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 0.002\n    else:\n        x = jnp.array([0.42, 2.4] * (8 * 128 // 2)).reshape(8, 128).astype(dtype)\n        rtol = 1e-06\n    self.assertAllClose(kernel(x), fn(x), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in ELEMENTWISE_OPS for fn, dtype in itertools.product(*args)))\ndef test_elementwise_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and fn in (jnp.ceil, jnp.floor, jnp.expm1, jnp.log1p, jnp.cbrt, lax.rsqrt, jnp.tan, jnp.asin, jnp.acos, jnp.atan, jnp.sinh, jnp.cosh, jnp.tanh, jnp.asinh, jnp.acosh, jnp.atanh) and (dtype == 'bfloat16'):\n        self.skipTest(f'bfloat16 {fn.__name__} is not supported on GPU')\n    if jtu.test_device_matches(['tpu']) and fn == lax.population_count and (not self.INTERPRET):\n        self.skipTest('Scalar population count on TPU is only supported in interpret mode')\n    if jtu.test_device_matches(['tpu']) and fn in (jnp.acos, jnp.acosh, jnp.asin, jnp.asinh, jnp.atan, jnp.atanh, jnp.cbrt, jnp.cosh, jnp.expm1, jnp.sinh):\n        self.skipTest(f'{fn.__name__} not implemented on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()),), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[0] = fn(x_ref[0])\n        o_ref[1] = fn(x_ref[1])\n    x = jnp.array([0.42, 1.4]).astype(dtype)\n    self.assertAllClose(kernel(x), fn(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "def test_abs_weak_type(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), floatx))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = jnp.abs(x_ref[...])\n    x = jnp.broadcast_to(-3.2, (4, 4))\n    np.testing.assert_allclose(kernel(x), jnp.abs(x), rtol=1e-06)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.parameters(('float32', 'int32'), ('float64', 'int32'), ('float32', 'float32'), ('float64', 'float64'))\ndef test_pow(self, x_dtype, y_dtype):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), x_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = lax.pow(x_ref[...], y_ref[...])\n    if not jax.config.x64_enabled and jnp.dtype(x_dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n    x = jnp.array([1, 2, 3, 4]).astype(x_dtype)\n    y = jnp.array([1, 2, 3, 4]).astype(y_dtype)\n    np.testing.assert_allclose(kernel(x, y), lax.pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.parameters(0, 1, 2, 3, 4, 5, -1, -2, -3)\ndef test_integer_pow(self, y):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[:] = lax.integer_pow(x_ref[...], y)\n    x = jnp.array([1, 2, 3, 4]).astype(jnp.float32) / 10\n    np.testing.assert_allclose(kernel(x), lax.integer_pow(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{dtype.__name__} (x={x!r}, y={y!r})', dtype, x, y) for dtype, x, y in itertools.product((jnp.float32, jnp.float64), _NEXTAFTER_VALUES, _NEXTAFTER_VALUES)))\ndef test_nextafter(self, dtype, x, y):\n    self.skip_if_mosaic_gpu()\n    if not jax.config.x64_enabled and jnp.dtype(dtype).itemsize == 8:\n        self.skipTest('64-bit types require x64_enabled')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.nextafter(x_ref[...], y_ref[...])\n    x = jnp.full((4,), x, dtype=dtype)\n    y = jnp.full((4,), y, dtype=dtype)\n    out = kernel(x, y)\n    expected = jnp.nextafter(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['gpu']) and dtype == jnp.bool_:\n        self.skipTest('Not implemented on GPU.')\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[:] = fn(x_ref[...], y_ref[...])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype.__name__}', fn, dtype) for fn, dtype in itertools.product(COMPARISON_OPS, (jnp.int32, jnp.uint32, jnp.float16, jnp.float32, jnp.bool_))))\ndef test_comparison_scalar(self, fn, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and dtype == jnp.float16:\n        self.skipTest('float16 is not supported on TPU')\n    if jtu.test_device_matches(['gpu']) and (not jtu.is_cuda_compute_capability_at_least('8.0')):\n        self.skipTest('Only works on GPUs with capability >= sm80')\n\n    @functools.partial(self.pallas_call, in_specs=(pl.BlockSpec(memory_space=smem_on_tpu()), pl.BlockSpec(memory_space=smem_on_tpu())), out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((8,), jnp.bool_))\n    def kernel(x_ref, y_ref, o_ref):\n        for i in range(8):\n            o_ref[i] = fn(x_ref[i], y_ref[i])\n    x = jnp.array([0, 3, -4, -6, 0, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 0, -2, 2, 4]).astype(dtype)\n    out = kernel(x, y)\n    expected = fn(x, y)\n    self.assertArraysEqual(out, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.parameters(('int32', 'float32'), ('float32', 'float32'), ('bfloat16', 'bfloat16'))\ndef test_true_divide(self, dtype, out_dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        if out_dtype == 'bfloat16' and (not jtu.is_device_tpu_at_least(6)):\n            self.skipTest('bfloat16 is not supported on older TPU generations')\n        if not jtu.if_cloud_tpu_at_least(2025, 1, 9):\n            self.skipTest('Requires libtpu built after 2025-01-09')\n    elif jtu.test_device_matches(['gpu']):\n        if dtype == 'bfloat16':\n            self.skipTest('bfloat16 not supported')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8, 8), out_dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    x = jnp.repeat(x, 8, axis=0).reshape(8, 8)\n    y = jnp.tile(y, 8).reshape(8, 8)\n    rtol = 0.008 if dtype == 'bfloat16' else 1e-06\n    np.testing.assert_allclose(jnp.true_divide(x, y).astype(jnp.float32), kernel(x, y).astype(jnp.float32), rtol=rtol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16')\ndef test_true_divide_unsupported(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('No lowering in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = jnp.true_divide(x_ref[...], y_ref[...])\n    x = jnp.array([2.4, 4.2]).astype(dtype)\n    y = jnp.array([4.2, 2.4]).astype(dtype)\n    with self.assertRaises(Exception):\n        kernel(x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.parameters('float16', 'bfloat16', 'float32')\ndef test_approx_tanh(self, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented on TPU')\n    if self.INTERPRET:\n        self.skipTest('approx_tanh is not supported in interpret mode')\n    if dtype == 'bfloat16' and (not jtu.is_cuda_compute_capability_at_least('9.0')):\n        self.skipTest('tanh.approx.bf16 requires a GPU with capability >= sm90')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), dtype))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = plgpu.approx_tanh(x_ref[...])\n    x = jnp.asarray([-1, 0.42, 0.24, 1]).astype(dtype)\n    np.testing.assert_allclose(kernel(x).astype(jnp.float32), jnp.tanh(x).astype(jnp.float32), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "def test_elementwise_inline_asm(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: elementwise_inline_asm_p')\n    if self.INTERPRET:\n        self.skipTest('elementwise_inline_asm is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((256,), jnp.float16))\n    def kernel(x_ref, o_ref):\n        [o_ref[...]] = plgpu.elementwise_inline_asm('tanh.approx.f16x2 $0, $1;', args=[x_ref[...]], constraints='=r,r', pack=2, result_shape_dtypes=[jax.ShapeDtypeStruct(x_ref.shape, x_ref.dtype)])\n    x = jnp.arange(256).astype(jnp.float16)\n    np.testing.assert_allclose(kernel(x), jnp.tanh(x), atol=0.005, rtol=0.005)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "def test_debug_barrier(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented: debug_barrier_p')\n    if self.INTERPRET:\n        self.skipTest('debug_barrier is not supported in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n        plgpu.debug_barrier()\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    np.testing.assert_array_equal(kernel(x), x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print(self):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('It works!')\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('It works!', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@unittest.skipIf(sys.platform == 'win32', 'plgpu.TritonCompilerParams unavailable on Windows')\ndef test_debug_print_with_values(self):\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Test for TPU is covered in tpu_pallas_test.py')\n    if jtu.test_device_matches(['gpu']):\n        self.skipTest('This test flakes on gpu')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32), compiler_params=plgpu.TritonCompilerParams(num_warps=1, num_stages=1))\n    def kernel(x_ref, o_ref):\n        pl.debug_print('x[0] =', x_ref[0])\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x[0] = 4.2', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "def test_num_programs(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_specs=pl.BlockSpec(memory_space=smem_on_tpu()), out_shape=jax.ShapeDtypeStruct((4,), intx), grid=4)\n    def kernel(o_ref):\n        o_ref[pl.program_id(0)] = pl.num_programs(0)\n    np.testing.assert_array_equal(kernel(), jnp.array([4, 4, 4, 4], dtype=intx))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.product(size=[1, 2, 64, 129, 1021], block_size=[1, 2, 32, 64, 128])\ndef test_masked_load_store(self, size, block_size):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not implemented')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((size,), floatx), grid=pl.cdiv(size, block_size))\n    def kernel(x_ref, o_ref):\n        idx = pl.program_id(0) * block_size + jnp.arange(block_size, dtype=jnp.int32)\n        mask = idx < x_ref.shape[0]\n        x = pl.load(x_ref, (idx,), mask=mask)\n        pl.store(o_ref, (idx,), x + 1.0, mask=mask)\n    key = random.key(0)\n    x = random.normal(key, (size,))\n    np.testing.assert_allclose(kernel(x), x + 1.0, atol=1e-05, rtol=1e-05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "def test_strided_load(self):\n    self.skip_if_mosaic_gpu()\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4, 4), jnp.float32))\n    def kernel(x_ref, o_ref):\n        o_ref[...] = x_ref[::4]\n    x = jnp.arange(64, dtype=jnp.float32).reshape((16, 4))\n    np.testing.assert_array_equal(kernel(x), x[::4])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.parameters(((16, 32), (16,)), ((16, 32), (32,)), ((16, 32), (16, 16)))\ndef test_invalid_broadcasted_load(self, x_shape, mask_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n    if self.INTERPRET:\n        self.skipTest('No broadcasting checks in pl.load in interpret mode')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((), jnp.float32))\n    def kernel(x_ref, mask_ref, o_ref):\n        del o_ref\n        pl.load(x_ref, slice(None), mask=mask_ref[:])\n    x = jnp.ones(x_shape, dtype=jnp.float32)\n    mask = jnp.ones(mask_shape, dtype=jnp.bool_)\n    try:\n        kernel(x, mask)\n    except Exception as e:\n        self.assertIn('Cannot broadcast', str(e.__cause__))\n    else:\n        self.fail('Expected exception due to invalid broadcasting')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "def test_debug_print(self):\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((2,), jnp.float32))\n    def kernel(x_ref, o_ref):\n        jax.debug.print('x = {}', x_ref)\n    x = jnp.array([4.2, 2.4]).astype(jnp.float32)\n    with jtu.capture_stdout() as output:\n        jax.block_until_ready(kernel(x))\n        jax.effects_barrier()\n    self.assertIn('x = [4.2 2.4]', output())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def kernel(x_hbm_ref, y_hbm_ref, o_hbm_ref):\n    grid = (pl.cdiv(m, bm), pl.cdiv(n, bn), pl.cdiv(k, bk))\n\n    def run(acc_scratch_ref):\n        pltpu.emit_pipeline(partial(basic_matmul_kernel, acc_scratch_ref=acc_scratch_ref, k=k), in_specs=[pl.BlockSpec((bm, bk), lambda i, j, k: (i, k)), pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))], out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)), grid=grid, core_axis=0, dimension_semantics=(pltpu.PARALLEL, pltpu.PARALLEL, pltpu.ARBITRARY))(x_hbm_ref, y_hbm_ref, o_hbm_ref)\n    accum_dtype = jnp.float32 if jnp.issubdtype(x.dtype, jnp.floating) else jnp.int32\n    pl.run_scoped(run, pltpu.VMEM((bm, bn), accum_dtype))"
  },
  {
    "test_code": "@parameterized.product(shape=((64,), (8, 8)), dtype=(jnp.int32, jnp.int16, jnp.int8))\ndef test_scalar_map(self, shape, dtype):\n    self.skip_if_mosaic_gpu()\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n    if dtype != jnp.int32 and len(shape) < 2:\n        self.skipTest('Loads and stores not implemented for 1D arrays of non-32bit types')\n\n    def kernel(x_ref, y_ref):\n        for idx in np.ndindex(shape):\n            x = x_ref[idx].astype(jnp.int32)\n            y_ref[idx] = (x * x).astype(y_ref.dtype)\n    f = self.pallas_call(kernel, in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.SMEM), out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    x = jnp.arange(np.prod(shape), dtype=dtype).reshape(shape)\n    self.assertAllClose(f(x), x * x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_extract_scalar(self):\n    if pltpu is None:\n        self.skipTest('No TPU module available.')\n\n    def kernel(x_ref, y_ref):\n        y_ref[0, 0] = x_ref[:][0, 0]\n    f = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((1, 1), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.SMEM))\n    x = np.arange(1024, dtype=jnp.float32).reshape(8, 128) + 10\n    self.assertAllClose(f(x).item(), 10.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[...] = f(x_ref[...], y_ref[...])\n    x = jnp.array([1, 3, -4, -6, 2, 5, 4, -7]).astype(dtype)\n    if f == jnp.bitwise_left_shift:\n        y = jnp.array([3, 1, 4, 5, 2, 2, 2, 4]).astype(dtype)\n    else:\n        y = jnp.array([3, 1, -4, -5, 2, -2, 2, 4]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((f'{fn.__name__}_{dtype}', fn, dtype) for args in BINARY_OPS for fn, dtype in itertools.product(*args)))\ndef test_binary_scalar(self, f, dtype):\n    self.skip_if_mosaic_gpu()\n    if not jtu.test_device_matches(['tpu']):\n        self.skipTest('Test only supported on TPU.')\n    if jtu.test_device_matches(['tpu']) and jnp.dtype(dtype).itemsize == 2:\n        self.skipTest('16-bit types are not supported on TPU')\n\n    @functools.partial(self.pallas_call, in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), out_shape=jax.ShapeDtypeStruct((1,), dtype))\n    def kernel(x_ref, y_ref, o_ref):\n        o_ref[0] = f(x_ref[0], y_ref[0])\n    x = jnp.array([1]).astype(dtype)\n    y = jnp.array([18]).astype(dtype)\n    np.testing.assert_allclose(f(x, y), kernel(x, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@parameterized.parameters(((8, 4), jnp.int32, 0), ((8, 16), jnp.float32, 1), ((8, 16, 2), jnp.int8, 1))\ndef test_broadcasted_iota(self, shape, dtype, dimension):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Only 32-bit integer iota supported')\n    f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(shape, dtype))\n    def kernel(o_ref):\n        o_ref[...] = f()\n    np.testing.assert_allclose(f(), kernel())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@parameterized.parameters(((2, 4), (8,)), ((2, 4), (8, 1)), ((2, 4), (1, 8)), ((64,), (32, 2)))\ndef test_reshape(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (1,)), ((), (1, 1)), ((2, 4), (2, 4)), ((2, 4), (2, 4, 1)), ((2, 4, 1), (2, 4)), ((2, 4), (1, 2, 4)), ((1, 2, 4), (2, 4)), ((2, 4), (2, 1, 4)), ((1, 2, 1, 4, 1), (2, 4)), ((2, 4), (1, 2, 1, 4)), ((2, 4), (1, 2, 4, 1)), ((1, 2, 4, 1), (1, 2, 1, 4, 1)))\ndef test_reshape_noop_or_singleton_dims(self, in_shape, out_shape):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        o_ref[...] = x_ref[...].reshape(out_shape)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = x.reshape(out_shape)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@parameterized.parameters(((), (2,), ()), ((1,), (2,), (0,)), ((1, 1), (2, 2), (0, 1)), ((), (2, 2), ()))\ndef test_broadcast_in_dim(self, in_shape, out_shape, dims):\n    self.skip_if_mosaic_gpu()\n    if jtu.test_device_matches(['tpu']):\n        self.skipTest('Not supported on TPU')\n\n    @functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\n    def f(x_ref, o_ref):\n        x = x_ref[...]\n        o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n    expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n    np.testing.assert_allclose(f(x), expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(((name, name, func, strategy) for name, func, strategy in UNARY_FUNCTIONS))\n@hp.given(hps.data())\ndef test_unary_primitives(self, name, func, shape_dtype_strategy, data):\n    self.skip_if_mosaic_gpu()\n    if self.INTERPRET:\n        self.skipTest('This hypothesis test is slow, even more so in interpret mode.')\n    tol = 0.0\n    if jtu.test_device_matches(['gpu']):\n        if func == jnp.round or func == jnp.rint:\n            self.skipTest('TODO: not implemented on GPU')\n        if name == 'tanh':\n            tol = 1e-06\n        elif name == 'exp2':\n            tol = 1e-06\n\n    def kernel(x_ref, y_ref):\n        y_ref[...] = func(x_ref[...])\n    x_shape_dtype = data.draw(shape_dtype_strategy)\n    key = random.key(0)\n    x = _random_value(key, x_shape_dtype)\n    out = self.pallas_call(kernel, out_shape=x_shape_dtype)(x)\n    self.assertAllClose(out, func(x), atol=tol, rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "def _random_value(key: jax.Array, shape_dtype: jax.ShapeDtypeStruct) -> jax.Array:\n    if jnp.issubdtype(shape_dtype.dtype, jnp.floating):\n        return random.normal(key, shape_dtype.shape, dtype=shape_dtype.dtype)\n    elif jnp.issubdtype(shape_dtype.dtype, jnp.integer):\n        return random.randint(key, shape_dtype.shape, minval=-4, maxval=4, dtype=shape_dtype.dtype)\n    raise NotImplementedError(shape_dtype)"
  },
  {
    "test_code": "@parameterized.product(from_dtype=_DTYPES_32BIT, to_dtype=_DTYPES)\n@hp.given(hps.data())\ndef test_cast_from_32bit(self, from_dtype, to_dtype, data):\n    self.skip_if_mosaic_gpu()\n    if to_dtype in {'float8_e4m3b11fnuz', 'float8_e5m2', 'float8_e4m3fn'}:\n        if not jtu.test_device_matches(['tpu']) or jtu.get_tpu_version() < 5:\n            self.skipTest('Not supported on this hardware')\n        if not jtu.if_cloud_tpu_at_least(2025, 3, 8):\n            self.skipTest('Test requires libtpu from 2025/3/8 or later')\n    if from_dtype == to_dtype:\n        self.skipTest('Unnecessary test')\n    if jtu.is_device_tpu(version=4):\n        if to_dtype in {'int8', 'uint8', 'int4', 'uint4'}:\n            self.skipTest('Not supported on this TPU generation')\n        if to_dtype in {'int16', 'uint16'} and (not jtu.if_cloud_tpu_at_least(2025, 1, 18)):\n            self.skipTest('Test requires libtpu from 2025/1/18 or later')\n    if jtu.test_device_matches(['tpu']) and jtu.get_tpu_version() < 4:\n        if to_dtype not in {'int32', 'uint32', 'float32', 'bfloat16'}:\n            self.skipTest('Not supported on this TPU generation')\n    if jtu.test_device_matches(['gpu']) and to_dtype in {'int4', 'uint4'}:\n        self.skipTest('int4/uint4 casts are buggy on GPU')\n    elements = dict(allow_nan=not jnp.issubdtype(to_dtype, jnp.integer))\n    x = data.draw(hnp.arrays(from_dtype, (8, 128), elements=elements))\n    x = jnp.asarray(x)\n\n    def kernel(x_ref, y_ref):\n        x = x_ref[...]\n        y = x.astype(to_dtype)\n        if to_dtype == jnp.bool:\n            y = y.astype(jnp.int32)\n        y_ref[...] = y\n    y_dtype = jnp.int32 if to_dtype == jnp.bool else to_dtype\n    try:\n        y = self.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, y_dtype))(x)\n    except Exception as e:\n        if 'Unsupported cast' in e.args[0]:\n            self.skipTest('Unsupported cast')\n        raise\n    if to_dtype == jnp.bool:\n        y = y.astype(jnp.bool)\n    y_ref = x.astype(to_dtype)\n    if jnp.dtype(to_dtype) in map(jnp.dtype, (jnp.bfloat16, jnp.int4, jnp.uint4)):\n        y, y_ref = (y.astype(np.float32), y_ref.astype(np.float32))\n    np.testing.assert_allclose(y, y_ref, atol=0.0, rtol=0.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/pallas/ops_test.py",
    "function": "@hps.composite\ndef arrays(draw, shape: tuple[int, ...], dtype: np.dtype, *, elements: hps.SearchStrategy[Any] | None=None) -> np.ndarray:\n    cast_to_bf16 = False\n    if dtype == np.dtype(jnp.bfloat16):\n        dtype = np.dtype('float32')\n        cast_to_bf16 = True\n    arr = draw(hnp.arrays(shape=shape, dtype=dtype, elements=elements))\n    if cast_to_bf16:\n        arr = arr.astype(np.dtype(jnp.bfloat16))\n    return arr"
  }
]