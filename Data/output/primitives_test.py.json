[
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def run(pos):\n    maxiter = 1000\n\n    def cond(v):\n        return v[0] < maxiter\n\n    def step(v):\n        i, pos = v\n        jax.debug.callback(print_it, i + 1, maxiter)\n        return (i + 1, pos + 1)\n    val = (jnp.array(0), pos)\n    val = jax.lax.while_loop(cond, step, val)\n    return val[1]"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def run(primal_ins, cotangent_outs):\n    primal_outs, vjp = jax.vjp(g, *primal_ins)\n    cotangent_ins = vjp(cotangent_outs)\n    return (primal_outs, cotangent_ins)"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "@jax.jit\ndef run(src_dst_ids):\n    return shard_map.shard_map(pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((8, 128), input_arr.dtype), in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM), pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)], out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY), scratch_shapes=[pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA], compiler_params=pltpu.TPUCompilerParams(collective_id=0), interpret=mosaic_interpret.TPUInterpretParams(dma_execution_mode='eager', detect_races=True)), mesh=mesh, in_specs=(P(None), P('x', None)), out_specs=P('x', None), check_rep=False)(src_dst_ids, input_arr)"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def run(interpret=False):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct((num_arrays, 128), jnp.float32), out_specs=pl.BlockSpec(memory_space=pltpu.VMEM), interpret=interpret)()"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def run(refs):\n    x_ref, acc_ref = refs\n\n    @pl.core_map(mesh)\n    def _kernel_entry():\n        pl.run_scoped(functools.partial(_scoped, x_gmem=x_ref, acc_gmem=acc_ref), plgpu.SMEM((blk_m, blk_n), jnp.float32))"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def run(*args, **kwargs):\n    mosaic_gpu_lib._mosaic_gpu_ext._cupti_init()\n    try:\n        results = jax.block_until_ready(jax.jit(f)(*args, **kwargs))\n    finally:\n        timings = mosaic_gpu_lib._mosaic_gpu_ext._cupti_get_timings()\n    return (results, timings)"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def run(refs):\n    q_ref, k_ref, v_ref, out_ref = refs\n\n    @pl.core_map(mesh, compiler_params=plgpu.GPUCompilerParams(approx_math=True))\n    def _kernel_entry():\n        qo_scratch = plgpu.SMEM((compute_wgs, block_q, head_dim), jnp.float16, transforms=(tiling, swizzle))\n        pl.run_scoped(lambda *args: fa3_kernel(q_ref, k_ref, v_ref, out_ref, args), qo_scratch, plgpu.Barrier(1, num_barriers=compute_wgs), plgpu.Barrier(num_arrivals=compute_wgs))"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "@pl.when(should_run)\ndef run():\n\n    @functools.partial(lax.fori_loop, 0, block_k_major // block_k, init_val=None, unroll=True)\n    def body(i, _):\n        m_prev = m_scratch_ref[batch_idx]\n        l_prev = l_scratch_ref[batch_idx]\n        q = q_tile_ref[batch_idx]\n        start_k = i * block_k\n        k = pl.load(k_tile_ref, (*batch_idx, pl.dslice(start_k, block_k), slice(None)))\n        s = jax.lax.dot_general(q, k, TRANS_B_DIM_NUMBERS, preferred_element_type=jnp.float32)\n        if ab_tile_ref is not None:\n            ab = pl.load(ab_tile_ref, (*batch_idx, pl.dslice(None), pl.dslice(start_k, block_k))).astype(jnp.float32)\n            s += ab\n        if sm_scale != 1.0:\n            s *= sm_scale\n        mask = None\n        if q_segment_ids_tile_ref is not None:\n            repeats, rem = divmod(block_k, NUM_LANES)\n            if rem:\n                raise NotImplementedError(f'kv block size must be a multiple of {NUM_LANES}')\n            q_segment_ids = pltpu.repeat(q_segment_ids_tile_ref[batch_idx[0]], repeats, axis=1)\n            kv_segment_ids = pl.load(kv_segment_ids_tile_ref, (batch_idx[0], pl.dslice(1), pl.dslice(start_k, block_k)))\n            mask = jnp.equal(q_segment_ids, kv_segment_ids).astype(jnp.bool_)\n        if causal:\n            mask_shape = (block_q, block_k)\n            row_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 0)\n            row_ids += q_seq_idx * block_q\n            col_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 1)\n            col_ids += kv_seq_idx * block_k_major + start_k\n            causal_mask = col_ids <= row_ids\n            mask = causal_mask if mask is None else jnp.logical_and(mask, causal_mask)\n        s = s if mask is None else s + jnp.where(mask, 0.0, mask_value)\n        m_curr = jnp.max(s, axis=1)[:, None]\n        m_next = jnp.maximum(m_prev, m_curr)\n        block_k_repeats, rem = divmod(block_k, MIN_BLOCK_SIZE)\n        if rem:\n            raise NotImplementedError(f'block_k={block_k!r} should be a multiple of {MIN_BLOCK_SIZE}')\n        p = jnp.exp(s - pltpu.repeat(m_next, block_k_repeats, 1))\n        alpha = jnp.exp(m_prev - m_next)\n        l_corr = alpha * l_prev\n        l_next = jnp.sum(p, axis=1)[:, None] + l_corr\n        head_dim_repeats, rem = divmod(head_dim, MIN_BLOCK_SIZE)\n        l_broadcast = lambda l: pltpu.repeat(l, head_dim_repeats, 1)\n        if rem:\n            if head_dim_repeats == 0:\n                l_broadcast = lambda l: l[:, :head_dim]\n            else:\n                raise NotImplementedError(f'head_dim={head_dim!r} should be a multiple of {MIN_BLOCK_SIZE} if larger')\n        l_scratch_ref[batch_idx] = l_next\n        m_scratch_ref[batch_idx] = m_next\n        l_next_inv_safe = jnp.where(l_next == 0.0, 1.0, 1.0 / l_next)\n        acc_scratch_ref[batch_idx] *= l_broadcast(l_corr * l_next_inv_safe)\n        v = pl.load(v_tile_ref, (*batch_idx, pl.dslice(start_k, block_k), slice(None)))\n        o_curr = jax.lax.dot(p.astype(v.dtype), v, preferred_element_type=jnp.float32)\n        acc_scratch_ref[batch_idx] += o_curr * l_broadcast(l_next_inv_safe)"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "@pl.when(should_run)\ndef run():\n    q = q_ref[...] if q_layout == HEAD_DIM_MINOR else q_ref[...].T\n    k = k_ref[...]\n    v = v_ref[...]\n    logsumexp = jnp.expand_dims(logsumexp_ref[0], -1)\n    do = do_ref[...]\n    di = jnp.expand_dims(di_ref[0], -1)\n    qk_dims = NT_DIM_NUMBERS if k_layout == HEAD_DIM_MINOR else NN_DIM_NUMBERS\n    qk_uncapped = lax.dot_general(q, k, qk_dims, preferred_element_type=float32)\n    qk = _apply_mask_and_soft_cap(qk_uncapped, mask_value, should_not_mask, mask_ref, q_sequence_ref, q_segment_ids_ref, kv_segment_ids_ref, attn_logits_soft_cap=attn_logits_soft_cap, k_slice=pl.ds(0, bkv), k_offset=global_kv_index * bkv, bq=bq, mask_function=mask_function)\n    p = jnp.exp(qk - logsumexp)\n    dp_dims = NT_DIM_NUMBERS if v_layout == HEAD_DIM_MINOR else NN_DIM_NUMBERS\n    dp = lax.dot_general(do.astype(v.dtype), v, dp_dims, preferred_element_type=jnp.float32)\n    ds = (dp - di) * p\n    if attn_logits_soft_cap is not None:\n        normalized = qk_uncapped / attn_logits_soft_cap\n        d = jnp.tanh(normalized)\n        g = ds * (1 - d)\n        ds = g + g * d\n    dq_dims = NN_DIM_NUMBERS if k_layout == HEAD_DIM_MINOR else NT_DIM_NUMBERS\n    dq_scratch_ref[...] += lax.dot_general(ds.astype(k.dtype), k, dq_dims, preferred_element_type=jnp.float32)"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@test_harnesses.parameterized(test_harnesses.all_harnesses, include_jax_unimpl=False)\n@jtu.ignore_warning(category=UserWarning, message='Using reduced precision for gradient.*')\ndef test_prim(self, harness: test_harnesses.Harness):\n    limitations = Jax2TfLimitation.limitations_for_harness(harness)\n    device = jtu.device_under_test()\n    limitations = tuple(filter(lambda l: l.filter(device=device, dtype=harness.dtype), limitations))\n    func_jax = harness.dyn_fun\n    args = harness.dyn_args_maker(self.rng())\n    if 'eigh' == harness.group_name and np.complex64 == harness.dtype and (device == 'tpu'):\n        raise unittest.SkipTest('b/264716764: error on tf.cast from c64 to f32')\n    if 'eigh' == harness.group_name and device == 'cpu':\n        raise unittest.SkipTest('Equality comparisons on eigendecompositions are not stable.')\n    if config.jax2tf_default_native_serialization.value and device == 'gpu' and ('lu' in harness.fullname):\n        raise unittest.SkipTest('b/269388847: lu failures on GPU')\n\n    def skipCustomCallTest(target: str):\n        raise unittest.SkipTest(f'TODO(b/272239584): custom call target not guaranteed stable: {target}')\n    if config.jax2tf_default_native_serialization.value:\n        if device == 'gpu':\n            if 'custom_linear_solve_' in harness.fullname:\n                skipCustomCallTest('cusolver_geqrf, cublas_geqrf_batched')\n            if 'svd_shape' in harness.fullname:\n                skipCustomCallTest('cusolver_gesvdj')\n            if 'tridiagonal_solve_shape' in harness.fullname:\n                skipCustomCallTest('cusparse_gtsv2_f32, cusparse_gtsv2_f64')\n    associative_scan_reductions = harness.params.get('associative_scan_reductions', False)\n    try:\n        with jax.jax2tf_associative_scan_reductions(associative_scan_reductions):\n            self.ConvertAndCompare(func_jax, *args, limitations=limitations)\n    except Exception as e:\n        if config.jax2tf_default_native_serialization.value and 'does not work with custom calls' in str(e):\n            logging.warning('Suppressing error %s', e)\n            raise unittest.SkipTest('b/264596006: custom calls in native serialization fail in TF')\n        else:\n            raise e",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'_{f_jax.__name__}', f_jax=f_jax) for f_jax in [jnp.add, jnp.subtract, jnp.multiply, jnp.divide, jnp.less, jnp.less_equal, jnp.equal, jnp.greater, jnp.greater_equal, jnp.not_equal, jnp.maximum, jnp.minimum]))\ndef test_type_promotion(self, f_jax=jnp.add):\n    types = [dtypes.bfloat16, np.int32, np.int64, np.float32]\n    for x_dtype in types:\n        for y_dtype in types:\n            x = np.array([1, 2], dtype=x_dtype)\n            y = np.array([3, 4], dtype=y_dtype)\n            self.ConvertAndCompare(f_jax, x, y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_integer_div(self):\n    x = jnp.array([-4, -3, -1, 0, 1, 3, 6])\n    y = np.int32(3)\n    self.ConvertAndCompare(jnp.floor_divide, x, y)\n    expected = jnp.floor_divide(x, y)\n    if not config.jax2tf_default_native_serialization.value:\n        with tf.compat.v1.Session() as sess:\n            tf1_res = sess.run(jax2tf.convert(jnp.floor_divide)(x, y))\n            self.assertAllClose(expected, tf1_res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_boolean_gather(self):\n    values = np.array([[True, True], [False, True], [False, False]], dtype=np.bool_)\n    indices = np.array([0, 1], dtype=np.int32)\n    for axis in [0, 1]:\n        f_jax = jax.jit(lambda v, i: jnp.take(v, i, axis=axis))\n        self.ConvertAndCompare(f_jax, values, indices)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_gather_rank_change(self):\n    params = jnp.array([[1.0, 1.5, 2.0], [2.0, 2.5, 3.0], [3.0, 3.5, 4.0]])\n    indices = jnp.array([[1, 1, 2], [0, 1, 0]])\n    f_jax = jax.jit(lambda i: params[i])\n    self.ConvertAndCompare(f_jax, indices)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "@jtu.sample_product(f_jax=REDUCE)\ndef test_reduce_ops_with_numerical_input(self, f_jax):\n    values = np.array([1, 2, 3], dtype=np.float32)\n    self.ConvertAndCompare(f_jax, values)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "@jtu.sample_product(op=['add', 'max', 'min', 'multiply', 'set'])\ndef test_scatter_static(self, op):\n    values = np.ones((5, 6), dtype=np.float32)\n    update = np.float32(6.0)\n    f_jax = jax.jit(lambda v, u: getattr(v.at[::2, 3:], op)(u))\n    self.ConvertAndCompare(f_jax, values, update)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "@jtu.sample_product(f_jax=REDUCE)\ndef test_reduce_ops_with_boolean_input(self, f_jax):\n    values = np.array([True, False, True], dtype=np.bool_)\n    self.ConvertAndCompare(f_jax, values)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/primitives_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  }
]