[
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def ppermute(input):\n    return jax.lax.ppermute(input, axis_name='i', perm=[[0, 1], [1, 0]])"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def ppermute(input):\n    return jax.lax.ppermute(input, axis_name='i', perm=[[0, 1], [1, 0]])"
  },
  {
    "test_code": "@jtu.ignore_warning(category=UserWarning, message='all_to_all .* are only implemented properly for TPUs and GPUs .*')\ndef test_shmap_all_to_all(self):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P(None, 'x'))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x'))\n    def f_jax(b):\n        return lax.all_to_all(b, 'x', split_axis=1, concat_axis=1, tiled=True)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(a)\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b00, b01 = np.split(b0, 2, axis=1)\n        b10, b11 = np.split(b1, 2, axis=1)\n        b0 = np.concatenate([b00, b10], axis=1)\n        b1 = np.concatenate([b01, b11], axis=1)\n        res = np.concatenate([b0, b1], axis=1)\n        self.assertAllClose(res_jax, res)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_variable_arg(self):\n\n    @partial(pjit.pjit, in_shardings=(P(None, 'x'), P('x', None)), out_shardings=None)\n    def f_jax(x, y):\n        return x @ y\n    shape_x = (10, 20)\n    x = np.arange(np.prod(shape_x), dtype=np.float32).reshape(shape_x)\n    shape_y = (20, 30)\n    y = np.arange(np.prod(shape_y), dtype=np.float32).reshape(shape_y)\n    self.log_jax_hlo(f_jax, [x, y], num_partitions=2)\n    x_v = tf.Variable(x)\n    f_tf = lambda y: jax2tf.convert(f_jax)(x_v, y)\n    self.check_sharding(f_tf, [y], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', 1), ('f32\\\\[20,30\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[10,30\\\\].*custom_call_target.*Sharding.*sharding.*replicated', 1), ('custom_call_target.*Sharding', 3)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_closed_over_const(self):\n    x = np.ones((10, 20), dtype=np.float32)\n    const = jnp.full((10, 20), 7, dtype=np.float32)\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_jax(x):\n        return (x * const).T\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', self.GEQ(1))])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_nested_pjit={nested_pjit}_constraint={constraint}_poly={poly}', nested_pjit=nested_pjit, constraint=constraint, poly=poly) for nested_pjit in (True, False) for constraint in (None, 'P') for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_sharding_constraint(self, nested_pjit=True, constraint='P', poly='2*b1,b2'):\n    constraint_sharding = P('x', None) if constraint == 'P' else None\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_jax(x):\n        y = jnp.concatenate([x, x], axis=1)\n        if nested_pjit:\n            y = pjit.pjit(lambda y: y, in_shardings=constraint_sharding, out_shardings=constraint_sharding)(y)\n        else:\n            y = jax.lax.with_sharding_constraint(y, constraint_sharding)\n        return jnp.concatenate([y, y], axis=1)\n    shape = (10, 20)\n    x = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=poly)\n    count_inner_sharding = (2 if nested_pjit else 1) if constraint == 'P' else 0\n    count_inner_replicated = (2 if nested_pjit else 1) if constraint != 'P' else 0\n    self.check_sharding(f_tf, [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', 1), ('f32\\\\[10,40\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_inner_sharding), ('f32\\\\[10,40\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_inner_replicated), ('f32\\\\[10,80\\\\].*custom_call_target.*Sharding.*sharding.*replicated', 1), ('custom_call_target.*Sharding', 2 + count_inner_sharding + count_inner_replicated)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\ndef test_grad_pjit(self, in_shardings='P', out_shardings=None):\n    if not config.jax2tf_default_native_serialization.value:\n        self.skipTest('TODO: failure in non-native serialization')\n    local_devices = list(jax.local_devices())\n    size = 2\n    if len(local_devices) < size:\n        raise unittest.SkipTest(f'Test requires {size} local devices')\n    mesh_devices = np.array(local_devices[:size]).reshape((2,))\n    mesh = jax.sharding.Mesh(mesh_devices, ('x',))\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = NamedSharding(mesh, P(None, 'x')) if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = NamedSharding(mesh, P('x', None)) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f_grad_tf(x_v, res_ct):\n        with tf.GradientTape(persistent=True) as tape:\n            tape.watch(x_v)\n            with tf.GradientTape() as tape2:\n                tape2.watch(x_v)\n                res_tf = jax2tf.convert(f_jax)(x_v)\n            dy_dx = tape.gradient(res_tf, x_v, output_gradients=res_ct)\n        d2y_dx2 = tape.gradient(dy_dx, x_v)\n        return d2y_dx2\n    count_in_P = self.GEQ(2) if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = self.GEQ(2) if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = self.GEQ(2) if in_shardings is None else 0\n    count_out_P = self.GEQ(1) if out_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_out_replicated = self.GEQ(1) if out_shardings in [None, 'missing'] else 0\n    else:\n        count_out_replicated = self.GEQ(1) if out_shardings is None else 0\n    self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if out_shardings not in [None, 'missing'] and in_shardings not in [None, 'missing']:\n        self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_grad_sharding_different_mesh(self):\n    devices = jax.local_devices()[:2]\n    if len(devices) < 2:\n        raise unittest.SkipTest('Test requires 2 local devices')\n\n    def f_jax(x):\n        return jnp.sum(x * 2.0)\n    mesh = Mesh(devices, 'i')\n    mesh_rev = Mesh(list(reversed(devices)), 'i')\n    shardings = NamedSharding(mesh, jax.sharding.PartitionSpec(('i',)))\n    shardings_rev = NamedSharding(mesh_rev, jax.sharding.PartitionSpec(('i',)))\n    f_tf = tf.function(jax2tf.convert(pjit.pjit(f_jax, in_shardings=shardings)), autograph=False)\n    f_tf_rev = tf.function(jax2tf.convert(pjit.pjit(f_jax, in_shardings=shardings_rev)), autograph=False)\n    inp = np.ones((2, 4), dtype=np.float32)\n    input_v = tf.Variable(inp)\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(input_v)\n        res_tf = f_tf(input_v)\n        g = tape.gradient(res_tf, input_v)\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(input_v)\n        res_tf_rev = f_tf_rev(input_v)\n        g_rev = tape.gradient(res_tf_rev, input_v)\n    self.assertAllClose(g, g_rev)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_func={func}', func=func) for func in ('pjit_sharded', 'pjit_replicated', 'nested_pjit_sharded', 'nested_pjit_replicated')])\ndef test_pjit_eager_error(self, func='pjit_sharded'):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('There is no error in eager mode for native serialization')\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_pjit_sharded(a):\n        return a + a\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_pjit_replicated(a):\n        return a + a\n\n    def f_nested_pjit_sharded(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=(P('x'),), out_shardings=None)(a)\n\n    def f_nested_pjit_replicated(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=None, out_shardings=None)(a)\n    shape = (8, 10)\n    a = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    if func == 'pjit_sharded':\n        f_jax = f_pjit_sharded\n    elif func == 'pjit_replicated':\n        f_jax = f_pjit_replicated\n    elif func == 'nested_pjit_sharded':\n        f_jax = f_nested_pjit_sharded\n    elif func == 'nested_pjit_replicated':\n        f_jax = f_nested_pjit_replicated\n    else:\n        assert False\n    with Mesh(self.devices, axis_names=('x',)):\n        _ = f_jax(a)\n        with self.assertRaisesRegex(ValueError, 'function with sharded arguments or results must be used under a `tf.function` context'):\n            jax2tf.convert(f_jax)(a)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.ignore_warning(category=UserWarning, message='all_to_all .* are only implemented properly for TPUs and GPUs .*')\ndef test_shmap_all_to_all(self):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P(None, 'x'))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x'))\n    def f_jax(b):\n        return lax.all_to_all(b, 'x', split_axis=1, concat_axis=1, tiled=True)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(a)\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b00, b01 = np.split(b0, 2, axis=1)\n        b10, b11 = np.split(b1, 2, axis=1)\n        b0 = np.concatenate([b00, b10], axis=1)\n        b1 = np.concatenate([b01, b11], axis=1)\n        res = np.concatenate([b0, b1], axis=1)\n        self.assertAllClose(res_jax, res)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_closed_over_const(self):\n    x = np.ones((10, 20), dtype=np.float32)\n    const = jnp.full((10, 20), 7, dtype=np.float32)\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_jax(x):\n        return (x * const).T\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', self.GEQ(1))])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_func={func}', func=func) for func in ('pjit_sharded', 'pjit_replicated', 'nested_pjit_sharded', 'nested_pjit_replicated')])\ndef test_pjit_eager_error(self, func='pjit_sharded'):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('There is no error in eager mode for native serialization')\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_pjit_sharded(a):\n        return a + a\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_pjit_replicated(a):\n        return a + a\n\n    def f_nested_pjit_sharded(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=(P('x'),), out_shardings=None)(a)\n\n    def f_nested_pjit_replicated(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=None, out_shardings=None)(a)\n    shape = (8, 10)\n    a = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    if func == 'pjit_sharded':\n        f_jax = f_pjit_sharded\n    elif func == 'pjit_replicated':\n        f_jax = f_pjit_replicated\n    elif func == 'nested_pjit_sharded':\n        f_jax = f_nested_pjit_sharded\n    elif func == 'nested_pjit_replicated':\n        f_jax = f_nested_pjit_replicated\n    else:\n        assert False\n    with Mesh(self.devices, axis_names=('x',)):\n        _ = f_jax(a)\n        with self.assertRaisesRegex(ValueError, 'function with sharded arguments or results must be used under a `tf.function` context'):\n            jax2tf.convert(f_jax)(a)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.ignore_warning(category=UserWarning, message='all_to_all .* are only implemented properly for TPUs and GPUs .*')\ndef test_shmap_all_to_all(self):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P(None, 'x'))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x'))\n    def f_jax(b):\n        return lax.all_to_all(b, 'x', split_axis=1, concat_axis=1, tiled=True)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(a)\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b00, b01 = np.split(b0, 2, axis=1)\n        b10, b11 = np.split(b1, 2, axis=1)\n        b0 = np.concatenate([b00, b10], axis=1)\n        b1 = np.concatenate([b01, b11], axis=1)\n        res = np.concatenate([b0, b1], axis=1)\n        self.assertAllClose(res_jax, res)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\ndef test_grad_pjit(self, in_shardings='P', out_shardings=None):\n    if not config.jax2tf_default_native_serialization.value:\n        self.skipTest('TODO: failure in non-native serialization')\n    local_devices = list(jax.local_devices())\n    size = 2\n    if len(local_devices) < size:\n        raise unittest.SkipTest(f'Test requires {size} local devices')\n    mesh_devices = np.array(local_devices[:size]).reshape((2,))\n    mesh = jax.sharding.Mesh(mesh_devices, ('x',))\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = NamedSharding(mesh, P(None, 'x')) if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = NamedSharding(mesh, P('x', None)) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f_grad_tf(x_v, res_ct):\n        with tf.GradientTape(persistent=True) as tape:\n            tape.watch(x_v)\n            with tf.GradientTape() as tape2:\n                tape2.watch(x_v)\n                res_tf = jax2tf.convert(f_jax)(x_v)\n            dy_dx = tape.gradient(res_tf, x_v, output_gradients=res_ct)\n        d2y_dx2 = tape.gradient(dy_dx, x_v)\n        return d2y_dx2\n    count_in_P = self.GEQ(2) if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = self.GEQ(2) if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = self.GEQ(2) if in_shardings is None else 0\n    count_out_P = self.GEQ(1) if out_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_out_replicated = self.GEQ(1) if out_shardings in [None, 'missing'] else 0\n    else:\n        count_out_replicated = self.GEQ(1) if out_shardings is None else 0\n    self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if out_shardings not in [None, 'missing'] and in_shardings not in [None, 'missing']:\n        self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_closed_over_const(self):\n    x = np.ones((10, 20), dtype=np.float32)\n    const = jnp.full((10, 20), 7, dtype=np.float32)\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_jax(x):\n        return (x * const).T\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', self.GEQ(1))])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_func={func}', func=func) for func in ('pjit_sharded', 'pjit_replicated', 'nested_pjit_sharded', 'nested_pjit_replicated')])\ndef test_pjit_eager_error(self, func='pjit_sharded'):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('There is no error in eager mode for native serialization')\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_pjit_sharded(a):\n        return a + a\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_pjit_replicated(a):\n        return a + a\n\n    def f_nested_pjit_sharded(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=(P('x'),), out_shardings=None)(a)\n\n    def f_nested_pjit_replicated(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=None, out_shardings=None)(a)\n    shape = (8, 10)\n    a = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    if func == 'pjit_sharded':\n        f_jax = f_pjit_sharded\n    elif func == 'pjit_replicated':\n        f_jax = f_pjit_replicated\n    elif func == 'nested_pjit_sharded':\n        f_jax = f_nested_pjit_sharded\n    elif func == 'nested_pjit_replicated':\n        f_jax = f_nested_pjit_replicated\n    else:\n        assert False\n    with Mesh(self.devices, axis_names=('x',)):\n        _ = f_jax(a)\n        with self.assertRaisesRegex(ValueError, 'function with sharded arguments or results must be used under a `tf.function` context'):\n            jax2tf.convert(f_jax)(a)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "@jtu.ignore_warning(category=UserWarning, message='all_to_all .* are only implemented properly for TPUs and GPUs .*')\ndef test_shmap_all_to_all(self):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P(None, 'x'))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x'))\n    def f_jax(b):\n        return lax.all_to_all(b, 'x', split_axis=1, concat_axis=1, tiled=True)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(a)\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b00, b01 = np.split(b0, 2, axis=1)\n        b10, b11 = np.split(b1, 2, axis=1)\n        b0 = np.concatenate([b00, b10], axis=1)\n        b1 = np.concatenate([b01, b11], axis=1)\n        res = np.concatenate([b0, b1], axis=1)\n        self.assertAllClose(res_jax, res)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\ndef test_grad_pjit(self, in_shardings='P', out_shardings=None):\n    if not config.jax2tf_default_native_serialization.value:\n        self.skipTest('TODO: failure in non-native serialization')\n    local_devices = list(jax.local_devices())\n    size = 2\n    if len(local_devices) < size:\n        raise unittest.SkipTest(f'Test requires {size} local devices')\n    mesh_devices = np.array(local_devices[:size]).reshape((2,))\n    mesh = jax.sharding.Mesh(mesh_devices, ('x',))\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = NamedSharding(mesh, P(None, 'x')) if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = NamedSharding(mesh, P('x', None)) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f_grad_tf(x_v, res_ct):\n        with tf.GradientTape(persistent=True) as tape:\n            tape.watch(x_v)\n            with tf.GradientTape() as tape2:\n                tape2.watch(x_v)\n                res_tf = jax2tf.convert(f_jax)(x_v)\n            dy_dx = tape.gradient(res_tf, x_v, output_gradients=res_ct)\n        d2y_dx2 = tape.gradient(dy_dx, x_v)\n        return d2y_dx2\n    count_in_P = self.GEQ(2) if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = self.GEQ(2) if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = self.GEQ(2) if in_shardings is None else 0\n    count_out_P = self.GEQ(1) if out_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_out_replicated = self.GEQ(1) if out_shardings in [None, 'missing'] else 0\n    else:\n        count_out_replicated = self.GEQ(1) if out_shardings is None else 0\n    self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if out_shardings not in [None, 'missing'] and in_shardings not in [None, 'missing']:\n        self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_closed_over_const(self):\n    x = np.ones((10, 20), dtype=np.float32)\n    const = jnp.full((10, 20), 7, dtype=np.float32)\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_jax(x):\n        return (x * const).T\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', self.GEQ(1))])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_func={func}', func=func) for func in ('pjit_sharded', 'pjit_replicated', 'nested_pjit_sharded', 'nested_pjit_replicated')])\ndef test_pjit_eager_error(self, func='pjit_sharded'):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('There is no error in eager mode for native serialization')\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_pjit_sharded(a):\n        return a + a\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_pjit_replicated(a):\n        return a + a\n\n    def f_nested_pjit_sharded(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=(P('x'),), out_shardings=None)(a)\n\n    def f_nested_pjit_replicated(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=None, out_shardings=None)(a)\n    shape = (8, 10)\n    a = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    if func == 'pjit_sharded':\n        f_jax = f_pjit_sharded\n    elif func == 'pjit_replicated':\n        f_jax = f_pjit_replicated\n    elif func == 'nested_pjit_sharded':\n        f_jax = f_nested_pjit_sharded\n    elif func == 'nested_pjit_replicated':\n        f_jax = f_nested_pjit_replicated\n    else:\n        assert False\n    with Mesh(self.devices, axis_names=('x',)):\n        _ = f_jax(a)\n        with self.assertRaisesRegex(ValueError, 'function with sharded arguments or results must be used under a `tf.function` context'):\n            jax2tf.convert(f_jax)(a)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "@jtu.ignore_warning(category=UserWarning, message='all_to_all .* are only implemented properly for TPUs and GPUs .*')\ndef test_shmap_all_to_all(self):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P(None, 'x'))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x'))\n    def f_jax(b):\n        return lax.all_to_all(b, 'x', split_axis=1, concat_axis=1, tiled=True)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(a)\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b00, b01 = np.split(b0, 2, axis=1)\n        b10, b11 = np.split(b1, 2, axis=1)\n        b0 = np.concatenate([b00, b10], axis=1)\n        b1 = np.concatenate([b01, b11], axis=1)\n        res = np.concatenate([b0, b1], axis=1)\n        self.assertAllClose(res_jax, res)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\ndef test_grad_pjit(self, in_shardings='P', out_shardings=None):\n    if not config.jax2tf_default_native_serialization.value:\n        self.skipTest('TODO: failure in non-native serialization')\n    local_devices = list(jax.local_devices())\n    size = 2\n    if len(local_devices) < size:\n        raise unittest.SkipTest(f'Test requires {size} local devices')\n    mesh_devices = np.array(local_devices[:size]).reshape((2,))\n    mesh = jax.sharding.Mesh(mesh_devices, ('x',))\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = NamedSharding(mesh, P(None, 'x')) if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = NamedSharding(mesh, P('x', None)) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f_grad_tf(x_v, res_ct):\n        with tf.GradientTape(persistent=True) as tape:\n            tape.watch(x_v)\n            with tf.GradientTape() as tape2:\n                tape2.watch(x_v)\n                res_tf = jax2tf.convert(f_jax)(x_v)\n            dy_dx = tape.gradient(res_tf, x_v, output_gradients=res_ct)\n        d2y_dx2 = tape.gradient(dy_dx, x_v)\n        return d2y_dx2\n    count_in_P = self.GEQ(2) if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = self.GEQ(2) if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = self.GEQ(2) if in_shardings is None else 0\n    count_out_P = self.GEQ(1) if out_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_out_replicated = self.GEQ(1) if out_shardings in [None, 'missing'] else 0\n    else:\n        count_out_replicated = self.GEQ(1) if out_shardings is None else 0\n    self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if out_shardings not in [None, 'missing'] and in_shardings not in [None, 'missing']:\n        self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_grad_sharding_different_mesh(self):\n    devices = jax.local_devices()[:2]\n    if len(devices) < 2:\n        raise unittest.SkipTest('Test requires 2 local devices')\n\n    def f_jax(x):\n        return jnp.sum(x * 2.0)\n    mesh = Mesh(devices, 'i')\n    mesh_rev = Mesh(list(reversed(devices)), 'i')\n    shardings = NamedSharding(mesh, jax.sharding.PartitionSpec(('i',)))\n    shardings_rev = NamedSharding(mesh_rev, jax.sharding.PartitionSpec(('i',)))\n    f_tf = tf.function(jax2tf.convert(pjit.pjit(f_jax, in_shardings=shardings)), autograph=False)\n    f_tf_rev = tf.function(jax2tf.convert(pjit.pjit(f_jax, in_shardings=shardings_rev)), autograph=False)\n    inp = np.ones((2, 4), dtype=np.float32)\n    input_v = tf.Variable(inp)\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(input_v)\n        res_tf = f_tf(input_v)\n        g = tape.gradient(res_tf, input_v)\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(input_v)\n        res_tf_rev = f_tf_rev(input_v)\n        g_rev = tape.gradient(res_tf_rev, input_v)\n    self.assertAllClose(g, g_rev)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_variable_arg(self):\n\n    @partial(pjit.pjit, in_shardings=(P(None, 'x'), P('x', None)), out_shardings=None)\n    def f_jax(x, y):\n        return x @ y\n    shape_x = (10, 20)\n    x = np.arange(np.prod(shape_x), dtype=np.float32).reshape(shape_x)\n    shape_y = (20, 30)\n    y = np.arange(np.prod(shape_y), dtype=np.float32).reshape(shape_y)\n    self.log_jax_hlo(f_jax, [x, y], num_partitions=2)\n    x_v = tf.Variable(x)\n    f_tf = lambda y: jax2tf.convert(f_jax)(x_v, y)\n    self.check_sharding(f_tf, [y], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', 1), ('f32\\\\[20,30\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[10,30\\\\].*custom_call_target.*Sharding.*sharding.*replicated', 1), ('custom_call_target.*Sharding', 3)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_closed_over_const(self):\n    x = np.ones((10, 20), dtype=np.float32)\n    const = jnp.full((10, 20), 7, dtype=np.float32)\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_jax(x):\n        return (x * const).T\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', self.GEQ(1))])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_nested_pjit={nested_pjit}_constraint={constraint}_poly={poly}', nested_pjit=nested_pjit, constraint=constraint, poly=poly) for nested_pjit in (True, False) for constraint in (None, 'P') for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_sharding_constraint(self, nested_pjit=True, constraint='P', poly='2*b1,b2'):\n    constraint_sharding = P('x', None) if constraint == 'P' else None\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_jax(x):\n        y = jnp.concatenate([x, x], axis=1)\n        if nested_pjit:\n            y = pjit.pjit(lambda y: y, in_shardings=constraint_sharding, out_shardings=constraint_sharding)(y)\n        else:\n            y = jax.lax.with_sharding_constraint(y, constraint_sharding)\n        return jnp.concatenate([y, y], axis=1)\n    shape = (10, 20)\n    x = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=poly)\n    count_inner_sharding = (2 if nested_pjit else 1) if constraint == 'P' else 0\n    count_inner_replicated = (2 if nested_pjit else 1) if constraint != 'P' else 0\n    self.check_sharding(f_tf, [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', 1), ('f32\\\\[10,40\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_inner_sharding), ('f32\\\\[10,40\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_inner_replicated), ('f32\\\\[10,80\\\\].*custom_call_target.*Sharding.*sharding.*replicated', 1), ('custom_call_target.*Sharding', 2 + count_inner_sharding + count_inner_replicated)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\ndef test_grad_pjit(self, in_shardings='P', out_shardings=None):\n    if not config.jax2tf_default_native_serialization.value:\n        self.skipTest('TODO: failure in non-native serialization')\n    local_devices = list(jax.local_devices())\n    size = 2\n    if len(local_devices) < size:\n        raise unittest.SkipTest(f'Test requires {size} local devices')\n    mesh_devices = np.array(local_devices[:size]).reshape((2,))\n    mesh = jax.sharding.Mesh(mesh_devices, ('x',))\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = NamedSharding(mesh, P(None, 'x')) if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = NamedSharding(mesh, P('x', None)) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f_grad_tf(x_v, res_ct):\n        with tf.GradientTape(persistent=True) as tape:\n            tape.watch(x_v)\n            with tf.GradientTape() as tape2:\n                tape2.watch(x_v)\n                res_tf = jax2tf.convert(f_jax)(x_v)\n            dy_dx = tape.gradient(res_tf, x_v, output_gradients=res_ct)\n        d2y_dx2 = tape.gradient(dy_dx, x_v)\n        return d2y_dx2\n    count_in_P = self.GEQ(2) if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = self.GEQ(2) if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = self.GEQ(2) if in_shardings is None else 0\n    count_out_P = self.GEQ(1) if out_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_out_replicated = self.GEQ(1) if out_shardings in [None, 'missing'] else 0\n    else:\n        count_out_replicated = self.GEQ(1) if out_shardings is None else 0\n    self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if out_shardings not in [None, 'missing'] and in_shardings not in [None, 'missing']:\n        self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_grad_sharding_different_mesh(self):\n    devices = jax.local_devices()[:2]\n    if len(devices) < 2:\n        raise unittest.SkipTest('Test requires 2 local devices')\n\n    def f_jax(x):\n        return jnp.sum(x * 2.0)\n    mesh = Mesh(devices, 'i')\n    mesh_rev = Mesh(list(reversed(devices)), 'i')\n    shardings = NamedSharding(mesh, jax.sharding.PartitionSpec(('i',)))\n    shardings_rev = NamedSharding(mesh_rev, jax.sharding.PartitionSpec(('i',)))\n    f_tf = tf.function(jax2tf.convert(pjit.pjit(f_jax, in_shardings=shardings)), autograph=False)\n    f_tf_rev = tf.function(jax2tf.convert(pjit.pjit(f_jax, in_shardings=shardings_rev)), autograph=False)\n    inp = np.ones((2, 4), dtype=np.float32)\n    input_v = tf.Variable(inp)\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(input_v)\n        res_tf = f_tf(input_v)\n        g = tape.gradient(res_tf, input_v)\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(input_v)\n        res_tf_rev = f_tf_rev(input_v)\n        g_rev = tape.gradient(res_tf_rev, input_v)\n    self.assertAllClose(g, g_rev)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_func={func}', func=func) for func in ('pjit_sharded', 'pjit_replicated', 'nested_pjit_sharded', 'nested_pjit_replicated')])\ndef test_pjit_eager_error(self, func='pjit_sharded'):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('There is no error in eager mode for native serialization')\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_pjit_sharded(a):\n        return a + a\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_pjit_replicated(a):\n        return a + a\n\n    def f_nested_pjit_sharded(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=(P('x'),), out_shardings=None)(a)\n\n    def f_nested_pjit_replicated(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=None, out_shardings=None)(a)\n    shape = (8, 10)\n    a = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    if func == 'pjit_sharded':\n        f_jax = f_pjit_sharded\n    elif func == 'pjit_replicated':\n        f_jax = f_pjit_replicated\n    elif func == 'nested_pjit_sharded':\n        f_jax = f_nested_pjit_sharded\n    elif func == 'nested_pjit_replicated':\n        f_jax = f_nested_pjit_replicated\n    else:\n        assert False\n    with Mesh(self.devices, axis_names=('x',)):\n        _ = f_jax(a)\n        with self.assertRaisesRegex(ValueError, 'function with sharded arguments or results must be used under a `tf.function` context'):\n            jax2tf.convert(f_jax)(a)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.ignore_warning(category=UserWarning, message='all_to_all .* are only implemented properly for TPUs and GPUs .*')\ndef test_shmap_all_to_all(self):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P(None, 'x'))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x'))\n    def f_jax(b):\n        return lax.all_to_all(b, 'x', split_axis=1, concat_axis=1, tiled=True)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(a)\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b00, b01 = np.split(b0, 2, axis=1)\n        b10, b11 = np.split(b1, 2, axis=1)\n        b0 = np.concatenate([b00, b10], axis=1)\n        b1 = np.concatenate([b01, b11], axis=1)\n        res = np.concatenate([b0, b1], axis=1)\n        self.assertAllClose(res_jax, res)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\ndef test_grad_pjit(self, in_shardings='P', out_shardings=None):\n    if not config.jax2tf_default_native_serialization.value:\n        self.skipTest('TODO: failure in non-native serialization')\n    local_devices = list(jax.local_devices())\n    size = 2\n    if len(local_devices) < size:\n        raise unittest.SkipTest(f'Test requires {size} local devices')\n    mesh_devices = np.array(local_devices[:size]).reshape((2,))\n    mesh = jax.sharding.Mesh(mesh_devices, ('x',))\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = NamedSharding(mesh, P(None, 'x')) if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = NamedSharding(mesh, P('x', None)) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f_grad_tf(x_v, res_ct):\n        with tf.GradientTape(persistent=True) as tape:\n            tape.watch(x_v)\n            with tf.GradientTape() as tape2:\n                tape2.watch(x_v)\n                res_tf = jax2tf.convert(f_jax)(x_v)\n            dy_dx = tape.gradient(res_tf, x_v, output_gradients=res_ct)\n        d2y_dx2 = tape.gradient(dy_dx, x_v)\n        return d2y_dx2\n    count_in_P = self.GEQ(2) if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = self.GEQ(2) if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = self.GEQ(2) if in_shardings is None else 0\n    count_out_P = self.GEQ(1) if out_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_out_replicated = self.GEQ(1) if out_shardings in [None, 'missing'] else 0\n    else:\n        count_out_replicated = self.GEQ(1) if out_shardings is None else 0\n    self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if out_shardings not in [None, 'missing'] and in_shardings not in [None, 'missing']:\n        self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_variable_arg(self):\n\n    @partial(pjit.pjit, in_shardings=(P(None, 'x'), P('x', None)), out_shardings=None)\n    def f_jax(x, y):\n        return x @ y\n    shape_x = (10, 20)\n    x = np.arange(np.prod(shape_x), dtype=np.float32).reshape(shape_x)\n    shape_y = (20, 30)\n    y = np.arange(np.prod(shape_y), dtype=np.float32).reshape(shape_y)\n    self.log_jax_hlo(f_jax, [x, y], num_partitions=2)\n    x_v = tf.Variable(x)\n    f_tf = lambda y: jax2tf.convert(f_jax)(x_v, y)\n    self.check_sharding(f_tf, [y], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', 1), ('f32\\\\[20,30\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[10,30\\\\].*custom_call_target.*Sharding.*sharding.*replicated', 1), ('custom_call_target.*Sharding', 3)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_closed_over_const(self):\n    x = np.ones((10, 20), dtype=np.float32)\n    const = jnp.full((10, 20), 7, dtype=np.float32)\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_jax(x):\n        return (x * const).T\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', self.GEQ(1))])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_nested_pjit={nested_pjit}_constraint={constraint}_poly={poly}', nested_pjit=nested_pjit, constraint=constraint, poly=poly) for nested_pjit in (True, False) for constraint in (None, 'P') for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_sharding_constraint(self, nested_pjit=True, constraint='P', poly='2*b1,b2'):\n    constraint_sharding = P('x', None) if constraint == 'P' else None\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_jax(x):\n        y = jnp.concatenate([x, x], axis=1)\n        if nested_pjit:\n            y = pjit.pjit(lambda y: y, in_shardings=constraint_sharding, out_shardings=constraint_sharding)(y)\n        else:\n            y = jax.lax.with_sharding_constraint(y, constraint_sharding)\n        return jnp.concatenate([y, y], axis=1)\n    shape = (10, 20)\n    x = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=poly)\n    count_inner_sharding = (2 if nested_pjit else 1) if constraint == 'P' else 0\n    count_inner_replicated = (2 if nested_pjit else 1) if constraint != 'P' else 0\n    self.check_sharding(f_tf, [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', 1), ('f32\\\\[10,40\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_inner_sharding), ('f32\\\\[10,40\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_inner_replicated), ('f32\\\\[10,80\\\\].*custom_call_target.*Sharding.*sharding.*replicated', 1), ('custom_call_target.*Sharding', 2 + count_inner_sharding + count_inner_replicated)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\ndef test_grad_pjit(self, in_shardings='P', out_shardings=None):\n    if not config.jax2tf_default_native_serialization.value:\n        self.skipTest('TODO: failure in non-native serialization')\n    local_devices = list(jax.local_devices())\n    size = 2\n    if len(local_devices) < size:\n        raise unittest.SkipTest(f'Test requires {size} local devices')\n    mesh_devices = np.array(local_devices[:size]).reshape((2,))\n    mesh = jax.sharding.Mesh(mesh_devices, ('x',))\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = NamedSharding(mesh, P(None, 'x')) if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = NamedSharding(mesh, P('x', None)) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f_grad_tf(x_v, res_ct):\n        with tf.GradientTape(persistent=True) as tape:\n            tape.watch(x_v)\n            with tf.GradientTape() as tape2:\n                tape2.watch(x_v)\n                res_tf = jax2tf.convert(f_jax)(x_v)\n            dy_dx = tape.gradient(res_tf, x_v, output_gradients=res_ct)\n        d2y_dx2 = tape.gradient(dy_dx, x_v)\n        return d2y_dx2\n    count_in_P = self.GEQ(2) if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = self.GEQ(2) if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = self.GEQ(2) if in_shardings is None else 0\n    count_out_P = self.GEQ(1) if out_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_out_replicated = self.GEQ(1) if out_shardings in [None, 'missing'] else 0\n    else:\n        count_out_replicated = self.GEQ(1) if out_shardings is None else 0\n    self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if out_shardings not in [None, 'missing'] and in_shardings not in [None, 'missing']:\n        self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_grad_sharding_different_mesh(self):\n    devices = jax.local_devices()[:2]\n    if len(devices) < 2:\n        raise unittest.SkipTest('Test requires 2 local devices')\n\n    def f_jax(x):\n        return jnp.sum(x * 2.0)\n    mesh = Mesh(devices, 'i')\n    mesh_rev = Mesh(list(reversed(devices)), 'i')\n    shardings = NamedSharding(mesh, jax.sharding.PartitionSpec(('i',)))\n    shardings_rev = NamedSharding(mesh_rev, jax.sharding.PartitionSpec(('i',)))\n    f_tf = tf.function(jax2tf.convert(pjit.pjit(f_jax, in_shardings=shardings)), autograph=False)\n    f_tf_rev = tf.function(jax2tf.convert(pjit.pjit(f_jax, in_shardings=shardings_rev)), autograph=False)\n    inp = np.ones((2, 4), dtype=np.float32)\n    input_v = tf.Variable(inp)\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(input_v)\n        res_tf = f_tf(input_v)\n        g = tape.gradient(res_tf, input_v)\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(input_v)\n        res_tf_rev = f_tf_rev(input_v)\n        g_rev = tape.gradient(res_tf_rev, input_v)\n    self.assertAllClose(g, g_rev)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_func={func}', func=func) for func in ('pjit_sharded', 'pjit_replicated', 'nested_pjit_sharded', 'nested_pjit_replicated')])\ndef test_pjit_eager_error(self, func='pjit_sharded'):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('There is no error in eager mode for native serialization')\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_pjit_sharded(a):\n        return a + a\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_pjit_replicated(a):\n        return a + a\n\n    def f_nested_pjit_sharded(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=(P('x'),), out_shardings=None)(a)\n\n    def f_nested_pjit_replicated(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=None, out_shardings=None)(a)\n    shape = (8, 10)\n    a = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    if func == 'pjit_sharded':\n        f_jax = f_pjit_sharded\n    elif func == 'pjit_replicated':\n        f_jax = f_pjit_replicated\n    elif func == 'nested_pjit_sharded':\n        f_jax = f_nested_pjit_sharded\n    elif func == 'nested_pjit_replicated':\n        f_jax = f_nested_pjit_replicated\n    else:\n        assert False\n    with Mesh(self.devices, axis_names=('x',)):\n        _ = f_jax(a)\n        with self.assertRaisesRegex(ValueError, 'function with sharded arguments or results must be used under a `tf.function` context'):\n            jax2tf.convert(f_jax)(a)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.ignore_warning(category=UserWarning, message='all_to_all .* are only implemented properly for TPUs and GPUs .*')\ndef test_shmap_all_to_all(self):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P(None, 'x'))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x'))\n    def f_jax(b):\n        return lax.all_to_all(b, 'x', split_axis=1, concat_axis=1, tiled=True)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(a)\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b00, b01 = np.split(b0, 2, axis=1)\n        b10, b11 = np.split(b1, 2, axis=1)\n        b0 = np.concatenate([b00, b10], axis=1)\n        b1 = np.concatenate([b01, b11], axis=1)\n        res = np.concatenate([b0, b1], axis=1)\n        self.assertAllClose(res_jax, res)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_variable_arg(self):\n\n    @partial(pjit.pjit, in_shardings=(P(None, 'x'), P('x', None)), out_shardings=None)\n    def f_jax(x, y):\n        return x @ y\n    shape_x = (10, 20)\n    x = np.arange(np.prod(shape_x), dtype=np.float32).reshape(shape_x)\n    shape_y = (20, 30)\n    y = np.arange(np.prod(shape_y), dtype=np.float32).reshape(shape_y)\n    self.log_jax_hlo(f_jax, [x, y], num_partitions=2)\n    x_v = tf.Variable(x)\n    f_tf = lambda y: jax2tf.convert(f_jax)(x_v, y)\n    self.check_sharding(f_tf, [y], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', 1), ('f32\\\\[20,30\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[10,30\\\\].*custom_call_target.*Sharding.*sharding.*replicated', 1), ('custom_call_target.*Sharding', 3)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_closed_over_const(self):\n    x = np.ones((10, 20), dtype=np.float32)\n    const = jnp.full((10, 20), 7, dtype=np.float32)\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_jax(x):\n        return (x * const).T\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', self.GEQ(1))])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_nested_pjit={nested_pjit}_constraint={constraint}_poly={poly}', nested_pjit=nested_pjit, constraint=constraint, poly=poly) for nested_pjit in (True, False) for constraint in (None, 'P') for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_sharding_constraint(self, nested_pjit=True, constraint='P', poly='2*b1,b2'):\n    constraint_sharding = P('x', None) if constraint == 'P' else None\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_jax(x):\n        y = jnp.concatenate([x, x], axis=1)\n        if nested_pjit:\n            y = pjit.pjit(lambda y: y, in_shardings=constraint_sharding, out_shardings=constraint_sharding)(y)\n        else:\n            y = jax.lax.with_sharding_constraint(y, constraint_sharding)\n        return jnp.concatenate([y, y], axis=1)\n    shape = (10, 20)\n    x = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n    f_tf = jax2tf.convert(f_jax, polymorphic_shapes=poly)\n    count_inner_sharding = (2 if nested_pjit else 1) if constraint == 'P' else 0\n    count_inner_replicated = (2 if nested_pjit else 1) if constraint != 'P' else 0\n    self.check_sharding(f_tf, [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', 1), ('f32\\\\[10,40\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_inner_sharding), ('f32\\\\[10,40\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_inner_replicated), ('f32\\\\[10,80\\\\].*custom_call_target.*Sharding.*sharding.*replicated', 1), ('custom_call_target.*Sharding', 2 + count_inner_sharding + count_inner_replicated)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\ndef test_grad_pjit(self, in_shardings='P', out_shardings=None):\n    if not config.jax2tf_default_native_serialization.value:\n        self.skipTest('TODO: failure in non-native serialization')\n    local_devices = list(jax.local_devices())\n    size = 2\n    if len(local_devices) < size:\n        raise unittest.SkipTest(f'Test requires {size} local devices')\n    mesh_devices = np.array(local_devices[:size]).reshape((2,))\n    mesh = jax.sharding.Mesh(mesh_devices, ('x',))\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = NamedSharding(mesh, P(None, 'x')) if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = NamedSharding(mesh, P('x', None)) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n\n    def f_grad_tf(x_v, res_ct):\n        with tf.GradientTape(persistent=True) as tape:\n            tape.watch(x_v)\n            with tf.GradientTape() as tape2:\n                tape2.watch(x_v)\n                res_tf = jax2tf.convert(f_jax)(x_v)\n            dy_dx = tape.gradient(res_tf, x_v, output_gradients=res_ct)\n        d2y_dx2 = tape.gradient(dy_dx, x_v)\n        return d2y_dx2\n    count_in_P = self.GEQ(2) if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = self.GEQ(2) if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = self.GEQ(2) if in_shardings is None else 0\n    count_out_P = self.GEQ(1) if out_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_out_replicated = self.GEQ(1) if out_shardings in [None, 'missing'] else 0\n    else:\n        count_out_replicated = self.GEQ(1) if out_shardings is None else 0\n    self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if out_shardings not in [None, 'missing'] and in_shardings not in [None, 'missing']:\n        self.check_sharding(f_grad_tf, [x, x.T], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_grad_sharding_different_mesh(self):\n    devices = jax.local_devices()[:2]\n    if len(devices) < 2:\n        raise unittest.SkipTest('Test requires 2 local devices')\n\n    def f_jax(x):\n        return jnp.sum(x * 2.0)\n    mesh = Mesh(devices, 'i')\n    mesh_rev = Mesh(list(reversed(devices)), 'i')\n    shardings = NamedSharding(mesh, jax.sharding.PartitionSpec(('i',)))\n    shardings_rev = NamedSharding(mesh_rev, jax.sharding.PartitionSpec(('i',)))\n    f_tf = tf.function(jax2tf.convert(pjit.pjit(f_jax, in_shardings=shardings)), autograph=False)\n    f_tf_rev = tf.function(jax2tf.convert(pjit.pjit(f_jax, in_shardings=shardings_rev)), autograph=False)\n    inp = np.ones((2, 4), dtype=np.float32)\n    input_v = tf.Variable(inp)\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(input_v)\n        res_tf = f_tf(input_v)\n        g = tape.gradient(res_tf, input_v)\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(input_v)\n        res_tf_rev = f_tf_rev(input_v)\n        g_rev = tape.gradient(res_tf_rev, input_v)\n    self.assertAllClose(g, g_rev)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_func={func}', func=func) for func in ('pjit_sharded', 'pjit_replicated', 'nested_pjit_sharded', 'nested_pjit_replicated')])\ndef test_pjit_eager_error(self, func='pjit_sharded'):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('There is no error in eager mode for native serialization')\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_pjit_sharded(a):\n        return a + a\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_pjit_replicated(a):\n        return a + a\n\n    def f_nested_pjit_sharded(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=(P('x'),), out_shardings=None)(a)\n\n    def f_nested_pjit_replicated(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=None, out_shardings=None)(a)\n    shape = (8, 10)\n    a = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    if func == 'pjit_sharded':\n        f_jax = f_pjit_sharded\n    elif func == 'pjit_replicated':\n        f_jax = f_pjit_replicated\n    elif func == 'nested_pjit_sharded':\n        f_jax = f_nested_pjit_sharded\n    elif func == 'nested_pjit_replicated':\n        f_jax = f_nested_pjit_replicated\n    else:\n        assert False\n    with Mesh(self.devices, axis_names=('x',)):\n        _ = f_jax(a)\n        with self.assertRaisesRegex(ValueError, 'function with sharded arguments or results must be used under a `tf.function` context'):\n            jax2tf.convert(f_jax)(a)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.ignore_warning(category=UserWarning, message='all_to_all .* are only implemented properly for TPUs and GPUs .*')\ndef test_shmap_all_to_all(self):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P(None, 'x'))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x'))\n    def f_jax(b):\n        return lax.all_to_all(b, 'x', split_axis=1, concat_axis=1, tiled=True)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(a)\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b00, b01 = np.split(b0, 2, axis=1)\n        b10, b11 = np.split(b1, 2, axis=1)\n        b0 = np.concatenate([b00, b10], axis=1)\n        b1 = np.concatenate([b01, b11], axis=1)\n        res = np.concatenate([b0, b1], axis=1)\n        self.assertAllClose(res_jax, res)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_closed_over_const(self):\n    x = np.ones((10, 20), dtype=np.float32)\n    const = jnp.full((10, 20), 7, dtype=np.float32)\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_jax(x):\n        return (x * const).T\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', self.GEQ(1))])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_func={func}', func=func) for func in ('pjit_sharded', 'pjit_replicated', 'nested_pjit_sharded', 'nested_pjit_replicated')])\ndef test_pjit_eager_error(self, func='pjit_sharded'):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('There is no error in eager mode for native serialization')\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_pjit_sharded(a):\n        return a + a\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_pjit_replicated(a):\n        return a + a\n\n    def f_nested_pjit_sharded(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=(P('x'),), out_shardings=None)(a)\n\n    def f_nested_pjit_replicated(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=None, out_shardings=None)(a)\n    shape = (8, 10)\n    a = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    if func == 'pjit_sharded':\n        f_jax = f_pjit_sharded\n    elif func == 'pjit_replicated':\n        f_jax = f_pjit_replicated\n    elif func == 'nested_pjit_sharded':\n        f_jax = f_nested_pjit_sharded\n    elif func == 'nested_pjit_replicated':\n        f_jax = f_nested_pjit_replicated\n    else:\n        assert False\n    with Mesh(self.devices, axis_names=('x',)):\n        _ = f_jax(a)\n        with self.assertRaisesRegex(ValueError, 'function with sharded arguments or results must be used under a `tf.function` context'):\n            jax2tf.convert(f_jax)(a)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "@jtu.ignore_warning(category=UserWarning, message='all_to_all .* are only implemented properly for TPUs and GPUs .*')\ndef test_shmap_all_to_all(self):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P(None, 'x'))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x'))\n    def f_jax(b):\n        return lax.all_to_all(b, 'x', split_axis=1, concat_axis=1, tiled=True)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(a)\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b00, b01 = np.split(b0, 2, axis=1)\n        b10, b11 = np.split(b1, 2, axis=1)\n        b0 = np.concatenate([b00, b10], axis=1)\n        b1 = np.concatenate([b01, b11], axis=1)\n        res = np.concatenate([b0, b1], axis=1)\n        self.assertAllClose(res_jax, res)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_closed_over_const(self):\n    x = np.ones((10, 20), dtype=np.float32)\n    const = jnp.full((10, 20), 7, dtype=np.float32)\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_jax(x):\n        return (x * const).T\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', self.GEQ(1))])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_func={func}', func=func) for func in ('pjit_sharded', 'pjit_replicated', 'nested_pjit_sharded', 'nested_pjit_replicated')])\ndef test_pjit_eager_error(self, func='pjit_sharded'):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('There is no error in eager mode for native serialization')\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_pjit_sharded(a):\n        return a + a\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_pjit_replicated(a):\n        return a + a\n\n    def f_nested_pjit_sharded(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=(P('x'),), out_shardings=None)(a)\n\n    def f_nested_pjit_replicated(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=None, out_shardings=None)(a)\n    shape = (8, 10)\n    a = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    if func == 'pjit_sharded':\n        f_jax = f_pjit_sharded\n    elif func == 'pjit_replicated':\n        f_jax = f_pjit_replicated\n    elif func == 'nested_pjit_sharded':\n        f_jax = f_nested_pjit_sharded\n    elif func == 'nested_pjit_replicated':\n        f_jax = f_nested_pjit_replicated\n    else:\n        assert False\n    with Mesh(self.devices, axis_names=('x',)):\n        _ = f_jax(a)\n        with self.assertRaisesRegex(ValueError, 'function with sharded arguments or results must be used under a `tf.function` context'):\n            jax2tf.convert(f_jax)(a)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.ignore_warning(category=UserWarning, message='all_to_all .* are only implemented properly for TPUs and GPUs .*')\ndef test_shmap_all_to_all(self):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P(None, 'x'))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x'))\n    def f_jax(b):\n        return lax.all_to_all(b, 'x', split_axis=1, concat_axis=1, tiled=True)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(a)\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b00, b01 = np.split(b0, 2, axis=1)\n        b10, b11 = np.split(b1, 2, axis=1)\n        b0 = np.concatenate([b00, b10], axis=1)\n        b1 = np.concatenate([b01, b11], axis=1)\n        res = np.concatenate([b0, b1], axis=1)\n        self.assertAllClose(res_jax, res)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_closed_over_const(self):\n    x = np.ones((10, 20), dtype=np.float32)\n    const = jnp.full((10, 20), 7, dtype=np.float32)\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_jax(x):\n        return (x * const).T\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', self.GEQ(1))])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_func={func}', func=func) for func in ('pjit_sharded', 'pjit_replicated', 'nested_pjit_sharded', 'nested_pjit_replicated')])\ndef test_pjit_eager_error(self, func='pjit_sharded'):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('There is no error in eager mode for native serialization')\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_pjit_sharded(a):\n        return a + a\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_pjit_replicated(a):\n        return a + a\n\n    def f_nested_pjit_sharded(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=(P('x'),), out_shardings=None)(a)\n\n    def f_nested_pjit_replicated(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=None, out_shardings=None)(a)\n    shape = (8, 10)\n    a = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    if func == 'pjit_sharded':\n        f_jax = f_pjit_sharded\n    elif func == 'pjit_replicated':\n        f_jax = f_pjit_replicated\n    elif func == 'nested_pjit_sharded':\n        f_jax = f_nested_pjit_sharded\n    elif func == 'nested_pjit_replicated':\n        f_jax = f_nested_pjit_replicated\n    else:\n        assert False\n    with Mesh(self.devices, axis_names=('x',)):\n        _ = f_jax(a)\n        with self.assertRaisesRegex(ValueError, 'function with sharded arguments or results must be used under a `tf.function` context'):\n            jax2tf.convert(f_jax)(a)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "@jtu.ignore_warning(category=UserWarning, message='all_to_all .* are only implemented properly for TPUs and GPUs .*')\ndef test_shmap_all_to_all(self):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P(None, 'x'))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x'))\n    def f_jax(b):\n        return lax.all_to_all(b, 'x', split_axis=1, concat_axis=1, tiled=True)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(a)\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b00, b01 = np.split(b0, 2, axis=1)\n        b10, b11 = np.split(b1, 2, axis=1)\n        b0 = np.concatenate([b00, b10], axis=1)\n        b1 = np.concatenate([b01, b11], axis=1)\n        res = np.concatenate([b0, b1], axis=1)\n        self.assertAllClose(res_jax, res)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_in_shardings={in_shardings}_out_shardings={out_shardings}', in_shardings=in_shardings, out_shardings=out_shardings) for in_shardings in ('missing', None, 'P') for out_shardings in ('missing', None, 'P')])\n@jtu.with_mesh([('x', 2)])\ndef test_pjit_basic(self, in_shardings='P', out_shardings='P'):\n\n    def f_jax(x):\n        return jnp.sin(x.T)\n    pjit_kwargs = {}\n    if in_shardings != 'missing':\n        pjit_kwargs['in_shardings'] = P(None, 'x') if in_shardings == 'P' else None\n    if out_shardings != 'missing':\n        pjit_kwargs['out_shardings'] = P('x', None) if out_shardings == 'P' else None\n    f_jax = pjit.pjit(f_jax, **pjit_kwargs)\n    x_shape = (10, 20)\n    x = np.arange(np.prod(x_shape), dtype=np.float32).reshape(x_shape)\n    self.log_jax_hlo(f_jax, [x], num_partitions=2)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    count_in_P = 1 if in_shardings == 'P' else 0\n    if config.jax2tf_default_native_serialization.value:\n        count_in_replicated = 1 if in_shardings in [None, 'missing'] else 0\n    else:\n        count_in_replicated = 1 if in_shardings is None else 0\n    count_out_P = 1 if out_shardings == 'P' else 0\n    count_out_replicated = 1 if out_shardings is None else 0\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[1,2\\\\]', count_in_P), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', count_out_P)])\n    if in_shardings not in [None, 'missing'] and out_shardings is not None:\n        self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_in_replicated), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', count_out_replicated), ('custom_call_target.*Sharding', count_in_P + count_in_replicated + count_out_P + count_out_replicated)])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf.numpy(), res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  },
  {
    "test_code": "@jtu.with_mesh([('x', 2)])\ndef test_pjit_closed_over_const(self):\n    x = np.ones((10, 20), dtype=np.float32)\n    const = jnp.full((10, 20), 7, dtype=np.float32)\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_jax(x):\n        return (x * const).T\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(x):\n        f_converted = jax2tf.convert(f_jax)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(x)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(x)\n    self.check_sharding(jax2tf.convert(f_jax), [x], checks=[('f32\\\\[10,20\\\\].*custom_call_target.*Sharding.*sharding.*devices=\\\\[2,1\\\\]', 1), ('f32\\\\[20,10\\\\].*custom_call_target.*Sharding.*sharding.*replicated', self.GEQ(1))])\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_func={func}', func=func) for func in ('pjit_sharded', 'pjit_replicated', 'nested_pjit_sharded', 'nested_pjit_replicated')])\ndef test_pjit_eager_error(self, func='pjit_sharded'):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('There is no error in eager mode for native serialization')\n\n    @partial(pjit.pjit, in_shardings=(P('x'),), out_shardings=None)\n    def f_pjit_sharded(a):\n        return a + a\n\n    @partial(pjit.pjit, in_shardings=None, out_shardings=None)\n    def f_pjit_replicated(a):\n        return a + a\n\n    def f_nested_pjit_sharded(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=(P('x'),), out_shardings=None)(a)\n\n    def f_nested_pjit_replicated(a):\n        return a + pjit.pjit(jnp.sin, in_shardings=None, out_shardings=None)(a)\n    shape = (8, 10)\n    a = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n    if func == 'pjit_sharded':\n        f_jax = f_pjit_sharded\n    elif func == 'pjit_replicated':\n        f_jax = f_pjit_replicated\n    elif func == 'nested_pjit_sharded':\n        f_jax = f_nested_pjit_sharded\n    elif func == 'nested_pjit_replicated':\n        f_jax = f_nested_pjit_replicated\n    else:\n        assert False\n    with Mesh(self.devices, axis_names=('x',)):\n        _ = f_jax(a)\n        with self.assertRaisesRegex(ValueError, 'function with sharded arguments or results must be used under a `tf.function` context'):\n            jax2tf.convert(f_jax)(a)",
    "assertions": [
      "assert False"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  },
  {
    "test_code": "@jtu.ignore_warning(category=UserWarning, message='all_to_all .* are only implemented properly for TPUs and GPUs .*')\ndef test_shmap_all_to_all(self):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P(None, 'x'))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x'))\n    def f_jax(b):\n        return lax.all_to_all(b, 'x', split_axis=1, concat_axis=1, tiled=True)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True)\n        if jtu.test_device_matches(['tpu']):\n            return tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            return f_converted(a)\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b00, b01 = np.split(b0, 2, axis=1)\n        b10, b11 = np.split(b1, 2, axis=1)\n        b0 = np.concatenate([b00, b10], axis=1)\n        b1 = np.concatenate([b01, b11], axis=1)\n        res = np.concatenate([b0, b1], axis=1)\n        self.assertAllClose(res_jax, res)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, res_jax)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  },
  {
    "test_code": "@unittest.skip('TODO(b/268295912): ShardingRemover crash,on all platforms!!!')\ndef test_repro_xla_bug_shmap_collective_permute(self):\n    mesh = Mesh(self.devices, axis_names='x')\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n    with mesh:\n        a = np.arange(4 * 4).reshape((4, 4))\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        f_tf = tf.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  },
  {
    "test_code": "@jtu.parameterized_filterable(kwargs=[dict(testcase_name=f'_poly={poly}', poly=poly) for poly in (None, '2*b1,_', '_,b2', '2*b1,b2')])\ndef test_shmap_collective_permute(self, poly=None):\n    if jtu.test_device_matches(['cpu']):\n        raise unittest.SkipTest('TODO(b/268295912): ShardingRemover crash')\n    mesh = Mesh(self.devices, axis_names='x')\n    a = np.arange(4 * 4, dtype=np.float32).reshape((4, 4))\n\n    @partial(pjit.pjit, in_shardings=(P('x', None),), out_shardings=P('x', None))\n    @partial(shard_map, mesh=mesh, in_specs=(P('x', None),), out_specs=P('x', None))\n    def f_jax(b):\n        axis_size = lax.psum(1, 'x')\n        perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n        return lax.ppermute(b, 'x', perm=perm)\n\n    @tf.function(autograph=False, jit_compile=True)\n    def f_tf(a):\n        f_converted = jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)\n        if jtu.test_device_matches(['tpu']):\n            res = tf.compat.v1.tpu.rewrite(f_converted, [tf.convert_to_tensor(a)], device_assignment=self.device_assignment(computation_shape=[1, 1, 1, 2]))[0]\n        else:\n            res = f_converted(a)\n        return res\n    with mesh:\n        res_jax = f_jax(a)\n        b0, b1 = np.split(a, 2, axis=0)\n        b0, b1 = (b1, b0)\n        expected = np.concatenate([b0, b1], axis=0)\n        self.assertAllClose(res_jax, expected)\n        res_tf = f_tf(a)\n        self.assertAllClose(res_tf, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/sharding_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  }
]