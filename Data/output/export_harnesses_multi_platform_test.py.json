[
  {
    "test_code": "@test_harnesses.parameterized(test_harnesses.all_harnesses, include_jax_unimpl=False)\n@jtu.ignore_warning(category=UserWarning, message='Using reduced precision for gradient of reduce-window min/max operator to work around missing XLA support for pair-reductions')\ndef test_prim(self, harness: test_harnesses.Harness):\n    if 'eigh_' in harness.fullname:\n        self.skipTest('Eigenvalues are sorted and it is not correct to compare decompositions for equality.')\n    if jtu.device_under_test() == 'gpu' and 'tridiagonal_solve_' in harness.fullname:\n        self.skipTest('tridiagonal_solve_ is not yet guaranteed stable.')\n    if harness.params.get('enable_xla', False):\n        self.skipTest('enable_xla=False is not relevant')\n    func_jax = harness.dyn_fun\n    args = harness.dyn_args_maker(self.rng())\n    unimplemented_platforms: set[str] = set()\n    for l in harness.jax_unimplemented:\n        if l.filter(dtype=harness.dtype):\n            unimplemented_platforms = unimplemented_platforms.union(l.devices)\n    if 'tridiagonal_solve_' in harness.fullname and all((d.platform != 'gpu' for d in self.devices)):\n        unimplemented_platforms.add('gpu')\n    if unimplemented_platforms:\n        logging.info('Harness is not implemented on %s', unimplemented_platforms)\n    tol = None\n    if 'conv_general_dilated' in harness.fullname and harness.dtype in [np.float32]:\n        tol = 0.0001\n    self.export_and_compare_to_native(func_jax, *args, unimplemented_platforms=unimplemented_platforms, tol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/export_harnesses_multi_platform_test.py",
    "function": "@staticmethod\ndef add(dt, x, y):\n    fromscale = partial(jax.lax.convert_element_type, new_dtype=dt.float_dtype)\n    toscale = partial(jax.lax.convert_element_type, new_dtype=dt)\n    return toscale(jax.lax.max(fromscale(x), fromscale(y)))"
  }
]