[
  {
    "test_code": "@jtu.sample_product(batch_size=[4], seq_len=[1024], num_heads=[8], head_dim=[64, 128], use_mask=[False, True], use_bias=[False, True], mask_type=[MaskType.NO_MASK], dropout_rate=[0], scale=[0.5], dtype=[jnp.float16, jnp.bfloat16])\n@jtu.run_on_devices('cuda')\ndef test_sdpa(self, batch_size: int, seq_len: int, num_heads: int, head_dim: int, use_mask: bool, use_bias: bool, mask_type: MaskType, dropout_rate: float, scale: float, dtype: jnp.dtype):\n    if len(jax.local_devices()) < 4:\n        self.skipTest('Require at least 4 devices to run sharding tests.')\n    if use_mask and mask_type != MaskType.NO_MASK:\n        self.skipTest('Either pass in mask or generate mask directly in cuDNN.')\n    k1, k2, k3, k4, k5, k6 = jax.random.split(jax.random.key(0), 6)\n    query = jax.random.normal(k1, (batch_size, seq_len, num_heads, head_dim), dtype=dtype)\n    key = jax.random.normal(k2, (batch_size, seq_len, num_heads, head_dim), dtype=dtype)\n    value = jax.random.normal(k3, (batch_size, seq_len, num_heads, head_dim), dtype=dtype)\n    grad = jax.random.normal(k4, (batch_size, seq_len, num_heads, head_dim), dtype=dtype)\n    if use_bias:\n        bias = jax.random.normal(k5, (batch_size, num_heads, seq_len, seq_len), dtype=dtype)\n    else:\n        bias = None\n    if use_mask:\n        mask = jax.random.bernoulli(k6, 0.5, (batch_size, num_heads, seq_len, seq_len))\n    else:\n        mask = None\n    devices = np.array(jax.local_devices()[:4])\n    devices = devices.reshape((2, 2))\n    with Mesh(devices, ('dp', 'tp')) as mesh:\n        qkv_spec = PartitionSpec('dp', None, 'tp', None)\n        qkv_sharding = NamedSharding(mesh, qkv_spec)\n        if bias is not None:\n            bias_spec = PartitionSpec('dp', 'tp', None, None)\n        else:\n            bias_spec = PartitionSpec()\n        if mask is not None:\n            mask_spec = PartitionSpec('dp', 'tp', None, None)\n        else:\n            mask_spec = PartitionSpec()\n        bias_sharding = NamedSharding(mesh, bias_spec)\n        mask_sharding = NamedSharding(mesh, mask_spec)\n        query = jax.device_put(query, qkv_sharding)\n        key = jax.device_put(key, qkv_sharding)\n        value = jax.device_put(value, qkv_sharding)\n        if bias is not None:\n            bias = jax.device_put(bias, bias_sharding)\n        if mask is not None:\n            mask = jax.device_put(mask, mask_sharding)\n        grad = jax.device_put(grad, qkv_sharding)\n        in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding, mask_sharding)\n        out_shardings = (qkv_sharding, (qkv_sharding, qkv_sharding, qkv_sharding))\n        jitted_sdpa_train = jax.jit(partial(sdpa_train, scale=scale, mask_type=mask_type, dropout_rate=dropout_rate), in_shardings=in_shardings, out_shardings=out_shardings)\n        jitted_sdpa_train_ref = jax.jit(partial(sdpa_train_ref, scale=scale, mask_type=mask_type, dropout_rate=dropout_rate), in_shardings=in_shardings, out_shardings=out_shardings)\n        out, (query_grad, key_grad, value_grad) = jitted_sdpa_train(query, key, value, grad, bias, mask)\n        out_ref, (query_grad_ref, key_grad_ref, value_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad, bias, mask)\n        self.assertArraysAllClose(out_ref, out, rtol=0.02, atol=0.02)\n        self.assertArraysAllClose(query_grad_ref, query_grad, rtol=0.2, atol=0.2)\n        self.assertArraysAllClose(key_grad_ref, key_grad, rtol=0.2, atol=0.2)\n        self.assertArraysAllClose(value_grad_ref, value_grad, rtol=0.2, atol=0.2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/fused_attention_stablehlo_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.run_on_devices('cuda')\ndef test_sdpa_inference(self):\n    if jax.device_count() < 4:\n        self.skipTest('Requires more than 4 devices.')\n    k1, k2, k3 = jax.random.split(jax.random.key(0), 3)\n    query = jax.random.normal(k1, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    key = jax.random.normal(k2, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    value = jax.random.normal(k3, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    devices = np.array(jax.local_devices()[:4])\n    devices = devices.reshape((2, 2))\n    with Mesh(devices, ('dp', 'tp')) as mesh:\n        qkv_spec = PartitionSpec('dp', None, 'tp', None)\n        qkv_sharding = NamedSharding(mesh, qkv_spec)\n        in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding)\n        out_shardings = qkv_sharding\n        query = jax.device_put(query, qkv_sharding)\n        key = jax.device_put(key, qkv_sharding)\n        value = jax.device_put(value, qkv_sharding)\n        jitted_sdpa_inference = jax.jit(partial(dot_product_attention, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0), in_shardings=in_shardings, out_shardings=out_shardings)\n        jitted_sdpa_inference_ref = jax.jit(partial(sdpa_ref, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0), in_shardings=in_shardings, out_shardings=out_shardings)\n        out = jitted_sdpa_inference(query, key, value)\n        out_ref = jitted_sdpa_inference_ref(query, key, value)\n        self.assertArraysAllClose(out_ref, out, rtol=0.02, atol=0.02)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/fused_attention_stablehlo_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.run_on_devices('cuda')\ndef test_sdpa_var_seq(self):\n    if jax.device_count() < 4:\n        self.skipTest('Requires more than 4 devices.')\n    k1, k2, k3, k4 = jax.random.split(jax.random.key(0), 4)\n    query = jax.random.normal(k1, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    key = jax.random.normal(k2, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    value = jax.random.normal(k3, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    grad = jax.random.normal(k4, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    jitted_sdpa_train = jax.jit(partial(sdpa_train, scale=1.0, mask_type=MaskType.PADDING, dropout_rate=0))\n    jitted_sdpa_train_ref = jax.jit(partial(sdpa_train_ref, scale=1.0, mask_type=MaskType.PADDING, dropout_rate=0))\n    out, (query_grad, key_grad, value_grad) = jitted_sdpa_train(query, key, value, grad)\n    out_ref, (query_grad_ref, key_grad_ref, value_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad)\n    self.assertArraysAllClose(out_ref, out, rtol=0.02, atol=0.02)\n    self.assertArraysAllClose(query_grad_ref, query_grad, rtol=0.2, atol=0.2)\n    self.assertArraysAllClose(key_grad_ref, key_grad, rtol=0.2, atol=0.2)\n    self.assertArraysAllClose(value_grad_ref, value_grad, rtol=0.2, atol=0.2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/fused_attention_stablehlo_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.run_on_devices('cuda')\ndef test_sdpa_broadcast_bias_and_dbias(self):\n    if jax.device_count() < 4:\n        self.skipTest('Requires more than 4 devices.')\n    try:\n        cudnn_version = check_cudnn_version()\n    except RuntimeError as e:\n        self.skipTest(str(e))\n        return\n    if cudnn_version < 8906:\n        self.skipTest('Requires >= cuDNN 8.9.6')\n    if not jtu.is_cuda_compute_capability_at_least('9.0'):\n        self.skipTest('Requires at least Hopper arch')\n    k1, k2, k3, k4, k5 = jax.random.split(jax.random.key(0), 5)\n    query = jax.random.normal(k1, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    key = jax.random.normal(k2, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    value = jax.random.normal(k3, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    grad = jax.random.normal(k4, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    bias = jax.random.normal(k5, (4, 1024, 1024), dtype=jnp.bfloat16)\n    devices = np.array(jax.local_devices()[:4])\n    devices = devices.reshape((2, 2))\n    with Mesh(devices, ('dp', 'tp')) as mesh:\n        qkv_spec = PartitionSpec('dp', None, 'tp', None)\n        qkv_sharding = NamedSharding(mesh, qkv_spec)\n        bias_spec = PartitionSpec('tp', None, None)\n        bias_sharding = NamedSharding(mesh, bias_spec)\n        in_shardings = (qkv_sharding, qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding)\n        out_shardings = (qkv_sharding, (qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding))\n        query = jax.device_put(query, qkv_sharding)\n        key = jax.device_put(key, qkv_sharding)\n        value = jax.device_put(value, qkv_sharding)\n        grad = jax.device_put(grad, qkv_sharding)\n        bias = jax.device_put(bias, bias_sharding)\n        jitted_sdpa_train = jax.jit(partial(sdpa_train, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0), in_shardings=in_shardings, out_shardings=out_shardings)\n        jitted_sdpa_train_ref = jax.jit(partial(sdpa_train_ref, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0), in_shardings=in_shardings, out_shardings=out_shardings)\n        out, (query_grad, key_grad, value_grad, bias_grad) = jitted_sdpa_train(query, key, value, grad, bias)\n        out_ref, (query_grad_ref, key_grad_ref, value_grad_ref, bias_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad, bias)\n        self.assertArraysAllClose(out_ref, out, rtol=0.02, atol=0.02)\n        self.assertArraysAllClose(query_grad_ref, query_grad, rtol=0.2, atol=0.2)\n        self.assertArraysAllClose(key_grad_ref, key_grad, rtol=0.2, atol=0.2)\n        self.assertArraysAllClose(value_grad_ref, value_grad, rtol=0.2, atol=0.2)\n        self.assertArraysAllClose(bias_grad_ref, bias_grad, rtol=0.2, atol=0.2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/fused_attention_stablehlo_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[1, 16])\n@jtu.run_on_devices('cuda')\ndef test_sdpa_dbias(self, batch_size: int):\n    if jax.device_count() < 4:\n        self.skipTest('Requires more than 4 devices.')\n    dtype = jnp.bfloat16\n    x_shape = (batch_size, 512, 16, 48)\n    bias_shape = (batch_size, 16, 512, 512)\n    mask_shape = (1, 1, 512)\n    keys = jax.random.split(jax.random.key(0), 2)\n    x = jax.random.normal(keys[0], x_shape, dtype=dtype)\n    bias = jax.random.normal(keys[1], bias_shape, dtype=dtype)\n    mask = jnp.ones(mask_shape, dtype=jnp.bool_)\n\n    def attn(x, bias, mask):\n        return dot_product_attention(x, x, x, bias, mask)\n\n    def attn_vjp(x, bias, mask, target_fn):\n        _, f_vjp = jax.vjp(target_fn, x, bias, mask)\n        return f_vjp(x)\n    attn_vmap = jax.vmap(attn, in_axes=(0, 0, None))\n    attn_ref = jax.jit(partial(attn_vjp, target_fn=attn))\n    attn_ans = jax.jit(partial(attn_vjp, target_fn=attn_vmap))\n    _, dbias_ref, _ = attn_ref(x, bias, mask)\n    x = jnp.expand_dims(x, axis=1)\n    bias = jnp.expand_dims(bias, axis=1)\n    _, dbias_ans, _ = attn_ans(x, bias, mask)\n    dbias_ans = jnp.squeeze(dbias_ans, axis=1)\n    self.assertArraysAllClose(dbias_ans, dbias_ref)\n    if batch_size != 1:\n        self.assertTrue(not jnp.any(dbias_ans))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/fused_attention_stablehlo_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.run_on_devices('cuda')\ndef test_sdpa_sliding_window_length(self):\n    if jax.device_count() < 4:\n        self.skipTest('Requires more than 4 devices.')\n    k1, k2, k3, k4 = jax.random.split(jax.random.key(0), 4)\n    query = jax.random.normal(k1, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    key = jax.random.normal(k2, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    value = jax.random.normal(k3, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    grad = jax.random.normal(k4, (4, 1024, 4, 64), dtype=jnp.bfloat16)\n    jitted_sdpa_train = jax.jit(partial(sdpa_train, scale=1.0, mask_type=MaskType.CAUSAL, dropout_rate=0, sliding_window_length=64))\n    jitted_sdpa_train_ref = jax.jit(partial(sdpa_train_ref, scale=1.0, mask_type=MaskType.NO_MASK, dropout_rate=0, sliding_window_length=64))\n    out, (query_grad, key_grad, value_grad) = jitted_sdpa_train(query, key, value, grad)\n    out_ref, (query_grad_ref, key_grad_ref, value_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad)\n    self.assertArraysAllClose(out_ref, out, rtol=0.02, atol=0.02)\n    self.assertArraysAllClose(query_grad_ref, query_grad, rtol=0.2, atol=0.2)\n    self.assertArraysAllClose(key_grad_ref, key_grad, rtol=0.2, atol=0.2)\n    self.assertArraysAllClose(value_grad_ref, value_grad, rtol=0.2, atol=0.2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/fused_attention_stablehlo_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.run_on_devices('cuda')\ndef test_sdpa_large_head_size(self):\n    try:\n        cudnn_version = check_cudnn_version()\n    except RuntimeError as e:\n        self.skipTest(str(e))\n        return\n    if cudnn_version < 90500:\n        self.skipTest('Requires >= cuDNN 9.5.0')\n    if not jtu.is_cuda_compute_capability_equal('9.0'):\n        self.skipTest('Requires Hopper arch')\n    B, T, N, H = (2, 64, 2, 256)\n    bf16 = jnp.bfloat16\n    keys = jax.random.split(jax.random.key(0), 4)\n    query = jax.random.normal(keys[0], (B, T, N, H), dtype=bf16)\n    key = jax.random.normal(keys[1], (B, T, N, H), dtype=bf16)\n    value = jax.random.normal(keys[2], (B, T, N, H), dtype=bf16)\n    grad = jax.random.normal(keys[3], (B, T, N, H), dtype=bf16)\n    sdpa_train_ans = jax.jit(partial(sdpa_train, scale=1.0, mask_type=MaskType.CAUSAL, dropout_rate=0))\n    sdpa_train_rfc = jax.jit(partial(sdpa_train_ref, scale=1.0, mask_type=MaskType.CAUSAL, dropout_rate=0))\n    out_ans, grads_ans = sdpa_train_ans(query, key, value, grad)\n    out_ref, grads_ref = sdpa_train_rfc(query, key, value, grad)\n    self.assertArraysAllClose(out_ref, out_ans)\n    self.assertArraysAllClose(grads_ref[0], grads_ans[0], rtol=0.2, atol=0.2)\n    self.assertArraysAllClose(grads_ref[1], grads_ans[1], rtol=0.2, atol=0.2)\n    self.assertArraysAllClose(grads_ref[2], grads_ans[2], rtol=0.2, atol=0.2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/fused_attention_stablehlo_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.run_on_devices('cuda')\ndef test_sdpa_packed_layout(self):\n    if jax.device_count() < 4:\n        self.skipTest('Requires more than 4 devices.')\n    try:\n        cudnn_version = check_cudnn_version()\n    except RuntimeError as e:\n        self.skipTest(str(e))\n        return\n    if cudnn_version < 90600:\n        self.skipTest('Requires >= cuDNN 9.6.0')\n    k1, k2, k3, k4 = jax.random.split(jax.random.key(0), 4)\n    query = jax.random.normal(k1, (4, 512, 4, 64), dtype=jnp.bfloat16)\n    key = jax.random.normal(k2, (4, 512, 4, 64), dtype=jnp.bfloat16)\n    value = jax.random.normal(k3, (4, 512, 4, 64), dtype=jnp.bfloat16)\n    grad = jax.random.normal(k4, (4, 512, 4, 64), dtype=jnp.bfloat16)\n\n    def generate_padding_mask(segment_ids, padding_id, shape, dtype):\n        encoded_padding = jnp.where(segment_ids >= padding_id, 0, 1).astype(dtype)\n        return jax.lax.broadcast_in_dim(encoded_padding, shape, broadcast_dimensions=[0, 1])\n\n    def generate_segment_mask(segment_ids, dtype):\n        segment_ids_1 = jnp.expand_dims(segment_ids, axis=-1)\n        segment_ids_2 = jnp.expand_dims(segment_ids, axis=1)\n        mask = jnp.not_equal(segment_ids_1, segment_ids_2).astype(dtype)\n        mask = jnp.expand_dims(mask, 1)\n        mask *= get_large_negative_number(dtype)\n        return mask\n    q_offsets = jnp.asarray([[0, 170, 340, -1], [0, 150, 340, -1], [0, 190, -1, -1], [0, -1, -1, -1]], dtype=np.int32)\n    q_seqlen = jnp.asarray([[170, 170, 172], [150, 187, 172], [190, 190, -1], [400, -1, -1]], dtype=np.int32)\n    segment_ids = jnp.asarray([[0] * 170 + [1] * 170 + [2] * 172, [0] * 150 + [1] * 187 + [3] * 3 + [2] * 172, [0] * 190 + [1] * 190 + [3] * 132, [0] * 400 + [3] * 112], dtype=np.int32)\n    kv_offsets = q_offsets.copy()\n    kv_seqlen = q_seqlen.copy()\n    mask = generate_padding_mask(segment_ids, q_seqlen.shape[1], query.shape, query.dtype)\n    bias = generate_segment_mask(segment_ids, jnp.float32)\n    devices = np.array(jax.local_devices()[:4])\n    devices = devices.reshape((2, 2))\n    with Mesh(devices, ('dp', 'tp')) as mesh:\n        qkv_spec = PartitionSpec('dp', None, 'tp', None)\n        qkv_sharding = NamedSharding(mesh, qkv_spec)\n        bias_spec = PartitionSpec('dp', None, None, None)\n        bias_sharding = NamedSharding(mesh, bias_spec)\n        offsets_specs = PartitionSpec('dp', None)\n        offsets_sharding = NamedSharding(mesh, offsets_specs)\n        query = jax.device_put(query, qkv_sharding)\n        key = jax.device_put(key, qkv_sharding)\n        value = jax.device_put(value, qkv_sharding)\n        grad = jax.device_put(grad, qkv_sharding)\n        bias = jax.device_put(bias, bias_sharding)\n        q_offsets = jax.device_put(q_offsets, offsets_sharding)\n        kv_offsets = jax.device_put(kv_offsets, offsets_sharding)\n        q_seqlen = jax.device_put(q_seqlen, offsets_sharding)\n        kv_seqlen = jax.device_put(kv_seqlen, offsets_sharding)\n        jitted_sdpa_train = jax.jit(partial(sdpa_train, scale=0.1, mask_type=MaskType.NO_MASK, dropout_rate=0), in_shardings=(qkv_sharding, qkv_sharding, qkv_sharding, qkv_sharding, None, None, offsets_sharding, offsets_sharding, offsets_sharding, offsets_sharding), out_shardings=(qkv_sharding, (qkv_sharding, qkv_sharding, qkv_sharding)))\n        jitted_sdpa_train_ref = jax.jit(partial(sdpa_train_ref, scale=0.1, mask_type=MaskType.NO_MASK, dropout_rate=0), in_shardings=(qkv_sharding, qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding), out_shardings=(qkv_sharding, (qkv_sharding, qkv_sharding, qkv_sharding)))\n        query = query * mask\n        key = key * mask\n        value = value * mask\n        grad = grad * mask\n        out, (query_grad, key_grad, value_grad) = jitted_sdpa_train(query, key, value, grad, None, None, q_seqlen, kv_seqlen, q_offsets, kv_offsets)\n        out_ref, (query_grad_ref, key_grad_ref, value_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad, bias)\n        out = out * mask\n        out_ref = out_ref * mask\n        query_grad = query_grad * mask\n        query_grad_ref = query_grad_ref * mask\n        key_grad = key_grad * mask\n        key_grad_ref = key_grad_ref * mask\n        value_grad = value_grad * mask\n        value_grad_ref = value_grad_ref * mask\n        self.assertArraysAllClose(out_ref, out, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(query_grad_ref, query_grad, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(key_grad_ref, key_grad, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(value_grad_ref, value_grad, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/fused_attention_stablehlo_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.run_on_devices('cuda')\ndef test_layouts(self):\n    if jax.device_count() < 4:\n        self.skipTest('Requires more than 4 devices.')\n    dtype = 'bfloat16'\n    B, T, N, H = (4, 1024, 8, 128)\n    S = T\n    k0, k1, k2, k3 = jax.random.split(jax.random.key(123), 4)\n    query = jax.random.normal(k0, (B, T, N, H), dtype=dtype)\n    key = jax.random.normal(k1, (B, S, N, H), dtype=dtype)\n    value = jax.random.normal(k2, (B, S, N, H), dtype=dtype)\n    grad = jax.random.normal(k3, (B, T, N, H), dtype=dtype)\n    btnh_fn = jax.jit(partial(sdpa_train, scale=0.5, mask_type=MaskType.CAUSAL, is_bnth=False, dropout_rate=0.0))\n    out_ref, (dq_ref, dk_ref, dv_ref) = btnh_fn(query, key, value, grad)\n\n    def _cvt(x):\n        return jnp.einsum('BTNH->BNTH', x)\n\n    def _cvt_back(x):\n        return jnp.einsum('BNTH->BTNH', x)\n    bnth_fn = jax.jit(partial(sdpa_train, scale=0.5, mask_type=MaskType.CAUSAL, is_bnth=True, dropout_rate=0.0))\n    out, (dq, dk, dv) = bnth_fn(_cvt(query), _cvt(key), _cvt(value), _cvt(grad))\n    self.assertArraysAllClose(out_ref, _cvt_back(out))\n    self.assertArraysAllClose(dq_ref, _cvt_back(dq))\n    self.assertArraysAllClose(dk_ref, _cvt_back(dk))\n    self.assertArraysAllClose(dv_ref, _cvt_back(dv))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/fused_attention_stablehlo_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[2, 4], seq_len=[128, 256], num_heads=[4, 8], head_dim=[128], mask_type=[MaskType.NO_MASK], scale=[1.0, 0.75], dtype=[jnp.bfloat16, jnp.float16])\n@jtu.run_on_devices('cuda')\ndef test_sdpa_fp8(self, batch_size: int, seq_len: int, num_heads: int, head_dim: int, mask_type: MaskType, scale: float, dtype: jnp.dtype):\n    k1, k2, k3, k4 = jax.random.split(jax.random.key(0), 4)\n    input_shape = (batch_size, seq_len, num_heads, head_dim)\n    query_h = jax.random.normal(k1, input_shape, dtype=dtype)\n    key_h = jax.random.normal(k2, input_shape, dtype=dtype)\n    value_h = jax.random.normal(k3, input_shape, dtype=dtype)\n    grad_h = jax.random.normal(k4, input_shape, dtype=dtype)\n    query = cast_to_representable(query_h, jnp.float8_e4m3fn)\n    key = cast_to_representable(key_h, jnp.float8_e4m3fn)\n    value = cast_to_representable(value_h, jnp.float8_e4m3fn)\n    grad = cast_to_representable(grad_h, jnp.float8_e4m3fn)\n    query_quantized = quantize(query, jnp.float8_e4m3fn, jnp.float32)\n    key_quantized = quantize(key, jnp.float8_e4m3fn, jnp.float32)\n    value_quantized = quantize(value, jnp.float8_e4m3fn, jnp.float32)\n    grad_quantized = quantize(grad, jnp.float8_e4m3fn, jnp.float32)\n    sdpa_train_fp8_p = partial(sdpa_train_fp8, scale=scale, mask_type=mask_type)\n    jitted_sdpa_train_fp8 = jax.jit(sdpa_train_fp8_p)\n    jitted_sdpa_train_ref = jax.jit(partial(sdpa_train_ref, scale=scale, mask_type=mask_type, dropout_rate=0.0))\n    fp8_metas = {name: jnp.ones((1, 1, 1, 1), dtype=jnp.float32) for name in fp8_meta_names}\n    out, (query_grad, key_grad, value_grad) = jitted_sdpa_train_fp8(query_quantized, key_quantized, value_quantized, grad_quantized, fp8_metas)\n    out_ref, (query_grad_ref, key_grad_ref, value_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad)\n    self.assertArraysAllClose(out_ref, out.astype(dtype), rtol=0.5, atol=0.5)\n    self.assertArraysAllClose(query_grad_ref, query_grad.astype(dtype), rtol=0.5, atol=3.0)\n    self.assertArraysAllClose(key_grad_ref, key_grad.astype(dtype), rtol=0.5, atol=3.0)\n    self.assertArraysAllClose(value_grad_ref, value_grad.astype(dtype), rtol=0.5, atol=0.5)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/fused_attention_stablehlo_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.sample_product(batch_size=[4, 2], seq_len=[4, 16], num_heads=[4, 16], head_dim=[16, 32], mask_type=[MaskType.NO_MASK], qkv_layout=['BNTH', 'BTNH'], scale=[1.0, 0.75], dtype=[jnp.bfloat16, jnp.float16])\n@jtu.run_on_devices('cuda')\ndef test_sdpa_fp8_inference(self, batch_size: int, seq_len: int, num_heads: int, head_dim: int, mask_type: MaskType, qkv_layout: str, scale: float, dtype: jnp.dtype):\n    k1, k2, k3 = jax.random.split(jax.random.key(0), 3)\n    if qkv_layout == 'BNTH':\n        input_shape = (batch_size, num_heads, seq_len, head_dim)\n    else:\n        input_shape = (batch_size, seq_len, num_heads, head_dim)\n    query_h = jax.random.normal(k1, input_shape, dtype=dtype)\n    key_h = jax.random.normal(k2, input_shape, dtype=dtype)\n    value_h = jax.random.normal(k3, input_shape, dtype=dtype)\n    query = cast_to_representable(query_h, jnp.float8_e4m3fn)\n    key = cast_to_representable(key_h, jnp.float8_e4m3fn)\n    value = cast_to_representable(value_h, jnp.float8_e4m3fn)\n    query_quantized = quantize(query, jnp.float8_e4m3fn, jnp.float32)\n    key_quantized = quantize(key, jnp.float8_e4m3fn, jnp.float32)\n    value_quantized = quantize(value, jnp.float8_e4m3fn, jnp.float32)\n\n    def dot_product_attention_fp8(query, key, value, fp8_metas):\n        f_p = partial(dot_product_attention, scale=scale, mask_type=mask_type, qkv_layout=qkv_layout, use_fp8=True)\n        return f_p(query, key, value, fp8_params=fp8_metas)\n    jitted_sdpa_inference = jax.jit(dot_product_attention_fp8)\n    jitted_sdpa_inference_ref = jax.jit(partial(dot_product_attention, scale=scale, mask_type=mask_type, qkv_layout=qkv_layout))\n    fp8_metas = {name: jnp.ones((1, 1, 1, 1), dtype=jnp.float32) for name in fp8_meta_names}\n    out, _, _ = jitted_sdpa_inference(query_quantized, key_quantized, value_quantized, fp8_metas)\n    out_ref = jitted_sdpa_inference_ref(query, key, value)\n    self.assertArraysAllClose(out_ref, out.astype(dtype), rtol=0.05, atol=0.05)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/fused_attention_stablehlo_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "@jtu.run_on_devices('cuda')\ndef test_sdpa_packed_layout(self):\n    if jax.device_count() < 4:\n        self.skipTest('Requires more than 4 devices.')\n    try:\n        cudnn_version = check_cudnn_version()\n    except RuntimeError as e:\n        self.skipTest(str(e))\n        return\n    if cudnn_version < 90600:\n        self.skipTest('Requires >= cuDNN 9.6.0')\n    k1, k2, k3, k4 = jax.random.split(jax.random.key(0), 4)\n    query = jax.random.normal(k1, (4, 512, 4, 64), dtype=jnp.bfloat16)\n    key = jax.random.normal(k2, (4, 512, 4, 64), dtype=jnp.bfloat16)\n    value = jax.random.normal(k3, (4, 512, 4, 64), dtype=jnp.bfloat16)\n    grad = jax.random.normal(k4, (4, 512, 4, 64), dtype=jnp.bfloat16)\n\n    def generate_padding_mask(segment_ids, padding_id, shape, dtype):\n        encoded_padding = jnp.where(segment_ids >= padding_id, 0, 1).astype(dtype)\n        return jax.lax.broadcast_in_dim(encoded_padding, shape, broadcast_dimensions=[0, 1])\n\n    def generate_segment_mask(segment_ids, dtype):\n        segment_ids_1 = jnp.expand_dims(segment_ids, axis=-1)\n        segment_ids_2 = jnp.expand_dims(segment_ids, axis=1)\n        mask = jnp.not_equal(segment_ids_1, segment_ids_2).astype(dtype)\n        mask = jnp.expand_dims(mask, 1)\n        mask *= get_large_negative_number(dtype)\n        return mask\n    q_offsets = jnp.asarray([[0, 170, 340, -1], [0, 150, 340, -1], [0, 190, -1, -1], [0, -1, -1, -1]], dtype=np.int32)\n    q_seqlen = jnp.asarray([[170, 170, 172], [150, 187, 172], [190, 190, -1], [400, -1, -1]], dtype=np.int32)\n    segment_ids = jnp.asarray([[0] * 170 + [1] * 170 + [2] * 172, [0] * 150 + [1] * 187 + [3] * 3 + [2] * 172, [0] * 190 + [1] * 190 + [3] * 132, [0] * 400 + [3] * 112], dtype=np.int32)\n    kv_offsets = q_offsets.copy()\n    kv_seqlen = q_seqlen.copy()\n    mask = generate_padding_mask(segment_ids, q_seqlen.shape[1], query.shape, query.dtype)\n    bias = generate_segment_mask(segment_ids, jnp.float32)\n    devices = np.array(jax.local_devices()[:4])\n    devices = devices.reshape((2, 2))\n    with Mesh(devices, ('dp', 'tp')) as mesh:\n        qkv_spec = PartitionSpec('dp', None, 'tp', None)\n        qkv_sharding = NamedSharding(mesh, qkv_spec)\n        bias_spec = PartitionSpec('dp', None, None, None)\n        bias_sharding = NamedSharding(mesh, bias_spec)\n        offsets_specs = PartitionSpec('dp', None)\n        offsets_sharding = NamedSharding(mesh, offsets_specs)\n        query = jax.device_put(query, qkv_sharding)\n        key = jax.device_put(key, qkv_sharding)\n        value = jax.device_put(value, qkv_sharding)\n        grad = jax.device_put(grad, qkv_sharding)\n        bias = jax.device_put(bias, bias_sharding)\n        q_offsets = jax.device_put(q_offsets, offsets_sharding)\n        kv_offsets = jax.device_put(kv_offsets, offsets_sharding)\n        q_seqlen = jax.device_put(q_seqlen, offsets_sharding)\n        kv_seqlen = jax.device_put(kv_seqlen, offsets_sharding)\n        jitted_sdpa_train = jax.jit(partial(sdpa_train, scale=0.1, mask_type=MaskType.NO_MASK, dropout_rate=0), in_shardings=(qkv_sharding, qkv_sharding, qkv_sharding, qkv_sharding, None, None, offsets_sharding, offsets_sharding, offsets_sharding, offsets_sharding), out_shardings=(qkv_sharding, (qkv_sharding, qkv_sharding, qkv_sharding)))\n        jitted_sdpa_train_ref = jax.jit(partial(sdpa_train_ref, scale=0.1, mask_type=MaskType.NO_MASK, dropout_rate=0), in_shardings=(qkv_sharding, qkv_sharding, qkv_sharding, qkv_sharding, bias_sharding), out_shardings=(qkv_sharding, (qkv_sharding, qkv_sharding, qkv_sharding)))\n        query = query * mask\n        key = key * mask\n        value = value * mask\n        grad = grad * mask\n        out, (query_grad, key_grad, value_grad) = jitted_sdpa_train(query, key, value, grad, None, None, q_seqlen, kv_seqlen, q_offsets, kv_offsets)\n        out_ref, (query_grad_ref, key_grad_ref, value_grad_ref) = jitted_sdpa_train_ref(query, key, value, grad, bias)\n        out = out * mask\n        out_ref = out_ref * mask\n        query_grad = query_grad * mask\n        query_grad_ref = query_grad_ref * mask\n        key_grad = key_grad * mask\n        key_grad_ref = key_grad_ref * mask\n        value_grad = value_grad * mask\n        value_grad_ref = value_grad_ref * mask\n        self.assertArraysAllClose(out_ref, out, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(query_grad_ref, query_grad, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(key_grad_ref, key_grad, rtol=0.01, atol=0.01)\n        self.assertArraysAllClose(value_grad_ref, value_grad, rtol=0.01, atol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/fused_attention_stablehlo_test.py",
    "function": "def get_large_negative_number(dtype):\n    return 0.7 * jnp.finfo(dtype).min"
  }
]