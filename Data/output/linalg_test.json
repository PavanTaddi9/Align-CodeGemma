[
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(0, 2), (2, 0), (0, 0)], dtype=float_types + complex_types, ord=[1, 2, np.inf, 'fro', 'nuc'])\ndef testEmptyMatrixNorm(self, shape, dtype, ord):\n    x = jnp.zeros(shape, dtype)\n    norm = jnp.linalg.matrix_norm(x, ord=ord)\n    self.assertEqual(norm, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.sample_product(dtype=float_types + complex_types, ord=[1, 2, np.inf])\ndef testEmptyVectorNorm(self, dtype, ord):\n    x = jnp.zeros(0, dtype)\n    norm = jnp.linalg.vector_norm(x, ord=ord)\n    self.assertEqual(norm, 0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(0, 0), (2, 0), (0, 2), (3, 3), (3, 4), (2, 10, 5), (2, 200, 100), (64, 16, 5), (33, 7, 3), (137, 9, 5), (20000, 2, 2)], dtype=float_types + complex_types, full_matrices=[False, True])\n@jax.default_matmul_precision('float32')\ndef testQr(self, shape, dtype, full_matrices):\n    if jtu.test_device_matches(['cuda']) and _is_required_cuda_version_satisfied(12000):\n        self.skipTest('Triggers a bug in cuda-12 b/287345077')\n    rng = jtu.rand_default(self.rng())\n    m, n = shape[-2:]\n    if full_matrices:\n        mode, k = ('complete', m)\n    else:\n        mode, k = ('reduced', min(m, n))\n    a = rng(shape, dtype)\n    lq, lr = jnp.linalg.qr(a, mode=mode)\n    nq = np.zeros(shape[:-2] + (m, k), dtype)\n    nr = np.zeros(shape[:-2] + (k, n), dtype)\n    for index in np.ndindex(*shape[:-2]):\n        nq[index], nr[index] = np.linalg.qr(a[index], mode=mode)\n    max_rank = max(m, n)\n\n    def norm(x):\n        n = np.linalg.norm(x, axis=(-2, -1))\n        return n / (max(1, max_rank) * jnp.finfo(dtype).eps)\n\n    def compare_orthogonal(q1, q2):\n        ratio = np.divide(np.where(q2 == 0, 0, q1), np.where(q2 == 0, 1, q2))\n        sum_of_ratios = ratio.sum(axis=-2, keepdims=True)\n        phases = np.divide(sum_of_ratios, np.abs(sum_of_ratios))\n        q1 *= phases\n        nm = norm(q1 - q2)\n        self.assertTrue(np.all(nm < 160), msg=f'norm={np.amax(nm)}')\n    norm_error = norm(a - np.matmul(lq, lr))\n    self.assertTrue(np.all(norm_error < 60), msg=np.amax(norm_error))\n    compare_orthogonal(nq[..., :k], lq[..., :k])\n    self.assertTrue(np.all(norm(np.eye(k) - np.matmul(np.conj(T(lq)), lq)) < 10))\n\n    def qr_and_mul(a):\n        q, r = jnp.linalg.qr(a, mode=mode)\n        return q @ r\n    if m == n or (m > n and (not full_matrices)):\n        jtu.check_jvp(qr_and_mul, partial(jvp, qr_and_mul), (a,), atol=0.003)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@parameterized.parameters(lax_linalg.lu, lax_linalg._lu_python)\ndef testLuOnZeroMatrix(self, lu):\n    x = jnp.zeros((2, 2), dtype=np.float32)\n    x_lu, _, _ = lu(x)\n    self.assertArraysEqual(x_lu, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.sample_product(n=[1, 4, 5, 20, 50, 100], dtype=float_types + complex_types)\ndef testIssue2131(self, n, dtype):\n    args_maker_zeros = lambda: [np.zeros((n, n), dtype)]\n    osp_fun = lambda a: osp.linalg.expm(a)\n    jsp_fun = lambda a: jsp.linalg.expm(a)\n    self._CheckAgainstNumpy(osp_fun, jsp_fun, args_maker_zeros)\n    self._CompileAndCheck(jsp_fun, args_maker_zeros)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(5, 1), (10, 4), (128, 12)], dtype=float_types, symmetrize_output=[True, False])\n@jtu.skip_on_devices('tpu')\ndef testSymmetricProduct(self, shape, dtype, symmetrize_output):\n    rng = jtu.rand_default(self.rng())\n    batch_size = 10\n    atol = 1e-06 if dtype == jnp.float64 else 0.001\n    a_matrix = rng((batch_size,) + shape, dtype)\n    c_shape = a_matrix.shape[:-1] + (a_matrix.shape[-2],)\n    c_matrix = jnp.zeros(c_shape, dtype)\n    old_product = jnp.einsum('...ij,...kj->...ik', a_matrix, a_matrix, precision=lax.Precision.HIGHEST)\n    new_product = lax_linalg.symmetric_product(a_matrix, c_matrix, symmetrize_output=symmetrize_output)\n    new_product_with_batching = jax.vmap(lambda a, c: lax_linalg.symmetric_product(a, c, symmetrize_output=symmetrize_output), in_axes=(0, 0))(a_matrix, c_matrix)\n    if not symmetrize_output:\n        old_product = jnp.tril(old_product)\n        new_product = jnp.tril(new_product)\n        new_product_with_batching = jnp.tril(new_product_with_batching)\n    self.assertAllClose(new_product, old_product, atol=atol)\n    self.assertAllClose(new_product_with_batching, old_product, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.sample_product([dict(m=m, n=n, full_matrices=full_matrices, hermitian=hermitian) for (m, n), full_matrices in list(itertools.product(itertools.product([0, 2, 7, 29, 32, 53], repeat=2), [False, True])) + [((400000, 2), False), ((2, 400000), False)] for hermitian in ([False, True] if m == n else [False])], b=[(), (3,), (2, 3)], dtype=float_types + complex_types, compute_uv=[False, True], algorithm=[None, lax.linalg.SvdAlgorithm.QR, lax.linalg.SvdAlgorithm.JACOBI])\n@jax.default_matmul_precision('float32')\ndef testSVD(self, b, m, n, dtype, full_matrices, compute_uv, hermitian, algorithm):\n    if algorithm is not None:\n        if hermitian:\n            self.skipTest(\"Hermitian SVD doesn't support the algorithm parameter.\")\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('SVD algorithm selection only supported on CPU and GPU.')\n        if jtu.test_device_matches(['cpu']) and jtu.jaxlib_version() <= (0, 5, 1):\n            self.skipTest('SVD algorithm selection on CPU requires a newer jaxlib version.')\n        if jtu.test_device_matches(['cpu']) and algorithm == lax.linalg.SvdAlgorithm.JACOBI:\n            self.skipTest('Jacobi SVD not supported on GPU.')\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(b + (m, n), dtype)]\n\n    def compute_max_backward_error(operand, reconstructed_operand):\n        error_norm = np.linalg.norm(operand - reconstructed_operand, axis=(-2, -1))\n        backward_error = error_norm / np.linalg.norm(operand, axis=(-2, -1))\n        max_backward_error = np.amax(backward_error)\n        return max_backward_error\n    tol = 100 * jnp.finfo(dtype).eps\n    reconstruction_tol = 2 * tol\n    unitariness_tol = 3 * tol\n    a, = args_maker()\n    if hermitian:\n        a = a + np.conj(T(a))\n    if algorithm is None:\n        fun = partial(jnp.linalg.svd, hermitian=hermitian)\n    else:\n        fun = partial(lax.linalg.svd, algorithm=algorithm)\n    out = fun(a, full_matrices=full_matrices, compute_uv=compute_uv)\n    if compute_uv:\n        out = list(out)\n        out[1] = out[1].astype(out[0].dtype)\n        if m and n:\n            if full_matrices:\n                k = min(m, n)\n                if m < n:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2][..., :k, :]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n                else:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0][..., :, :k], out[2]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n            else:\n                max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2]))\n                self.assertLess(max_backward_error, reconstruction_tol)\n        unitary_mat = np.real(np.matmul(np.conj(T(out[0])), out[0]))\n        eye_slice = np.eye(out[0].shape[-1], dtype=unitary_mat.dtype)\n        self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        if m >= n:\n            unitary_mat = np.real(np.matmul(np.conj(T(out[2])), out[2]))\n            eye_slice = np.eye(out[2].shape[-1], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        else:\n            unitary_mat = np.real(np.matmul(out[2], np.conj(T(out[2]))))\n            eye_slice = np.eye(out[2].shape[-2], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n    else:\n        self.assertTrue(np.allclose(np.linalg.svd(a, compute_uv=False), np.asarray(out), atol=0.0001, rtol=0.0001))\n    self._CompileAndCheck(partial(fun, full_matrices=full_matrices, compute_uv=compute_uv), args_maker)\n    if not compute_uv and a.size < 100000:\n        svd = partial(fun, full_matrices=full_matrices, compute_uv=compute_uv)\n        if dtype == np.complex128:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.0001, atol=0.0001, eps=1e-08)\n        else:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.05, atol=0.2)\n    if compute_uv and (not full_matrices):\n        b, = args_maker()\n\n        def f(x):\n            u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n            vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n            return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real\n        _, t_out = jvp(f, (1.0,), (1.0,))\n        if dtype == np.complex128:\n            atol = 2e-13\n        else:\n            atol = 0.0006\n        self.assertArraysAllClose(t_out, b.real, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@partial(jax.jit, abstracted_axes=('n',))\ndef fun(x):\n    return jnp.sum(x)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(0, 0), (4, 4), (5, 5), (50, 50), (2, 6, 6)], dtype=float_types + complex_types, compute_left_eigenvectors=[False, True], compute_right_eigenvectors=[False, True])\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEig(self, shape, dtype, compute_left_eigenvectors, compute_right_eigenvectors):\n    rng = jtu.rand_default(self.rng())\n    n = shape[-1]\n    args_maker = lambda: [rng(shape, dtype)]\n\n    def norm(x):\n        norm = np.linalg.norm(x, axis=(-2, -1))\n        return norm / ((n + 1) * jnp.finfo(dtype).eps)\n\n    def check_right_eigenvectors(a, w, vr):\n        self.assertTrue(np.all(norm(np.matmul(a, vr) - w[..., None, :] * vr) < 100))\n\n    def check_left_eigenvectors(a, w, vl):\n        rank = len(a.shape)\n        aH = jnp.conj(a.transpose(list(range(rank - 2)) + [rank - 1, rank - 2]))\n        wC = jnp.conj(w)\n        check_right_eigenvectors(aH, wC, vl)\n    a, = args_maker()\n    results = lax.linalg.eig(a, compute_left_eigenvectors=compute_left_eigenvectors, compute_right_eigenvectors=compute_right_eigenvectors)\n    w = results[0]\n    if compute_left_eigenvectors:\n        check_left_eigenvectors(a, w, results[1])\n    if compute_right_eigenvectors:\n        check_right_eigenvectors(a, w, results[1 + compute_left_eigenvectors])\n    self._CompileAndCheck(partial(jnp.linalg.eig), args_maker, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5)], dtype=float_types + complex_types)\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEigBatching(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    shape = (10,) + shape\n    args = rng(shape, dtype)\n    ws, vs = vmap(jnp.linalg.eig)(args)\n    self.assertTrue(np.all(np.linalg.norm(np.matmul(args, vs) - ws[..., None, :] * vs) < 0.001))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(n=[0, 4, 5, 50, 512], dtype=float_types + complex_types, lower=[True, False])\ndef testEigh(self, n, dtype, lower):\n    rng = jtu.rand_default(self.rng())\n    eps = np.finfo(dtype).eps\n    args_maker = lambda: [rng((n, n), dtype)]\n    uplo = 'L' if lower else 'U'\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    w, v = jnp.linalg.eigh(np.tril(a) if lower else np.triu(a), UPLO=uplo, symmetrize_input=False)\n    w = w.astype(v.dtype)\n    tol = 2 * n * eps\n    self.assertAllClose(np.eye(n, dtype=v.dtype), np.matmul(np.conj(T(v)), v), atol=tol, rtol=tol)\n    with jax.numpy_rank_promotion('allow'):\n        tol = 100 * eps\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    self._CompileAndCheck(partial(jnp.linalg.eigh, UPLO=uplo), args_maker, rtol=eps)\n    double_type = dtype\n    if dtype == np.float32:\n        double_type = np.float64\n    if dtype == np.complex64:\n        double_type = np.complex128\n    w_np = np.linalg.eigvalsh(a.astype(double_type))\n    tol = 8 * eps\n    self.assertAllClose(w_np.astype(w.dtype), w, atol=tol * np.linalg.norm(a), rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(start=[0, 1, 63, 64, 65, 255], end=[1, 63, 64, 65, 256])\n@jtu.run_on_devices('tpu')\ndef testEighSubsetByIndex(self, start, end):\n    if start >= end:\n        return\n    dtype = np.float32\n    n = 256\n    rng = jtu.rand_default(self.rng())\n    eps = np.finfo(dtype).eps\n    args_maker = lambda: [rng((n, n), dtype)]\n    subset_by_index = (start, end)\n    k = end - start\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    v, w = lax.linalg.eigh(a, symmetrize_input=False, subset_by_index=subset_by_index)\n    w = w.astype(v.dtype)\n    self.assertEqual(v.shape, (n, k))\n    self.assertEqual(w.shape, (k,))\n    with jax.numpy_rank_promotion('allow'):\n        tol = 200 * eps\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    tol = 3 * n * eps\n    self.assertAllClose(np.eye(k, dtype=v.dtype), np.matmul(np.conj(T(v)), v), atol=tol, rtol=tol)\n    self._CompileAndCheck(partial(jnp.linalg.eigh), args_maker, rtol=eps)\n    double_type = dtype\n    if dtype == np.float32:\n        double_type = np.float64\n    if dtype == np.complex64:\n        double_type = np.complex128\n    w_np = np.linalg.eigvalsh(a.astype(double_type))[subset_by_index[0]:subset_by_index[1]]\n    tol = 20 * eps\n    self.assertAllClose(w_np.astype(w.dtype), w, atol=tol * np.linalg.norm(a), rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "def testEighZeroDiagonal(self):\n    a = np.array([[0.0, -1.0, -1.0, 1.0], [-1.0, 0.0, 1.0, -1.0], [-1.0, 1.0, 0.0, -1.0], [1.0, -1.0, -1.0, 0.0]], dtype=np.float32)\n    w, v = jnp.linalg.eigh(a)\n    w = w.astype(v.dtype)\n    eps = jnp.finfo(a.dtype).eps\n    with jax.numpy_rank_promotion('allow'):\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), 2 * eps * np.linalg.norm(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "def testEighTinyNorm(self):\n    rng = jtu.rand_default(self.rng())\n    a = rng((300, 300), dtype=np.float32)\n    eps = jnp.finfo(a.dtype).eps\n    a = eps * (a + np.conj(a.T))\n    w, v = jnp.linalg.eigh(a)\n    w = w.astype(v.dtype)\n    with jax.numpy_rank_promotion('allow'):\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), 80 * eps * np.linalg.norm(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(rank=[1, 3, 299])\ndef testEighRankDeficient(self, rank):\n    rng = jtu.rand_default(self.rng())\n    eps = jnp.finfo(np.float32).eps\n    a = rng((300, rank), dtype=np.float32)\n    a = a @ np.conj(a.T)\n    w, v = jnp.linalg.eigh(a)\n    w = w.astype(v.dtype)\n    with jax.numpy_rank_promotion('allow'):\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), 85 * eps * np.linalg.norm(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(n=[0, 4, 5, 50, 512], dtype=float_types + complex_types, lower=[True, False])\ndef testEighIdentity(self, n, dtype, lower):\n    tol = np.finfo(dtype).eps\n    uplo = 'L' if lower else 'U'\n    a = jnp.eye(n, dtype=dtype)\n    w, v = jnp.linalg.eigh(a, UPLO=uplo, symmetrize_input=False)\n    w = w.astype(v.dtype)\n    self.assertLessEqual(np.linalg.norm(np.eye(n) - np.matmul(np.conj(T(v)), v)), tol)\n    with jax.numpy_rank_promotion('allow'):\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (300, 300)], dtype=float_types + complex_types)\ndef testEighBatching(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    shape = (10,) + shape\n    args = rng(shape, dtype)\n    args = (args + np.conj(T(args))) / 2\n    ws, vs = vmap(jsp.linalg.eigh)(args)\n    ws = ws.astype(vs.dtype)\n    norm = np.max(np.linalg.norm(np.matmul(args, vs) - ws[..., None, :] * vs))\n    self.assertLess(norm, 0.014)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "def testStringInfNorm(self):\n    err, msg = (ValueError, \"Invalid order 'inf' for vector norm.\")\n    with self.assertRaisesRegex(err, msg):\n        jnp.linalg.norm(jnp.array([1.0, 2.0, 3.0]), ord='inf')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(m=m, n=n, full_matrices=full_matrices, hermitian=hermitian) for (m, n), full_matrices in list(itertools.product(itertools.product([0, 2, 7, 29, 32, 53], repeat=2), [False, True])) + [((400000, 2), False), ((2, 400000), False)] for hermitian in ([False, True] if m == n else [False])], b=[(), (3,), (2, 3)], dtype=float_types + complex_types, compute_uv=[False, True], algorithm=[None, lax.linalg.SvdAlgorithm.QR, lax.linalg.SvdAlgorithm.JACOBI])\n@jax.default_matmul_precision('float32')\ndef testSVD(self, b, m, n, dtype, full_matrices, compute_uv, hermitian, algorithm):\n    if algorithm is not None:\n        if hermitian:\n            self.skipTest(\"Hermitian SVD doesn't support the algorithm parameter.\")\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('SVD algorithm selection only supported on CPU and GPU.')\n        if jtu.test_device_matches(['cpu']) and jtu.jaxlib_version() <= (0, 5, 1):\n            self.skipTest('SVD algorithm selection on CPU requires a newer jaxlib version.')\n        if jtu.test_device_matches(['cpu']) and algorithm == lax.linalg.SvdAlgorithm.JACOBI:\n            self.skipTest('Jacobi SVD not supported on GPU.')\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(b + (m, n), dtype)]\n\n    def compute_max_backward_error(operand, reconstructed_operand):\n        error_norm = np.linalg.norm(operand - reconstructed_operand, axis=(-2, -1))\n        backward_error = error_norm / np.linalg.norm(operand, axis=(-2, -1))\n        max_backward_error = np.amax(backward_error)\n        return max_backward_error\n    tol = 100 * jnp.finfo(dtype).eps\n    reconstruction_tol = 2 * tol\n    unitariness_tol = 3 * tol\n    a, = args_maker()\n    if hermitian:\n        a = a + np.conj(T(a))\n    if algorithm is None:\n        fun = partial(jnp.linalg.svd, hermitian=hermitian)\n    else:\n        fun = partial(lax.linalg.svd, algorithm=algorithm)\n    out = fun(a, full_matrices=full_matrices, compute_uv=compute_uv)\n    if compute_uv:\n        out = list(out)\n        out[1] = out[1].astype(out[0].dtype)\n        if m and n:\n            if full_matrices:\n                k = min(m, n)\n                if m < n:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2][..., :k, :]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n                else:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0][..., :, :k], out[2]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n            else:\n                max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2]))\n                self.assertLess(max_backward_error, reconstruction_tol)\n        unitary_mat = np.real(np.matmul(np.conj(T(out[0])), out[0]))\n        eye_slice = np.eye(out[0].shape[-1], dtype=unitary_mat.dtype)\n        self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        if m >= n:\n            unitary_mat = np.real(np.matmul(np.conj(T(out[2])), out[2]))\n            eye_slice = np.eye(out[2].shape[-1], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        else:\n            unitary_mat = np.real(np.matmul(out[2], np.conj(T(out[2]))))\n            eye_slice = np.eye(out[2].shape[-2], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n    else:\n        self.assertTrue(np.allclose(np.linalg.svd(a, compute_uv=False), np.asarray(out), atol=0.0001, rtol=0.0001))\n    self._CompileAndCheck(partial(fun, full_matrices=full_matrices, compute_uv=compute_uv), args_maker)\n    if not compute_uv and a.size < 100000:\n        svd = partial(fun, full_matrices=full_matrices, compute_uv=compute_uv)\n        if dtype == np.complex128:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.0001, atol=0.0001, eps=1e-08)\n        else:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.05, atol=0.2)\n    if compute_uv and (not full_matrices):\n        b, = args_maker()\n\n        def f(x):\n            u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n            vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n            return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real\n        _, t_out = jvp(f, (1.0,), (1.0,))\n        if dtype == np.complex128:\n            atol = 2e-13\n        else:\n            atol = 0.0006\n        self.assertArraysAllClose(t_out, b.real, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(0, 0), (2, 0), (0, 2), (3, 3), (3, 4), (2, 10, 5), (2, 200, 100), (64, 16, 5), (33, 7, 3), (137, 9, 5), (20000, 2, 2)], dtype=float_types + complex_types, full_matrices=[False, True])\n@jax.default_matmul_precision('float32')\ndef testQr(self, shape, dtype, full_matrices):\n    if jtu.test_device_matches(['cuda']) and _is_required_cuda_version_satisfied(12000):\n        self.skipTest('Triggers a bug in cuda-12 b/287345077')\n    rng = jtu.rand_default(self.rng())\n    m, n = shape[-2:]\n    if full_matrices:\n        mode, k = ('complete', m)\n    else:\n        mode, k = ('reduced', min(m, n))\n    a = rng(shape, dtype)\n    lq, lr = jnp.linalg.qr(a, mode=mode)\n    nq = np.zeros(shape[:-2] + (m, k), dtype)\n    nr = np.zeros(shape[:-2] + (k, n), dtype)\n    for index in np.ndindex(*shape[:-2]):\n        nq[index], nr[index] = np.linalg.qr(a[index], mode=mode)\n    max_rank = max(m, n)\n\n    def norm(x):\n        n = np.linalg.norm(x, axis=(-2, -1))\n        return n / (max(1, max_rank) * jnp.finfo(dtype).eps)\n\n    def compare_orthogonal(q1, q2):\n        ratio = np.divide(np.where(q2 == 0, 0, q1), np.where(q2 == 0, 1, q2))\n        sum_of_ratios = ratio.sum(axis=-2, keepdims=True)\n        phases = np.divide(sum_of_ratios, np.abs(sum_of_ratios))\n        q1 *= phases\n        nm = norm(q1 - q2)\n        self.assertTrue(np.all(nm < 160), msg=f'norm={np.amax(nm)}')\n    norm_error = norm(a - np.matmul(lq, lr))\n    self.assertTrue(np.all(norm_error < 60), msg=np.amax(norm_error))\n    compare_orthogonal(nq[..., :k], lq[..., :k])\n    self.assertTrue(np.all(norm(np.eye(k) - np.matmul(np.conj(T(lq)), lq)) < 10))\n\n    def qr_and_mul(a):\n        q, r = jnp.linalg.qr(a, mode=mode)\n        return q @ r\n    if m == n or (m > n and (not full_matrices)):\n        jtu.check_jvp(qr_and_mul, partial(jvp, qr_and_mul), (a,), atol=0.003)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(10, 4, 5), (5, 3, 3), (7, 6, 4)], dtype=float_types + complex_types)\ndef testQrBatching(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args = rng(shape, jnp.float32)\n    qs, rs = vmap(jsp.linalg.qr)(args)\n    self.assertTrue(np.all(np.linalg.norm(args - np.matmul(qs, rs)) < 0.001))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(n=[1, 4, 5, 20, 50, 100], dtype=float_types + complex_types)\ndef testExpmFrechet(self, n, dtype):\n    rng = jtu.rand_small(self.rng())\n    if dtype == np.float64 or dtype == np.complex128:\n        target_norms = [0.01, 0.2, 0.9, 2.0, 3.0]\n        tol = {np.dtype(np.float64): 1e-14, np.dtype(np.complex128): 1e-14}\n    elif dtype == np.float32 or dtype == np.complex64:\n        target_norms = [0.4, 1.0, 3.0]\n        tol = None\n    else:\n        raise TypeError(f'dtype={dtype!r} is not supported.')\n    for norm in target_norms:\n\n        def args_maker():\n            a = rng((n, n), dtype)\n            a = a / np.linalg.norm(a, 1) * norm\n            e = rng((n, n), dtype)\n            return [a, e]\n        osp_fun = lambda a, e: osp.linalg.expm_frechet(a, e, compute_expm=True)\n        jsp_fun = lambda a, e: jsp.linalg.expm_frechet(a, e, compute_expm=True)\n        self._CheckAgainstNumpy(osp_fun, jsp_fun, args_maker, check_dtypes=False, tol=tol)\n        self._CompileAndCheck(jsp_fun, args_maker, check_dtypes=False)\n        osp_fun = lambda a, e: osp.linalg.expm_frechet(a, e, compute_expm=False)\n        jsp_fun = lambda a, e: jsp.linalg.expm_frechet(a, e, compute_expm=False)\n        self._CheckAgainstNumpy(osp_fun, jsp_fun, args_maker, check_dtypes=False, tol=tol)\n        self._CompileAndCheck(jsp_fun, args_maker, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(n=[1, 4, 5, 20, 50], dtype=float_types + complex_types)\ndef testExpmGrad(self, n, dtype):\n    rng = jtu.rand_small(self.rng())\n    a = rng((n, n), dtype)\n    if dtype == np.float64 or dtype == np.complex128:\n        target_norms = [0.01, 0.2, 0.9, 2.0, 3.0]\n    elif dtype == np.float32 or dtype == np.complex64:\n        target_norms = [0.4, 1.0, 3.0]\n    else:\n        raise TypeError(f'dtype={dtype!r} is not supported.')\n    tol = {np.dtype(np.float32): 0.02, np.dtype(np.complex64): 0.02, np.dtype(np.float64): 0.0001, np.dtype(np.complex128): 0.0001}\n    for norm in target_norms:\n        a = a / np.linalg.norm(a, 1) * norm\n\n        def expm(x):\n            return jsp.linalg.expm(x, upper_triangular=False, max_squarings=16)\n        jtu.check_grads(expm, (a,), modes=['fwd', 'rev'], order=1, atol=tol, rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(n=[0, 4, 5, 50], dtype=float_types + complex_types, lower=[True, False], sort_eigenvalues=[True, False])\ndef testEigh(self, n, dtype, lower, sort_eigenvalues):\n    rng = jtu.rand_default(self.rng())\n    tol = 0.001\n    args_maker = lambda: [rng((n, n), dtype)]\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    v, w = lax.linalg.eigh(np.tril(a) if lower else np.triu(a), lower=lower, symmetrize_input=False, sort_eigenvalues=sort_eigenvalues)\n    w = np.asarray(w)\n    v = np.asarray(v)\n    self.assertLessEqual(np.linalg.norm(np.eye(n) - np.matmul(np.conj(T(v)), v)), 0.001)\n    self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    w_expected, v_expected = np.linalg.eigh(np.asarray(a))\n    self.assertAllClose(w_expected, w if sort_eigenvalues else np.sort(w), rtol=0.0001, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((inner_dimension + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(0, 0), (4, 4), (5, 5), (50, 50), (2, 6, 6)], dtype=float_types + complex_types, compute_left_eigenvectors=[False, True], compute_right_eigenvectors=[False, True])\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEig(self, shape, dtype, compute_left_eigenvectors, compute_right_eigenvectors):\n    rng = jtu.rand_default(self.rng())\n    n = shape[-1]\n    args_maker = lambda: [rng(shape, dtype)]\n\n    def norm(x):\n        norm = np.linalg.norm(x, axis=(-2, -1))\n        return norm / ((n + 1) * jnp.finfo(dtype).eps)\n\n    def check_right_eigenvectors(a, w, vr):\n        self.assertTrue(np.all(norm(np.matmul(a, vr) - w[..., None, :] * vr) < 100))\n\n    def check_left_eigenvectors(a, w, vl):\n        rank = len(a.shape)\n        aH = jnp.conj(a.transpose(list(range(rank - 2)) + [rank - 1, rank - 2]))\n        wC = jnp.conj(w)\n        check_right_eigenvectors(aH, wC, vl)\n    a, = args_maker()\n    results = lax.linalg.eig(a, compute_left_eigenvectors=compute_left_eigenvectors, compute_right_eigenvectors=compute_right_eigenvectors)\n    w = results[0]\n    if compute_left_eigenvectors:\n        check_left_eigenvectors(a, w, results[1])\n    if compute_right_eigenvectors:\n        check_right_eigenvectors(a, w, results[1 + compute_left_eigenvectors])\n    self._CompileAndCheck(partial(jnp.linalg.eig), args_maker, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def check_left_eigenvectors(a, w, vl):\n    rank = len(a.shape)\n    aH = jnp.conj(a.transpose(list(range(rank - 2)) + [rank - 1, rank - 2]))\n    wC = jnp.conj(w)\n    check_right_eigenvectors(aH, wC, vl)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (2, 3, 5), (5, 5, 5), (20, 20), (5, 10)], pnorm=[jnp.inf, -jnp.inf, 1, -1, 2, -2, 'fro'], dtype=float_types + complex_types)\n@jtu.skip_on_devices('gpu')\ndef testCond(self, shape, pnorm, dtype):\n\n    def gen_mat():\n        arr_gen = jtu.rand_default(self.rng())\n        res = arr_gen(shape, dtype)\n        return res\n\n    def args_gen(p):\n\n        def _args_gen():\n            return [gen_mat(), p]\n        return _args_gen\n    args_maker = args_gen(pnorm)\n    if pnorm not in [2, -2] and len(set(shape[-2:])) != 1:\n        with self.assertRaises(ValueError):\n            jnp.linalg.cond(*args_maker())\n    else:\n        self._CheckAgainstNumpy(np.linalg.cond, jnp.linalg.cond, args_maker, check_dtypes=False, tol=0.001)\n        partial_norm = partial(jnp.linalg.cond, p=pnorm)\n        self._CompileAndCheck(partial_norm, lambda: [gen_mat()], check_dtypes=False, rtol=0.001, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@jtu.sample_product([dict(m=m, n=n, full_matrices=full_matrices, hermitian=hermitian) for (m, n), full_matrices in list(itertools.product(itertools.product([0, 2, 7, 29, 32, 53], repeat=2), [False, True])) + [((400000, 2), False), ((2, 400000), False)] for hermitian in ([False, True] if m == n else [False])], b=[(), (3,), (2, 3)], dtype=float_types + complex_types, compute_uv=[False, True], algorithm=[None, lax.linalg.SvdAlgorithm.QR, lax.linalg.SvdAlgorithm.JACOBI])\n@jax.default_matmul_precision('float32')\ndef testSVD(self, b, m, n, dtype, full_matrices, compute_uv, hermitian, algorithm):\n    if algorithm is not None:\n        if hermitian:\n            self.skipTest(\"Hermitian SVD doesn't support the algorithm parameter.\")\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('SVD algorithm selection only supported on CPU and GPU.')\n        if jtu.test_device_matches(['cpu']) and jtu.jaxlib_version() <= (0, 5, 1):\n            self.skipTest('SVD algorithm selection on CPU requires a newer jaxlib version.')\n        if jtu.test_device_matches(['cpu']) and algorithm == lax.linalg.SvdAlgorithm.JACOBI:\n            self.skipTest('Jacobi SVD not supported on GPU.')\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(b + (m, n), dtype)]\n\n    def compute_max_backward_error(operand, reconstructed_operand):\n        error_norm = np.linalg.norm(operand - reconstructed_operand, axis=(-2, -1))\n        backward_error = error_norm / np.linalg.norm(operand, axis=(-2, -1))\n        max_backward_error = np.amax(backward_error)\n        return max_backward_error\n    tol = 100 * jnp.finfo(dtype).eps\n    reconstruction_tol = 2 * tol\n    unitariness_tol = 3 * tol\n    a, = args_maker()\n    if hermitian:\n        a = a + np.conj(T(a))\n    if algorithm is None:\n        fun = partial(jnp.linalg.svd, hermitian=hermitian)\n    else:\n        fun = partial(lax.linalg.svd, algorithm=algorithm)\n    out = fun(a, full_matrices=full_matrices, compute_uv=compute_uv)\n    if compute_uv:\n        out = list(out)\n        out[1] = out[1].astype(out[0].dtype)\n        if m and n:\n            if full_matrices:\n                k = min(m, n)\n                if m < n:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2][..., :k, :]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n                else:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0][..., :, :k], out[2]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n            else:\n                max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2]))\n                self.assertLess(max_backward_error, reconstruction_tol)\n        unitary_mat = np.real(np.matmul(np.conj(T(out[0])), out[0]))\n        eye_slice = np.eye(out[0].shape[-1], dtype=unitary_mat.dtype)\n        self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        if m >= n:\n            unitary_mat = np.real(np.matmul(np.conj(T(out[2])), out[2]))\n            eye_slice = np.eye(out[2].shape[-1], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        else:\n            unitary_mat = np.real(np.matmul(out[2], np.conj(T(out[2]))))\n            eye_slice = np.eye(out[2].shape[-2], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n    else:\n        self.assertTrue(np.allclose(np.linalg.svd(a, compute_uv=False), np.asarray(out), atol=0.0001, rtol=0.0001))\n    self._CompileAndCheck(partial(fun, full_matrices=full_matrices, compute_uv=compute_uv), args_maker)\n    if not compute_uv and a.size < 100000:\n        svd = partial(fun, full_matrices=full_matrices, compute_uv=compute_uv)\n        if dtype == np.complex128:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.0001, atol=0.0001, eps=1e-08)\n        else:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.05, atol=0.2)\n    if compute_uv and (not full_matrices):\n        b, = args_maker()\n\n        def f(x):\n            u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n            vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n            return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real\n        _, t_out = jvp(f, (1.0,), (1.0,))\n        if dtype == np.complex128:\n            atol = 2e-13\n        else:\n            atol = 0.0006\n        self.assertArraysAllClose(t_out, b.real, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@jax.jit\ndef fun(x, y):\n    return cond(x < 3, None, lambda _: 2.0 * jnp.sin(y), x, lambda x: 2.0 * x)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(0, 0), (4, 4), (5, 5), (50, 50), (2, 6, 6)], dtype=float_types + complex_types, compute_left_eigenvectors=[False, True], compute_right_eigenvectors=[False, True])\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEig(self, shape, dtype, compute_left_eigenvectors, compute_right_eigenvectors):\n    rng = jtu.rand_default(self.rng())\n    n = shape[-1]\n    args_maker = lambda: [rng(shape, dtype)]\n\n    def norm(x):\n        norm = np.linalg.norm(x, axis=(-2, -1))\n        return norm / ((n + 1) * jnp.finfo(dtype).eps)\n\n    def check_right_eigenvectors(a, w, vr):\n        self.assertTrue(np.all(norm(np.matmul(a, vr) - w[..., None, :] * vr) < 100))\n\n    def check_left_eigenvectors(a, w, vl):\n        rank = len(a.shape)\n        aH = jnp.conj(a.transpose(list(range(rank - 2)) + [rank - 1, rank - 2]))\n        wC = jnp.conj(w)\n        check_right_eigenvectors(aH, wC, vl)\n    a, = args_maker()\n    results = lax.linalg.eig(a, compute_left_eigenvectors=compute_left_eigenvectors, compute_right_eigenvectors=compute_right_eigenvectors)\n    w = results[0]\n    if compute_left_eigenvectors:\n        check_left_eigenvectors(a, w, results[1])\n    if compute_right_eigenvectors:\n        check_right_eigenvectors(a, w, results[1 + compute_left_eigenvectors])\n    self._CompileAndCheck(partial(jnp.linalg.eig), args_maker, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5)], dtype=float_types + complex_types)\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEigBatching(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    shape = (10,) + shape\n    args = rng(shape, dtype)\n    ws, vs = vmap(jnp.linalg.eig)(args)\n    self.assertTrue(np.all(np.linalg.norm(np.matmul(args, vs) - ws[..., None, :] * vs) < 0.001))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(n=[0, 4, 5, 50, 512], dtype=float_types + complex_types, lower=[True, False])\ndef testEigh(self, n, dtype, lower):\n    rng = jtu.rand_default(self.rng())\n    eps = np.finfo(dtype).eps\n    args_maker = lambda: [rng((n, n), dtype)]\n    uplo = 'L' if lower else 'U'\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    w, v = jnp.linalg.eigh(np.tril(a) if lower else np.triu(a), UPLO=uplo, symmetrize_input=False)\n    w = w.astype(v.dtype)\n    tol = 2 * n * eps\n    self.assertAllClose(np.eye(n, dtype=v.dtype), np.matmul(np.conj(T(v)), v), atol=tol, rtol=tol)\n    with jax.numpy_rank_promotion('allow'):\n        tol = 100 * eps\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    self._CompileAndCheck(partial(jnp.linalg.eigh, UPLO=uplo), args_maker, rtol=eps)\n    double_type = dtype\n    if dtype == np.float32:\n        double_type = np.float64\n    if dtype == np.complex64:\n        double_type = np.complex128\n    w_np = np.linalg.eigvalsh(a.astype(double_type))\n    tol = 8 * eps\n    self.assertAllClose(w_np.astype(w.dtype), w, atol=tol * np.linalg.norm(a), rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(start=[0, 1, 63, 64, 65, 255], end=[1, 63, 64, 65, 256])\n@jtu.run_on_devices('tpu')\ndef testEighSubsetByIndex(self, start, end):\n    if start >= end:\n        return\n    dtype = np.float32\n    n = 256\n    rng = jtu.rand_default(self.rng())\n    eps = np.finfo(dtype).eps\n    args_maker = lambda: [rng((n, n), dtype)]\n    subset_by_index = (start, end)\n    k = end - start\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    v, w = lax.linalg.eigh(a, symmetrize_input=False, subset_by_index=subset_by_index)\n    w = w.astype(v.dtype)\n    self.assertEqual(v.shape, (n, k))\n    self.assertEqual(w.shape, (k,))\n    with jax.numpy_rank_promotion('allow'):\n        tol = 200 * eps\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    tol = 3 * n * eps\n    self.assertAllClose(np.eye(k, dtype=v.dtype), np.matmul(np.conj(T(v)), v), atol=tol, rtol=tol)\n    self._CompileAndCheck(partial(jnp.linalg.eigh), args_maker, rtol=eps)\n    double_type = dtype\n    if dtype == np.float32:\n        double_type = np.float64\n    if dtype == np.complex64:\n        double_type = np.complex128\n    w_np = np.linalg.eigvalsh(a.astype(double_type))[subset_by_index[0]:subset_by_index[1]]\n    tol = 20 * eps\n    self.assertAllClose(w_np.astype(w.dtype), w, atol=tol * np.linalg.norm(a), rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "def testEighZeroDiagonal(self):\n    a = np.array([[0.0, -1.0, -1.0, 1.0], [-1.0, 0.0, 1.0, -1.0], [-1.0, 1.0, 0.0, -1.0], [1.0, -1.0, -1.0, 0.0]], dtype=np.float32)\n    w, v = jnp.linalg.eigh(a)\n    w = w.astype(v.dtype)\n    eps = jnp.finfo(a.dtype).eps\n    with jax.numpy_rank_promotion('allow'):\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), 2 * eps * np.linalg.norm(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "def testEighTinyNorm(self):\n    rng = jtu.rand_default(self.rng())\n    a = rng((300, 300), dtype=np.float32)\n    eps = jnp.finfo(a.dtype).eps\n    a = eps * (a + np.conj(a.T))\n    w, v = jnp.linalg.eigh(a)\n    w = w.astype(v.dtype)\n    with jax.numpy_rank_promotion('allow'):\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), 80 * eps * np.linalg.norm(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(rank=[1, 3, 299])\ndef testEighRankDeficient(self, rank):\n    rng = jtu.rand_default(self.rng())\n    eps = jnp.finfo(np.float32).eps\n    a = rng((300, rank), dtype=np.float32)\n    a = a @ np.conj(a.T)\n    w, v = jnp.linalg.eigh(a)\n    w = w.astype(v.dtype)\n    with jax.numpy_rank_promotion('allow'):\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), 85 * eps * np.linalg.norm(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(n=[0, 4, 5, 50, 512], dtype=float_types + complex_types, lower=[True, False])\ndef testEighIdentity(self, n, dtype, lower):\n    tol = np.finfo(dtype).eps\n    uplo = 'L' if lower else 'U'\n    a = jnp.eye(n, dtype=dtype)\n    w, v = jnp.linalg.eigh(a, UPLO=uplo, symmetrize_input=False)\n    w = w.astype(v.dtype)\n    self.assertLessEqual(np.linalg.norm(np.eye(n) - np.matmul(np.conj(T(v)), v)), tol)\n    with jax.numpy_rank_promotion('allow'):\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (300, 300)], dtype=float_types + complex_types)\ndef testEighBatching(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    shape = (10,) + shape\n    args = rng(shape, dtype)\n    args = (args + np.conj(T(args))) / 2\n    ws, vs = vmap(jsp.linalg.eigh)(args)\n    ws = ws.astype(vs.dtype)\n    norm = np.max(np.linalg.norm(np.matmul(args, vs) - ws[..., None, :] * vs))\n    self.assertLess(norm, 0.014)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "def testStringInfNorm(self):\n    err, msg = (ValueError, \"Invalid order 'inf' for vector norm.\")\n    with self.assertRaisesRegex(err, msg):\n        jnp.linalg.norm(jnp.array([1.0, 2.0, 3.0]), ord='inf')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product([dict(m=m, n=n, full_matrices=full_matrices, hermitian=hermitian) for (m, n), full_matrices in list(itertools.product(itertools.product([0, 2, 7, 29, 32, 53], repeat=2), [False, True])) + [((400000, 2), False), ((2, 400000), False)] for hermitian in ([False, True] if m == n else [False])], b=[(), (3,), (2, 3)], dtype=float_types + complex_types, compute_uv=[False, True], algorithm=[None, lax.linalg.SvdAlgorithm.QR, lax.linalg.SvdAlgorithm.JACOBI])\n@jax.default_matmul_precision('float32')\ndef testSVD(self, b, m, n, dtype, full_matrices, compute_uv, hermitian, algorithm):\n    if algorithm is not None:\n        if hermitian:\n            self.skipTest(\"Hermitian SVD doesn't support the algorithm parameter.\")\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('SVD algorithm selection only supported on CPU and GPU.')\n        if jtu.test_device_matches(['cpu']) and jtu.jaxlib_version() <= (0, 5, 1):\n            self.skipTest('SVD algorithm selection on CPU requires a newer jaxlib version.')\n        if jtu.test_device_matches(['cpu']) and algorithm == lax.linalg.SvdAlgorithm.JACOBI:\n            self.skipTest('Jacobi SVD not supported on GPU.')\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(b + (m, n), dtype)]\n\n    def compute_max_backward_error(operand, reconstructed_operand):\n        error_norm = np.linalg.norm(operand - reconstructed_operand, axis=(-2, -1))\n        backward_error = error_norm / np.linalg.norm(operand, axis=(-2, -1))\n        max_backward_error = np.amax(backward_error)\n        return max_backward_error\n    tol = 100 * jnp.finfo(dtype).eps\n    reconstruction_tol = 2 * tol\n    unitariness_tol = 3 * tol\n    a, = args_maker()\n    if hermitian:\n        a = a + np.conj(T(a))\n    if algorithm is None:\n        fun = partial(jnp.linalg.svd, hermitian=hermitian)\n    else:\n        fun = partial(lax.linalg.svd, algorithm=algorithm)\n    out = fun(a, full_matrices=full_matrices, compute_uv=compute_uv)\n    if compute_uv:\n        out = list(out)\n        out[1] = out[1].astype(out[0].dtype)\n        if m and n:\n            if full_matrices:\n                k = min(m, n)\n                if m < n:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2][..., :k, :]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n                else:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0][..., :, :k], out[2]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n            else:\n                max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2]))\n                self.assertLess(max_backward_error, reconstruction_tol)\n        unitary_mat = np.real(np.matmul(np.conj(T(out[0])), out[0]))\n        eye_slice = np.eye(out[0].shape[-1], dtype=unitary_mat.dtype)\n        self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        if m >= n:\n            unitary_mat = np.real(np.matmul(np.conj(T(out[2])), out[2]))\n            eye_slice = np.eye(out[2].shape[-1], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        else:\n            unitary_mat = np.real(np.matmul(out[2], np.conj(T(out[2]))))\n            eye_slice = np.eye(out[2].shape[-2], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n    else:\n        self.assertTrue(np.allclose(np.linalg.svd(a, compute_uv=False), np.asarray(out), atol=0.0001, rtol=0.0001))\n    self._CompileAndCheck(partial(fun, full_matrices=full_matrices, compute_uv=compute_uv), args_maker)\n    if not compute_uv and a.size < 100000:\n        svd = partial(fun, full_matrices=full_matrices, compute_uv=compute_uv)\n        if dtype == np.complex128:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.0001, atol=0.0001, eps=1e-08)\n        else:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.05, atol=0.2)\n    if compute_uv and (not full_matrices):\n        b, = args_maker()\n\n        def f(x):\n            u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n            vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n            return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real\n        _, t_out = jvp(f, (1.0,), (1.0,))\n        if dtype == np.complex128:\n            atol = 2e-13\n        else:\n            atol = 0.0006\n        self.assertArraysAllClose(t_out, b.real, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(0, 0), (2, 0), (0, 2), (3, 3), (3, 4), (2, 10, 5), (2, 200, 100), (64, 16, 5), (33, 7, 3), (137, 9, 5), (20000, 2, 2)], dtype=float_types + complex_types, full_matrices=[False, True])\n@jax.default_matmul_precision('float32')\ndef testQr(self, shape, dtype, full_matrices):\n    if jtu.test_device_matches(['cuda']) and _is_required_cuda_version_satisfied(12000):\n        self.skipTest('Triggers a bug in cuda-12 b/287345077')\n    rng = jtu.rand_default(self.rng())\n    m, n = shape[-2:]\n    if full_matrices:\n        mode, k = ('complete', m)\n    else:\n        mode, k = ('reduced', min(m, n))\n    a = rng(shape, dtype)\n    lq, lr = jnp.linalg.qr(a, mode=mode)\n    nq = np.zeros(shape[:-2] + (m, k), dtype)\n    nr = np.zeros(shape[:-2] + (k, n), dtype)\n    for index in np.ndindex(*shape[:-2]):\n        nq[index], nr[index] = np.linalg.qr(a[index], mode=mode)\n    max_rank = max(m, n)\n\n    def norm(x):\n        n = np.linalg.norm(x, axis=(-2, -1))\n        return n / (max(1, max_rank) * jnp.finfo(dtype).eps)\n\n    def compare_orthogonal(q1, q2):\n        ratio = np.divide(np.where(q2 == 0, 0, q1), np.where(q2 == 0, 1, q2))\n        sum_of_ratios = ratio.sum(axis=-2, keepdims=True)\n        phases = np.divide(sum_of_ratios, np.abs(sum_of_ratios))\n        q1 *= phases\n        nm = norm(q1 - q2)\n        self.assertTrue(np.all(nm < 160), msg=f'norm={np.amax(nm)}')\n    norm_error = norm(a - np.matmul(lq, lr))\n    self.assertTrue(np.all(norm_error < 60), msg=np.amax(norm_error))\n    compare_orthogonal(nq[..., :k], lq[..., :k])\n    self.assertTrue(np.all(norm(np.eye(k) - np.matmul(np.conj(T(lq)), lq)) < 10))\n\n    def qr_and_mul(a):\n        q, r = jnp.linalg.qr(a, mode=mode)\n        return q @ r\n    if m == n or (m > n and (not full_matrices)):\n        jtu.check_jvp(qr_and_mul, partial(jvp, qr_and_mul), (a,), atol=0.003)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(10, 4, 5), (5, 3, 3), (7, 6, 4)], dtype=float_types + complex_types)\ndef testQrBatching(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args = rng(shape, jnp.float32)\n    qs, rs = vmap(jsp.linalg.qr)(args)\n    self.assertTrue(np.all(np.linalg.norm(args - np.matmul(qs, rs)) < 0.001))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(n=[1, 4, 5, 20, 50, 100], dtype=float_types + complex_types)\ndef testExpmFrechet(self, n, dtype):\n    rng = jtu.rand_small(self.rng())\n    if dtype == np.float64 or dtype == np.complex128:\n        target_norms = [0.01, 0.2, 0.9, 2.0, 3.0]\n        tol = {np.dtype(np.float64): 1e-14, np.dtype(np.complex128): 1e-14}\n    elif dtype == np.float32 or dtype == np.complex64:\n        target_norms = [0.4, 1.0, 3.0]\n        tol = None\n    else:\n        raise TypeError(f'dtype={dtype!r} is not supported.')\n    for norm in target_norms:\n\n        def args_maker():\n            a = rng((n, n), dtype)\n            a = a / np.linalg.norm(a, 1) * norm\n            e = rng((n, n), dtype)\n            return [a, e]\n        osp_fun = lambda a, e: osp.linalg.expm_frechet(a, e, compute_expm=True)\n        jsp_fun = lambda a, e: jsp.linalg.expm_frechet(a, e, compute_expm=True)\n        self._CheckAgainstNumpy(osp_fun, jsp_fun, args_maker, check_dtypes=False, tol=tol)\n        self._CompileAndCheck(jsp_fun, args_maker, check_dtypes=False)\n        osp_fun = lambda a, e: osp.linalg.expm_frechet(a, e, compute_expm=False)\n        jsp_fun = lambda a, e: jsp.linalg.expm_frechet(a, e, compute_expm=False)\n        self._CheckAgainstNumpy(osp_fun, jsp_fun, args_maker, check_dtypes=False, tol=tol)\n        self._CompileAndCheck(jsp_fun, args_maker, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(n=[1, 4, 5, 20, 50], dtype=float_types + complex_types)\ndef testExpmGrad(self, n, dtype):\n    rng = jtu.rand_small(self.rng())\n    a = rng((n, n), dtype)\n    if dtype == np.float64 or dtype == np.complex128:\n        target_norms = [0.01, 0.2, 0.9, 2.0, 3.0]\n    elif dtype == np.float32 or dtype == np.complex64:\n        target_norms = [0.4, 1.0, 3.0]\n    else:\n        raise TypeError(f'dtype={dtype!r} is not supported.')\n    tol = {np.dtype(np.float32): 0.02, np.dtype(np.complex64): 0.02, np.dtype(np.float64): 0.0001, np.dtype(np.complex128): 0.0001}\n    for norm in target_norms:\n        a = a / np.linalg.norm(a, 1) * norm\n\n        def expm(x):\n            return jsp.linalg.expm(x, upper_triangular=False, max_squarings=16)\n        jtu.check_grads(expm, (a,), modes=['fwd', 'rev'], order=1, atol=tol, rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(n=[0, 4, 5, 50], dtype=float_types + complex_types, lower=[True, False], sort_eigenvalues=[True, False])\ndef testEigh(self, n, dtype, lower, sort_eigenvalues):\n    rng = jtu.rand_default(self.rng())\n    tol = 0.001\n    args_maker = lambda: [rng((n, n), dtype)]\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    v, w = lax.linalg.eigh(np.tril(a) if lower else np.triu(a), lower=lower, symmetrize_input=False, sort_eigenvalues=sort_eigenvalues)\n    w = np.asarray(w)\n    v = np.asarray(v)\n    self.assertLessEqual(np.linalg.norm(np.eye(n) - np.matmul(np.conj(T(v)), v)), 0.001)\n    self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    w_expected, v_expected = np.linalg.eigh(np.asarray(a))\n    self.assertAllClose(w_expected, w if sort_eigenvalues else np.sort(w), rtol=0.0001, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def norm(x):\n    norm = np.linalg.norm(x, axis=(-2, -1))\n    return norm / ((n + 1) * jnp.finfo(dtype).eps)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(0, 0), (4, 4), (5, 5), (50, 50), (2, 6, 6)], dtype=float_types + complex_types, compute_left_eigenvectors=[False, True], compute_right_eigenvectors=[False, True])\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEig(self, shape, dtype, compute_left_eigenvectors, compute_right_eigenvectors):\n    rng = jtu.rand_default(self.rng())\n    n = shape[-1]\n    args_maker = lambda: [rng(shape, dtype)]\n\n    def norm(x):\n        norm = np.linalg.norm(x, axis=(-2, -1))\n        return norm / ((n + 1) * jnp.finfo(dtype).eps)\n\n    def check_right_eigenvectors(a, w, vr):\n        self.assertTrue(np.all(norm(np.matmul(a, vr) - w[..., None, :] * vr) < 100))\n\n    def check_left_eigenvectors(a, w, vl):\n        rank = len(a.shape)\n        aH = jnp.conj(a.transpose(list(range(rank - 2)) + [rank - 1, rank - 2]))\n        wC = jnp.conj(w)\n        check_right_eigenvectors(aH, wC, vl)\n    a, = args_maker()\n    results = lax.linalg.eig(a, compute_left_eigenvectors=compute_left_eigenvectors, compute_right_eigenvectors=compute_right_eigenvectors)\n    w = results[0]\n    if compute_left_eigenvectors:\n        check_left_eigenvectors(a, w, results[1])\n    if compute_right_eigenvectors:\n        check_right_eigenvectors(a, w, results[1 + compute_left_eigenvectors])\n    self._CompileAndCheck(partial(jnp.linalg.eig), args_maker, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def check_left_eigenvectors(a, w, vl):\n    rank = len(a.shape)\n    aH = jnp.conj(a.transpose(list(range(rank - 2)) + [rank - 1, rank - 2]))\n    wC = jnp.conj(w)\n    check_right_eigenvectors(aH, wC, vl)"
  },
  {
    "test_code": "@jtu.sample_product([dict(m=m, n=n, full_matrices=full_matrices, hermitian=hermitian) for (m, n), full_matrices in list(itertools.product(itertools.product([0, 2, 7, 29, 32, 53], repeat=2), [False, True])) + [((400000, 2), False), ((2, 400000), False)] for hermitian in ([False, True] if m == n else [False])], b=[(), (3,), (2, 3)], dtype=float_types + complex_types, compute_uv=[False, True], algorithm=[None, lax.linalg.SvdAlgorithm.QR, lax.linalg.SvdAlgorithm.JACOBI])\n@jax.default_matmul_precision('float32')\ndef testSVD(self, b, m, n, dtype, full_matrices, compute_uv, hermitian, algorithm):\n    if algorithm is not None:\n        if hermitian:\n            self.skipTest(\"Hermitian SVD doesn't support the algorithm parameter.\")\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('SVD algorithm selection only supported on CPU and GPU.')\n        if jtu.test_device_matches(['cpu']) and jtu.jaxlib_version() <= (0, 5, 1):\n            self.skipTest('SVD algorithm selection on CPU requires a newer jaxlib version.')\n        if jtu.test_device_matches(['cpu']) and algorithm == lax.linalg.SvdAlgorithm.JACOBI:\n            self.skipTest('Jacobi SVD not supported on GPU.')\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(b + (m, n), dtype)]\n\n    def compute_max_backward_error(operand, reconstructed_operand):\n        error_norm = np.linalg.norm(operand - reconstructed_operand, axis=(-2, -1))\n        backward_error = error_norm / np.linalg.norm(operand, axis=(-2, -1))\n        max_backward_error = np.amax(backward_error)\n        return max_backward_error\n    tol = 100 * jnp.finfo(dtype).eps\n    reconstruction_tol = 2 * tol\n    unitariness_tol = 3 * tol\n    a, = args_maker()\n    if hermitian:\n        a = a + np.conj(T(a))\n    if algorithm is None:\n        fun = partial(jnp.linalg.svd, hermitian=hermitian)\n    else:\n        fun = partial(lax.linalg.svd, algorithm=algorithm)\n    out = fun(a, full_matrices=full_matrices, compute_uv=compute_uv)\n    if compute_uv:\n        out = list(out)\n        out[1] = out[1].astype(out[0].dtype)\n        if m and n:\n            if full_matrices:\n                k = min(m, n)\n                if m < n:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2][..., :k, :]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n                else:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0][..., :, :k], out[2]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n            else:\n                max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2]))\n                self.assertLess(max_backward_error, reconstruction_tol)\n        unitary_mat = np.real(np.matmul(np.conj(T(out[0])), out[0]))\n        eye_slice = np.eye(out[0].shape[-1], dtype=unitary_mat.dtype)\n        self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        if m >= n:\n            unitary_mat = np.real(np.matmul(np.conj(T(out[2])), out[2]))\n            eye_slice = np.eye(out[2].shape[-1], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        else:\n            unitary_mat = np.real(np.matmul(out[2], np.conj(T(out[2]))))\n            eye_slice = np.eye(out[2].shape[-2], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n    else:\n        self.assertTrue(np.allclose(np.linalg.svd(a, compute_uv=False), np.asarray(out), atol=0.0001, rtol=0.0001))\n    self._CompileAndCheck(partial(fun, full_matrices=full_matrices, compute_uv=compute_uv), args_maker)\n    if not compute_uv and a.size < 100000:\n        svd = partial(fun, full_matrices=full_matrices, compute_uv=compute_uv)\n        if dtype == np.complex128:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.0001, atol=0.0001, eps=1e-08)\n        else:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.05, atol=0.2)\n    if compute_uv and (not full_matrices):\n        b, = args_maker()\n\n        def f(x):\n            u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n            vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n            return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real\n        _, t_out = jvp(f, (1.0,), (1.0,))\n        if dtype == np.complex128:\n            atol = 2e-13\n        else:\n            atol = 0.0006\n        self.assertArraysAllClose(t_out, b.real, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def fun():\n    return jax.ffi.ffi_call('test', jax.ShapeDtypeStruct((), np.int64))()"
  },
  {
    "test_code": "@jtu.sample_product([dict(m=m, n=n, full_matrices=full_matrices, hermitian=hermitian) for (m, n), full_matrices in list(itertools.product(itertools.product([0, 2, 7, 29, 32, 53], repeat=2), [False, True])) + [((400000, 2), False), ((2, 400000), False)] for hermitian in ([False, True] if m == n else [False])], b=[(), (3,), (2, 3)], dtype=float_types + complex_types, compute_uv=[False, True], algorithm=[None, lax.linalg.SvdAlgorithm.QR, lax.linalg.SvdAlgorithm.JACOBI])\n@jax.default_matmul_precision('float32')\ndef testSVD(self, b, m, n, dtype, full_matrices, compute_uv, hermitian, algorithm):\n    if algorithm is not None:\n        if hermitian:\n            self.skipTest(\"Hermitian SVD doesn't support the algorithm parameter.\")\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('SVD algorithm selection only supported on CPU and GPU.')\n        if jtu.test_device_matches(['cpu']) and jtu.jaxlib_version() <= (0, 5, 1):\n            self.skipTest('SVD algorithm selection on CPU requires a newer jaxlib version.')\n        if jtu.test_device_matches(['cpu']) and algorithm == lax.linalg.SvdAlgorithm.JACOBI:\n            self.skipTest('Jacobi SVD not supported on GPU.')\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(b + (m, n), dtype)]\n\n    def compute_max_backward_error(operand, reconstructed_operand):\n        error_norm = np.linalg.norm(operand - reconstructed_operand, axis=(-2, -1))\n        backward_error = error_norm / np.linalg.norm(operand, axis=(-2, -1))\n        max_backward_error = np.amax(backward_error)\n        return max_backward_error\n    tol = 100 * jnp.finfo(dtype).eps\n    reconstruction_tol = 2 * tol\n    unitariness_tol = 3 * tol\n    a, = args_maker()\n    if hermitian:\n        a = a + np.conj(T(a))\n    if algorithm is None:\n        fun = partial(jnp.linalg.svd, hermitian=hermitian)\n    else:\n        fun = partial(lax.linalg.svd, algorithm=algorithm)\n    out = fun(a, full_matrices=full_matrices, compute_uv=compute_uv)\n    if compute_uv:\n        out = list(out)\n        out[1] = out[1].astype(out[0].dtype)\n        if m and n:\n            if full_matrices:\n                k = min(m, n)\n                if m < n:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2][..., :k, :]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n                else:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0][..., :, :k], out[2]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n            else:\n                max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2]))\n                self.assertLess(max_backward_error, reconstruction_tol)\n        unitary_mat = np.real(np.matmul(np.conj(T(out[0])), out[0]))\n        eye_slice = np.eye(out[0].shape[-1], dtype=unitary_mat.dtype)\n        self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        if m >= n:\n            unitary_mat = np.real(np.matmul(np.conj(T(out[2])), out[2]))\n            eye_slice = np.eye(out[2].shape[-1], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        else:\n            unitary_mat = np.real(np.matmul(out[2], np.conj(T(out[2]))))\n            eye_slice = np.eye(out[2].shape[-2], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n    else:\n        self.assertTrue(np.allclose(np.linalg.svd(a, compute_uv=False), np.asarray(out), atol=0.0001, rtol=0.0001))\n    self._CompileAndCheck(partial(fun, full_matrices=full_matrices, compute_uv=compute_uv), args_maker)\n    if not compute_uv and a.size < 100000:\n        svd = partial(fun, full_matrices=full_matrices, compute_uv=compute_uv)\n        if dtype == np.complex128:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.0001, atol=0.0001, eps=1e-08)\n        else:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.05, atol=0.2)\n    if compute_uv and (not full_matrices):\n        b, = args_maker()\n\n        def f(x):\n            u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n            vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n            return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real\n        _, t_out = jvp(f, (1.0,), (1.0,))\n        if dtype == np.complex128:\n            atol = 2e-13\n        else:\n            atol = 0.0006\n        self.assertArraysAllClose(t_out, b.real, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def fun(x, t):\n    return jnp.sum(jnp.power(jnp.maximum(x, 0.0), 2)) + t"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@jtu.sample_product([dict(m=m, n=n, full_matrices=full_matrices, hermitian=hermitian) for (m, n), full_matrices in list(itertools.product(itertools.product([0, 2, 7, 29, 32, 53], repeat=2), [False, True])) + [((400000, 2), False), ((2, 400000), False)] for hermitian in ([False, True] if m == n else [False])], b=[(), (3,), (2, 3)], dtype=float_types + complex_types, compute_uv=[False, True], algorithm=[None, lax.linalg.SvdAlgorithm.QR, lax.linalg.SvdAlgorithm.JACOBI])\n@jax.default_matmul_precision('float32')\ndef testSVD(self, b, m, n, dtype, full_matrices, compute_uv, hermitian, algorithm):\n    if algorithm is not None:\n        if hermitian:\n            self.skipTest(\"Hermitian SVD doesn't support the algorithm parameter.\")\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('SVD algorithm selection only supported on CPU and GPU.')\n        if jtu.test_device_matches(['cpu']) and jtu.jaxlib_version() <= (0, 5, 1):\n            self.skipTest('SVD algorithm selection on CPU requires a newer jaxlib version.')\n        if jtu.test_device_matches(['cpu']) and algorithm == lax.linalg.SvdAlgorithm.JACOBI:\n            self.skipTest('Jacobi SVD not supported on GPU.')\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(b + (m, n), dtype)]\n\n    def compute_max_backward_error(operand, reconstructed_operand):\n        error_norm = np.linalg.norm(operand - reconstructed_operand, axis=(-2, -1))\n        backward_error = error_norm / np.linalg.norm(operand, axis=(-2, -1))\n        max_backward_error = np.amax(backward_error)\n        return max_backward_error\n    tol = 100 * jnp.finfo(dtype).eps\n    reconstruction_tol = 2 * tol\n    unitariness_tol = 3 * tol\n    a, = args_maker()\n    if hermitian:\n        a = a + np.conj(T(a))\n    if algorithm is None:\n        fun = partial(jnp.linalg.svd, hermitian=hermitian)\n    else:\n        fun = partial(lax.linalg.svd, algorithm=algorithm)\n    out = fun(a, full_matrices=full_matrices, compute_uv=compute_uv)\n    if compute_uv:\n        out = list(out)\n        out[1] = out[1].astype(out[0].dtype)\n        if m and n:\n            if full_matrices:\n                k = min(m, n)\n                if m < n:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2][..., :k, :]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n                else:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0][..., :, :k], out[2]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n            else:\n                max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2]))\n                self.assertLess(max_backward_error, reconstruction_tol)\n        unitary_mat = np.real(np.matmul(np.conj(T(out[0])), out[0]))\n        eye_slice = np.eye(out[0].shape[-1], dtype=unitary_mat.dtype)\n        self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        if m >= n:\n            unitary_mat = np.real(np.matmul(np.conj(T(out[2])), out[2]))\n            eye_slice = np.eye(out[2].shape[-1], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        else:\n            unitary_mat = np.real(np.matmul(out[2], np.conj(T(out[2]))))\n            eye_slice = np.eye(out[2].shape[-2], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n    else:\n        self.assertTrue(np.allclose(np.linalg.svd(a, compute_uv=False), np.asarray(out), atol=0.0001, rtol=0.0001))\n    self._CompileAndCheck(partial(fun, full_matrices=full_matrices, compute_uv=compute_uv), args_maker)\n    if not compute_uv and a.size < 100000:\n        svd = partial(fun, full_matrices=full_matrices, compute_uv=compute_uv)\n        if dtype == np.complex128:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.0001, atol=0.0001, eps=1e-08)\n        else:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.05, atol=0.2)\n    if compute_uv and (not full_matrices):\n        b, = args_maker()\n\n        def f(x):\n            u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n            vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n            return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real\n        _, t_out = jvp(f, (1.0,), (1.0,))\n        if dtype == np.complex128:\n            atol = 2e-13\n        else:\n            atol = 0.0006\n        self.assertArraysAllClose(t_out, b.real, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@partial(jax.jit, backend=backend)\ndef fun(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (2, 3, 5), (5, 5, 5), (20, 20), (5, 10)], pnorm=[jnp.inf, -jnp.inf, 1, -1, 2, -2, 'fro'], dtype=float_types + complex_types)\n@jtu.skip_on_devices('gpu')\ndef testCond(self, shape, pnorm, dtype):\n\n    def gen_mat():\n        arr_gen = jtu.rand_default(self.rng())\n        res = arr_gen(shape, dtype)\n        return res\n\n    def args_gen(p):\n\n        def _args_gen():\n            return [gen_mat(), p]\n        return _args_gen\n    args_maker = args_gen(pnorm)\n    if pnorm not in [2, -2] and len(set(shape[-2:])) != 1:\n        with self.assertRaises(ValueError):\n            jnp.linalg.cond(*args_maker())\n    else:\n        self._CheckAgainstNumpy(np.linalg.cond, jnp.linalg.cond, args_maker, check_dtypes=False, tol=0.001)\n        partial_norm = partial(jnp.linalg.cond, p=pnorm)\n        self._CompileAndCheck(partial_norm, lambda: [gen_mat()], check_dtypes=False, rtol=0.001, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (2, 5, 5), (200, 200), (1000, 0, 0)], dtype=float_types + complex_types, upper=[True, False])\ndef testCholesky(self, shape, dtype, upper):\n    rng = jtu.rand_default(self.rng())\n\n    def args_maker():\n        factor_shape = shape[:-1] + (2 * shape[-1],)\n        a = rng(factor_shape, dtype)\n        return [np.matmul(a, jnp.conj(T(a)))]\n    jnp_fun = partial(jnp.linalg.cholesky, upper=upper)\n\n    def np_fun(x, upper=upper):\n        if jtu.numpy_version() >= (2, 0, 0):\n            return np.linalg.cholesky(x, upper=upper)\n        result = np.linalg.cholesky(x)\n        if upper:\n            axes = list(range(x.ndim))\n            axes[-1], axes[-2] = (axes[-2], axes[-1])\n            return np.transpose(result, axes).conj()\n        return result\n    self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, tol=0.001)\n    self._CompileAndCheck(jnp_fun, args_maker)\n    if jnp.finfo(dtype).bits == 64:\n        jtu.check_grads(jnp.linalg.cholesky, args_maker(), order=2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(m=[1, 5, 7, 23], nq=zip([2, 4, 6, 36], [(1, 2), (2, 2), (1, 2, 3), (3, 3, 1, 4)]), dtype=float_types)\ndef testTensorsolve(self, m, nq, dtype):\n    rng = jtu.rand_default(self.rng())\n    n, q = nq\n    b_shape = (n, m)\n    Q = q + (m,)\n    args_maker = lambda: [rng(b_shape + Q, dtype), rng(b_shape, dtype)]\n    a, b = args_maker()\n    result = jnp.linalg.tensorsolve(*args_maker())\n    self.assertEqual(result.shape, Q)\n    self._CheckAgainstNumpy(np.linalg.tensorsolve, jnp.linalg.tensorsolve, args_maker, tol={np.float32: 0.01, np.float64: 0.001})\n    self._CompileAndCheck(jnp.linalg.tensorsolve, args_maker, rtol={np.float64: 1e-13})",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(0, 0), (4, 4), (5, 5), (50, 50), (2, 6, 6)], dtype=float_types + complex_types, compute_left_eigenvectors=[False, True], compute_right_eigenvectors=[False, True])\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEig(self, shape, dtype, compute_left_eigenvectors, compute_right_eigenvectors):\n    rng = jtu.rand_default(self.rng())\n    n = shape[-1]\n    args_maker = lambda: [rng(shape, dtype)]\n\n    def norm(x):\n        norm = np.linalg.norm(x, axis=(-2, -1))\n        return norm / ((n + 1) * jnp.finfo(dtype).eps)\n\n    def check_right_eigenvectors(a, w, vr):\n        self.assertTrue(np.all(norm(np.matmul(a, vr) - w[..., None, :] * vr) < 100))\n\n    def check_left_eigenvectors(a, w, vl):\n        rank = len(a.shape)\n        aH = jnp.conj(a.transpose(list(range(rank - 2)) + [rank - 1, rank - 2]))\n        wC = jnp.conj(w)\n        check_right_eigenvectors(aH, wC, vl)\n    a, = args_maker()\n    results = lax.linalg.eig(a, compute_left_eigenvectors=compute_left_eigenvectors, compute_right_eigenvectors=compute_right_eigenvectors)\n    w = results[0]\n    if compute_left_eigenvectors:\n        check_left_eigenvectors(a, w, results[1])\n    if compute_right_eigenvectors:\n        check_right_eigenvectors(a, w, results[1 + compute_left_eigenvectors])\n    self._CompileAndCheck(partial(jnp.linalg.eig), args_maker, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(4, 4), (5, 5), (8, 8), (7, 6, 6)], dtype=float_types + complex_types)\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEigvalsGrad(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    a, = args_maker()\n    tol = 0.0001 if dtype in (np.float64, np.complex128) else 0.1\n    jtu.check_grads(lambda x: jnp.linalg.eigvals(x), (a,), order=1, modes=['fwd', 'rev'], rtol=tol, atol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(4, 4), (5, 5), (50, 50)], dtype=float_types + complex_types)\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEigvals(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    a, = args_maker()\n    w1, _ = jnp.linalg.eig(a)\n    w2 = jnp.linalg.eigvals(a)\n    self.assertAllClose(w1, w2, rtol={np.complex64: 1e-05, np.complex128: 2e-14})",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(n=[0, 4, 5, 50, 512], dtype=float_types + complex_types, lower=[True, False])\ndef testEigh(self, n, dtype, lower):\n    rng = jtu.rand_default(self.rng())\n    eps = np.finfo(dtype).eps\n    args_maker = lambda: [rng((n, n), dtype)]\n    uplo = 'L' if lower else 'U'\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    w, v = jnp.linalg.eigh(np.tril(a) if lower else np.triu(a), UPLO=uplo, symmetrize_input=False)\n    w = w.astype(v.dtype)\n    tol = 2 * n * eps\n    self.assertAllClose(np.eye(n, dtype=v.dtype), np.matmul(np.conj(T(v)), v), atol=tol, rtol=tol)\n    with jax.numpy_rank_promotion('allow'):\n        tol = 100 * eps\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    self._CompileAndCheck(partial(jnp.linalg.eigh, UPLO=uplo), args_maker, rtol=eps)\n    double_type = dtype\n    if dtype == np.float32:\n        double_type = np.float64\n    if dtype == np.complex64:\n        double_type = np.complex128\n    w_np = np.linalg.eigvalsh(a.astype(double_type))\n    tol = 8 * eps\n    self.assertAllClose(w_np.astype(w.dtype), w, atol=tol * np.linalg.norm(a), rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(start=[0, 1, 63, 64, 65, 255], end=[1, 63, 64, 65, 256])\n@jtu.run_on_devices('tpu')\ndef testEighSubsetByIndex(self, start, end):\n    if start >= end:\n        return\n    dtype = np.float32\n    n = 256\n    rng = jtu.rand_default(self.rng())\n    eps = np.finfo(dtype).eps\n    args_maker = lambda: [rng((n, n), dtype)]\n    subset_by_index = (start, end)\n    k = end - start\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    v, w = lax.linalg.eigh(a, symmetrize_input=False, subset_by_index=subset_by_index)\n    w = w.astype(v.dtype)\n    self.assertEqual(v.shape, (n, k))\n    self.assertEqual(w.shape, (k,))\n    with jax.numpy_rank_promotion('allow'):\n        tol = 200 * eps\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    tol = 3 * n * eps\n    self.assertAllClose(np.eye(k, dtype=v.dtype), np.matmul(np.conj(T(v)), v), atol=tol, rtol=tol)\n    self._CompileAndCheck(partial(jnp.linalg.eigh), args_maker, rtol=eps)\n    double_type = dtype\n    if dtype == np.float32:\n        double_type = np.float64\n    if dtype == np.complex64:\n        double_type = np.complex128\n    w_np = np.linalg.eigvalsh(a.astype(double_type))[subset_by_index[0]:subset_by_index[1]]\n    tol = 20 * eps\n    self.assertAllClose(w_np.astype(w.dtype), w, atol=tol * np.linalg.norm(a), rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product([dict(m=m, n=n, full_matrices=full_matrices, hermitian=hermitian) for (m, n), full_matrices in list(itertools.product(itertools.product([0, 2, 7, 29, 32, 53], repeat=2), [False, True])) + [((400000, 2), False), ((2, 400000), False)] for hermitian in ([False, True] if m == n else [False])], b=[(), (3,), (2, 3)], dtype=float_types + complex_types, compute_uv=[False, True], algorithm=[None, lax.linalg.SvdAlgorithm.QR, lax.linalg.SvdAlgorithm.JACOBI])\n@jax.default_matmul_precision('float32')\ndef testSVD(self, b, m, n, dtype, full_matrices, compute_uv, hermitian, algorithm):\n    if algorithm is not None:\n        if hermitian:\n            self.skipTest(\"Hermitian SVD doesn't support the algorithm parameter.\")\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('SVD algorithm selection only supported on CPU and GPU.')\n        if jtu.test_device_matches(['cpu']) and jtu.jaxlib_version() <= (0, 5, 1):\n            self.skipTest('SVD algorithm selection on CPU requires a newer jaxlib version.')\n        if jtu.test_device_matches(['cpu']) and algorithm == lax.linalg.SvdAlgorithm.JACOBI:\n            self.skipTest('Jacobi SVD not supported on GPU.')\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(b + (m, n), dtype)]\n\n    def compute_max_backward_error(operand, reconstructed_operand):\n        error_norm = np.linalg.norm(operand - reconstructed_operand, axis=(-2, -1))\n        backward_error = error_norm / np.linalg.norm(operand, axis=(-2, -1))\n        max_backward_error = np.amax(backward_error)\n        return max_backward_error\n    tol = 100 * jnp.finfo(dtype).eps\n    reconstruction_tol = 2 * tol\n    unitariness_tol = 3 * tol\n    a, = args_maker()\n    if hermitian:\n        a = a + np.conj(T(a))\n    if algorithm is None:\n        fun = partial(jnp.linalg.svd, hermitian=hermitian)\n    else:\n        fun = partial(lax.linalg.svd, algorithm=algorithm)\n    out = fun(a, full_matrices=full_matrices, compute_uv=compute_uv)\n    if compute_uv:\n        out = list(out)\n        out[1] = out[1].astype(out[0].dtype)\n        if m and n:\n            if full_matrices:\n                k = min(m, n)\n                if m < n:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2][..., :k, :]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n                else:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0][..., :, :k], out[2]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n            else:\n                max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2]))\n                self.assertLess(max_backward_error, reconstruction_tol)\n        unitary_mat = np.real(np.matmul(np.conj(T(out[0])), out[0]))\n        eye_slice = np.eye(out[0].shape[-1], dtype=unitary_mat.dtype)\n        self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        if m >= n:\n            unitary_mat = np.real(np.matmul(np.conj(T(out[2])), out[2]))\n            eye_slice = np.eye(out[2].shape[-1], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        else:\n            unitary_mat = np.real(np.matmul(out[2], np.conj(T(out[2]))))\n            eye_slice = np.eye(out[2].shape[-2], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n    else:\n        self.assertTrue(np.allclose(np.linalg.svd(a, compute_uv=False), np.asarray(out), atol=0.0001, rtol=0.0001))\n    self._CompileAndCheck(partial(fun, full_matrices=full_matrices, compute_uv=compute_uv), args_maker)\n    if not compute_uv and a.size < 100000:\n        svd = partial(fun, full_matrices=full_matrices, compute_uv=compute_uv)\n        if dtype == np.complex128:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.0001, atol=0.0001, eps=1e-08)\n        else:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.05, atol=0.2)\n    if compute_uv and (not full_matrices):\n        b, = args_maker()\n\n        def f(x):\n            u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n            vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n            return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real\n        _, t_out = jvp(f, (1.0,), (1.0,))\n        if dtype == np.complex128:\n            atol = 2e-13\n        else:\n            atol = 0.0006\n        self.assertArraysAllClose(t_out, b.real, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (2, 3, 5), (5, 5, 5), (20, 20), (5, 10)], pnorm=[jnp.inf, -jnp.inf, 1, -1, 2, -2, 'fro'], dtype=float_types + complex_types)\n@jtu.skip_on_devices('gpu')\ndef testCond(self, shape, pnorm, dtype):\n\n    def gen_mat():\n        arr_gen = jtu.rand_default(self.rng())\n        res = arr_gen(shape, dtype)\n        return res\n\n    def args_gen(p):\n\n        def _args_gen():\n            return [gen_mat(), p]\n        return _args_gen\n    args_maker = args_gen(pnorm)\n    if pnorm not in [2, -2] and len(set(shape[-2:])) != 1:\n        with self.assertRaises(ValueError):\n            jnp.linalg.cond(*args_maker())\n    else:\n        self._CheckAgainstNumpy(np.linalg.cond, jnp.linalg.cond, args_maker, check_dtypes=False, tol=0.001)\n        partial_norm = partial(jnp.linalg.cond, p=pnorm)\n        self._CompileAndCheck(partial_norm, lambda: [gen_mat()], check_dtypes=False, rtol=0.001, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, hermitian=hermitian) for shape in [(1, 1), (4, 4), (3, 10, 10), (2, 70, 7), (2000, 7), (7, 1000), (70, 7, 2), (2, 0, 0), (3, 0, 2), (1, 0), (400000, 2), (2, 400000)] for hermitian in ([False, True] if shape[-1] == shape[-2] else [False])], dtype=float_types + complex_types)\ndef testPinv(self, shape, hermitian, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    jnp_fn = partial(jnp.linalg.pinv, hermitian=hermitian)\n\n    def np_fn(a):\n        if hermitian:\n            a = (a + T(a.conj())) / 2\n        return np.linalg.pinv(a, hermitian=hermitian)\n    self._CheckAgainstNumpy(np_fn, jnp_fn, args_maker, tol=0.0001)\n    self._CompileAndCheck(jnp_fn, args_maker, atol=1e-05)\n    jtu.check_grads(jnp_fn, args_maker(), 1, rtol=0.06, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(3,), (1, 2), (8, 5), (4, 4), (5, 5), (50, 50), (3, 4, 5), (2, 3, 4, 5)], dtype=float_types + complex_types)\ndef testMatrixRank(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    a, = args_maker()\n    self._CheckAgainstNumpy(np.linalg.matrix_rank, jnp.linalg.matrix_rank, args_maker, check_dtypes=False, tol=0.001)\n    self._CompileAndCheck(jnp.linalg.matrix_rank, args_maker, check_dtypes=False, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 5), (10, 5), (50, 50)], dtype=float_types + complex_types)\ndef testLu(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    x, = args_maker()\n    p, l, u = jsp.linalg.lu(x)\n    self.assertAllClose(x, np.matmul(p, np.matmul(l, u)), rtol={np.float32: 0.001, np.float64: 5e-12, np.complex64: 0.001, np.complex128: 1e-12}, atol={np.float32: 1e-05})\n    self._CompileAndCheck(jsp.linalg.lu, args_maker)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(n=[1, 4, 5, 200], dtype=float_types + complex_types)\ndef testLuFactor(self, n, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng((n, n), dtype)]\n    x, = args_maker()\n    lu, piv = jsp.linalg.lu_factor(x)\n    l = np.tril(lu, -1) + np.eye(n, dtype=dtype)\n    u = np.triu(lu)\n    for i in range(n):\n        x[[i, piv[i]],] = x[[piv[i], i],]\n    self.assertAllClose(x, np.matmul(l, u), rtol=0.001, atol=0.001)\n    self._CompileAndCheck(jsp.linalg.lu_factor, args_maker)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(3, 4), (3, 3), (4, 3)], dtype=float_types + complex_types, mode=['full', 'r', 'economic'], pivoting=[False, True])\n@jax.default_matmul_precision('float32')\ndef testScipyQrModes(self, shape, dtype, mode, pivoting):\n    if pivoting:\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('Pivoting is only supported on CPU and GPU.')\n    rng = jtu.rand_default(self.rng())\n    jsp_func = partial(jax.scipy.linalg.qr, mode=mode, pivoting=pivoting)\n    sp_func = partial(scipy.linalg.qr, mode=mode, pivoting=pivoting)\n    args_maker = lambda: [rng(shape, dtype)]\n    self._CheckAgainstNumpy(sp_func, jsp_func, args_maker, rtol=1e-05, atol=1e-05)\n    self._CompileAndCheck(jsp_func, args_maker)\n\n    def qr_and_mul(a):\n        q, r, *p = jsp_func(a)\n        inverted_pivots = jnp.argsort(p[0])\n        return (q @ r)[:, inverted_pivots]\n    m, n = shape\n    if pivoting and mode != 'r' and (m == n or (m > n and mode != 'full')):\n        for a in args_maker():\n            jtu.check_jvp(qr_and_mul, partial(jvp, qr_and_mul), (a,), atol=0.003)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (2, 4, 4), (0, 100, 100), (10, 10)], dtype=float_types + complex_types, calc_q=[False, True])\n@jtu.run_on_devices('cpu')\ndef testHessenberg(self, shape, dtype, calc_q):\n    rng = jtu.rand_default(self.rng())\n    jsp_func = partial(jax.scipy.linalg.hessenberg, calc_q=calc_q)\n    if calc_q:\n        sp_func = np.vectorize(partial(scipy.linalg.hessenberg, calc_q=True), otypes=(dtype, dtype), signature='(n,n)->(n,n),(n,n)')\n    else:\n        sp_func = np.vectorize(scipy.linalg.hessenberg, signature='(n,n)->(n,n)', otypes=(dtype,))\n    args_maker = lambda: [rng(shape, dtype)]\n    self._CheckAgainstNumpy(sp_func, jsp_func, args_maker, rtol=1e-05, atol=1e-05, check_dtypes=not calc_q)\n    self._CompileAndCheck(jsp_func, args_maker)\n    if len(shape) == 3:\n        args = args_maker()\n        self.assertAllClose(jax.vmap(jsp_func)(*args), jsp_func(*args))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (2, 2, 2), (4, 4), (10, 10), (2, 5, 5)], dtype=float_types + complex_types, lower=[False, True])\n@jtu.skip_on_devices('tpu', 'rocm')\ndef testTridiagonal(self, shape, dtype, lower):\n    rng = jtu.rand_default(self.rng())\n\n    def jax_func(a):\n        return lax.linalg.tridiagonal(a, lower=lower)\n    real_dtype = jnp.finfo(dtype).dtype\n\n    @partial(np.vectorize, otypes=(dtype, real_dtype, real_dtype, dtype), signature='(n,n)->(n,n),(n),(k),(k)')\n    def sp_func(a):\n        if dtype == np.float32:\n            c, d, e, tau, info = scipy.linalg.lapack.ssytrd(a, lower=lower)\n        elif dtype == np.float64:\n            c, d, e, tau, info = scipy.linalg.lapack.dsytrd(a, lower=lower)\n        elif dtype == np.complex64:\n            c, d, e, tau, info = scipy.linalg.lapack.chetrd(a, lower=lower)\n        elif dtype == np.complex128:\n            c, d, e, tau, info = scipy.linalg.lapack.zhetrd(a, lower=lower)\n        else:\n            assert False, dtype\n        assert info == 0\n        return (c, d, e, tau)\n    args_maker = lambda: [rng(shape, dtype)]\n    self._CheckAgainstNumpy(sp_func, jax_func, args_maker, rtol=0.0001, atol=0.0001, check_dtypes=False)\n    if len(shape) == 3:\n        args = args_maker()\n        self.assertAllClose(jax.vmap(jax_func)(*args), jax_func(*args))",
    "assertions": [
      "assert info == 0",
      "assert False, dtype"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(n=[0, 4, 5, 50], dtype=float_types + complex_types, lower=[True, False], sort_eigenvalues=[True, False])\ndef testEigh(self, n, dtype, lower, sort_eigenvalues):\n    rng = jtu.rand_default(self.rng())\n    tol = 0.001\n    args_maker = lambda: [rng((n, n), dtype)]\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    v, w = lax.linalg.eigh(np.tril(a) if lower else np.triu(a), lower=lower, symmetrize_input=False, sort_eigenvalues=sort_eigenvalues)\n    w = np.asarray(w)\n    v = np.asarray(v)\n    self.assertLessEqual(np.linalg.norm(np.eye(n) - np.matmul(np.conj(T(v)), v)), 0.001)\n    self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    w_expected, v_expected = np.linalg.eigh(np.asarray(a))\n    self.assertAllClose(w_expected, w if sort_eigenvalues else np.sort(w), rtol=0.0001, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (2, 3, 5), (5, 5, 5), (20, 20), (5, 10)], pnorm=[jnp.inf, -jnp.inf, 1, -1, 2, -2, 'fro'], dtype=float_types + complex_types)\n@jtu.skip_on_devices('gpu')\ndef testCond(self, shape, pnorm, dtype):\n\n    def gen_mat():\n        arr_gen = jtu.rand_default(self.rng())\n        res = arr_gen(shape, dtype)\n        return res\n\n    def args_gen(p):\n\n        def _args_gen():\n            return [gen_mat(), p]\n        return _args_gen\n    args_maker = args_gen(pnorm)\n    if pnorm not in [2, -2] and len(set(shape[-2:])) != 1:\n        with self.assertRaises(ValueError):\n            jnp.linalg.cond(*args_maker())\n    else:\n        self._CheckAgainstNumpy(np.linalg.cond, jnp.linalg.cond, args_maker, check_dtypes=False, tol=0.001)\n        partial_norm = partial(jnp.linalg.cond, p=pnorm)\n        self._CompileAndCheck(partial_norm, lambda: [gen_mat()], check_dtypes=False, rtol=0.001, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (2, 5, 5), (200, 200), (1000, 0, 0)], dtype=float_types + complex_types, upper=[True, False])\ndef testCholesky(self, shape, dtype, upper):\n    rng = jtu.rand_default(self.rng())\n\n    def args_maker():\n        factor_shape = shape[:-1] + (2 * shape[-1],)\n        a = rng(factor_shape, dtype)\n        return [np.matmul(a, jnp.conj(T(a)))]\n    jnp_fun = partial(jnp.linalg.cholesky, upper=upper)\n\n    def np_fun(x, upper=upper):\n        if jtu.numpy_version() >= (2, 0, 0):\n            return np.linalg.cholesky(x, upper=upper)\n        result = np.linalg.cholesky(x)\n        if upper:\n            axes = list(range(x.ndim))\n            axes[-1], axes[-2] = (axes[-2], axes[-1])\n            return np.transpose(result, axes).conj()\n        return result\n    self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, tol=0.001)\n    self._CompileAndCheck(jnp_fun, args_maker)\n    if jnp.finfo(dtype).bits == 64:\n        jtu.check_grads(jnp.linalg.cholesky, args_maker(), order=2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(m=[1, 5, 7, 23], nq=zip([2, 4, 6, 36], [(1, 2), (2, 2), (1, 2, 3), (3, 3, 1, 4)]), dtype=float_types)\ndef testTensorsolve(self, m, nq, dtype):\n    rng = jtu.rand_default(self.rng())\n    n, q = nq\n    b_shape = (n, m)\n    Q = q + (m,)\n    args_maker = lambda: [rng(b_shape + Q, dtype), rng(b_shape, dtype)]\n    a, b = args_maker()\n    result = jnp.linalg.tensorsolve(*args_maker())\n    self.assertEqual(result.shape, Q)\n    self._CheckAgainstNumpy(np.linalg.tensorsolve, jnp.linalg.tensorsolve, args_maker, tol={np.float32: 0.01, np.float64: 0.001})\n    self._CompileAndCheck(jnp.linalg.tensorsolve, args_maker, rtol={np.float64: 1e-13})",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(0, 0), (4, 4), (5, 5), (50, 50), (2, 6, 6)], dtype=float_types + complex_types, compute_left_eigenvectors=[False, True], compute_right_eigenvectors=[False, True])\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEig(self, shape, dtype, compute_left_eigenvectors, compute_right_eigenvectors):\n    rng = jtu.rand_default(self.rng())\n    n = shape[-1]\n    args_maker = lambda: [rng(shape, dtype)]\n\n    def norm(x):\n        norm = np.linalg.norm(x, axis=(-2, -1))\n        return norm / ((n + 1) * jnp.finfo(dtype).eps)\n\n    def check_right_eigenvectors(a, w, vr):\n        self.assertTrue(np.all(norm(np.matmul(a, vr) - w[..., None, :] * vr) < 100))\n\n    def check_left_eigenvectors(a, w, vl):\n        rank = len(a.shape)\n        aH = jnp.conj(a.transpose(list(range(rank - 2)) + [rank - 1, rank - 2]))\n        wC = jnp.conj(w)\n        check_right_eigenvectors(aH, wC, vl)\n    a, = args_maker()\n    results = lax.linalg.eig(a, compute_left_eigenvectors=compute_left_eigenvectors, compute_right_eigenvectors=compute_right_eigenvectors)\n    w = results[0]\n    if compute_left_eigenvectors:\n        check_left_eigenvectors(a, w, results[1])\n    if compute_right_eigenvectors:\n        check_right_eigenvectors(a, w, results[1 + compute_left_eigenvectors])\n    self._CompileAndCheck(partial(jnp.linalg.eig), args_maker, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(4, 4), (5, 5), (8, 8), (7, 6, 6)], dtype=float_types + complex_types)\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEigvalsGrad(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    a, = args_maker()\n    tol = 0.0001 if dtype in (np.float64, np.complex128) else 0.1\n    jtu.check_grads(lambda x: jnp.linalg.eigvals(x), (a,), order=1, modes=['fwd', 'rev'], rtol=tol, atol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(4, 4), (5, 5), (50, 50)], dtype=float_types + complex_types)\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEigvals(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    a, = args_maker()\n    w1, _ = jnp.linalg.eig(a)\n    w2 = jnp.linalg.eigvals(a)\n    self.assertAllClose(w1, w2, rtol={np.complex64: 1e-05, np.complex128: 2e-14})",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(n=[0, 4, 5, 50, 512], dtype=float_types + complex_types, lower=[True, False])\ndef testEigh(self, n, dtype, lower):\n    rng = jtu.rand_default(self.rng())\n    eps = np.finfo(dtype).eps\n    args_maker = lambda: [rng((n, n), dtype)]\n    uplo = 'L' if lower else 'U'\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    w, v = jnp.linalg.eigh(np.tril(a) if lower else np.triu(a), UPLO=uplo, symmetrize_input=False)\n    w = w.astype(v.dtype)\n    tol = 2 * n * eps\n    self.assertAllClose(np.eye(n, dtype=v.dtype), np.matmul(np.conj(T(v)), v), atol=tol, rtol=tol)\n    with jax.numpy_rank_promotion('allow'):\n        tol = 100 * eps\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    self._CompileAndCheck(partial(jnp.linalg.eigh, UPLO=uplo), args_maker, rtol=eps)\n    double_type = dtype\n    if dtype == np.float32:\n        double_type = np.float64\n    if dtype == np.complex64:\n        double_type = np.complex128\n    w_np = np.linalg.eigvalsh(a.astype(double_type))\n    tol = 8 * eps\n    self.assertAllClose(w_np.astype(w.dtype), w, atol=tol * np.linalg.norm(a), rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(start=[0, 1, 63, 64, 65, 255], end=[1, 63, 64, 65, 256])\n@jtu.run_on_devices('tpu')\ndef testEighSubsetByIndex(self, start, end):\n    if start >= end:\n        return\n    dtype = np.float32\n    n = 256\n    rng = jtu.rand_default(self.rng())\n    eps = np.finfo(dtype).eps\n    args_maker = lambda: [rng((n, n), dtype)]\n    subset_by_index = (start, end)\n    k = end - start\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    v, w = lax.linalg.eigh(a, symmetrize_input=False, subset_by_index=subset_by_index)\n    w = w.astype(v.dtype)\n    self.assertEqual(v.shape, (n, k))\n    self.assertEqual(w.shape, (k,))\n    with jax.numpy_rank_promotion('allow'):\n        tol = 200 * eps\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    tol = 3 * n * eps\n    self.assertAllClose(np.eye(k, dtype=v.dtype), np.matmul(np.conj(T(v)), v), atol=tol, rtol=tol)\n    self._CompileAndCheck(partial(jnp.linalg.eigh), args_maker, rtol=eps)\n    double_type = dtype\n    if dtype == np.float32:\n        double_type = np.float64\n    if dtype == np.complex64:\n        double_type = np.complex128\n    w_np = np.linalg.eigvalsh(a.astype(double_type))[subset_by_index[0]:subset_by_index[1]]\n    tol = 20 * eps\n    self.assertAllClose(w_np.astype(w.dtype), w, atol=tol * np.linalg.norm(a), rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product([dict(m=m, n=n, full_matrices=full_matrices, hermitian=hermitian) for (m, n), full_matrices in list(itertools.product(itertools.product([0, 2, 7, 29, 32, 53], repeat=2), [False, True])) + [((400000, 2), False), ((2, 400000), False)] for hermitian in ([False, True] if m == n else [False])], b=[(), (3,), (2, 3)], dtype=float_types + complex_types, compute_uv=[False, True], algorithm=[None, lax.linalg.SvdAlgorithm.QR, lax.linalg.SvdAlgorithm.JACOBI])\n@jax.default_matmul_precision('float32')\ndef testSVD(self, b, m, n, dtype, full_matrices, compute_uv, hermitian, algorithm):\n    if algorithm is not None:\n        if hermitian:\n            self.skipTest(\"Hermitian SVD doesn't support the algorithm parameter.\")\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('SVD algorithm selection only supported on CPU and GPU.')\n        if jtu.test_device_matches(['cpu']) and jtu.jaxlib_version() <= (0, 5, 1):\n            self.skipTest('SVD algorithm selection on CPU requires a newer jaxlib version.')\n        if jtu.test_device_matches(['cpu']) and algorithm == lax.linalg.SvdAlgorithm.JACOBI:\n            self.skipTest('Jacobi SVD not supported on GPU.')\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(b + (m, n), dtype)]\n\n    def compute_max_backward_error(operand, reconstructed_operand):\n        error_norm = np.linalg.norm(operand - reconstructed_operand, axis=(-2, -1))\n        backward_error = error_norm / np.linalg.norm(operand, axis=(-2, -1))\n        max_backward_error = np.amax(backward_error)\n        return max_backward_error\n    tol = 100 * jnp.finfo(dtype).eps\n    reconstruction_tol = 2 * tol\n    unitariness_tol = 3 * tol\n    a, = args_maker()\n    if hermitian:\n        a = a + np.conj(T(a))\n    if algorithm is None:\n        fun = partial(jnp.linalg.svd, hermitian=hermitian)\n    else:\n        fun = partial(lax.linalg.svd, algorithm=algorithm)\n    out = fun(a, full_matrices=full_matrices, compute_uv=compute_uv)\n    if compute_uv:\n        out = list(out)\n        out[1] = out[1].astype(out[0].dtype)\n        if m and n:\n            if full_matrices:\n                k = min(m, n)\n                if m < n:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2][..., :k, :]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n                else:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0][..., :, :k], out[2]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n            else:\n                max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2]))\n                self.assertLess(max_backward_error, reconstruction_tol)\n        unitary_mat = np.real(np.matmul(np.conj(T(out[0])), out[0]))\n        eye_slice = np.eye(out[0].shape[-1], dtype=unitary_mat.dtype)\n        self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        if m >= n:\n            unitary_mat = np.real(np.matmul(np.conj(T(out[2])), out[2]))\n            eye_slice = np.eye(out[2].shape[-1], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        else:\n            unitary_mat = np.real(np.matmul(out[2], np.conj(T(out[2]))))\n            eye_slice = np.eye(out[2].shape[-2], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n    else:\n        self.assertTrue(np.allclose(np.linalg.svd(a, compute_uv=False), np.asarray(out), atol=0.0001, rtol=0.0001))\n    self._CompileAndCheck(partial(fun, full_matrices=full_matrices, compute_uv=compute_uv), args_maker)\n    if not compute_uv and a.size < 100000:\n        svd = partial(fun, full_matrices=full_matrices, compute_uv=compute_uv)\n        if dtype == np.complex128:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.0001, atol=0.0001, eps=1e-08)\n        else:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.05, atol=0.2)\n    if compute_uv and (not full_matrices):\n        b, = args_maker()\n\n        def f(x):\n            u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n            vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n            return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real\n        _, t_out = jvp(f, (1.0,), (1.0,))\n        if dtype == np.complex128:\n            atol = 2e-13\n        else:\n            atol = 0.0006\n        self.assertArraysAllClose(t_out, b.real, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (2, 3, 5), (5, 5, 5), (20, 20), (5, 10)], pnorm=[jnp.inf, -jnp.inf, 1, -1, 2, -2, 'fro'], dtype=float_types + complex_types)\n@jtu.skip_on_devices('gpu')\ndef testCond(self, shape, pnorm, dtype):\n\n    def gen_mat():\n        arr_gen = jtu.rand_default(self.rng())\n        res = arr_gen(shape, dtype)\n        return res\n\n    def args_gen(p):\n\n        def _args_gen():\n            return [gen_mat(), p]\n        return _args_gen\n    args_maker = args_gen(pnorm)\n    if pnorm not in [2, -2] and len(set(shape[-2:])) != 1:\n        with self.assertRaises(ValueError):\n            jnp.linalg.cond(*args_maker())\n    else:\n        self._CheckAgainstNumpy(np.linalg.cond, jnp.linalg.cond, args_maker, check_dtypes=False, tol=0.001)\n        partial_norm = partial(jnp.linalg.cond, p=pnorm)\n        self._CompileAndCheck(partial_norm, lambda: [gen_mat()], check_dtypes=False, rtol=0.001, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, hermitian=hermitian) for shape in [(1, 1), (4, 4), (3, 10, 10), (2, 70, 7), (2000, 7), (7, 1000), (70, 7, 2), (2, 0, 0), (3, 0, 2), (1, 0), (400000, 2), (2, 400000)] for hermitian in ([False, True] if shape[-1] == shape[-2] else [False])], dtype=float_types + complex_types)\ndef testPinv(self, shape, hermitian, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    jnp_fn = partial(jnp.linalg.pinv, hermitian=hermitian)\n\n    def np_fn(a):\n        if hermitian:\n            a = (a + T(a.conj())) / 2\n        return np.linalg.pinv(a, hermitian=hermitian)\n    self._CheckAgainstNumpy(np_fn, jnp_fn, args_maker, tol=0.0001)\n    self._CompileAndCheck(jnp_fn, args_maker, atol=1e-05)\n    jtu.check_grads(jnp_fn, args_maker(), 1, rtol=0.06, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(3,), (1, 2), (8, 5), (4, 4), (5, 5), (50, 50), (3, 4, 5), (2, 3, 4, 5)], dtype=float_types + complex_types)\ndef testMatrixRank(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    a, = args_maker()\n    self._CheckAgainstNumpy(np.linalg.matrix_rank, jnp.linalg.matrix_rank, args_maker, check_dtypes=False, tol=0.001)\n    self._CompileAndCheck(jnp.linalg.matrix_rank, args_maker, check_dtypes=False, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 5), (10, 5), (50, 50)], dtype=float_types + complex_types)\ndef testLu(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    x, = args_maker()\n    p, l, u = jsp.linalg.lu(x)\n    self.assertAllClose(x, np.matmul(p, np.matmul(l, u)), rtol={np.float32: 0.001, np.float64: 5e-12, np.complex64: 0.001, np.complex128: 1e-12}, atol={np.float32: 1e-05})\n    self._CompileAndCheck(jsp.linalg.lu, args_maker)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(n=[1, 4, 5, 200], dtype=float_types + complex_types)\ndef testLuFactor(self, n, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng((n, n), dtype)]\n    x, = args_maker()\n    lu, piv = jsp.linalg.lu_factor(x)\n    l = np.tril(lu, -1) + np.eye(n, dtype=dtype)\n    u = np.triu(lu)\n    for i in range(n):\n        x[[i, piv[i]],] = x[[piv[i], i],]\n    self.assertAllClose(x, np.matmul(l, u), rtol=0.001, atol=0.001)\n    self._CompileAndCheck(jsp.linalg.lu_factor, args_maker)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(3, 4), (3, 3), (4, 3)], dtype=float_types + complex_types, mode=['full', 'r', 'economic'], pivoting=[False, True])\n@jax.default_matmul_precision('float32')\ndef testScipyQrModes(self, shape, dtype, mode, pivoting):\n    if pivoting:\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('Pivoting is only supported on CPU and GPU.')\n    rng = jtu.rand_default(self.rng())\n    jsp_func = partial(jax.scipy.linalg.qr, mode=mode, pivoting=pivoting)\n    sp_func = partial(scipy.linalg.qr, mode=mode, pivoting=pivoting)\n    args_maker = lambda: [rng(shape, dtype)]\n    self._CheckAgainstNumpy(sp_func, jsp_func, args_maker, rtol=1e-05, atol=1e-05)\n    self._CompileAndCheck(jsp_func, args_maker)\n\n    def qr_and_mul(a):\n        q, r, *p = jsp_func(a)\n        inverted_pivots = jnp.argsort(p[0])\n        return (q @ r)[:, inverted_pivots]\n    m, n = shape\n    if pivoting and mode != 'r' and (m == n or (m > n and mode != 'full')):\n        for a in args_maker():\n            jtu.check_jvp(qr_and_mul, partial(jvp, qr_and_mul), (a,), atol=0.003)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (2, 4, 4), (0, 100, 100), (10, 10)], dtype=float_types + complex_types, calc_q=[False, True])\n@jtu.run_on_devices('cpu')\ndef testHessenberg(self, shape, dtype, calc_q):\n    rng = jtu.rand_default(self.rng())\n    jsp_func = partial(jax.scipy.linalg.hessenberg, calc_q=calc_q)\n    if calc_q:\n        sp_func = np.vectorize(partial(scipy.linalg.hessenberg, calc_q=True), otypes=(dtype, dtype), signature='(n,n)->(n,n),(n,n)')\n    else:\n        sp_func = np.vectorize(scipy.linalg.hessenberg, signature='(n,n)->(n,n)', otypes=(dtype,))\n    args_maker = lambda: [rng(shape, dtype)]\n    self._CheckAgainstNumpy(sp_func, jsp_func, args_maker, rtol=1e-05, atol=1e-05, check_dtypes=not calc_q)\n    self._CompileAndCheck(jsp_func, args_maker)\n    if len(shape) == 3:\n        args = args_maker()\n        self.assertAllClose(jax.vmap(jsp_func)(*args), jsp_func(*args))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (2, 2, 2), (4, 4), (10, 10), (2, 5, 5)], dtype=float_types + complex_types, lower=[False, True])\n@jtu.skip_on_devices('tpu', 'rocm')\ndef testTridiagonal(self, shape, dtype, lower):\n    rng = jtu.rand_default(self.rng())\n\n    def jax_func(a):\n        return lax.linalg.tridiagonal(a, lower=lower)\n    real_dtype = jnp.finfo(dtype).dtype\n\n    @partial(np.vectorize, otypes=(dtype, real_dtype, real_dtype, dtype), signature='(n,n)->(n,n),(n),(k),(k)')\n    def sp_func(a):\n        if dtype == np.float32:\n            c, d, e, tau, info = scipy.linalg.lapack.ssytrd(a, lower=lower)\n        elif dtype == np.float64:\n            c, d, e, tau, info = scipy.linalg.lapack.dsytrd(a, lower=lower)\n        elif dtype == np.complex64:\n            c, d, e, tau, info = scipy.linalg.lapack.chetrd(a, lower=lower)\n        elif dtype == np.complex128:\n            c, d, e, tau, info = scipy.linalg.lapack.zhetrd(a, lower=lower)\n        else:\n            assert False, dtype\n        assert info == 0\n        return (c, d, e, tau)\n    args_maker = lambda: [rng(shape, dtype)]\n    self._CheckAgainstNumpy(sp_func, jax_func, args_maker, rtol=0.0001, atol=0.0001, check_dtypes=False)\n    if len(shape) == 3:\n        args = args_maker()\n        self.assertAllClose(jax.vmap(jax_func)(*args), jax_func(*args))",
    "assertions": [
      "assert info == 0",
      "assert False, dtype"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(n=[0, 4, 5, 50], dtype=float_types + complex_types, lower=[True, False], sort_eigenvalues=[True, False])\ndef testEigh(self, n, dtype, lower, sort_eigenvalues):\n    rng = jtu.rand_default(self.rng())\n    tol = 0.001\n    args_maker = lambda: [rng((n, n), dtype)]\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    v, w = lax.linalg.eigh(np.tril(a) if lower else np.triu(a), lower=lower, symmetrize_input=False, sort_eigenvalues=sort_eigenvalues)\n    w = np.asarray(w)\n    v = np.asarray(v)\n    self.assertLessEqual(np.linalg.norm(np.eye(n) - np.matmul(np.conj(T(v)), v)), 0.001)\n    self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    w_expected, v_expected = np.linalg.eigh(np.asarray(a))\n    self.assertAllClose(w_expected, w if sort_eigenvalues else np.sort(w), rtol=0.0001, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    \"\"\"Test the set of inputs np.geomspace is well-defined on.\"\"\"\n    start, stop = self._GetArgsMaker(rng, [start_shape, stop_shape], [dtype, dtype])()\n    start, stop = jnp.broadcast_arrays(start, stop)\n    if dtype in complex_dtypes:\n        return (start, stop)\n    start = start * jnp.sign(start) * jnp.sign(stop)\n    return (start, stop)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (2, 5, 5), (200, 200), (1000, 0, 0)], dtype=float_types + complex_types, upper=[True, False])\ndef testCholesky(self, shape, dtype, upper):\n    rng = jtu.rand_default(self.rng())\n\n    def args_maker():\n        factor_shape = shape[:-1] + (2 * shape[-1],)\n        a = rng(factor_shape, dtype)\n        return [np.matmul(a, jnp.conj(T(a)))]\n    jnp_fun = partial(jnp.linalg.cholesky, upper=upper)\n\n    def np_fun(x, upper=upper):\n        if jtu.numpy_version() >= (2, 0, 0):\n            return np.linalg.cholesky(x, upper=upper)\n        result = np.linalg.cholesky(x)\n        if upper:\n            axes = list(range(x.ndim))\n            axes[-1], axes[-2] = (axes[-2], axes[-1])\n            return np.transpose(result, axes).conj()\n        return result\n    self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, tol=0.001)\n    self._CompileAndCheck(jnp_fun, args_maker)\n    if jnp.finfo(dtype).bits == 64:\n        jtu.check_grads(jnp.linalg.cholesky, args_maker(), order=2)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(m=[1, 5, 7, 23], nq=zip([2, 4, 6, 36], [(1, 2), (2, 2), (1, 2, 3), (3, 3, 1, 4)]), dtype=float_types)\ndef testTensorsolve(self, m, nq, dtype):\n    rng = jtu.rand_default(self.rng())\n    n, q = nq\n    b_shape = (n, m)\n    Q = q + (m,)\n    args_maker = lambda: [rng(b_shape + Q, dtype), rng(b_shape, dtype)]\n    a, b = args_maker()\n    result = jnp.linalg.tensorsolve(*args_maker())\n    self.assertEqual(result.shape, Q)\n    self._CheckAgainstNumpy(np.linalg.tensorsolve, jnp.linalg.tensorsolve, args_maker, tol={np.float32: 0.01, np.float64: 0.001})\n    self._CompileAndCheck(jnp.linalg.tensorsolve, args_maker, rtol={np.float64: 1e-13})",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(0, 0), (4, 4), (5, 5), (50, 50), (2, 6, 6)], dtype=float_types + complex_types, compute_left_eigenvectors=[False, True], compute_right_eigenvectors=[False, True])\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEig(self, shape, dtype, compute_left_eigenvectors, compute_right_eigenvectors):\n    rng = jtu.rand_default(self.rng())\n    n = shape[-1]\n    args_maker = lambda: [rng(shape, dtype)]\n\n    def norm(x):\n        norm = np.linalg.norm(x, axis=(-2, -1))\n        return norm / ((n + 1) * jnp.finfo(dtype).eps)\n\n    def check_right_eigenvectors(a, w, vr):\n        self.assertTrue(np.all(norm(np.matmul(a, vr) - w[..., None, :] * vr) < 100))\n\n    def check_left_eigenvectors(a, w, vl):\n        rank = len(a.shape)\n        aH = jnp.conj(a.transpose(list(range(rank - 2)) + [rank - 1, rank - 2]))\n        wC = jnp.conj(w)\n        check_right_eigenvectors(aH, wC, vl)\n    a, = args_maker()\n    results = lax.linalg.eig(a, compute_left_eigenvectors=compute_left_eigenvectors, compute_right_eigenvectors=compute_right_eigenvectors)\n    w = results[0]\n    if compute_left_eigenvectors:\n        check_left_eigenvectors(a, w, results[1])\n    if compute_right_eigenvectors:\n        check_right_eigenvectors(a, w, results[1 + compute_left_eigenvectors])\n    self._CompileAndCheck(partial(jnp.linalg.eig), args_maker, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(4, 4), (5, 5), (8, 8), (7, 6, 6)], dtype=float_types + complex_types)\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEigvalsGrad(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    a, = args_maker()\n    tol = 0.0001 if dtype in (np.float64, np.complex128) else 0.1\n    jtu.check_grads(lambda x: jnp.linalg.eigvals(x), (a,), order=1, modes=['fwd', 'rev'], rtol=tol, atol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(4, 4), (5, 5), (50, 50)], dtype=float_types + complex_types)\n@jtu.run_on_devices('cpu', 'gpu')\ndef testEigvals(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    a, = args_maker()\n    w1, _ = jnp.linalg.eig(a)\n    w2 = jnp.linalg.eigvals(a)\n    self.assertAllClose(w1, w2, rtol={np.complex64: 1e-05, np.complex128: 2e-14})",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(n=[0, 4, 5, 50, 512], dtype=float_types + complex_types, lower=[True, False])\ndef testEigh(self, n, dtype, lower):\n    rng = jtu.rand_default(self.rng())\n    eps = np.finfo(dtype).eps\n    args_maker = lambda: [rng((n, n), dtype)]\n    uplo = 'L' if lower else 'U'\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    w, v = jnp.linalg.eigh(np.tril(a) if lower else np.triu(a), UPLO=uplo, symmetrize_input=False)\n    w = w.astype(v.dtype)\n    tol = 2 * n * eps\n    self.assertAllClose(np.eye(n, dtype=v.dtype), np.matmul(np.conj(T(v)), v), atol=tol, rtol=tol)\n    with jax.numpy_rank_promotion('allow'):\n        tol = 100 * eps\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    self._CompileAndCheck(partial(jnp.linalg.eigh, UPLO=uplo), args_maker, rtol=eps)\n    double_type = dtype\n    if dtype == np.float32:\n        double_type = np.float64\n    if dtype == np.complex64:\n        double_type = np.complex128\n    w_np = np.linalg.eigvalsh(a.astype(double_type))\n    tol = 8 * eps\n    self.assertAllClose(w_np.astype(w.dtype), w, atol=tol * np.linalg.norm(a), rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(start=[0, 1, 63, 64, 65, 255], end=[1, 63, 64, 65, 256])\n@jtu.run_on_devices('tpu')\ndef testEighSubsetByIndex(self, start, end):\n    if start >= end:\n        return\n    dtype = np.float32\n    n = 256\n    rng = jtu.rand_default(self.rng())\n    eps = np.finfo(dtype).eps\n    args_maker = lambda: [rng((n, n), dtype)]\n    subset_by_index = (start, end)\n    k = end - start\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    v, w = lax.linalg.eigh(a, symmetrize_input=False, subset_by_index=subset_by_index)\n    w = w.astype(v.dtype)\n    self.assertEqual(v.shape, (n, k))\n    self.assertEqual(w.shape, (k,))\n    with jax.numpy_rank_promotion('allow'):\n        tol = 200 * eps\n        self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    tol = 3 * n * eps\n    self.assertAllClose(np.eye(k, dtype=v.dtype), np.matmul(np.conj(T(v)), v), atol=tol, rtol=tol)\n    self._CompileAndCheck(partial(jnp.linalg.eigh), args_maker, rtol=eps)\n    double_type = dtype\n    if dtype == np.float32:\n        double_type = np.float64\n    if dtype == np.complex64:\n        double_type = np.complex128\n    w_np = np.linalg.eigvalsh(a.astype(double_type))[subset_by_index[0]:subset_by_index[1]]\n    tol = 20 * eps\n    self.assertAllClose(w_np.astype(w.dtype), w, atol=tol * np.linalg.norm(a), rtol=tol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product([dict(m=m, n=n, full_matrices=full_matrices, hermitian=hermitian) for (m, n), full_matrices in list(itertools.product(itertools.product([0, 2, 7, 29, 32, 53], repeat=2), [False, True])) + [((400000, 2), False), ((2, 400000), False)] for hermitian in ([False, True] if m == n else [False])], b=[(), (3,), (2, 3)], dtype=float_types + complex_types, compute_uv=[False, True], algorithm=[None, lax.linalg.SvdAlgorithm.QR, lax.linalg.SvdAlgorithm.JACOBI])\n@jax.default_matmul_precision('float32')\ndef testSVD(self, b, m, n, dtype, full_matrices, compute_uv, hermitian, algorithm):\n    if algorithm is not None:\n        if hermitian:\n            self.skipTest(\"Hermitian SVD doesn't support the algorithm parameter.\")\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('SVD algorithm selection only supported on CPU and GPU.')\n        if jtu.test_device_matches(['cpu']) and jtu.jaxlib_version() <= (0, 5, 1):\n            self.skipTest('SVD algorithm selection on CPU requires a newer jaxlib version.')\n        if jtu.test_device_matches(['cpu']) and algorithm == lax.linalg.SvdAlgorithm.JACOBI:\n            self.skipTest('Jacobi SVD not supported on GPU.')\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(b + (m, n), dtype)]\n\n    def compute_max_backward_error(operand, reconstructed_operand):\n        error_norm = np.linalg.norm(operand - reconstructed_operand, axis=(-2, -1))\n        backward_error = error_norm / np.linalg.norm(operand, axis=(-2, -1))\n        max_backward_error = np.amax(backward_error)\n        return max_backward_error\n    tol = 100 * jnp.finfo(dtype).eps\n    reconstruction_tol = 2 * tol\n    unitariness_tol = 3 * tol\n    a, = args_maker()\n    if hermitian:\n        a = a + np.conj(T(a))\n    if algorithm is None:\n        fun = partial(jnp.linalg.svd, hermitian=hermitian)\n    else:\n        fun = partial(lax.linalg.svd, algorithm=algorithm)\n    out = fun(a, full_matrices=full_matrices, compute_uv=compute_uv)\n    if compute_uv:\n        out = list(out)\n        out[1] = out[1].astype(out[0].dtype)\n        if m and n:\n            if full_matrices:\n                k = min(m, n)\n                if m < n:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2][..., :k, :]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n                else:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0][..., :, :k], out[2]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n            else:\n                max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2]))\n                self.assertLess(max_backward_error, reconstruction_tol)\n        unitary_mat = np.real(np.matmul(np.conj(T(out[0])), out[0]))\n        eye_slice = np.eye(out[0].shape[-1], dtype=unitary_mat.dtype)\n        self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        if m >= n:\n            unitary_mat = np.real(np.matmul(np.conj(T(out[2])), out[2]))\n            eye_slice = np.eye(out[2].shape[-1], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        else:\n            unitary_mat = np.real(np.matmul(out[2], np.conj(T(out[2]))))\n            eye_slice = np.eye(out[2].shape[-2], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n    else:\n        self.assertTrue(np.allclose(np.linalg.svd(a, compute_uv=False), np.asarray(out), atol=0.0001, rtol=0.0001))\n    self._CompileAndCheck(partial(fun, full_matrices=full_matrices, compute_uv=compute_uv), args_maker)\n    if not compute_uv and a.size < 100000:\n        svd = partial(fun, full_matrices=full_matrices, compute_uv=compute_uv)\n        if dtype == np.complex128:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.0001, atol=0.0001, eps=1e-08)\n        else:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.05, atol=0.2)\n    if compute_uv and (not full_matrices):\n        b, = args_maker()\n\n        def f(x):\n            u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n            vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n            return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real\n        _, t_out = jvp(f, (1.0,), (1.0,))\n        if dtype == np.complex128:\n            atol = 2e-13\n        else:\n            atol = 0.0006\n        self.assertArraysAllClose(t_out, b.real, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (2, 3, 5), (5, 5, 5), (20, 20), (5, 10)], pnorm=[jnp.inf, -jnp.inf, 1, -1, 2, -2, 'fro'], dtype=float_types + complex_types)\n@jtu.skip_on_devices('gpu')\ndef testCond(self, shape, pnorm, dtype):\n\n    def gen_mat():\n        arr_gen = jtu.rand_default(self.rng())\n        res = arr_gen(shape, dtype)\n        return res\n\n    def args_gen(p):\n\n        def _args_gen():\n            return [gen_mat(), p]\n        return _args_gen\n    args_maker = args_gen(pnorm)\n    if pnorm not in [2, -2] and len(set(shape[-2:])) != 1:\n        with self.assertRaises(ValueError):\n            jnp.linalg.cond(*args_maker())\n    else:\n        self._CheckAgainstNumpy(np.linalg.cond, jnp.linalg.cond, args_maker, check_dtypes=False, tol=0.001)\n        partial_norm = partial(jnp.linalg.cond, p=pnorm)\n        self._CompileAndCheck(partial_norm, lambda: [gen_mat()], check_dtypes=False, rtol=0.001, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product([dict(shape=shape, hermitian=hermitian) for shape in [(1, 1), (4, 4), (3, 10, 10), (2, 70, 7), (2000, 7), (7, 1000), (70, 7, 2), (2, 0, 0), (3, 0, 2), (1, 0), (400000, 2), (2, 400000)] for hermitian in ([False, True] if shape[-1] == shape[-2] else [False])], dtype=float_types + complex_types)\ndef testPinv(self, shape, hermitian, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    jnp_fn = partial(jnp.linalg.pinv, hermitian=hermitian)\n\n    def np_fn(a):\n        if hermitian:\n            a = (a + T(a.conj())) / 2\n        return np.linalg.pinv(a, hermitian=hermitian)\n    self._CheckAgainstNumpy(np_fn, jnp_fn, args_maker, tol=0.0001)\n    self._CompileAndCheck(jnp_fn, args_maker, atol=1e-05)\n    jtu.check_grads(jnp_fn, args_maker(), 1, rtol=0.06, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(3,), (1, 2), (8, 5), (4, 4), (5, 5), (50, 50), (3, 4, 5), (2, 3, 4, 5)], dtype=float_types + complex_types)\ndef testMatrixRank(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    a, = args_maker()\n    self._CheckAgainstNumpy(np.linalg.matrix_rank, jnp.linalg.matrix_rank, args_maker, check_dtypes=False, tol=0.001)\n    self._CompileAndCheck(jnp.linalg.matrix_rank, args_maker, check_dtypes=False, rtol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 5), (10, 5), (50, 50)], dtype=float_types + complex_types)\ndef testLu(self, shape, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(shape, dtype)]\n    x, = args_maker()\n    p, l, u = jsp.linalg.lu(x)\n    self.assertAllClose(x, np.matmul(p, np.matmul(l, u)), rtol={np.float32: 0.001, np.float64: 5e-12, np.complex64: 0.001, np.complex128: 1e-12}, atol={np.float32: 1e-05})\n    self._CompileAndCheck(jsp.linalg.lu, args_maker)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(n=[1, 4, 5, 200], dtype=float_types + complex_types)\ndef testLuFactor(self, n, dtype):\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng((n, n), dtype)]\n    x, = args_maker()\n    lu, piv = jsp.linalg.lu_factor(x)\n    l = np.tril(lu, -1) + np.eye(n, dtype=dtype)\n    u = np.triu(lu)\n    for i in range(n):\n        x[[i, piv[i]],] = x[[piv[i], i],]\n    self.assertAllClose(x, np.matmul(l, u), rtol=0.001, atol=0.001)\n    self._CompileAndCheck(jsp.linalg.lu_factor, args_maker)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(3, 4), (3, 3), (4, 3)], dtype=float_types + complex_types, mode=['full', 'r', 'economic'], pivoting=[False, True])\n@jax.default_matmul_precision('float32')\ndef testScipyQrModes(self, shape, dtype, mode, pivoting):\n    if pivoting:\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('Pivoting is only supported on CPU and GPU.')\n    rng = jtu.rand_default(self.rng())\n    jsp_func = partial(jax.scipy.linalg.qr, mode=mode, pivoting=pivoting)\n    sp_func = partial(scipy.linalg.qr, mode=mode, pivoting=pivoting)\n    args_maker = lambda: [rng(shape, dtype)]\n    self._CheckAgainstNumpy(sp_func, jsp_func, args_maker, rtol=1e-05, atol=1e-05)\n    self._CompileAndCheck(jsp_func, args_maker)\n\n    def qr_and_mul(a):\n        q, r, *p = jsp_func(a)\n        inverted_pivots = jnp.argsort(p[0])\n        return (q @ r)[:, inverted_pivots]\n    m, n = shape\n    if pivoting and mode != 'r' and (m == n or (m > n and mode != 'full')):\n        for a in args_maker():\n            jtu.check_jvp(qr_and_mul, partial(jvp, qr_and_mul), (a,), atol=0.003)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (2, 4, 4), (0, 100, 100), (10, 10)], dtype=float_types + complex_types, calc_q=[False, True])\n@jtu.run_on_devices('cpu')\ndef testHessenberg(self, shape, dtype, calc_q):\n    rng = jtu.rand_default(self.rng())\n    jsp_func = partial(jax.scipy.linalg.hessenberg, calc_q=calc_q)\n    if calc_q:\n        sp_func = np.vectorize(partial(scipy.linalg.hessenberg, calc_q=True), otypes=(dtype, dtype), signature='(n,n)->(n,n),(n,n)')\n    else:\n        sp_func = np.vectorize(scipy.linalg.hessenberg, signature='(n,n)->(n,n)', otypes=(dtype,))\n    args_maker = lambda: [rng(shape, dtype)]\n    self._CheckAgainstNumpy(sp_func, jsp_func, args_maker, rtol=1e-05, atol=1e-05, check_dtypes=not calc_q)\n    self._CompileAndCheck(jsp_func, args_maker)\n    if len(shape) == 3:\n        args = args_maker()\n        self.assertAllClose(jax.vmap(jsp_func)(*args), jsp_func(*args))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (2, 2, 2), (4, 4), (10, 10), (2, 5, 5)], dtype=float_types + complex_types, lower=[False, True])\n@jtu.skip_on_devices('tpu', 'rocm')\ndef testTridiagonal(self, shape, dtype, lower):\n    rng = jtu.rand_default(self.rng())\n\n    def jax_func(a):\n        return lax.linalg.tridiagonal(a, lower=lower)\n    real_dtype = jnp.finfo(dtype).dtype\n\n    @partial(np.vectorize, otypes=(dtype, real_dtype, real_dtype, dtype), signature='(n,n)->(n,n),(n),(k),(k)')\n    def sp_func(a):\n        if dtype == np.float32:\n            c, d, e, tau, info = scipy.linalg.lapack.ssytrd(a, lower=lower)\n        elif dtype == np.float64:\n            c, d, e, tau, info = scipy.linalg.lapack.dsytrd(a, lower=lower)\n        elif dtype == np.complex64:\n            c, d, e, tau, info = scipy.linalg.lapack.chetrd(a, lower=lower)\n        elif dtype == np.complex128:\n            c, d, e, tau, info = scipy.linalg.lapack.zhetrd(a, lower=lower)\n        else:\n            assert False, dtype\n        assert info == 0\n        return (c, d, e, tau)\n    args_maker = lambda: [rng(shape, dtype)]\n    self._CheckAgainstNumpy(sp_func, jax_func, args_maker, rtol=0.0001, atol=0.0001, check_dtypes=False)\n    if len(shape) == 3:\n        args = args_maker()\n        self.assertAllClose(jax.vmap(jax_func)(*args), jax_func(*args))",
    "assertions": [
      "assert info == 0",
      "assert False, dtype"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(n=[0, 4, 5, 50], dtype=float_types + complex_types, lower=[True, False], sort_eigenvalues=[True, False])\ndef testEigh(self, n, dtype, lower, sort_eigenvalues):\n    rng = jtu.rand_default(self.rng())\n    tol = 0.001\n    args_maker = lambda: [rng((n, n), dtype)]\n    a, = args_maker()\n    a = (a + np.conj(a.T)) / 2\n    v, w = lax.linalg.eigh(np.tril(a) if lower else np.triu(a), lower=lower, symmetrize_input=False, sort_eigenvalues=sort_eigenvalues)\n    w = np.asarray(w)\n    v = np.asarray(v)\n    self.assertLessEqual(np.linalg.norm(np.eye(n) - np.matmul(np.conj(T(v)), v)), 0.001)\n    self.assertLessEqual(np.linalg.norm(np.matmul(a, v) - w * v), tol * np.linalg.norm(a))\n    w_expected, v_expected = np.linalg.eigh(np.asarray(a))\n    self.assertAllClose(w_expected, w if sort_eigenvalues else np.sort(w), rtol=0.0001, atol=0.0001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def args_maker():\n    x = jnp.array(rng(shape, dtype))\n    if out_dtype in unsigned_dtypes:\n        x = 10 * jnp.abs(x)\n    return [x]"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@jtu.sample_product([dict(m=m, n=n, full_matrices=full_matrices, hermitian=hermitian) for (m, n), full_matrices in list(itertools.product(itertools.product([0, 2, 7, 29, 32, 53], repeat=2), [False, True])) + [((400000, 2), False), ((2, 400000), False)] for hermitian in ([False, True] if m == n else [False])], b=[(), (3,), (2, 3)], dtype=float_types + complex_types, compute_uv=[False, True], algorithm=[None, lax.linalg.SvdAlgorithm.QR, lax.linalg.SvdAlgorithm.JACOBI])\n@jax.default_matmul_precision('float32')\ndef testSVD(self, b, m, n, dtype, full_matrices, compute_uv, hermitian, algorithm):\n    if algorithm is not None:\n        if hermitian:\n            self.skipTest(\"Hermitian SVD doesn't support the algorithm parameter.\")\n        if not jtu.test_device_matches(['cpu', 'gpu']):\n            self.skipTest('SVD algorithm selection only supported on CPU and GPU.')\n        if jtu.test_device_matches(['cpu']) and jtu.jaxlib_version() <= (0, 5, 1):\n            self.skipTest('SVD algorithm selection on CPU requires a newer jaxlib version.')\n        if jtu.test_device_matches(['cpu']) and algorithm == lax.linalg.SvdAlgorithm.JACOBI:\n            self.skipTest('Jacobi SVD not supported on GPU.')\n    rng = jtu.rand_default(self.rng())\n    args_maker = lambda: [rng(b + (m, n), dtype)]\n\n    def compute_max_backward_error(operand, reconstructed_operand):\n        error_norm = np.linalg.norm(operand - reconstructed_operand, axis=(-2, -1))\n        backward_error = error_norm / np.linalg.norm(operand, axis=(-2, -1))\n        max_backward_error = np.amax(backward_error)\n        return max_backward_error\n    tol = 100 * jnp.finfo(dtype).eps\n    reconstruction_tol = 2 * tol\n    unitariness_tol = 3 * tol\n    a, = args_maker()\n    if hermitian:\n        a = a + np.conj(T(a))\n    if algorithm is None:\n        fun = partial(jnp.linalg.svd, hermitian=hermitian)\n    else:\n        fun = partial(lax.linalg.svd, algorithm=algorithm)\n    out = fun(a, full_matrices=full_matrices, compute_uv=compute_uv)\n    if compute_uv:\n        out = list(out)\n        out[1] = out[1].astype(out[0].dtype)\n        if m and n:\n            if full_matrices:\n                k = min(m, n)\n                if m < n:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2][..., :k, :]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n                else:\n                    max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0][..., :, :k], out[2]))\n                    self.assertLess(max_backward_error, reconstruction_tol)\n            else:\n                max_backward_error = compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2]))\n                self.assertLess(max_backward_error, reconstruction_tol)\n        unitary_mat = np.real(np.matmul(np.conj(T(out[0])), out[0]))\n        eye_slice = np.eye(out[0].shape[-1], dtype=unitary_mat.dtype)\n        self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        if m >= n:\n            unitary_mat = np.real(np.matmul(np.conj(T(out[2])), out[2]))\n            eye_slice = np.eye(out[2].shape[-1], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n        else:\n            unitary_mat = np.real(np.matmul(out[2], np.conj(T(out[2]))))\n            eye_slice = np.eye(out[2].shape[-2], dtype=unitary_mat.dtype)\n            self.assertAllClose(np.broadcast_to(eye_slice, b + eye_slice.shape), unitary_mat, rtol=unitariness_tol, atol=unitariness_tol)\n    else:\n        self.assertTrue(np.allclose(np.linalg.svd(a, compute_uv=False), np.asarray(out), atol=0.0001, rtol=0.0001))\n    self._CompileAndCheck(partial(fun, full_matrices=full_matrices, compute_uv=compute_uv), args_maker)\n    if not compute_uv and a.size < 100000:\n        svd = partial(fun, full_matrices=full_matrices, compute_uv=compute_uv)\n        if dtype == np.complex128:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.0001, atol=0.0001, eps=1e-08)\n        else:\n            jtu.check_jvp(svd, partial(jvp, svd), (a,), rtol=0.05, atol=0.2)\n    if compute_uv and (not full_matrices):\n        b, = args_maker()\n\n        def f(x):\n            u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n            vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n            return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real\n        _, t_out = jvp(f, (1.0,), (1.0,))\n        if dtype == np.complex128:\n            atol = 2e-13\n        else:\n            atol = 0.0006\n        self.assertArraysAllClose(t_out, b.real, atol=atol)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def fun(x):\n    self.assertEqual(x.dtype, jnp.complex64)\n    out_type = (jax.ShapeDtypeStruct(x.shape[:-1], x.dtype), jax.ShapeDtypeStruct(x.shape, x.dtype))\n    return jax.pure_callback(callback, out_type, x)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (2, 3, 5), (5, 5, 5), (20, 20), (5, 10)], pnorm=[jnp.inf, -jnp.inf, 1, -1, 2, -2, 'fro'], dtype=float_types + complex_types)\n@jtu.skip_on_devices('gpu')\ndef testCond(self, shape, pnorm, dtype):\n\n    def gen_mat():\n        arr_gen = jtu.rand_default(self.rng())\n        res = arr_gen(shape, dtype)\n        return res\n\n    def args_gen(p):\n\n        def _args_gen():\n            return [gen_mat(), p]\n        return _args_gen\n    args_maker = args_gen(pnorm)\n    if pnorm not in [2, -2] and len(set(shape[-2:])) != 1:\n        with self.assertRaises(ValueError):\n            jnp.linalg.cond(*args_maker())\n    else:\n        self._CheckAgainstNumpy(np.linalg.cond, jnp.linalg.cond, args_maker, check_dtypes=False, tol=0.001)\n        partial_norm = partial(jnp.linalg.cond, p=pnorm)\n        self._CompileAndCheck(partial_norm, lambda: [gen_mat()], check_dtypes=False, rtol=0.001, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def cond(x):\n    return jax.pure_callback(_cond_callback, jax.ShapeDtypeStruct((), np.bool_), x)"
  },
  {
    "test_code": "@jtu.sample_product(shape=[(1, 1), (4, 4), (5, 5), (50, 50)], dtype=complex_types, lower=[True, False], eps=[1e-05])\ndef testEighGradVectorComplex(self, shape, dtype, lower, eps):\n    rng = jtu.rand_default(self.rng())\n    uplo = 'L' if lower else 'U'\n    a = rng(shape, dtype)\n    a = (a + np.conj(a.T)) / 2\n    a = np.tril(a) if lower else np.triu(a)\n    a_dot = eps * rng(shape, dtype)\n    a_dot = (a_dot + np.conj(a_dot.T)) / 2\n    a_dot = np.tril(a_dot) if lower else np.triu(a_dot)\n    f = partial(jnp.linalg.eigh, UPLO=uplo)\n    (w, v), (dw, dv) = jvp(f, primals=(a,), tangents=(a_dot,))\n    self.assertTrue(jnp.issubdtype(w.dtype, jnp.floating))\n    self.assertTrue(jnp.issubdtype(dw.dtype, jnp.floating))\n    new_a = a + a_dot\n    new_w, new_v = f(new_a)\n    new_a = (new_a + np.conj(new_a.T)) / 2\n    new_w = new_w.astype(new_a.dtype)\n    RTOL = 0.01\n    with jax.numpy_rank_promotion('allow'):\n        assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL\n        assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL",
    "assertions": [
      "assert np.max(np.abs((np.diag(np.dot(np.conj((v + dv).T), np.dot(new_a, v + dv))) - new_w) / new_w)) < RTOL",
      "assert np.max(np.linalg.norm(np.abs(new_w * (v + dv) - np.dot(new_a, v + dv)), axis=0) / np.linalg.norm(np.abs(new_w * (v + dv)), axis=0)) < RTOL"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/linalg_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  }
]