[
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(x):\n    x, = promote_dtypes_complex(x)\n    return jnp.fft.irfft(jnp.concatenate([jnp.zeros_like(x, shape=1), x[:2] + 1j * x[2:]]))"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(x):\n    x, = promote_dtypes_complex(x)\n    return jnp.fft.irfft(jnp.concatenate([jnp.zeros_like(x, shape=1), x[:2] + 1j * x[2:]]))"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(x):\n    x, = promote_dtypes_complex(x)\n    return jnp.fft.irfft(jnp.concatenate([jnp.zeros_like(x, shape=1), x[:2] + 1j * x[2:]]))"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_int_argument(self, with_function=False):\n    state = dict(float_used=np.array([0.7, 0.9], dtype=np.float32), float_passthrough=np.float16(1.0), float_unused=np.array([1.1, 2.2, 3.3], dtype=np.float32), int_used=np.int16(5), int_passthrough=np.int8(7), int_unused=np.array([1, 2, 3], dtype=np.uint32), bool_used=np.array([True, False, False, True], dtype=np.bool_), bool_passthrough=np.array([True, False, False, True, False], dtype=np.bool_), bool_unused=np.array([[True, False], [False, True]], dtype=np.bool_))\n\n    def jax_f(state):\n        res = dict(state, float_used=2.0 * state['float_used'], int_used=3 * state['int_used'], bool_used=state['bool_used'] == state['bool_used'])\n        del res['float_unused']\n        del res['int_unused']\n        del res['bool_unused']\n        return res\n    args = (state,)\n    res_jax = jax_f(*args)\n    vjp_jax_fun, args_vjp = tf_test_util.TransformJaxVJP(jax_f, args, res_jax)\n    grad_jax, = vjp_jax_fun(*args_vjp)\n\n    def compare_with_overrides(*, what, expected, **expected_overrides):\n        what_keys = set(what.keys())\n        expected_keys = set(expected.keys())\n        self.assertEqual(what_keys, expected_keys)\n        for k, w in what.items():\n            e = expected[k]\n            if k in expected_overrides:\n                if expected_overrides[k] == 'ZERO':\n                    e = np.zeros_like(w)\n                elif expected_overrides[k] == 'ZERO_BOOL':\n                    e = np.zeros(np.shape(w), dtype=np.bool_)\n                elif expected_overrides[k] == 'ONE':\n                    e = np.ones_like(w)\n                else:\n                    e = expected_overrides[k]\n            if e is None:\n                self.assertIsNone(w, msg=k)\n            else:\n                self.assertIsNotNone(w, msg=k)\n            w = w.numpy() if isinstance(w, tf.Tensor) else e\n            e = e.numpy() if isinstance(e, tf.Tensor) else e\n            try:\n                self.assertAllClose(e, w, err_msg=k)\n            except:\n                print(f'Failed at {k}')\n                raise\n    _, (grad_tf_0,) = tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    compare_with_overrides(what=grad_tf_0, expected=grad_jax, float_unused='ZERO', bool_used='ZERO', bool_passthrough='ONE', bool_unused='ZERO', int_used='ZERO', int_passthrough='ONE', int_unused='ZERO')\n    _, (grad_tf_None,) = tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.NONE)\n    compare_with_overrides(what=grad_tf_None, expected=grad_tf_0, float_unused=None, int_used=None, int_unused=None, bool_used=None, bool_unused=None)\n    f_tf_jax = jax2tf.convert(jax_f)\n    if with_function:\n        f_tf_jax = tf.function(f_tf_jax, autograph=False)\n    _, (grad_tf_jax_0,) = tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args)\n    compare_with_overrides(what=grad_tf_jax_0, expected=grad_tf_0, int_passthrough='ZERO', bool_passthrough='ZERO')\n    _, (grad_tf_jax_None,) = tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args, unconnected_gradients=tf.UnconnectedGradients.NONE)\n    compare_with_overrides(what=grad_tf_jax_None, expected=grad_tf_0, int_used=None, int_passthrough=None, int_unused=None, bool_unused=None, bool_used=None, bool_passthrough=None)\n    tf_vjp_jax_fun = jax2tf.convert(vjp_jax_fun)\n    grad_tf_vjp_jax, = tf_vjp_jax_fun(*args_vjp)\n    compare_with_overrides(what=grad_tf_vjp_jax, expected=grad_tf_0, bool_passthrough='ZERO_BOOL', bool_unused='ZERO_BOOL', bool_used='ZERO_BOOL', int_passthrough='ZERO_BOOL', int_unused='ZERO_BOOL', int_used='ZERO_BOOL')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_shared_constants_under_scan(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    xs = np.ones((8, const_size), dtype=np.float32)\n\n    def f1(xs):\n        res, _ = lax.scan(lambda carry, x: (carry + x + const, None), jnp.zeros((const_size,), dtype=np.float32), xs)\n        return res\n\n    def f2(xs):\n        return f1(xs) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), xs, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), xs, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_kwargs(self, with_function=False):\n\n    def f_jax(*, x):\n        return jnp.sum(x)\n    f_tf = jax2tf.convert(f_jax)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(f_tf(x=np.zeros(3, dtype=np.float32)), np.zeros((), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_grad_kwargs(self, with_function=False):\n    x = (np.zeros(3, dtype=np.float32), np.zeros(4, dtype=np.float32))\n\n    def f_jax(*, x=(1.0, 2.0)):\n        return jnp.sum(x[0]) + 2.0 * jnp.sum(x[1])\n    f_tf = jax2tf.convert(f_jax)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    xv = tf.nest.map_structure(tf.Variable, x)\n    with tf.GradientTape() as tape:\n        res = f_tf(x=xv)\n    grad_tf = tape.gradient(res, xv)\n    self.assertAllClose((np.full_like(x[0], fill_value=1.0), np.full_like(x[1], fill_value=2.0)), (grad_tf[0].numpy(), grad_tf[1].numpy()))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_device_array_arg(self):\n    self.ConvertAndCompare(jnp.sin, jnp.zeros((2, 3), jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_name_scope_polymorphic(self):\n    if config.jax2tf_default_native_serialization.value and (not config.dynamic_shapes.value):\n        self.skipTest('shape polymorphism but --jax_dynamic_shapes is not set.')\n\n    def func_jax(x, y):\n        return jnp.sin(x) + jnp.cos(y)\n    func_tf = jax2tf.convert(func_jax, polymorphic_shapes='(b,...)', with_gradient=True)\n    outer_scope = 'output_a'\n    g = tf.Graph()\n    with g.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = func_tf(x, y)\n    self.assertAllOperationStartWith(g, outer_scope)\n    g2 = tf.Graph()\n    with g2.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = tf.function(func_tf, jit_compile=True, autograph=False)(x, y)\n    self.assertAllOperationStartWith(g2, outer_scope)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_name_scope_while_loop(self):\n\n    def f(x):\n        with tf.name_scope('outer_scope'):\n\n            def condition(x):\n                return jnp.sum(x, keepdims=False) < 100\n\n            def body(x):\n                return jnp.add(x, 2.0)\n            result = jax.lax.while_loop(condition, body, x)\n            return result\n    tf_f = tf.function(jax2tf.convert(f), jit_compile=True, autograph=False)\n    g = tf_f.get_concrete_function(tf.zeros((1, 3))).graph\n    for func in g._functions.values():\n        for op in func.graph.get_operations():\n            if op.name.count(f'outer_scope/jax2tf_{f.__name__}_/while') > 1:\n                self.fail(f'tf graph has repeated name issue on when converting lax.while to tf.while.See op.name = : {op.name}')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef fn(n, x):\n    return lax.broadcast_in_dim(x, (n,), ())"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(size):\n    lhs_one_d = jnp.arange(size, dtype='int32') + 1\n    lhs_two_d = jax.lax.broadcast_in_dim(lhs_one_d, (size, 2), (0,))\n    rhs = jax.lax.broadcasted_iota('int32', (2, 4), 0) + 1\n    return jnp.dot(lhs_two_d, rhs)"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(size):\n    lhs_one_d = jnp.arange(size, dtype='int32') + 1\n    lhs_two_d = jax.lax.broadcast_in_dim(lhs_one_d, (size, 2), (0,))\n    rhs = jax.lax.broadcasted_iota('int32', (2, 4), 0) + 1\n    return jnp.dot(lhs_two_d, rhs)"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(size):\n    lhs_one_d = jnp.arange(size, dtype='int32') + 1\n    lhs_two_d = jax.lax.broadcast_in_dim(lhs_one_d, (size, 2), (0,))\n    rhs = jax.lax.broadcasted_iota('int32', (2, 4), 0) + 1\n    return jnp.dot(lhs_two_d, rhs)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g():\n    return jnp.zeros(n) + x"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(x):\n    return jax.random.uniform(x, (2, 4), dtype=np.float32)"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(x):\n    return jax.random.uniform(x, (2, 4), dtype=np.float32)"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(x):\n    return jax.random.uniform(x, (2, 4), dtype=np.float32)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "def test_op_metadata_while_and_cond(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_while_cond(x):\n\n        def body_fun(i_acc):\n            i, acc = i_acc\n            return (i + 1, jnp.cos(acc) + lax.cond(jnp.mod(i, 2) == 0, lambda acc: jnp.sin(acc), lambda acc: acc, acc))\n        _, acc = lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))\n        return acc\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_while_cond, x, [tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 5, op_name='jax2tf(f_while_cond)/while/body/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 7, op_name='jax2tf(f_while_cond)/while/body/branch_1_fun/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='FloorMod', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_while_cond)/while/body/rem', op_type='rem')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "def test_name_scope_cond(self):\n\n    def f(x):\n\n        def f_pos(x):\n            with jax.named_scope('jax_f_pos'):\n                return lax.cond(x < 1.0, jnp.cos, jnp.sin, x)\n        with jax.named_scope('jax_f_outer'):\n            return lax.cond(x > 0.0, f_pos, lambda x: x, x)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_forward():\n        with tf.name_scope('tf_outer_forward'):\n            x = 0.5\n            f_tf = jax2tf.convert(f)\n            _ = f_tf(x)\n    g = outer_forward.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_forward')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_forward/jax2tf_f_/jax_f_outer')\n    x = tf.Variable(0.5, name='tf_outer_back/x')\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_back():\n        with tf.name_scope('tf_outer_back'):\n            f_tf = jax2tf.convert(f)\n            with tf.GradientTape() as tape:\n                res_tf = f_tf(x)\n                _ = tape.gradient(res_tf, x)\n    g = outer_back.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_back')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_back')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(state):\n    it, _, fx, _ = state\n    return (jnp.max(jnp.abs(fx)) > tol) & (it < max_it)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g():\n    return jax.lax.cond(True, lambda: data[0], lambda: data[1])"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.vmap\ndef fn(a, b):\n    m1, v1 = a\n    m2, v2 = b\n    return (m1 + m2, jsp.linalg.solve(m1, v2) + jsp.linalg.solve(m2, v1))"
  },
  {
    "test_code": "def test_shared_constants_under_scan(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    xs = np.ones((8, const_size), dtype=np.float32)\n\n    def f1(xs):\n        res, _ = lax.scan(lambda carry, x: (carry + x + const, None), jnp.zeros((const_size,), dtype=np.float32), xs)\n        return res\n\n    def f2(xs):\n        return f1(xs) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), xs, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), xs, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def scan(y):\n\n    def body(carry, x):\n        return (carry, jnp.dot(x, x))\n    return jax.lax.scan(body, 1.0, y, unroll=False)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g():\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g(a, b):\n    c = jnp.zeros_like(a)\n    _, b, c, _ = for_impl(5, body2, (a, b, c, 0))\n    return (b, c)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(xs):\n    return jnp.array(list(xs))"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(xs):\n    return jnp.array(list(xs))"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(xs):\n    return jnp.array(list(xs))"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g():\n    _, _, attr_tangents = attrs.jvp(f, (), (), [(thing, 'x', 1.0)])\n    (thing_, attr_, tangent_), = attr_tangents\n    self.assertIs(thing, thing_)\n    self.assertEqual(attr_, 'x')\n    return (jax_getattr(thing, 'x'), tangent_)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_disabled(self, with_function=False):\n    if tf.version.VERSION.split('.') <= ['2', '17', '0']:\n        self.skipTest('This test works only with newer versions of TF')\n    f_tf = jax2tf.convert(jnp.tan, with_gradient=False)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    x = tf.ones([])\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            y = f_tf(x)\n            _ = tape.gradient(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "def test_dot_algorithm(self):\n    if tf.version.VERSION.split('.') <= ['2', '18', '0']:\n        self.skipTest('Because of an XLA bug this test segfaults with TF v2.18.0')\n    if jtu.test_device_matches(['tpu']):\n        algorithm = 'BF16_BF16_F32'\n    else:\n        algorithm = 'F32_F32_F32'\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision=algorithm)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    f_tf(np.ones((128, 128), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def split(self) -> jax.Array:\n    key = jax_getattr(self, 'key')\n    new_key, returned_key = jax.random.split(key)\n    jax_setattr(self, 'key', new_key)\n    return returned_key"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@pjit\ndef g(y):\n    return jnp.sin(y)"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "def test_op_metadata_while_and_cond(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_while_cond(x):\n\n        def body_fun(i_acc):\n            i, acc = i_acc\n            return (i + 1, jnp.cos(acc) + lax.cond(jnp.mod(i, 2) == 0, lambda acc: jnp.sin(acc), lambda acc: acc, acc))\n        _, acc = lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))\n        return acc\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_while_cond, x, [tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 5, op_name='jax2tf(f_while_cond)/while/body/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 7, op_name='jax2tf(f_while_cond)/while/body/branch_1_fun/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='FloorMod', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_while_cond)/while/body/rem', op_type='rem')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "def test_name_scope_cond(self):\n\n    def f(x):\n\n        def f_pos(x):\n            with jax.named_scope('jax_f_pos'):\n                return lax.cond(x < 1.0, jnp.cos, jnp.sin, x)\n        with jax.named_scope('jax_f_outer'):\n            return lax.cond(x > 0.0, f_pos, lambda x: x, x)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_forward():\n        with tf.name_scope('tf_outer_forward'):\n            x = 0.5\n            f_tf = jax2tf.convert(f)\n            _ = f_tf(x)\n    g = outer_forward.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_forward')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_forward/jax2tf_f_/jax_f_outer')\n    x = tf.Variable(0.5, name='tf_outer_back/x')\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_back():\n        with tf.name_scope('tf_outer_back'):\n            f_tf = jax2tf.convert(f)\n            with tf.GradientTape() as tape:\n                res_tf = f_tf(x)\n                _ = tape.gradient(res_tf, x)\n    g = outer_back.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_back')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_back')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.named_scope('bar_cond')\ndef cond(x):\n    return x < 5.0"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.remat\ndef g(x):\n    jax.jit(lambda: 0 if jnp.add(1, 1) else 0)()\n    return lax.sin(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@custom_transpose_with_example_out(jnp.ones(2))\ndef fn(r, x):\n    tracer_spy.append(r)\n    tracer_spy.append(x['c'])\n    return dict(b=x['c'] / r)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    if x > 0.0:\n        return x * 2\n    else:\n        return x + 2"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.pmap\ndef g(z):\n    return f(z, z + 77)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    z = x * 2\n    return shard_alike(x, z)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "def test_op_metadata_while_and_cond(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_while_cond(x):\n\n        def body_fun(i_acc):\n            i, acc = i_acc\n            return (i + 1, jnp.cos(acc) + lax.cond(jnp.mod(i, 2) == 0, lambda acc: jnp.sin(acc), lambda acc: acc, acc))\n        _, acc = lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))\n        return acc\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_while_cond, x, [tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 5, op_name='jax2tf(f_while_cond)/while/body/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 7, op_name='jax2tf(f_while_cond)/while/body/branch_1_fun/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='FloorMod', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_while_cond)/while/body/rem', op_type='rem')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "def test_name_scope_cond(self):\n\n    def f(x):\n\n        def f_pos(x):\n            with jax.named_scope('jax_f_pos'):\n                return lax.cond(x < 1.0, jnp.cos, jnp.sin, x)\n        with jax.named_scope('jax_f_outer'):\n            return lax.cond(x > 0.0, f_pos, lambda x: x, x)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_forward():\n        with tf.name_scope('tf_outer_forward'):\n            x = 0.5\n            f_tf = jax2tf.convert(f)\n            _ = f_tf(x)\n    g = outer_forward.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_forward')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_forward/jax2tf_f_/jax_f_outer')\n    x = tf.Variable(0.5, name='tf_outer_back/x')\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_back():\n        with tf.name_scope('tf_outer_back'):\n            f_tf = jax2tf.convert(f)\n            with tf.GradientTape() as tape:\n                res_tf = f_tf(x)\n                _ = tape.gradient(res_tf, x)\n    g = outer_back.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_back')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_back')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(x):\n    return jnp.linalg.norm(matvec(x) - b) > tolerance"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef g(x):\n    return x"
  },
  {
    "test_code": "def test_sin(self):\n    f_tf = jax2tf.convert(jnp.sin)\n    x = np.float32(0.5)\n    sin_x = np.sin(x)\n    self.assertAllClose(sin_x, f_tf(x))\n    self.assertAllClose(sin_x, tf.function(f_tf, autograph=False, jit_compile=True)(x))\n    tf_preferred_device = (tf.config.list_logical_devices('TPU') + tf.config.list_logical_devices('GPU') + tf.config.list_logical_devices())[0]\n    logging.info('Running TF on %s', tf_preferred_device)\n\n    @tf.function(autograph=False, jit_compile=False)\n    def f_tf_wrapped(x):\n        with tf.device(tf_preferred_device.name):\n            return f_tf(x)\n    with tf.device(tf_preferred_device.name):\n        self.assertAllClose(sin_x, f_tf_wrapped(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_nested_jit_is_compiled(self):\n\n    def has_xla_must_compile(f_tf, x):\n        f_conc = tf.function(f_tf, autograph=True).get_concrete_function(tf.convert_to_tensor(x))\n        for n in f_conc.graph._nodes_by_id.values():\n            try:\n                n.get_attr('_XlaMustCompile')\n                return True\n            except ValueError:\n                continue\n        return False\n    x = np.array(0.7)\n    f_no_jit = lambda x: x\n    self.assertFalse(has_xla_must_compile(jax2tf.convert(f_no_jit), x))\n    f_jit = lambda x: jax.jit(jnp.sin)(x)\n    self.assertFalse(has_xla_must_compile(jax2tf.convert(f_jit), x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_bfloat16_passed_by_tf(self):\n    f_jax = lambda a, b: a + b\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([512, 512], tf.bfloat16), tf.TensorSpec([512, 512], tf.bfloat16)])\n    self.assertIsNotNone(f_tf.get_concrete_function())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_bfloat16_returned_by_jax(self):\n    f_jax = lambda a, b: (a + b).astype(jnp.bfloat16)\n    f_tf = jax2tf.convert(f_jax)\n    self.assertEqual(f_tf(1.0, 2.0).dtype, tf.bfloat16)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_bfloat16_tf_grad(self):\n    f_jax = lambda a, b: a + b\n\n    def _tf_grad(a, b):\n        with tf.GradientTape() as tape:\n            tape.watch(a)\n            result = jax2tf.convert(f_jax)(a, b)\n        return (result, tape.gradient(result, a))\n    f_tf = tf.function(_tf_grad, autograph=False, input_signature=[tf.TensorSpec([512, 512], tf.bfloat16), tf.TensorSpec([512, 512], tf.bfloat16)])\n    self.assertIsNotNone(f_tf.get_concrete_function())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(dtype=[np.int64, np.float64], with_function=[True, False])\ndef test_converts_64bit(self, dtype=np.int64, with_function=False):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    big_const = np.full((5,), 2 ** 33, dtype=dtype)\n    self.ConvertAndCompare(jnp.sin, big_const)\n    f_conv = jax2tf.convert(jnp.sin)\n    if with_function:\n        f_conv = tf.function(f_conv, autograph=False)\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.Variable(big_const)))\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.constant(big_const)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_64bit_behavior_enable_x64_readme(self):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float64)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_64bit_behavior_not_enable_x64_readme(self):\n    if config.enable_x64.value:\n        self.skipTest('requires not x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float32)\n    self.assertEqual(tf.math.sin(np.float64(3.14)).dtype, tf.float64)\n    self.assertEqual(jnp.sin(np.float64(3.14)).dtype, jnp.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(np.float64(3.14)).dtype, tf.float32)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_disabled(self, with_function=False):\n    if tf.version.VERSION.split('.') <= ['2', '17', '0']:\n        self.skipTest('This test works only with newer versions of TF')\n    f_tf = jax2tf.convert(jnp.tan, with_gradient=False)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    x = tf.ones([])\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            y = f_tf(x)\n            _ = tape.gradient(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients(self, with_function=True):\n\n    def f(x, y):\n        return (x * x, x * y)\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_type = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    y = tf.Variable(5.0, dtype=default_float_type)\n    with tf.GradientTape(persistent=True) as tape:\n        u, v = f_tf(x, y)\n    self.assertAllClose(2.0 * 4.0, tape.gradient(u, x))\n    self.assertAllClose(0.0, tape.gradient(u, y))\n    self.assertAllClose(5.0, tape.gradient(v, x))\n    self.assertAllClose(4.0, tape.gradient(v, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_higher_order_gradients(self):\n    f = lambda x: x ** 3\n    f_tf = jax2tf.convert(f)\n    x = tf.Variable(4.0, dtype=tf.float32)\n    with tf.GradientTape() as t2:\n        with tf.GradientTape() as t1:\n            y = f_tf(x)\n        dy_dx = t1.gradient(y, x)\n    d2y_dx2 = t2.gradient(dy_dx, x)\n    self.assertAllClose(np.float32(48.0), dy_dx.numpy())\n    self.assertAllClose(np.float32(24.0), d2y_dx2.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_pytree(self, with_function=False):\n\n    def f(xy: tuple[float, float]) -> dict[str, float]:\n        x, y = xy\n        return dict(one=x * x, two=x * y)\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_dtype = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable(4.0, dtype=default_float_dtype)\n    y = tf.Variable(5.0, dtype=default_float_dtype)\n    with tf.GradientTape(persistent=True) as tape:\n        uv = f_tf((x, y))\n    self.assertAllClose(2.0 * 4.0, tape.gradient(uv['one'], x))\n    self.assertAllClose(0.0, tape.gradient(uv['one'], y))\n    self.assertAllClose(5.0, tape.gradient(uv['two'], x))\n    self.assertAllClose(4.0, tape.gradient(uv['two'], y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_ordered_dict_input(self, with_function=True):\n\n    def f(inputs):\n        out = 0.0\n        for v in inputs.values():\n            out += jnp.sum(v)\n        return out\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_type = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable([4.0], dtype=default_float_type)\n    y = tf.Variable([4.0, 5.0], dtype=default_float_type)\n    inputs = collections.OrderedDict()\n    inputs['r'] = x\n    inputs['d'] = y\n    with tf.GradientTape(persistent=True) as tape:\n        u = f_tf(inputs)\n    self.assertAllClose(np.array([1.0]), tape.gradient(u, x).numpy())\n    self.assertAllClose(np.array([1.0, 1.0]), tape.gradient(u, y).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_int_argument(self, with_function=False):\n    state = dict(float_used=np.array([0.7, 0.9], dtype=np.float32), float_passthrough=np.float16(1.0), float_unused=np.array([1.1, 2.2, 3.3], dtype=np.float32), int_used=np.int16(5), int_passthrough=np.int8(7), int_unused=np.array([1, 2, 3], dtype=np.uint32), bool_used=np.array([True, False, False, True], dtype=np.bool_), bool_passthrough=np.array([True, False, False, True, False], dtype=np.bool_), bool_unused=np.array([[True, False], [False, True]], dtype=np.bool_))\n\n    def jax_f(state):\n        res = dict(state, float_used=2.0 * state['float_used'], int_used=3 * state['int_used'], bool_used=state['bool_used'] == state['bool_used'])\n        del res['float_unused']\n        del res['int_unused']\n        del res['bool_unused']\n        return res\n    args = (state,)\n    res_jax = jax_f(*args)\n    vjp_jax_fun, args_vjp = tf_test_util.TransformJaxVJP(jax_f, args, res_jax)\n    grad_jax, = vjp_jax_fun(*args_vjp)\n\n    def compare_with_overrides(*, what, expected, **expected_overrides):\n        what_keys = set(what.keys())\n        expected_keys = set(expected.keys())\n        self.assertEqual(what_keys, expected_keys)\n        for k, w in what.items():\n            e = expected[k]\n            if k in expected_overrides:\n                if expected_overrides[k] == 'ZERO':\n                    e = np.zeros_like(w)\n                elif expected_overrides[k] == 'ZERO_BOOL':\n                    e = np.zeros(np.shape(w), dtype=np.bool_)\n                elif expected_overrides[k] == 'ONE':\n                    e = np.ones_like(w)\n                else:\n                    e = expected_overrides[k]\n            if e is None:\n                self.assertIsNone(w, msg=k)\n            else:\n                self.assertIsNotNone(w, msg=k)\n            w = w.numpy() if isinstance(w, tf.Tensor) else e\n            e = e.numpy() if isinstance(e, tf.Tensor) else e\n            try:\n                self.assertAllClose(e, w, err_msg=k)\n            except:\n                print(f'Failed at {k}')\n                raise\n    _, (grad_tf_0,) = tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    compare_with_overrides(what=grad_tf_0, expected=grad_jax, float_unused='ZERO', bool_used='ZERO', bool_passthrough='ONE', bool_unused='ZERO', int_used='ZERO', int_passthrough='ONE', int_unused='ZERO')\n    _, (grad_tf_None,) = tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.NONE)\n    compare_with_overrides(what=grad_tf_None, expected=grad_tf_0, float_unused=None, int_used=None, int_unused=None, bool_used=None, bool_unused=None)\n    f_tf_jax = jax2tf.convert(jax_f)\n    if with_function:\n        f_tf_jax = tf.function(f_tf_jax, autograph=False)\n    _, (grad_tf_jax_0,) = tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args)\n    compare_with_overrides(what=grad_tf_jax_0, expected=grad_tf_0, int_passthrough='ZERO', bool_passthrough='ZERO')\n    _, (grad_tf_jax_None,) = tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args, unconnected_gradients=tf.UnconnectedGradients.NONE)\n    compare_with_overrides(what=grad_tf_jax_None, expected=grad_tf_0, int_used=None, int_passthrough=None, int_unused=None, bool_unused=None, bool_used=None, bool_passthrough=None)\n    tf_vjp_jax_fun = jax2tf.convert(vjp_jax_fun)\n    grad_tf_vjp_jax, = tf_vjp_jax_fun(*args_vjp)\n    compare_with_overrides(what=grad_tf_vjp_jax, expected=grad_tf_0, bool_passthrough='ZERO_BOOL', bool_unused='ZERO_BOOL', bool_used='ZERO_BOOL', int_passthrough='ZERO_BOOL', int_unused='ZERO_BOOL', int_used='ZERO_BOOL')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_readme_gradient_int(self):\n    x = np.array(2, dtype=np.int16)\n\n    def f_jax(x):\n        return x.astype(np.float32) * 2.0\n    print(jax.grad(f_jax, allow_int=True)(x))\n    print(jax2tf.convert(jax.grad(f_jax, allow_int=True))(x))\n\n    def f_tf(x):\n        return tf.cast(x, tf.float32) * 2.0\n    xv = tf.Variable(x)\n    with tf.GradientTape(persistent=True) as tape:\n        print(tape.gradient(f_tf(xv), xv))\n        print(tape.gradient(f_tf(xv), xv, unconnected_gradients=tf.UnconnectedGradients.ZERO))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_convert_argument_non_callable_error(self):\n    with self.assertRaisesRegex(TypeError, 'Expected a callable value'):\n        jax2tf.convert(5.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_convert_argument_non_tensor_error(self):\n    with self.assertRaisesRegex(TypeError, 'Argument.*is not a valid JAX type'):\n        jax2tf.convert(lambda x: x)(lambda y: y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_argument_eager_tensor(self):\n    x = jax2tf.convert(jnp.sin)(1.0)\n    jax2tf.convert(jnp.cos)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@unittest.skip('Test fails at head')\ndef test_issue_10586(self):\n\n    class JaxModule(tf.Module):\n\n        def __init__(self):\n            self._params = {'w': tf.Variable(tf.ones([784, 10]), name='w'), 'b': tf.Variable(tf.ones([10]), name='b')}\n\n        def __call__(self, x):\n            return jax2tf.convert(lambda p, x: x @ p['w'] + p['b'])(self._params, x)\n    net = JaxModule()\n    images = tf.ones([1, 784])\n    with tf.GradientTape() as tape:\n        loss = tf.reduce_sum(net(images))\n    params = tape.watched_variables()\n    grads = tape.gradient(loss, params)\n    for var, grad in zip(params, grads):\n        self.assertEqual(var.shape, grad.shape, msg=var.name)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_remat(self):\n\n    def f(x1):\n        x2 = jnp.sin(x1)\n        x3 = jnp.sin(x2)\n        x4 = jnp.sin(x3)\n        return x4\n    remat_f = ad_checkpoint.checkpoint(f)\n    arg = np.array(3.0)\n    f_tf = jax2tf.convert(jax.grad(remat_f))\n    f_tf_hlo = self.TfToHlo(f_tf, arg)\n    if config.remat_opt_barrier.value:\n        self.assertRegex(f_tf_hlo, 'opt-barrier')\n    else:\n        self.assertRegex(f_tf_hlo, 'transpose/jax2tf_f_/jvp/checkpoint/cond/branch_1_fun/Sin')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_checkpoint_name(self):\n\n    def f_jax(x):\n        return ad_checkpoint.checkpoint_name(jnp.sin(x), 'sin')\n    jax2tf.convert(f_jax)(1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_convert_of_nested_independent_jit(self):\n\n    def func(x):\n\n        def inner1(y):\n            return x + y\n        return jax.jit(inner1)(1.0)\n    jax2tf.convert(func)(2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_convert_of_nested_dependent_jit(self):\n\n    def func(x):\n\n        def inner1(y):\n            return x + y\n        return jax.jit(inner1)(x)\n    jax2tf.convert(func)(2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_jit_unused(self):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))(x, y_unused)\n    self.assertAllClose(f_jax(x, None), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=mode, mode=mode) for mode in ('eager', 'graph', 'compiled')))\ndef test_jit_unused_grad(self, mode='eager'):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_jax = f_jax(x, y_unused)\n    f_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))\n    x_tf, y_unused_tf = (tf.constant(x), tf.constant(y_unused))\n\n    def grad_tf(x, y_unused):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            tape.watch(y_unused)\n            res_tf = f_tf(x, y_unused)\n            grad_tf_x, grad_tf_y = tape.gradient(res_tf, (x, y_unused))\n        return (res_tf, grad_tf_x, grad_tf_y)\n    if mode == 'graph':\n        grad_tf = tf.function(grad_tf, autograph=False)\n    elif mode == 'compiled':\n        grad_tf = tf.function(grad_tf, autograph=False, jit_compile=True)\n    res_tf, grad_tf_x, grad_tf_y = grad_tf(x_tf, y_unused_tf)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(np.float32(2.0), grad_tf_x)\n    self.assertIsNone(grad_tf_y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_nested_convert_error(self):\n\n    def outer(y):\n        return jax2tf.convert(jnp.sin)(y)\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        jax2tf.convert(outer)(np.ones((4,), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_nested_convert_error_non_tracer(self):\n    \"\"\"The inner convert takes non-tracer arguments\"\"\"\n\n    def outer(y):\n        sin_1 = jax2tf.convert(jnp.sin)(1.0)\n        return y + sin_1\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        jax2tf.convert(outer)(2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(transform=['jit', 'jvp', 'grad', 'vmap'])\ndef test_convert_under_transform_error(self, transform='vmap'):\n\n    def outer(y):\n        return jax2tf.convert(jnp.sin)(y)\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        self.TransformConvertAndCompare(outer, np.ones((4,)), transform)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(transform=['jit', 'jvp', 'grad', 'vmap'])\ndef test_convert_under_transform_error_non_tracer(self, transform='vmap'):\n\n    def outer(y):\n        sin_1 = jax2tf.convert(jnp.sin)(1.0)\n        return y + sin_1\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        self.TransformConvertAndCompare(outer, np.ones((4,)), transform)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_name_scope(self):\n\n    def run_tf():\n\n        @jax.named_call\n        def my_test_function_jax(x):\n            return x * x\n\n        def caller_jax(x):\n            return my_test_function_jax(jnp.sin(x))\n        out = jax2tf.convert(caller_jax, with_gradient=False)(2.0)\n        return out\n    if config.jax2tf_default_native_serialization.value:\n        self.assertIn('my_test_function_jax/mul', self.TfToHlo(run_tf))\n    else:\n        graph_def = str(tf.function(run_tf, autograph=False).get_concrete_function().graph.as_graph_def())\n        if 'my_test_function_jax/pjit_multiply_/Mul' not in graph_def:\n            self.assertIn('my_test_function_jax/jit_multiply_/Mul', graph_def)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_bfloat16_constant(self):\n\n    def jax_fn_scalar(x):\n        x = x.astype(jnp.bfloat16)\n        x *= 2.0\n        return x\n\n    def jax_fn_array(x):\n        x = x.astype(jnp.bfloat16)\n        x *= np.array([1.5, 2.5, 3.5], jnp.bfloat16)\n        return x\n    tf_fn_scalar = jax2tf.convert(jax_fn_scalar)\n    self.assertAllClose(tf_fn_scalar(1.375).numpy(), jnp.bfloat16(2.75))\n    tf_fn_array = jax2tf.convert(jax_fn_array)\n    self.assertAllClose(tf_fn_array(np.array([3, 4, 5])), np.array([4.5, 10, 17.5], jnp.bfloat16))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_shared_constants(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const = np.random.uniform(size=256).astype(np.float32)\n\n    def f(x):\n        return x + const + const + const + const\n    f_tf_consts = self.FindLargeTfConstants(jax2tf.convert(f), const)\n    self.assertLen(f_tf_consts, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_shared_constants_under_scan(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    xs = np.ones((8, const_size), dtype=np.float32)\n\n    def f1(xs):\n        res, _ = lax.scan(lambda carry, x: (carry + x + const, None), jnp.zeros((const_size,), dtype=np.float32), xs)\n        return res\n\n    def f2(xs):\n        return f1(xs) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), xs, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), xs, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_shared_constants_under_jit(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const = np.random.uniform(size=(16, 16)).astype(np.float32)\n\n    @jax.jit\n    def g_jit(x):\n        return x * const\n\n    def f(x):\n        return g_jit(x) + const + const\n    f_tf_graph_consts = self.FindLargeTfConstants(jax2tf.convert(f), const)\n    self.assertLen(f_tf_graph_consts, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_weak_types(self):\n    mul = jax.jit(jnp.multiply)\n    tf_fn = jax2tf.convert(lambda x: mul(x, 2.0))\n    self.assertAllClose(tf_fn(tf.constant(1.375, tf.bfloat16)).numpy(), jnp.bfloat16(2.75))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_kwargs(self, with_function=False):\n\n    def f_jax(*, x):\n        return jnp.sum(x)\n    f_tf = jax2tf.convert(f_jax)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(f_tf(x=np.zeros(3, dtype=np.float32)), np.zeros((), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_grad_kwargs(self, with_function=False):\n    x = (np.zeros(3, dtype=np.float32), np.zeros(4, dtype=np.float32))\n\n    def f_jax(*, x=(1.0, 2.0)):\n        return jnp.sum(x[0]) + 2.0 * jnp.sum(x[1])\n    f_tf = jax2tf.convert(f_jax)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    xv = tf.nest.map_structure(tf.Variable, x)\n    with tf.GradientTape() as tape:\n        res = f_tf(x=xv)\n    grad_tf = tape.gradient(res, xv)\n    self.assertAllClose((np.full_like(x[0], fill_value=1.0), np.full_like(x[1], fill_value=2.0)), (grad_tf[0].numpy(), grad_tf[1].numpy()))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_name_scope_polymorphic(self):\n    if config.jax2tf_default_native_serialization.value and (not config.dynamic_shapes.value):\n        self.skipTest('shape polymorphism but --jax_dynamic_shapes is not set.')\n\n    def func_jax(x, y):\n        return jnp.sin(x) + jnp.cos(y)\n    func_tf = jax2tf.convert(func_jax, polymorphic_shapes='(b,...)', with_gradient=True)\n    outer_scope = 'output_a'\n    g = tf.Graph()\n    with g.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = func_tf(x, y)\n    self.assertAllOperationStartWith(g, outer_scope)\n    g2 = tf.Graph()\n    with g2.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = tf.function(func_tf, jit_compile=True, autograph=False)(x, y)\n    self.assertAllOperationStartWith(g2, outer_scope)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_name_scope_cond(self):\n\n    def f(x):\n\n        def f_pos(x):\n            with jax.named_scope('jax_f_pos'):\n                return lax.cond(x < 1.0, jnp.cos, jnp.sin, x)\n        with jax.named_scope('jax_f_outer'):\n            return lax.cond(x > 0.0, f_pos, lambda x: x, x)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_forward():\n        with tf.name_scope('tf_outer_forward'):\n            x = 0.5\n            f_tf = jax2tf.convert(f)\n            _ = f_tf(x)\n    g = outer_forward.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_forward')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_forward/jax2tf_f_/jax_f_outer')\n    x = tf.Variable(0.5, name='tf_outer_back/x')\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_back():\n        with tf.name_scope('tf_outer_back'):\n            f_tf = jax2tf.convert(f)\n            with tf.GradientTape() as tape:\n                res_tf = f_tf(x)\n                _ = tape.gradient(res_tf, x)\n    g = outer_back.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_back')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_back')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_name_scope_while_loop(self):\n\n    def f(x):\n        with tf.name_scope('outer_scope'):\n\n            def condition(x):\n                return jnp.sum(x, keepdims=False) < 100\n\n            def body(x):\n                return jnp.add(x, 2.0)\n            result = jax.lax.while_loop(condition, body, x)\n            return result\n    tf_f = tf.function(jax2tf.convert(f), jit_compile=True, autograph=False)\n    g = tf_f.get_concrete_function(tf.zeros((1, 3))).graph\n    for func in g._functions.values():\n        for op in func.graph.get_operations():\n            if op.name.count(f'outer_scope/jax2tf_{f.__name__}_/while') > 1:\n                self.fail(f'tf graph has repeated name issue on when converting lax.while to tf.while.See op.name = : {op.name}')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'{('with_mesh_' if with_mesh else '')}2={(transform2 if transform2 != 'none' else '')}_1={(transform1 if transform1 != 'none' else '')}{('_nullary' if nullary else '')}', with_mesh=with_mesh, transform1=transform1, transform2=transform2, nullary=nullary) for transform1 in ['none', 'jit', 'pjit', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding', 'shard_map', 'pmap'] for transform2 in ['none', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] for nullary in ([True, False] if transform2 == 'none' else [False]) for with_mesh in ([True] if transform1 not in ['base', 'jit', 'pjit'] or transform2 != 'none' else [False, True])))\ndef test_cross_platform(self, with_mesh=True, transform1='pjit_in_shardings_P', transform2='pjit_in_shardings_P', nullary=False):\n    if transform2 == 'none' and (transform1 == 'shard_map' or (transform1 in ['pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] and nullary)):\n        raise unittest.SkipTest('Skip because must have pjit at top level')\n    x = np.ones((4, 6), dtype=np.float32)\n    mesh = sharding.Mesh(jax.devices()[:1], ('a',))\n    func = lambda x: lax.cummax(x, axis=0, reverse=False)\n    func_shard_map = lambda x: lax.all_gather(x, 'a', axis=1, tiled=True)\n\n    def apply_transform(func, transform: str):\n        transformed_func = dict(none=func, jit=jax.jit(func), jit_in_shardings_None=jax.jit(func, in_shardings=None), jit_in_shardings_P=jax.jit(func, in_shardings=(P('a'),)), jit_in_shardings_Sharding=jax.jit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),)), pjit=pjit.pjit(func), pjit_in_shardings_None=pjit.pjit(func, in_shardings=None, out_shardings=None), pjit_in_shardings_P=pjit.pjit(func, in_shardings=(P('a'),), out_shardings=P('a')), pjit_in_shardings_Sharding=pjit.pjit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),), out_shardings=sharding.NamedSharding(mesh, P('a'))), shard_map=shard_map(func, mesh, in_specs=(P('a', None),), out_specs=P('a', None)), pmap=jax.pmap(func, in_axes=0, out_axes=0))[transform]\n        return transformed_func\n    transformed1_func = apply_transform(func_shard_map if transform1 == 'shard_map' else func, transform1)\n    assert transform2 not in ['shard_map']\n    transformed2_func = apply_transform(transformed1_func, transform2)\n    if transform1 == 'pmap':\n        x = x.reshape((1, -1))\n    if not nullary:\n        func_to_convert = transformed2_func\n        args = [x]\n    else:\n        func_to_convert = lambda: transformed2_func(jnp.ones(x.shape, dtype=x.dtype))\n        args = []\n    if transform1 == 'pmap':\n        if nullary:\n            raise unittest.SkipTest('Cannot lower nested pmap: jit-of-pmap warning')\n        raise unittest.SkipTest('TODO: figure out how to invoke pmap from TF')\n    f_tf = jax2tf.convert(func_to_convert, native_serialization=True, native_serialization_platforms=('tpu',))\n    f_tf = tf.function(f_tf, jit_compile=True, autograph=False)\n    with contextlib.ExitStack() as stack:\n        if with_mesh:\n            stack.enter_context(mesh)\n        _ = func_to_convert(*args)\n        exported = export.export(jax.jit(func_to_convert) if not hasattr(func_to_convert, 'trace') else func_to_convert, platforms=('tpu',))(*(core.ShapedArray(a.shape, a.dtype) for a in args))\n    if transform1 == 'shard_map':\n        self.assertIn('stablehlo.all_gather', str(exported.mlir_module()))\n    else:\n        self.assertIn('stablehlo.reduce_window', str(exported.mlir_module()))",
    "assertions": [
      "assert transform2 not in ['shard_map']"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_cross_platform_error(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization=True, native_serialization_platforms=('tpu',))\n    x = np.float32(0.5)\n    if jtu.test_device_matches(['tpu']):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    else:\n        f_tf_fun = tf.function(f_tf, jit_compile=True, autograph=False)\n        graph_def = f_tf_fun.get_concrete_function(x).graph.as_graph_def()\n        self.assertIn('XlaCallModule', str(graph_def))\n        with self.assertRaisesRegex(tf.errors.NotFoundError, 'The current platform .* is not among the platforms required by the module'):\n            f_tf(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.ignore_warning(message='using native_serialization_platforms without native_serialization')\ndef test_native_parameters_for_non_native(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_platforms=('cpu',))\n    x = np.float32(0.5)\n    tf_cpus = tf.config.list_logical_devices('CPU')\n    self.assertNotEmpty(tf_cpus)\n    with tf.device(tf_cpus[0]):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_disabled_checks=(jax2tf.DisabledSafetyCheck.platform(),))\n    self.assertAllClose(jnp.sin(x), f_tf(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_native_serialization_grad(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization=True, native_serialization_platforms=('tpu',))\n    x = np.arange(4, dtype=np.float32)\n    x_v = tf.Variable(x)\n\n    @tf.function(autograph=False)\n    def f_grad_tf(x_v):\n        with tf.GradientTape() as tape:\n            tape.watch(x_v)\n            res_tf = f_tf(x_v)\n            return tape.gradient(res_tf, x_v)\n    f_grad_tf_fun = tf.function(f_grad_tf, autograph=False)\n    graph_def = f_grad_tf_fun.get_concrete_function(x).graph.as_graph_def()\n    logging.info('Found graph_def: %s', graph_def)\n    self.assertLen(re.findall('op:\\\\s*\"XlaCallModule\"', str(graph_def)), 2)\n    if not jtu.test_device_matches(['tpu']):\n        with self.assertRaisesRegex(tf.errors.NotFoundError, 'The current platform .* is not among the platforms required by the module: \\\\[TPU\\\\]'):\n            f_grad_tf(x_v)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_effects_error(self):\n\n    def f_jax(x):\n        jax.debug.print('{}', x)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_jax, native_serialization=True)(np.float32(42.0))\n\n    def f_ordered_jax(x):\n        jax.debug.print('{}', x, ordered=True)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_ordered_jax, native_serialization=True)(np.float32(42.0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_multi_platform(self):\n    if config.enable_x64.value:\n        self.skipTest('TODO: enable when we can handle i64 platform_index_argument')\n    _testing_multi_platform_to_add = dict(cpu=2.0, tpu=3.0, cuda=4.0, rocm=5.0)\n\n    def f_jax(x):\n        return x + lax.platform_dependent(tpu=lambda: _testing_multi_platform_to_add['tpu'], cuda=lambda: _testing_multi_platform_to_add['cuda'], rocm=lambda: _testing_multi_platform_to_add['rocm'], default=lambda: _testing_multi_platform_to_add['cpu'])\n    x = np.float32(0.42)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=('cpu', 'cuda', 'tpu'))\n    for tf_device in self.tf_devices:\n        logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n        with tf.device(tf_device):\n            res = f_tf(x)\n        tf_device_jax_platform = dict(CPU='cpu', GPU='cuda', TPU='tpu')[tf_device.device_type]\n        self.assertAllClose(res, x + _testing_multi_platform_to_add[tf_device_jax_platform])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_dot_algorithm(self):\n    if tf.version.VERSION.split('.') <= ['2', '18', '0']:\n        self.skipTest('Because of an XLA bug this test segfaults with TF v2.18.0')\n    if jtu.test_device_matches(['tpu']):\n        algorithm = 'BF16_BF16_F32'\n    else:\n        algorithm = 'F32_F32_F32'\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision=algorithm)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    f_tf(np.ones((128, 128), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_dot_algorithm_non_native_unsupported(self):\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision='F32_F32_F32')\n    x = np.ones((128, 128), dtype=np.float32)\n    with self.assertRaisesRegex(NotImplementedError, 'Unsupported precision in dot_general'):\n        jax2tf.convert(f_jax, native_serialization=False)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@staticmethod\ndef add(dt, x, y):\n    fromscale = partial(jax.lax.convert_element_type, new_dtype=dt.float_dtype)\n    toscale = partial(jax.lax.convert_element_type, new_dtype=dt)\n    return toscale(jax.lax.max(fromscale(x), fromscale(y)))"
  },
  {
    "test_code": "def test_name_scope_while_loop(self):\n\n    def f(x):\n        with tf.name_scope('outer_scope'):\n\n            def condition(x):\n                return jnp.sum(x, keepdims=False) < 100\n\n            def body(x):\n                return jnp.add(x, 2.0)\n            result = jax.lax.while_loop(condition, body, x)\n            return result\n    tf_f = tf.function(jax2tf.convert(f), jit_compile=True, autograph=False)\n    g = tf_f.get_concrete_function(tf.zeros((1, 3))).graph\n    for func in g._functions.values():\n        for op in func.graph.get_operations():\n            if op.name.count(f'outer_scope/jax2tf_{f.__name__}_/while') > 1:\n                self.fail(f'tf graph has repeated name issue on when converting lax.while to tf.while.See op.name = : {op.name}')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@staticmethod\ndef add(dt, x, y):\n    fromscale = partial(jax.lax.convert_element_type, new_dtype=dt.float_dtype)\n    toscale = partial(jax.lax.convert_element_type, new_dtype=dt)\n    return toscale(jax.lax.max(fromscale(x), fromscale(y)))"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    if x > 0.0:\n        return x * 2\n    else:\n        return x + 2"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('x', 'y'), out_specs=out_spec)\ndef g(x):\n    result = lax.psum(x, axis_name=reduce_along)\n\n    def check_rep(result):\n        self.assertEqual(jax.experimental.shard_map.get_replication(result), set(reduce_along))\n        return result\n    result = check_rep(result)\n    result = jax.vmap(check_rep)(result)\n    return result"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_shared_constants_under_jit(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const = np.random.uniform(size=(16, 16)).astype(np.float32)\n\n    @jax.jit\n    def g_jit(x):\n        return x * const\n\n    def f(x):\n        return g_jit(x) + const + const\n    f_tf_graph_consts = self.FindLargeTfConstants(jax2tf.convert(f), const)\n    self.assertLen(f_tf_graph_consts, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g_jit(x):\n    return jnp.zeros(shape=(2, x))"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    return x"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_jit_unused(self):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))(x, y_unused)\n    self.assertAllClose(f_jax(x, None), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=mode, mode=mode) for mode in ('eager', 'graph', 'compiled')))\ndef test_jit_unused_grad(self, mode='eager'):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_jax = f_jax(x, y_unused)\n    f_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))\n    x_tf, y_unused_tf = (tf.constant(x), tf.constant(y_unused))\n\n    def grad_tf(x, y_unused):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            tape.watch(y_unused)\n            res_tf = f_tf(x, y_unused)\n            grad_tf_x, grad_tf_y = tape.gradient(res_tf, (x, y_unused))\n        return (res_tf, grad_tf_x, grad_tf_y)\n    if mode == 'graph':\n        grad_tf = tf.function(grad_tf, autograph=False)\n    elif mode == 'compiled':\n        grad_tf = tf.function(grad_tf, autograph=False, jit_compile=True)\n    res_tf, grad_tf_x, grad_tf_y = grad_tf(x_tf, y_unused_tf)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(np.float32(2.0), grad_tf_x)\n    self.assertIsNone(grad_tf_y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g(x):\n    return jax.pure_callback(lambda x: x, x, x)"
  },
  {
    "test_code": "def test_sin(self):\n    f_tf = jax2tf.convert(jnp.sin)\n    x = np.float32(0.5)\n    sin_x = np.sin(x)\n    self.assertAllClose(sin_x, f_tf(x))\n    self.assertAllClose(sin_x, tf.function(f_tf, autograph=False, jit_compile=True)(x))\n    tf_preferred_device = (tf.config.list_logical_devices('TPU') + tf.config.list_logical_devices('GPU') + tf.config.list_logical_devices())[0]\n    logging.info('Running TF on %s', tf_preferred_device)\n\n    @tf.function(autograph=False, jit_compile=False)\n    def f_tf_wrapped(x):\n        with tf.device(tf_preferred_device.name):\n            return f_tf(x)\n    with tf.device(tf_preferred_device.name):\n        self.assertAllClose(sin_x, f_tf_wrapped(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_basics(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_jit(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_nested_jit(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jax.jit(jnp.cos)(x)))\n    x = 0.7\n    self.ConvertAndCompare(f_jax, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@jtu.sample_product(dtype=[np.int64, np.float64], with_function=[True, False])\ndef test_converts_64bit(self, dtype=np.int64, with_function=False):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    big_const = np.full((5,), 2 ** 33, dtype=dtype)\n    self.ConvertAndCompare(jnp.sin, big_const)\n    f_conv = jax2tf.convert(jnp.sin)\n    if with_function:\n        f_conv = tf.function(f_conv, autograph=False)\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.Variable(big_const)))\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.constant(big_const)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_64bit_behavior_enable_x64_readme(self):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float64)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_64bit_behavior_not_enable_x64_readme(self):\n    if config.enable_x64.value:\n        self.skipTest('requires not x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float32)\n    self.assertEqual(tf.math.sin(np.float64(3.14)).dtype, tf.float64)\n    self.assertEqual(jnp.sin(np.float64(3.14)).dtype, jnp.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(np.float64(3.14)).dtype, tf.float32)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_function(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_remat(self):\n\n    def f(x1):\n        x2 = jnp.sin(x1)\n        x3 = jnp.sin(x2)\n        x4 = jnp.sin(x3)\n        return x4\n    remat_f = ad_checkpoint.checkpoint(f)\n    arg = np.array(3.0)\n    f_tf = jax2tf.convert(jax.grad(remat_f))\n    f_tf_hlo = self.TfToHlo(f_tf, arg)\n    if config.remat_opt_barrier.value:\n        self.assertRegex(f_tf_hlo, 'opt-barrier')\n    else:\n        self.assertRegex(f_tf_hlo, 'transpose/jax2tf_f_/jvp/checkpoint/cond/branch_1_fun/Sin')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_checkpoint_name(self):\n\n    def f_jax(x):\n        return ad_checkpoint.checkpoint_name(jnp.sin(x), 'sin')\n    jax2tf.convert(f_jax)(1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_name_scope(self):\n\n    def run_tf():\n\n        @jax.named_call\n        def my_test_function_jax(x):\n            return x * x\n\n        def caller_jax(x):\n            return my_test_function_jax(jnp.sin(x))\n        out = jax2tf.convert(caller_jax, with_gradient=False)(2.0)\n        return out\n    if config.jax2tf_default_native_serialization.value:\n        self.assertIn('my_test_function_jax/mul', self.TfToHlo(run_tf))\n    else:\n        graph_def = str(tf.function(run_tf, autograph=False).get_concrete_function().graph.as_graph_def())\n        if 'my_test_function_jax/pjit_multiply_/Mul' not in graph_def:\n            self.assertIn('my_test_function_jax/jit_multiply_/Mul', graph_def)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_op_metadata_simple(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_simple(x):\n        return jnp.sin(x)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_simple, x, [tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 2, op_name='jax2tf(f_simple)/sin', op_type='sin')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_op_metadata_sub_jit(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_callee(x):\n        return jnp.cos(x)\n\n    def f_caller(x):\n        y = jnp.tanh(x)\n        z = jax.jit(f_callee)(y)\n        return jnp.sin(z)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_caller, x, [tf_test_util.OpMetadataGraph(tf_type='Tanh', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_caller)/tanh', op_type='tanh'), tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 2, op_name='jax2tf(f_caller)/jit(f_callee)/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_caller)/sin', op_type='sin')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_op_metadata_named(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_callee(x):\n        return jnp.cos(x)\n\n    def f_caller(x):\n        y = jnp.tanh(x)\n        z = jax.named_call(f_callee, name='callee')(y)\n        return jnp.sin(z)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_caller, x, [tf_test_util.OpMetadataGraph(tf_type='Tanh', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_caller)/tanh', op_type='tanh'), tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 2, op_name='jax2tf(f_caller)/named(callee)/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_caller)/sin', op_type='sin')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_op_metadata_while_and_cond(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_while_cond(x):\n\n        def body_fun(i_acc):\n            i, acc = i_acc\n            return (i + 1, jnp.cos(acc) + lax.cond(jnp.mod(i, 2) == 0, lambda acc: jnp.sin(acc), lambda acc: acc, acc))\n        _, acc = lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))\n        return acc\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_while_cond, x, [tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 5, op_name='jax2tf(f_while_cond)/while/body/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 7, op_name='jax2tf(f_while_cond)/while/body/branch_1_fun/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='FloorMod', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_while_cond)/while/body/rem', op_type='rem')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_op_metadata_batched_while(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    @jax.vmap\n    def f_while(x):\n\n        def body_fun(carry):\n            new_carry = jnp.sin(carry)\n            return new_carry\n        _, carry = lax.while_loop(lambda carry: jnp.all(carry <= x), body_fun, x)\n        return carry\n    shape = (3, 2)\n    x = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n    jax_comp = jax.jit(f_while).lower(x).compiler_ir('hlo')\n    backend = xb.get_backend()\n    modules = backend.compile(jax_comp).hlo_modules()\n    jax_opt_hlo = modules[0].to_string()\n    print(f'JAX OPT HLO = {jax_opt_hlo}')\n    self.CheckOpMetadata(f_while, x, [tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_while)/while/body/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='LessEqual', source_file=__file__, source_line=user_frame.start_line + 8, op_name='jax2tf(f_while)/while/body_pred/le', op_type='le')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_op_metadata_disabled(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n\n    def f_simple(x):\n        return jnp.sin(x)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_simple, x, [], include_xla_op_metadata=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_name_scope_polymorphic(self):\n    if config.jax2tf_default_native_serialization.value and (not config.dynamic_shapes.value):\n        self.skipTest('shape polymorphism but --jax_dynamic_shapes is not set.')\n\n    def func_jax(x, y):\n        return jnp.sin(x) + jnp.cos(y)\n    func_tf = jax2tf.convert(func_jax, polymorphic_shapes='(b,...)', with_gradient=True)\n    outer_scope = 'output_a'\n    g = tf.Graph()\n    with g.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = func_tf(x, y)\n    self.assertAllOperationStartWith(g, outer_scope)\n    g2 = tf.Graph()\n    with g2.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = tf.function(func_tf, jit_compile=True, autograph=False)(x, y)\n    self.assertAllOperationStartWith(g2, outer_scope)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_cross_platform_error(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization=True, native_serialization_platforms=('tpu',))\n    x = np.float32(0.5)\n    if jtu.test_device_matches(['tpu']):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    else:\n        f_tf_fun = tf.function(f_tf, jit_compile=True, autograph=False)\n        graph_def = f_tf_fun.get_concrete_function(x).graph.as_graph_def()\n        self.assertIn('XlaCallModule', str(graph_def))\n        with self.assertRaisesRegex(tf.errors.NotFoundError, 'The current platform .* is not among the platforms required by the module'):\n            f_tf(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='using native_serialization_platforms without native_serialization')\ndef test_native_parameters_for_non_native(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_platforms=('cpu',))\n    x = np.float32(0.5)\n    tf_cpus = tf.config.list_logical_devices('CPU')\n    self.assertNotEmpty(tf_cpus)\n    with tf.device(tf_cpus[0]):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_disabled_checks=(jax2tf.DisabledSafetyCheck.platform(),))\n    self.assertAllClose(jnp.sin(x), f_tf(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_effects_error(self):\n\n    def f_jax(x):\n        jax.debug.print('{}', x)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_jax, native_serialization=True)(np.float32(42.0))\n\n    def f_ordered_jax(x):\n        jax.debug.print('{}', x, ordered=True)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_ordered_jax, native_serialization=True)(np.float32(42.0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(x):\n    return jax.pure_callback(_cond_callback, jax.ShapeDtypeStruct((), np.bool_), x)"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(x):\n    return jax.pure_callback(_cond_callback, jax.ShapeDtypeStruct((), np.bool_), x)"
  },
  {
    "test_code": "def test_op_metadata_while_and_cond(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_while_cond(x):\n\n        def body_fun(i_acc):\n            i, acc = i_acc\n            return (i + 1, jnp.cos(acc) + lax.cond(jnp.mod(i, 2) == 0, lambda acc: jnp.sin(acc), lambda acc: acc, acc))\n        _, acc = lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))\n        return acc\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_while_cond, x, [tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 5, op_name='jax2tf(f_while_cond)/while/body/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 7, op_name='jax2tf(f_while_cond)/while/body/branch_1_fun/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='FloorMod', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_while_cond)/while/body/rem', op_type='rem')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(x):\n    return jax.pure_callback(_cond_callback, jax.ShapeDtypeStruct((), np.bool_), x)"
  },
  {
    "test_code": "def test_name_scope_cond(self):\n\n    def f(x):\n\n        def f_pos(x):\n            with jax.named_scope('jax_f_pos'):\n                return lax.cond(x < 1.0, jnp.cos, jnp.sin, x)\n        with jax.named_scope('jax_f_outer'):\n            return lax.cond(x > 0.0, f_pos, lambda x: x, x)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_forward():\n        with tf.name_scope('tf_outer_forward'):\n            x = 0.5\n            f_tf = jax2tf.convert(f)\n            _ = f_tf(x)\n    g = outer_forward.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_forward')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_forward/jax2tf_f_/jax_f_outer')\n    x = tf.Variable(0.5, name='tf_outer_back/x')\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_back():\n        with tf.name_scope('tf_outer_back'):\n            f_tf = jax2tf.convert(f)\n            with tf.GradientTape() as tape:\n                res_tf = f_tf(x)\n                _ = tape.gradient(res_tf, x)\n    g = outer_back.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_back')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_back')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(x):\n    return jax.pure_callback(_cond_callback, jax.ShapeDtypeStruct((), np.bool_), x)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g(a, b):\n    an, ai = a\n    bn, bi = b\n    which = an >= bn\n    return (jnp.where(which, an, bn), jnp.where(which, ai, bi))"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.profiler.annotate_function, name='aname')\ndef g(x):\n    return x + 2"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def fn(data, segment_ids):\n    return jax.ops.segment_sum(data, segment_ids, num_segments).sum()"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, donate_argnums=0, out_shardings=Layout(DLL.AUTO))\ndef g(x):\n    return x * 2"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g():\n    x = x_ref[...] * y_ref[...]\n    y_ref[...] = x * 2\n    x_ref[...] = y_ref[...] + x_ref[...]"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(state):\n    idx, x, _ = state\n    chunk = jax.lax.dynamic_slice_in_dim(x, idx * chunk_size, chunk_size)\n    return (idx * chunk_size < x.shape[0]) & jnp.any(chunk > 0)"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(state):\n    idx, x, _ = state\n    chunk = jax.lax.dynamic_slice_in_dim(x, idx * chunk_size, chunk_size)\n    return (idx * chunk_size < x.shape[0]) & jnp.any(chunk > 0)"
  },
  {
    "test_code": "def test_op_metadata_while_and_cond(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_while_cond(x):\n\n        def body_fun(i_acc):\n            i, acc = i_acc\n            return (i + 1, jnp.cos(acc) + lax.cond(jnp.mod(i, 2) == 0, lambda acc: jnp.sin(acc), lambda acc: acc, acc))\n        _, acc = lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))\n        return acc\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_while_cond, x, [tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 5, op_name='jax2tf(f_while_cond)/while/body/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 7, op_name='jax2tf(f_while_cond)/while/body/branch_1_fun/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='FloorMod', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_while_cond)/while/body/rem', op_type='rem')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(state):\n    idx, x, _ = state\n    chunk = jax.lax.dynamic_slice_in_dim(x, idx * chunk_size, chunk_size)\n    return (idx * chunk_size < x.shape[0]) & jnp.any(chunk > 0)"
  },
  {
    "test_code": "def test_name_scope_cond(self):\n\n    def f(x):\n\n        def f_pos(x):\n            with jax.named_scope('jax_f_pos'):\n                return lax.cond(x < 1.0, jnp.cos, jnp.sin, x)\n        with jax.named_scope('jax_f_outer'):\n            return lax.cond(x > 0.0, f_pos, lambda x: x, x)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_forward():\n        with tf.name_scope('tf_outer_forward'):\n            x = 0.5\n            f_tf = jax2tf.convert(f)\n            _ = f_tf(x)\n    g = outer_forward.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_forward')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_forward/jax2tf_f_/jax_f_outer')\n    x = tf.Variable(0.5, name='tf_outer_back/x')\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_back():\n        with tf.name_scope('tf_outer_back'):\n            f_tf = jax2tf.convert(f)\n            with tf.GradientTape() as tape:\n                res_tf = f_tf(x)\n                _ = tape.gradient(res_tf, x)\n    g = outer_back.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_back')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_back')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(state):\n    idx, x, _ = state\n    chunk = jax.lax.dynamic_slice_in_dim(x, idx * chunk_size, chunk_size)\n    return (idx * chunk_size < x.shape[0]) & jnp.any(chunk > 0)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g(ys, _):\n    y, _ = ys\n    y = checkpoint_name(jnp.sin(y), 'y')\n    z = checkpoint_name(jnp.sin(y), 'z')\n    z = jax.lax.with_sharding_constraint(z, s)\n    z = z.T\n    w = checkpoint_name(jnp.sin(z), 'w')\n    return ((w.T, jnp.sum(w)), None)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@compute_on('device_host')\ndef fn():\n    k = jax.random.key(0)\n    return jax.nn.initializers.lecun_normal()(k, (2, 2), jnp.float32)"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@compute_on('device_host')\n@jax.jit\ndef f1(x):\n    x = x * 3\n    return f0(x)"
  },
  {
    "test_code": "def test_shared_constants_under_scan(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    xs = np.ones((8, const_size), dtype=np.float32)\n\n    def f1(xs):\n        res, _ = lax.scan(lambda carry, x: (carry + x + const, None), jnp.zeros((const_size,), dtype=np.float32), xs)\n        return res\n\n    def f2(xs):\n        return f1(xs) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), xs, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), xs, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@compute_on('device_host')\n@jax.jit\ndef f1(x):\n    x = x * 3\n    return f0(x)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f1(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_shared_constants_under_scan(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    xs = np.ones((8, const_size), dtype=np.float32)\n\n    def f1(xs):\n        res, _ = lax.scan(lambda carry, x: (carry + x + const, None), jnp.zeros((const_size,), dtype=np.float32), xs)\n        return res\n\n    def f2(xs):\n        return f1(xs) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), xs, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), xs, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f1(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_jit_unused(self):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))(x, y_unused)\n    self.assertAllClose(f_jax(x, None), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=mode, mode=mode) for mode in ('eager', 'graph', 'compiled')))\ndef test_jit_unused_grad(self, mode='eager'):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_jax = f_jax(x, y_unused)\n    f_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))\n    x_tf, y_unused_tf = (tf.constant(x), tf.constant(y_unused))\n\n    def grad_tf(x, y_unused):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            tape.watch(y_unused)\n            res_tf = f_tf(x, y_unused)\n            grad_tf_x, grad_tf_y = tape.gradient(res_tf, (x, y_unused))\n        return (res_tf, grad_tf_x, grad_tf_y)\n    if mode == 'graph':\n        grad_tf = tf.function(grad_tf, autograph=False)\n    elif mode == 'compiled':\n        grad_tf = tf.function(grad_tf, autograph=False, jit_compile=True)\n    res_tf, grad_tf_x, grad_tf_y = grad_tf(x_tf, y_unused_tf)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(np.float32(2.0), grad_tf_x)\n    self.assertIsNone(grad_tf_y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnums=[1])\ndef g(x, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_batched_while(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    @jax.vmap\n    def f_while(x):\n\n        def body_fun(carry):\n            new_carry = jnp.sin(carry)\n            return new_carry\n        _, carry = lax.while_loop(lambda carry: jnp.all(carry <= x), body_fun, x)\n        return carry\n    shape = (3, 2)\n    x = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n    jax_comp = jax.jit(f_while).lower(x).compiler_ir('hlo')\n    backend = xb.get_backend()\n    modules = backend.compile(jax_comp).hlo_modules()\n    jax_opt_hlo = modules[0].to_string()\n    print(f'JAX OPT HLO = {jax_opt_hlo}')\n    self.CheckOpMetadata(f_while, x, [tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_while)/while/body/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='LessEqual', source_file=__file__, source_line=user_frame.start_line + 8, op_name='jax2tf(f_while)/while/body_pred/le', op_type='le')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def lower(self, x, _experimental_lowering_parameters=None):\n    return jax.jit(self.__call__).lower(x, _experimental_lowering_parameters=_experimental_lowering_parameters)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jit\ndef g(z):\n    return self.pmap(lambda x: x[jnp.newaxis] * y)(z)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def fn(indices):\n    return jnp.equal(indices, jnp.arange(3)).astype(jnp.float32)"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(self.pmap, axis_name='i')\ndef func(_):\n    return jax.lax.psum(dtype(0), axis_name='i')"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(self.pmap, axis_name='i')\ndef func(_):\n    return jax.lax.psum(dtype(0), axis_name='i')"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(self.pmap, axis_name='i')\ndef func(_):\n    return jax.lax.psum(dtype(0), axis_name='i')"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_weak_types(self):\n    mul = jax.jit(jnp.multiply)\n    tf_fn = jax2tf.convert(lambda x: mul(x, 2.0))\n    self.assertAllClose(tf_fn(tf.constant(1.375, tf.bfloat16)).numpy(), jnp.bfloat16(2.75))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef mul(x):\n    return x @ x.T"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g(y):\n    return y"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef fn(key):\n    x = jnp.arange(113003)\n    x = with_sharding_constraint(x, P('data'))\n    y = jnp.arange(65536)\n    y = with_sharding_constraint(y.reshape(-1), P('data'))\n    z = jnp.concatenate([x, y], axis=0)\n    z = with_sharding_constraint(z, P('data'))\n    return (x, y, z)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g(weights, x, h_0, c_0):\n    W_ih, W_hh, b_ih, b_hh = rnn.unpack_lstm_weights(weights, input_size, hidden_size, num_layers, bidirectional)\n    y_ref, h_n_ref, c_n_ref = rnn.lstm_ref(x, h_0, c_0, W_ih, W_hh, b_ih, b_hh, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)\n    seq_length_mask = jnp.tile(jnp.arange(seq_len, dtype=jnp.int32)[None], [batch_size, 1]) < seq_lengths[:, None]\n    loss = jnp.sum(jnp.where(seq_length_mask[..., None], y_ref, 0.0))\n    return (loss, (y_ref, h_n_ref, c_n_ref))"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def add(self, x: jax.Array) -> jax.Array:\n    self.value += np.asarray(x)\n    return jax.device_put(self.value, x.sharding)"
  },
  {
    "test_code": "def test_name_scope_while_loop(self):\n\n    def f(x):\n        with tf.name_scope('outer_scope'):\n\n            def condition(x):\n                return jnp.sum(x, keepdims=False) < 100\n\n            def body(x):\n                return jnp.add(x, 2.0)\n            result = jax.lax.while_loop(condition, body, x)\n            return result\n    tf_f = tf.function(jax2tf.convert(f), jit_compile=True, autograph=False)\n    g = tf_f.get_concrete_function(tf.zeros((1, 3))).graph\n    for func in g._functions.values():\n        for op in func.graph.get_operations():\n            if op.name.count(f'outer_scope/jax2tf_{f.__name__}_/while') > 1:\n                self.fail(f'tf graph has repeated name issue on when converting lax.while to tf.while.See op.name = : {op.name}')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def add(self, x: jax.Array) -> jax.Array:\n    self.value += np.asarray(x)\n    return jax.device_put(self.value, x.sharding)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    return x + 4"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.vmap\ndef fn(x):\n    R1 = jnp.array([[x[0], 0, 0], [0, x[0], 0], [0, 0, x[0]]])\n    R2 = jnp.array([[x[0], 0, 0], [0, x[1], 0], [0, 0, x[2]]])\n    H = jnp.eye(4)\n    H = H.at[:3, :3].set(R2.T)\n    pos = H @ jnp.concatenate([x, jnp.array([1.0])])\n    return (pos, R1)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    return callback_p.bind(x, callback=log_value, effect=log_effect, out_avals=[])"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g(x):\n    branch = jax.named_call(lambda x: x)\n    out = jax.lax.cond(True, branch, branch, x)\n    return out"
  },
  {
    "test_code": "def test_sin(self):\n    f_tf = jax2tf.convert(jnp.sin)\n    x = np.float32(0.5)\n    sin_x = np.sin(x)\n    self.assertAllClose(sin_x, f_tf(x))\n    self.assertAllClose(sin_x, tf.function(f_tf, autograph=False, jit_compile=True)(x))\n    tf_preferred_device = (tf.config.list_logical_devices('TPU') + tf.config.list_logical_devices('GPU') + tf.config.list_logical_devices())[0]\n    logging.info('Running TF on %s', tf_preferred_device)\n\n    @tf.function(autograph=False, jit_compile=False)\n    def f_tf_wrapped(x):\n        with tf.device(tf_preferred_device.name):\n            return f_tf(x)\n    with tf.device(tf_preferred_device.name):\n        self.assertAllClose(sin_x, f_tf_wrapped(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_basics(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_jit(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_nested_jit(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jax.jit(jnp.cos)(x)))\n    x = 0.7\n    self.ConvertAndCompare(f_jax, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.sample_product(dtype=[np.int64, np.float64], with_function=[True, False])\ndef test_converts_64bit(self, dtype=np.int64, with_function=False):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    big_const = np.full((5,), 2 ** 33, dtype=dtype)\n    self.ConvertAndCompare(jnp.sin, big_const)\n    f_conv = jax2tf.convert(jnp.sin)\n    if with_function:\n        f_conv = tf.function(f_conv, autograph=False)\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.Variable(big_const)))\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.constant(big_const)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_64bit_behavior_enable_x64_readme(self):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float64)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_64bit_behavior_not_enable_x64_readme(self):\n    if config.enable_x64.value:\n        self.skipTest('requires not x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float32)\n    self.assertEqual(tf.math.sin(np.float64(3.14)).dtype, tf.float64)\n    self.assertEqual(jnp.sin(np.float64(3.14)).dtype, jnp.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(np.float64(3.14)).dtype, tf.float32)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_function(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_remat(self):\n\n    def f(x1):\n        x2 = jnp.sin(x1)\n        x3 = jnp.sin(x2)\n        x4 = jnp.sin(x3)\n        return x4\n    remat_f = ad_checkpoint.checkpoint(f)\n    arg = np.array(3.0)\n    f_tf = jax2tf.convert(jax.grad(remat_f))\n    f_tf_hlo = self.TfToHlo(f_tf, arg)\n    if config.remat_opt_barrier.value:\n        self.assertRegex(f_tf_hlo, 'opt-barrier')\n    else:\n        self.assertRegex(f_tf_hlo, 'transpose/jax2tf_f_/jvp/checkpoint/cond/branch_1_fun/Sin')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_checkpoint_name(self):\n\n    def f_jax(x):\n        return ad_checkpoint.checkpoint_name(jnp.sin(x), 'sin')\n    jax2tf.convert(f_jax)(1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_name_scope(self):\n\n    def run_tf():\n\n        @jax.named_call\n        def my_test_function_jax(x):\n            return x * x\n\n        def caller_jax(x):\n            return my_test_function_jax(jnp.sin(x))\n        out = jax2tf.convert(caller_jax, with_gradient=False)(2.0)\n        return out\n    if config.jax2tf_default_native_serialization.value:\n        self.assertIn('my_test_function_jax/mul', self.TfToHlo(run_tf))\n    else:\n        graph_def = str(tf.function(run_tf, autograph=False).get_concrete_function().graph.as_graph_def())\n        if 'my_test_function_jax/pjit_multiply_/Mul' not in graph_def:\n            self.assertIn('my_test_function_jax/jit_multiply_/Mul', graph_def)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_simple(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_simple(x):\n        return jnp.sin(x)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_simple, x, [tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 2, op_name='jax2tf(f_simple)/sin', op_type='sin')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_sub_jit(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_callee(x):\n        return jnp.cos(x)\n\n    def f_caller(x):\n        y = jnp.tanh(x)\n        z = jax.jit(f_callee)(y)\n        return jnp.sin(z)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_caller, x, [tf_test_util.OpMetadataGraph(tf_type='Tanh', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_caller)/tanh', op_type='tanh'), tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 2, op_name='jax2tf(f_caller)/jit(f_callee)/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_caller)/sin', op_type='sin')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_named(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_callee(x):\n        return jnp.cos(x)\n\n    def f_caller(x):\n        y = jnp.tanh(x)\n        z = jax.named_call(f_callee, name='callee')(y)\n        return jnp.sin(z)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_caller, x, [tf_test_util.OpMetadataGraph(tf_type='Tanh', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_caller)/tanh', op_type='tanh'), tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 2, op_name='jax2tf(f_caller)/named(callee)/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_caller)/sin', op_type='sin')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_while_and_cond(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_while_cond(x):\n\n        def body_fun(i_acc):\n            i, acc = i_acc\n            return (i + 1, jnp.cos(acc) + lax.cond(jnp.mod(i, 2) == 0, lambda acc: jnp.sin(acc), lambda acc: acc, acc))\n        _, acc = lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))\n        return acc\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_while_cond, x, [tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 5, op_name='jax2tf(f_while_cond)/while/body/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 7, op_name='jax2tf(f_while_cond)/while/body/branch_1_fun/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='FloorMod', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_while_cond)/while/body/rem', op_type='rem')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_batched_while(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    @jax.vmap\n    def f_while(x):\n\n        def body_fun(carry):\n            new_carry = jnp.sin(carry)\n            return new_carry\n        _, carry = lax.while_loop(lambda carry: jnp.all(carry <= x), body_fun, x)\n        return carry\n    shape = (3, 2)\n    x = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n    jax_comp = jax.jit(f_while).lower(x).compiler_ir('hlo')\n    backend = xb.get_backend()\n    modules = backend.compile(jax_comp).hlo_modules()\n    jax_opt_hlo = modules[0].to_string()\n    print(f'JAX OPT HLO = {jax_opt_hlo}')\n    self.CheckOpMetadata(f_while, x, [tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_while)/while/body/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='LessEqual', source_file=__file__, source_line=user_frame.start_line + 8, op_name='jax2tf(f_while)/while/body_pred/le', op_type='le')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_disabled(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n\n    def f_simple(x):\n        return jnp.sin(x)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_simple, x, [], include_xla_op_metadata=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_name_scope_polymorphic(self):\n    if config.jax2tf_default_native_serialization.value and (not config.dynamic_shapes.value):\n        self.skipTest('shape polymorphism but --jax_dynamic_shapes is not set.')\n\n    def func_jax(x, y):\n        return jnp.sin(x) + jnp.cos(y)\n    func_tf = jax2tf.convert(func_jax, polymorphic_shapes='(b,...)', with_gradient=True)\n    outer_scope = 'output_a'\n    g = tf.Graph()\n    with g.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = func_tf(x, y)\n    self.assertAllOperationStartWith(g, outer_scope)\n    g2 = tf.Graph()\n    with g2.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = tf.function(func_tf, jit_compile=True, autograph=False)(x, y)\n    self.assertAllOperationStartWith(g2, outer_scope)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_cross_platform_error(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization=True, native_serialization_platforms=('tpu',))\n    x = np.float32(0.5)\n    if jtu.test_device_matches(['tpu']):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    else:\n        f_tf_fun = tf.function(f_tf, jit_compile=True, autograph=False)\n        graph_def = f_tf_fun.get_concrete_function(x).graph.as_graph_def()\n        self.assertIn('XlaCallModule', str(graph_def))\n        with self.assertRaisesRegex(tf.errors.NotFoundError, 'The current platform .* is not among the platforms required by the module'):\n            f_tf(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='using native_serialization_platforms without native_serialization')\ndef test_native_parameters_for_non_native(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_platforms=('cpu',))\n    x = np.float32(0.5)\n    tf_cpus = tf.config.list_logical_devices('CPU')\n    self.assertNotEmpty(tf_cpus)\n    with tf.device(tf_cpus[0]):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_disabled_checks=(jax2tf.DisabledSafetyCheck.platform(),))\n    self.assertAllClose(jnp.sin(x), f_tf(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_effects_error(self):\n\n    def f_jax(x):\n        jax.debug.print('{}', x)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_jax, native_serialization=True)(np.float32(42.0))\n\n    def f_ordered_jax(x):\n        jax.debug.print('{}', x, ordered=True)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_ordered_jax, native_serialization=True)(np.float32(42.0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def fn(x: jax.Array):\n    checkify.check(jnp.all(x > 0), 'x must be positive')\n    return x + 1"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g(x):\n    y0_arr = jnp.array([[x, 0.1], [x, 0.2]])\n    t = jnp.array([0.0, 5.0])\n    y = jax.vmap(lambda y0: odeint(dx_dt, y0, t))(y0_arr)\n    return y[:, -1].sum()"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@self.sparsify\ndef func(x):\n    return jit(lambda x: jnp.sum(x, 1))(x)"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@self.sparsify\ndef func(x):\n    return jit(lambda x: jnp.sum(x, 1))(x)"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@self.sparsify\ndef func(x):\n    return jit(lambda x: jnp.sum(x, 1))(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef fn(x):\n    return x * x"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef g(x):\n    debug_print('hello: {x}', x=x)\n    return x"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g(key1, key2):\n    assert_unconsumed(key1)\n    assert_unconsumed(key2)\n    return jax.random.bits(key1)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@custom_transpose(jnp.ones(2))\ndef fn(r, x):\n    return x / r"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_jit_unused(self):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))(x, y_unused)\n    self.assertAllClose(f_jax(x, None), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=mode, mode=mode) for mode in ('eager', 'graph', 'compiled')))\ndef test_jit_unused_grad(self, mode='eager'):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_jax = f_jax(x, y_unused)\n    f_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))\n    x_tf, y_unused_tf = (tf.constant(x), tf.constant(y_unused))\n\n    def grad_tf(x, y_unused):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            tape.watch(y_unused)\n            res_tf = f_tf(x, y_unused)\n            grad_tf_x, grad_tf_y = tape.gradient(res_tf, (x, y_unused))\n        return (res_tf, grad_tf_x, grad_tf_y)\n    if mode == 'graph':\n        grad_tf = tf.function(grad_tf, autograph=False)\n    elif mode == 'compiled':\n        grad_tf = tf.function(grad_tf, autograph=False, jit_compile=True)\n    res_tf, grad_tf_x, grad_tf_y = grad_tf(x_tf, y_unused_tf)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(np.float32(2.0), grad_tf_x)\n    self.assertIsNone(grad_tf_y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef g(y):\n    return x * y"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef add(x):\n    return x * 2"
  },
  {
    "test_code": "def test_name_scope_while_loop(self):\n\n    def f(x):\n        with tf.name_scope('outer_scope'):\n\n            def condition(x):\n                return jnp.sum(x, keepdims=False) < 100\n\n            def body(x):\n                return jnp.add(x, 2.0)\n            result = jax.lax.while_loop(condition, body, x)\n            return result\n    tf_f = tf.function(jax2tf.convert(f), jit_compile=True, autograph=False)\n    g = tf_f.get_concrete_function(tf.zeros((1, 3))).graph\n    for func in g._functions.values():\n        for op in func.graph.get_operations():\n            if op.name.count(f'outer_scope/jax2tf_{f.__name__}_/while') > 1:\n                self.fail(f'tf graph has repeated name issue on when converting lax.while to tf.while.See op.name = : {op.name}')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef add(x):\n    return x * 2"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f1(y, z):\n    v = api.vmap(lambda _y: f2(_y, z))(y)\n    return jnp.sum(v)"
  },
  {
    "test_code": "def test_shared_constants_under_scan(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    xs = np.ones((8, const_size), dtype=np.float32)\n\n    def f1(xs):\n        res, _ = lax.scan(lambda carry, x: (carry + x + const, None), jnp.zeros((const_size,), dtype=np.float32), xs)\n        return res\n\n    def f2(xs):\n        return f1(xs) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), xs, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), xs, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f1(y, z):\n    v = api.vmap(lambda _y: f2(_y, z))(y)\n    return jnp.sum(v)"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(D0):\n\n    def shift(R, dR, **unused_kwargs):\n        return R + dR\n\n    def apply_fn(R):\n        return D0 * R\n    Rinit = jax.random.uniform(split, (n, 3), minval=0.0, maxval=5.0, dtype=jnp.float32)\n\n    def move(R, i):\n        F = apply_fn(R)\n        return (shift(R, 0.001 * F), jnp.array([0.0]))\n    move = remat(move)\n    R, temp = lax.scan(move, Rinit, jnp.arange(2))\n    return R[0, 0]"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(D0):\n\n    def shift(R, dR, **unused_kwargs):\n        return R + dR\n\n    def apply_fn(R):\n        return D0 * R\n    Rinit = jax.random.uniform(split, (n, 3), minval=0.0, maxval=5.0, dtype=jnp.float32)\n\n    def move(R, i):\n        F = apply_fn(R)\n        return (shift(R, 0.001 * F), jnp.array([0.0]))\n    move = remat(move)\n    R, temp = lax.scan(move, Rinit, jnp.arange(2))\n    return R[0, 0]"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(D0):\n\n    def shift(R, dR, **unused_kwargs):\n        return R + dR\n\n    def apply_fn(R):\n        return D0 * R\n    Rinit = jax.random.uniform(split, (n, 3), minval=0.0, maxval=5.0, dtype=jnp.float32)\n\n    def move(R, i):\n        F = apply_fn(R)\n        return (shift(R, 0.001 * F), jnp.array([0.0]))\n    move = remat(move)\n    R, temp = lax.scan(move, Rinit, jnp.arange(2))\n    return R[0, 0]"
  },
  {
    "test_code": "def test_sin(self):\n    f_tf = jax2tf.convert(jnp.sin)\n    x = np.float32(0.5)\n    sin_x = np.sin(x)\n    self.assertAllClose(sin_x, f_tf(x))\n    self.assertAllClose(sin_x, tf.function(f_tf, autograph=False, jit_compile=True)(x))\n    tf_preferred_device = (tf.config.list_logical_devices('TPU') + tf.config.list_logical_devices('GPU') + tf.config.list_logical_devices())[0]\n    logging.info('Running TF on %s', tf_preferred_device)\n\n    @tf.function(autograph=False, jit_compile=False)\n    def f_tf_wrapped(x):\n        with tf.device(tf_preferred_device.name):\n            return f_tf(x)\n    with tf.device(tf_preferred_device.name):\n        self.assertAllClose(sin_x, f_tf_wrapped(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_basics(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_jit(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_nested_jit(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jax.jit(jnp.cos)(x)))\n    x = 0.7\n    self.ConvertAndCompare(f_jax, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.sample_product(dtype=[np.int64, np.float64], with_function=[True, False])\ndef test_converts_64bit(self, dtype=np.int64, with_function=False):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    big_const = np.full((5,), 2 ** 33, dtype=dtype)\n    self.ConvertAndCompare(jnp.sin, big_const)\n    f_conv = jax2tf.convert(jnp.sin)\n    if with_function:\n        f_conv = tf.function(f_conv, autograph=False)\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.Variable(big_const)))\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.constant(big_const)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_64bit_behavior_enable_x64_readme(self):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float64)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_64bit_behavior_not_enable_x64_readme(self):\n    if config.enable_x64.value:\n        self.skipTest('requires not x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float32)\n    self.assertEqual(tf.math.sin(np.float64(3.14)).dtype, tf.float64)\n    self.assertEqual(jnp.sin(np.float64(3.14)).dtype, jnp.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(np.float64(3.14)).dtype, tf.float32)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_function(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_remat(self):\n\n    def f(x1):\n        x2 = jnp.sin(x1)\n        x3 = jnp.sin(x2)\n        x4 = jnp.sin(x3)\n        return x4\n    remat_f = ad_checkpoint.checkpoint(f)\n    arg = np.array(3.0)\n    f_tf = jax2tf.convert(jax.grad(remat_f))\n    f_tf_hlo = self.TfToHlo(f_tf, arg)\n    if config.remat_opt_barrier.value:\n        self.assertRegex(f_tf_hlo, 'opt-barrier')\n    else:\n        self.assertRegex(f_tf_hlo, 'transpose/jax2tf_f_/jvp/checkpoint/cond/branch_1_fun/Sin')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_checkpoint_name(self):\n\n    def f_jax(x):\n        return ad_checkpoint.checkpoint_name(jnp.sin(x), 'sin')\n    jax2tf.convert(f_jax)(1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_name_scope(self):\n\n    def run_tf():\n\n        @jax.named_call\n        def my_test_function_jax(x):\n            return x * x\n\n        def caller_jax(x):\n            return my_test_function_jax(jnp.sin(x))\n        out = jax2tf.convert(caller_jax, with_gradient=False)(2.0)\n        return out\n    if config.jax2tf_default_native_serialization.value:\n        self.assertIn('my_test_function_jax/mul', self.TfToHlo(run_tf))\n    else:\n        graph_def = str(tf.function(run_tf, autograph=False).get_concrete_function().graph.as_graph_def())\n        if 'my_test_function_jax/pjit_multiply_/Mul' not in graph_def:\n            self.assertIn('my_test_function_jax/jit_multiply_/Mul', graph_def)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_simple(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_simple(x):\n        return jnp.sin(x)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_simple, x, [tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 2, op_name='jax2tf(f_simple)/sin', op_type='sin')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_sub_jit(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_callee(x):\n        return jnp.cos(x)\n\n    def f_caller(x):\n        y = jnp.tanh(x)\n        z = jax.jit(f_callee)(y)\n        return jnp.sin(z)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_caller, x, [tf_test_util.OpMetadataGraph(tf_type='Tanh', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_caller)/tanh', op_type='tanh'), tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 2, op_name='jax2tf(f_caller)/jit(f_callee)/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_caller)/sin', op_type='sin')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_named(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_callee(x):\n        return jnp.cos(x)\n\n    def f_caller(x):\n        y = jnp.tanh(x)\n        z = jax.named_call(f_callee, name='callee')(y)\n        return jnp.sin(z)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_caller, x, [tf_test_util.OpMetadataGraph(tf_type='Tanh', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_caller)/tanh', op_type='tanh'), tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 2, op_name='jax2tf(f_caller)/named(callee)/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_caller)/sin', op_type='sin')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_while_and_cond(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_while_cond(x):\n\n        def body_fun(i_acc):\n            i, acc = i_acc\n            return (i + 1, jnp.cos(acc) + lax.cond(jnp.mod(i, 2) == 0, lambda acc: jnp.sin(acc), lambda acc: acc, acc))\n        _, acc = lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))\n        return acc\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_while_cond, x, [tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 5, op_name='jax2tf(f_while_cond)/while/body/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 7, op_name='jax2tf(f_while_cond)/while/body/branch_1_fun/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='FloorMod', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_while_cond)/while/body/rem', op_type='rem')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_batched_while(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    @jax.vmap\n    def f_while(x):\n\n        def body_fun(carry):\n            new_carry = jnp.sin(carry)\n            return new_carry\n        _, carry = lax.while_loop(lambda carry: jnp.all(carry <= x), body_fun, x)\n        return carry\n    shape = (3, 2)\n    x = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n    jax_comp = jax.jit(f_while).lower(x).compiler_ir('hlo')\n    backend = xb.get_backend()\n    modules = backend.compile(jax_comp).hlo_modules()\n    jax_opt_hlo = modules[0].to_string()\n    print(f'JAX OPT HLO = {jax_opt_hlo}')\n    self.CheckOpMetadata(f_while, x, [tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_while)/while/body/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='LessEqual', source_file=__file__, source_line=user_frame.start_line + 8, op_name='jax2tf(f_while)/while/body_pred/le', op_type='le')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_op_metadata_disabled(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n\n    def f_simple(x):\n        return jnp.sin(x)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_simple, x, [], include_xla_op_metadata=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_name_scope_polymorphic(self):\n    if config.jax2tf_default_native_serialization.value and (not config.dynamic_shapes.value):\n        self.skipTest('shape polymorphism but --jax_dynamic_shapes is not set.')\n\n    def func_jax(x, y):\n        return jnp.sin(x) + jnp.cos(y)\n    func_tf = jax2tf.convert(func_jax, polymorphic_shapes='(b,...)', with_gradient=True)\n    outer_scope = 'output_a'\n    g = tf.Graph()\n    with g.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = func_tf(x, y)\n    self.assertAllOperationStartWith(g, outer_scope)\n    g2 = tf.Graph()\n    with g2.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = tf.function(func_tf, jit_compile=True, autograph=False)(x, y)\n    self.assertAllOperationStartWith(g2, outer_scope)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_cross_platform_error(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization=True, native_serialization_platforms=('tpu',))\n    x = np.float32(0.5)\n    if jtu.test_device_matches(['tpu']):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    else:\n        f_tf_fun = tf.function(f_tf, jit_compile=True, autograph=False)\n        graph_def = f_tf_fun.get_concrete_function(x).graph.as_graph_def()\n        self.assertIn('XlaCallModule', str(graph_def))\n        with self.assertRaisesRegex(tf.errors.NotFoundError, 'The current platform .* is not among the platforms required by the module'):\n            f_tf(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='using native_serialization_platforms without native_serialization')\ndef test_native_parameters_for_non_native(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_platforms=('cpu',))\n    x = np.float32(0.5)\n    tf_cpus = tf.config.list_logical_devices('CPU')\n    self.assertNotEmpty(tf_cpus)\n    with tf.device(tf_cpus[0]):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_disabled_checks=(jax2tf.DisabledSafetyCheck.platform(),))\n    self.assertAllClose(jnp.sin(x), f_tf(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_effects_error(self):\n\n    def f_jax(x):\n        jax.debug.print('{}', x)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_jax, native_serialization=True)(np.float32(42.0))\n\n    def f_ordered_jax(x):\n        jax.debug.print('{}', x, ordered=True)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_ordered_jax, native_serialization=True)(np.float32(42.0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_ordered_dict_input(self, with_function=True):\n\n    def f(inputs):\n        out = 0.0\n        for v in inputs.values():\n            out += jnp.sum(v)\n        return out\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_type = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable([4.0], dtype=default_float_type)\n    y = tf.Variable([4.0, 5.0], dtype=default_float_type)\n    inputs = collections.OrderedDict()\n    inputs['r'] = x\n    inputs['d'] = y\n    with tf.GradientTape(persistent=True) as tape:\n        u = f_tf(inputs)\n    self.assertAllClose(np.array([1.0]), tape.gradient(u, x).numpy())\n    self.assertAllClose(np.array([1.0, 1.0]), tape.gradient(u, y).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_kwargs(self, with_function=False):\n\n    def f_jax(*, x):\n        return jnp.sum(x)\n    f_tf = jax2tf.convert(f_jax)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(f_tf(x=np.zeros(3, dtype=np.float32)), np.zeros((), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_grad_kwargs(self, with_function=False):\n    x = (np.zeros(3, dtype=np.float32), np.zeros(4, dtype=np.float32))\n\n    def f_jax(*, x=(1.0, 2.0)):\n        return jnp.sum(x[0]) + 2.0 * jnp.sum(x[1])\n    f_tf = jax2tf.convert(f_jax)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    xv = tf.nest.map_structure(tf.Variable, x)\n    with tf.GradientTape() as tape:\n        res = f_tf(x=xv)\n    grad_tf = tape.gradient(res, xv)\n    self.assertAllClose((np.full_like(x[0], fill_value=1.0), np.full_like(x[1], fill_value=2.0)), (grad_tf[0].numpy(), grad_tf[1].numpy()))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_name_scope_while_loop(self):\n\n    def f(x):\n        with tf.name_scope('outer_scope'):\n\n            def condition(x):\n                return jnp.sum(x, keepdims=False) < 100\n\n            def body(x):\n                return jnp.add(x, 2.0)\n            result = jax.lax.while_loop(condition, body, x)\n            return result\n    tf_f = tf.function(jax2tf.convert(f), jit_compile=True, autograph=False)\n    g = tf_f.get_concrete_function(tf.zeros((1, 3))).graph\n    for func in g._functions.values():\n        for op in func.graph.get_operations():\n            if op.name.count(f'outer_scope/jax2tf_{f.__name__}_/while') > 1:\n                self.fail(f'tf graph has repeated name issue on when converting lax.while to tf.while.See op.name = : {op.name}')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef sum(x):\n    return jnp.sum(x, axis=0)"
  },
  {
    "test_code": "def test_dot_algorithm(self):\n    if tf.version.VERSION.split('.') <= ['2', '18', '0']:\n        self.skipTest('Because of an XLA bug this test segfaults with TF v2.18.0')\n    if jtu.test_device_matches(['tpu']):\n        algorithm = 'BF16_BF16_F32'\n    else:\n        algorithm = 'F32_F32_F32'\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision=algorithm)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    f_tf(np.ones((128, 128), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "def test_dot_algorithm_non_native_unsupported(self):\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision='F32_F32_F32')\n    x = np.ones((128, 128), dtype=np.float32)\n    with self.assertRaisesRegex(NotImplementedError, 'Unsupported precision in dot_general'):\n        jax2tf.convert(f_jax, native_serialization=False)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef dot(x):\n    return jnp.dot(x, x)"
  },
  {
    "test_code": "def test_weak_types(self):\n    mul = jax.jit(jnp.multiply)\n    tf_fn = jax2tf.convert(lambda x: mul(x, 2.0))\n    self.assertAllClose(tf_fn(tf.constant(1.375, tf.bfloat16)).numpy(), jnp.bfloat16(2.75))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef mul(x, coeff):\n    return x * coeff"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g(z):\n    with set_xla_metadata(c='d'):\n        return z ** 2 + 1"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g(eps):\n    x = jnp.array(1.0)\n    return jax.grad(f)(x, eps)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    return x * y"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g(y, Xtree):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    return out"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g(hiddens):\n    hiddens_aug = jnp.vstack((hiddens[0], hiddens))\n    new_hiddens = hiddens_aug.copy()\n    diff = new_hiddens[:-1] - hiddens\n    diff = new_hiddens[:-1] - hiddens\n    out = jnp.trace(jnp.conj(diff).T @ diff).real\n    return jnp.array(out, dtype=jnp.complex64)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def fn(*args, split_transpose=False):\n    v, fn_transpose = jax.vjp(partial(loss, split_transpose=split_transpose), *args)\n    grads = fn_transpose(1.0)\n    return (*grads, v)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef fn(x: int, static_y: BlackBox):\n    nonlocal num_called\n    num_called += 1\n    return x + static_y.value"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g(x):\n    return f(x * 2)"
  },
  {
    "test_code": "def test_op_metadata_batched_while(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    @jax.vmap\n    def f_while(x):\n\n        def body_fun(carry):\n            new_carry = jnp.sin(carry)\n            return new_carry\n        _, carry = lax.while_loop(lambda carry: jnp.all(carry <= x), body_fun, x)\n        return carry\n    shape = (3, 2)\n    x = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n    jax_comp = jax.jit(f_while).lower(x).compiler_ir('hlo')\n    backend = xb.get_backend()\n    modules = backend.compile(jax_comp).hlo_modules()\n    jax_opt_hlo = modules[0].to_string()\n    print(f'JAX OPT HLO = {jax_opt_hlo}')\n    self.CheckOpMetadata(f_while, x, [tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_while)/while/body/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='LessEqual', source_file=__file__, source_line=user_frame.start_line + 8, op_name='jax2tf(f_while)/while/body_pred/le', op_type='le')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def lower(f):\n    \"\"\"Prints the MLIR IR that results from lowering `f`.\n\n    The arguments to `f` are taken to be arrays shaped like `prototypes`.\"\"\"\n    inputs = jax.tree.map(np.array, prototypes)\n    flat_inputs, _ = jax.tree.flatten(inputs)\n    shape_strs = ' '.join([f'{x.dtype.name}[{','.join(map(str, x.shape))}]' for x in flat_inputs])\n    name = f.func.__name__ if hasattr(f, 'func') else f.__name__\n    print(f'\\nTEST: {name} {shape_strs}')\n    print(jax.jit(f).lower(*inputs).compiler_ir())"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_dot_algorithm(self):\n    if tf.version.VERSION.split('.') <= ['2', '18', '0']:\n        self.skipTest('Because of an XLA bug this test segfaults with TF v2.18.0')\n    if jtu.test_device_matches(['tpu']):\n        algorithm = 'BF16_BF16_F32'\n    else:\n        algorithm = 'F32_F32_F32'\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision=algorithm)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    f_tf(np.ones((128, 128), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "def test_dot_algorithm_non_native_unsupported(self):\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision='F32_F32_F32')\n    x = np.ones((128, 128), dtype=np.float32)\n    with self.assertRaisesRegex(NotImplementedError, 'Unsupported precision in dot_general'):\n        jax2tf.convert(f_jax, native_serialization=False)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def dot(lhs: jnp.ndarray, rhs: jnp.ndarray, transpose_lhs: bool=False, transpose_rhs: bool=False, preferred_element_type: jnp.dtype=jnp.float32) -> jnp.ndarray:\n    lhs = jnp.transpose(lhs) if transpose_lhs else lhs\n    rhs = jnp.transpose(rhs) if transpose_rhs else rhs\n    return jax.lax.dot(lhs, rhs, preferred_element_type=preferred_element_type)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func():\n\n    def dma_kernel(x, y):\n\n        def body(dma_sem, sem):\n            pltpu.async_copy(x, y, dma_sem).wait()\n            pltpu.semaphore_signal(sem)\n            pltpu.semaphore_wait(sem)\n        pl.run_scoped(body, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.REGULAR)\n    x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n    y = pl.pallas_call(dma_kernel, out_shape=x)(x)\n    return jnp.array_equal(x, y).astype(jnp.float32)"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func():\n\n    def dma_kernel(x, y):\n\n        def body(dma_sem, sem):\n            pltpu.async_copy(x, y, dma_sem).wait()\n            pltpu.semaphore_signal(sem)\n            pltpu.semaphore_wait(sem)\n        pl.run_scoped(body, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.REGULAR)\n    x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n    y = pl.pallas_call(dma_kernel, out_shape=x)(x)\n    return jnp.array_equal(x, y).astype(jnp.float32)"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func():\n\n    def dma_kernel(x, y):\n\n        def body(dma_sem, sem):\n            pltpu.async_copy(x, y, dma_sem).wait()\n            pltpu.semaphore_signal(sem)\n            pltpu.semaphore_wait(sem)\n        pl.run_scoped(body, pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.REGULAR)\n    x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n    y = pl.pallas_call(dma_kernel, out_shape=x)(x)\n    return jnp.array_equal(x, y).astype(jnp.float32)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_dot_algorithm(self):\n    if tf.version.VERSION.split('.') <= ['2', '18', '0']:\n        self.skipTest('Because of an XLA bug this test segfaults with TF v2.18.0')\n    if jtu.test_device_matches(['tpu']):\n        algorithm = 'BF16_BF16_F32'\n    else:\n        algorithm = 'F32_F32_F32'\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision=algorithm)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    f_tf(np.ones((128, 128), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, dtype))\ndef dot(x_ref, y_ref, o_ref):\n    x = x_ref[:, :]\n    y = y_ref[:, :]\n    o_ref[:, :] = pl.dot(x, y, trans_x, trans_y).astype(o_ref.dtype)"
  },
  {
    "test_code": "def test_dot_algorithm_non_native_unsupported(self):\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision='F32_F32_F32')\n    x = np.ones((128, 128), dtype=np.float32)\n    with self.assertRaisesRegex(NotImplementedError, 'Unsupported precision in dot_general'):\n        jax2tf.convert(f_jax, native_serialization=False)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, dtype))\ndef dot(x_ref, y_ref, o_ref):\n    x = x_ref[:, :]\n    y = y_ref[:, :]\n    o_ref[:, :] = pl.dot(x, y, trans_x, trans_y).astype(o_ref.dtype)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), floatx))\ndef load(x_ref, o_ref):\n    x = pl.load(x_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]))\n    pl.store(o_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]), x + 1.0)"
  },
  {
    "test_code": "def test_sin(self):\n    f_tf = jax2tf.convert(jnp.sin)\n    x = np.float32(0.5)\n    sin_x = np.sin(x)\n    self.assertAllClose(sin_x, f_tf(x))\n    self.assertAllClose(sin_x, tf.function(f_tf, autograph=False, jit_compile=True)(x))\n    tf_preferred_device = (tf.config.list_logical_devices('TPU') + tf.config.list_logical_devices('GPU') + tf.config.list_logical_devices())[0]\n    logging.info('Running TF on %s', tf_preferred_device)\n\n    @tf.function(autograph=False, jit_compile=False)\n    def f_tf_wrapped(x):\n        with tf.device(tf_preferred_device.name):\n            return f_tf(x)\n    with tf.device(tf_preferred_device.name):\n        self.assertAllClose(sin_x, f_tf_wrapped(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_nested_jit_is_compiled(self):\n\n    def has_xla_must_compile(f_tf, x):\n        f_conc = tf.function(f_tf, autograph=True).get_concrete_function(tf.convert_to_tensor(x))\n        for n in f_conc.graph._nodes_by_id.values():\n            try:\n                n.get_attr('_XlaMustCompile')\n                return True\n            except ValueError:\n                continue\n        return False\n    x = np.array(0.7)\n    f_no_jit = lambda x: x\n    self.assertFalse(has_xla_must_compile(jax2tf.convert(f_no_jit), x))\n    f_jit = lambda x: jax.jit(jnp.sin)(x)\n    self.assertFalse(has_xla_must_compile(jax2tf.convert(f_jit), x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_bfloat16_passed_by_tf(self):\n    f_jax = lambda a, b: a + b\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([512, 512], tf.bfloat16), tf.TensorSpec([512, 512], tf.bfloat16)])\n    self.assertIsNotNone(f_tf.get_concrete_function())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_bfloat16_returned_by_jax(self):\n    f_jax = lambda a, b: (a + b).astype(jnp.bfloat16)\n    f_tf = jax2tf.convert(f_jax)\n    self.assertEqual(f_tf(1.0, 2.0).dtype, tf.bfloat16)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_bfloat16_tf_grad(self):\n    f_jax = lambda a, b: a + b\n\n    def _tf_grad(a, b):\n        with tf.GradientTape() as tape:\n            tape.watch(a)\n            result = jax2tf.convert(f_jax)(a, b)\n        return (result, tape.gradient(result, a))\n    f_tf = tf.function(_tf_grad, autograph=False, input_signature=[tf.TensorSpec([512, 512], tf.bfloat16), tf.TensorSpec([512, 512], tf.bfloat16)])\n    self.assertIsNotNone(f_tf.get_concrete_function())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(dtype=[np.int64, np.float64], with_function=[True, False])\ndef test_converts_64bit(self, dtype=np.int64, with_function=False):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    big_const = np.full((5,), 2 ** 33, dtype=dtype)\n    self.ConvertAndCompare(jnp.sin, big_const)\n    f_conv = jax2tf.convert(jnp.sin)\n    if with_function:\n        f_conv = tf.function(f_conv, autograph=False)\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.Variable(big_const)))\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.constant(big_const)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_64bit_behavior_enable_x64_readme(self):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float64)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_64bit_behavior_not_enable_x64_readme(self):\n    if config.enable_x64.value:\n        self.skipTest('requires not x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float32)\n    self.assertEqual(tf.math.sin(np.float64(3.14)).dtype, tf.float64)\n    self.assertEqual(jnp.sin(np.float64(3.14)).dtype, jnp.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(np.float64(3.14)).dtype, tf.float32)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_disabled(self, with_function=False):\n    if tf.version.VERSION.split('.') <= ['2', '17', '0']:\n        self.skipTest('This test works only with newer versions of TF')\n    f_tf = jax2tf.convert(jnp.tan, with_gradient=False)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    x = tf.ones([])\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            y = f_tf(x)\n            _ = tape.gradient(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients(self, with_function=True):\n\n    def f(x, y):\n        return (x * x, x * y)\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_type = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    y = tf.Variable(5.0, dtype=default_float_type)\n    with tf.GradientTape(persistent=True) as tape:\n        u, v = f_tf(x, y)\n    self.assertAllClose(2.0 * 4.0, tape.gradient(u, x))\n    self.assertAllClose(0.0, tape.gradient(u, y))\n    self.assertAllClose(5.0, tape.gradient(v, x))\n    self.assertAllClose(4.0, tape.gradient(v, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_higher_order_gradients(self):\n    f = lambda x: x ** 3\n    f_tf = jax2tf.convert(f)\n    x = tf.Variable(4.0, dtype=tf.float32)\n    with tf.GradientTape() as t2:\n        with tf.GradientTape() as t1:\n            y = f_tf(x)\n        dy_dx = t1.gradient(y, x)\n    d2y_dx2 = t2.gradient(dy_dx, x)\n    self.assertAllClose(np.float32(48.0), dy_dx.numpy())\n    self.assertAllClose(np.float32(24.0), d2y_dx2.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_pytree(self, with_function=False):\n\n    def f(xy: tuple[float, float]) -> dict[str, float]:\n        x, y = xy\n        return dict(one=x * x, two=x * y)\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_dtype = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable(4.0, dtype=default_float_dtype)\n    y = tf.Variable(5.0, dtype=default_float_dtype)\n    with tf.GradientTape(persistent=True) as tape:\n        uv = f_tf((x, y))\n    self.assertAllClose(2.0 * 4.0, tape.gradient(uv['one'], x))\n    self.assertAllClose(0.0, tape.gradient(uv['one'], y))\n    self.assertAllClose(5.0, tape.gradient(uv['two'], x))\n    self.assertAllClose(4.0, tape.gradient(uv['two'], y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_ordered_dict_input(self, with_function=True):\n\n    def f(inputs):\n        out = 0.0\n        for v in inputs.values():\n            out += jnp.sum(v)\n        return out\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_type = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable([4.0], dtype=default_float_type)\n    y = tf.Variable([4.0, 5.0], dtype=default_float_type)\n    inputs = collections.OrderedDict()\n    inputs['r'] = x\n    inputs['d'] = y\n    with tf.GradientTape(persistent=True) as tape:\n        u = f_tf(inputs)\n    self.assertAllClose(np.array([1.0]), tape.gradient(u, x).numpy())\n    self.assertAllClose(np.array([1.0, 1.0]), tape.gradient(u, y).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_int_argument(self, with_function=False):\n    state = dict(float_used=np.array([0.7, 0.9], dtype=np.float32), float_passthrough=np.float16(1.0), float_unused=np.array([1.1, 2.2, 3.3], dtype=np.float32), int_used=np.int16(5), int_passthrough=np.int8(7), int_unused=np.array([1, 2, 3], dtype=np.uint32), bool_used=np.array([True, False, False, True], dtype=np.bool_), bool_passthrough=np.array([True, False, False, True, False], dtype=np.bool_), bool_unused=np.array([[True, False], [False, True]], dtype=np.bool_))\n\n    def jax_f(state):\n        res = dict(state, float_used=2.0 * state['float_used'], int_used=3 * state['int_used'], bool_used=state['bool_used'] == state['bool_used'])\n        del res['float_unused']\n        del res['int_unused']\n        del res['bool_unused']\n        return res\n    args = (state,)\n    res_jax = jax_f(*args)\n    vjp_jax_fun, args_vjp = tf_test_util.TransformJaxVJP(jax_f, args, res_jax)\n    grad_jax, = vjp_jax_fun(*args_vjp)\n\n    def compare_with_overrides(*, what, expected, **expected_overrides):\n        what_keys = set(what.keys())\n        expected_keys = set(expected.keys())\n        self.assertEqual(what_keys, expected_keys)\n        for k, w in what.items():\n            e = expected[k]\n            if k in expected_overrides:\n                if expected_overrides[k] == 'ZERO':\n                    e = np.zeros_like(w)\n                elif expected_overrides[k] == 'ZERO_BOOL':\n                    e = np.zeros(np.shape(w), dtype=np.bool_)\n                elif expected_overrides[k] == 'ONE':\n                    e = np.ones_like(w)\n                else:\n                    e = expected_overrides[k]\n            if e is None:\n                self.assertIsNone(w, msg=k)\n            else:\n                self.assertIsNotNone(w, msg=k)\n            w = w.numpy() if isinstance(w, tf.Tensor) else e\n            e = e.numpy() if isinstance(e, tf.Tensor) else e\n            try:\n                self.assertAllClose(e, w, err_msg=k)\n            except:\n                print(f'Failed at {k}')\n                raise\n    _, (grad_tf_0,) = tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    compare_with_overrides(what=grad_tf_0, expected=grad_jax, float_unused='ZERO', bool_used='ZERO', bool_passthrough='ONE', bool_unused='ZERO', int_used='ZERO', int_passthrough='ONE', int_unused='ZERO')\n    _, (grad_tf_None,) = tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.NONE)\n    compare_with_overrides(what=grad_tf_None, expected=grad_tf_0, float_unused=None, int_used=None, int_unused=None, bool_used=None, bool_unused=None)\n    f_tf_jax = jax2tf.convert(jax_f)\n    if with_function:\n        f_tf_jax = tf.function(f_tf_jax, autograph=False)\n    _, (grad_tf_jax_0,) = tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args)\n    compare_with_overrides(what=grad_tf_jax_0, expected=grad_tf_0, int_passthrough='ZERO', bool_passthrough='ZERO')\n    _, (grad_tf_jax_None,) = tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args, unconnected_gradients=tf.UnconnectedGradients.NONE)\n    compare_with_overrides(what=grad_tf_jax_None, expected=grad_tf_0, int_used=None, int_passthrough=None, int_unused=None, bool_unused=None, bool_used=None, bool_passthrough=None)\n    tf_vjp_jax_fun = jax2tf.convert(vjp_jax_fun)\n    grad_tf_vjp_jax, = tf_vjp_jax_fun(*args_vjp)\n    compare_with_overrides(what=grad_tf_vjp_jax, expected=grad_tf_0, bool_passthrough='ZERO_BOOL', bool_unused='ZERO_BOOL', bool_used='ZERO_BOOL', int_passthrough='ZERO_BOOL', int_unused='ZERO_BOOL', int_used='ZERO_BOOL')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_readme_gradient_int(self):\n    x = np.array(2, dtype=np.int16)\n\n    def f_jax(x):\n        return x.astype(np.float32) * 2.0\n    print(jax.grad(f_jax, allow_int=True)(x))\n    print(jax2tf.convert(jax.grad(f_jax, allow_int=True))(x))\n\n    def f_tf(x):\n        return tf.cast(x, tf.float32) * 2.0\n    xv = tf.Variable(x)\n    with tf.GradientTape(persistent=True) as tape:\n        print(tape.gradient(f_tf(xv), xv))\n        print(tape.gradient(f_tf(xv), xv, unconnected_gradients=tf.UnconnectedGradients.ZERO))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_convert_argument_non_callable_error(self):\n    with self.assertRaisesRegex(TypeError, 'Expected a callable value'):\n        jax2tf.convert(5.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_convert_argument_non_tensor_error(self):\n    with self.assertRaisesRegex(TypeError, 'Argument.*is not a valid JAX type'):\n        jax2tf.convert(lambda x: x)(lambda y: y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_argument_eager_tensor(self):\n    x = jax2tf.convert(jnp.sin)(1.0)\n    jax2tf.convert(jnp.cos)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@unittest.skip('Test fails at head')\ndef test_issue_10586(self):\n\n    class JaxModule(tf.Module):\n\n        def __init__(self):\n            self._params = {'w': tf.Variable(tf.ones([784, 10]), name='w'), 'b': tf.Variable(tf.ones([10]), name='b')}\n\n        def __call__(self, x):\n            return jax2tf.convert(lambda p, x: x @ p['w'] + p['b'])(self._params, x)\n    net = JaxModule()\n    images = tf.ones([1, 784])\n    with tf.GradientTape() as tape:\n        loss = tf.reduce_sum(net(images))\n    params = tape.watched_variables()\n    grads = tape.gradient(loss, params)\n    for var, grad in zip(params, grads):\n        self.assertEqual(var.shape, grad.shape, msg=var.name)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_remat(self):\n\n    def f(x1):\n        x2 = jnp.sin(x1)\n        x3 = jnp.sin(x2)\n        x4 = jnp.sin(x3)\n        return x4\n    remat_f = ad_checkpoint.checkpoint(f)\n    arg = np.array(3.0)\n    f_tf = jax2tf.convert(jax.grad(remat_f))\n    f_tf_hlo = self.TfToHlo(f_tf, arg)\n    if config.remat_opt_barrier.value:\n        self.assertRegex(f_tf_hlo, 'opt-barrier')\n    else:\n        self.assertRegex(f_tf_hlo, 'transpose/jax2tf_f_/jvp/checkpoint/cond/branch_1_fun/Sin')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_checkpoint_name(self):\n\n    def f_jax(x):\n        return ad_checkpoint.checkpoint_name(jnp.sin(x), 'sin')\n    jax2tf.convert(f_jax)(1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_convert_of_nested_independent_jit(self):\n\n    def func(x):\n\n        def inner1(y):\n            return x + y\n        return jax.jit(inner1)(1.0)\n    jax2tf.convert(func)(2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_convert_of_nested_dependent_jit(self):\n\n    def func(x):\n\n        def inner1(y):\n            return x + y\n        return jax.jit(inner1)(x)\n    jax2tf.convert(func)(2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_jit_unused(self):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))(x, y_unused)\n    self.assertAllClose(f_jax(x, None), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=mode, mode=mode) for mode in ('eager', 'graph', 'compiled')))\ndef test_jit_unused_grad(self, mode='eager'):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_jax = f_jax(x, y_unused)\n    f_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))\n    x_tf, y_unused_tf = (tf.constant(x), tf.constant(y_unused))\n\n    def grad_tf(x, y_unused):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            tape.watch(y_unused)\n            res_tf = f_tf(x, y_unused)\n            grad_tf_x, grad_tf_y = tape.gradient(res_tf, (x, y_unused))\n        return (res_tf, grad_tf_x, grad_tf_y)\n    if mode == 'graph':\n        grad_tf = tf.function(grad_tf, autograph=False)\n    elif mode == 'compiled':\n        grad_tf = tf.function(grad_tf, autograph=False, jit_compile=True)\n    res_tf, grad_tf_x, grad_tf_y = grad_tf(x_tf, y_unused_tf)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(np.float32(2.0), grad_tf_x)\n    self.assertIsNone(grad_tf_y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_nested_convert_error(self):\n\n    def outer(y):\n        return jax2tf.convert(jnp.sin)(y)\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        jax2tf.convert(outer)(np.ones((4,), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_nested_convert_error_non_tracer(self):\n    \"\"\"The inner convert takes non-tracer arguments\"\"\"\n\n    def outer(y):\n        sin_1 = jax2tf.convert(jnp.sin)(1.0)\n        return y + sin_1\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        jax2tf.convert(outer)(2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(transform=['jit', 'jvp', 'grad', 'vmap'])\ndef test_convert_under_transform_error(self, transform='vmap'):\n\n    def outer(y):\n        return jax2tf.convert(jnp.sin)(y)\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        self.TransformConvertAndCompare(outer, np.ones((4,)), transform)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(transform=['jit', 'jvp', 'grad', 'vmap'])\ndef test_convert_under_transform_error_non_tracer(self, transform='vmap'):\n\n    def outer(y):\n        sin_1 = jax2tf.convert(jnp.sin)(1.0)\n        return y + sin_1\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        self.TransformConvertAndCompare(outer, np.ones((4,)), transform)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_name_scope(self):\n\n    def run_tf():\n\n        @jax.named_call\n        def my_test_function_jax(x):\n            return x * x\n\n        def caller_jax(x):\n            return my_test_function_jax(jnp.sin(x))\n        out = jax2tf.convert(caller_jax, with_gradient=False)(2.0)\n        return out\n    if config.jax2tf_default_native_serialization.value:\n        self.assertIn('my_test_function_jax/mul', self.TfToHlo(run_tf))\n    else:\n        graph_def = str(tf.function(run_tf, autograph=False).get_concrete_function().graph.as_graph_def())\n        if 'my_test_function_jax/pjit_multiply_/Mul' not in graph_def:\n            self.assertIn('my_test_function_jax/jit_multiply_/Mul', graph_def)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_bfloat16_constant(self):\n\n    def jax_fn_scalar(x):\n        x = x.astype(jnp.bfloat16)\n        x *= 2.0\n        return x\n\n    def jax_fn_array(x):\n        x = x.astype(jnp.bfloat16)\n        x *= np.array([1.5, 2.5, 3.5], jnp.bfloat16)\n        return x\n    tf_fn_scalar = jax2tf.convert(jax_fn_scalar)\n    self.assertAllClose(tf_fn_scalar(1.375).numpy(), jnp.bfloat16(2.75))\n    tf_fn_array = jax2tf.convert(jax_fn_array)\n    self.assertAllClose(tf_fn_array(np.array([3, 4, 5])), np.array([4.5, 10, 17.5], jnp.bfloat16))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_shared_constants(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const = np.random.uniform(size=256).astype(np.float32)\n\n    def f(x):\n        return x + const + const + const + const\n    f_tf_consts = self.FindLargeTfConstants(jax2tf.convert(f), const)\n    self.assertLen(f_tf_consts, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_shared_constants_under_scan(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    xs = np.ones((8, const_size), dtype=np.float32)\n\n    def f1(xs):\n        res, _ = lax.scan(lambda carry, x: (carry + x + const, None), jnp.zeros((const_size,), dtype=np.float32), xs)\n        return res\n\n    def f2(xs):\n        return f1(xs) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), xs, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), xs, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_shared_constants_under_jit(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const = np.random.uniform(size=(16, 16)).astype(np.float32)\n\n    @jax.jit\n    def g_jit(x):\n        return x * const\n\n    def f(x):\n        return g_jit(x) + const + const\n    f_tf_graph_consts = self.FindLargeTfConstants(jax2tf.convert(f), const)\n    self.assertLen(f_tf_graph_consts, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_weak_types(self):\n    mul = jax.jit(jnp.multiply)\n    tf_fn = jax2tf.convert(lambda x: mul(x, 2.0))\n    self.assertAllClose(tf_fn(tf.constant(1.375, tf.bfloat16)).numpy(), jnp.bfloat16(2.75))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_kwargs(self, with_function=False):\n\n    def f_jax(*, x):\n        return jnp.sum(x)\n    f_tf = jax2tf.convert(f_jax)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(f_tf(x=np.zeros(3, dtype=np.float32)), np.zeros((), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_grad_kwargs(self, with_function=False):\n    x = (np.zeros(3, dtype=np.float32), np.zeros(4, dtype=np.float32))\n\n    def f_jax(*, x=(1.0, 2.0)):\n        return jnp.sum(x[0]) + 2.0 * jnp.sum(x[1])\n    f_tf = jax2tf.convert(f_jax)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    xv = tf.nest.map_structure(tf.Variable, x)\n    with tf.GradientTape() as tape:\n        res = f_tf(x=xv)\n    grad_tf = tape.gradient(res, xv)\n    self.assertAllClose((np.full_like(x[0], fill_value=1.0), np.full_like(x[1], fill_value=2.0)), (grad_tf[0].numpy(), grad_tf[1].numpy()))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_name_scope_polymorphic(self):\n    if config.jax2tf_default_native_serialization.value and (not config.dynamic_shapes.value):\n        self.skipTest('shape polymorphism but --jax_dynamic_shapes is not set.')\n\n    def func_jax(x, y):\n        return jnp.sin(x) + jnp.cos(y)\n    func_tf = jax2tf.convert(func_jax, polymorphic_shapes='(b,...)', with_gradient=True)\n    outer_scope = 'output_a'\n    g = tf.Graph()\n    with g.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = func_tf(x, y)\n    self.assertAllOperationStartWith(g, outer_scope)\n    g2 = tf.Graph()\n    with g2.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = tf.function(func_tf, jit_compile=True, autograph=False)(x, y)\n    self.assertAllOperationStartWith(g2, outer_scope)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_name_scope_cond(self):\n\n    def f(x):\n\n        def f_pos(x):\n            with jax.named_scope('jax_f_pos'):\n                return lax.cond(x < 1.0, jnp.cos, jnp.sin, x)\n        with jax.named_scope('jax_f_outer'):\n            return lax.cond(x > 0.0, f_pos, lambda x: x, x)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_forward():\n        with tf.name_scope('tf_outer_forward'):\n            x = 0.5\n            f_tf = jax2tf.convert(f)\n            _ = f_tf(x)\n    g = outer_forward.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_forward')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_forward/jax2tf_f_/jax_f_outer')\n    x = tf.Variable(0.5, name='tf_outer_back/x')\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_back():\n        with tf.name_scope('tf_outer_back'):\n            f_tf = jax2tf.convert(f)\n            with tf.GradientTape() as tape:\n                res_tf = f_tf(x)\n                _ = tape.gradient(res_tf, x)\n    g = outer_back.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_back')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_back')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_name_scope_while_loop(self):\n\n    def f(x):\n        with tf.name_scope('outer_scope'):\n\n            def condition(x):\n                return jnp.sum(x, keepdims=False) < 100\n\n            def body(x):\n                return jnp.add(x, 2.0)\n            result = jax.lax.while_loop(condition, body, x)\n            return result\n    tf_f = tf.function(jax2tf.convert(f), jit_compile=True, autograph=False)\n    g = tf_f.get_concrete_function(tf.zeros((1, 3))).graph\n    for func in g._functions.values():\n        for op in func.graph.get_operations():\n            if op.name.count(f'outer_scope/jax2tf_{f.__name__}_/while') > 1:\n                self.fail(f'tf graph has repeated name issue on when converting lax.while to tf.while.See op.name = : {op.name}')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'{('with_mesh_' if with_mesh else '')}2={(transform2 if transform2 != 'none' else '')}_1={(transform1 if transform1 != 'none' else '')}{('_nullary' if nullary else '')}', with_mesh=with_mesh, transform1=transform1, transform2=transform2, nullary=nullary) for transform1 in ['none', 'jit', 'pjit', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding', 'shard_map', 'pmap'] for transform2 in ['none', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] for nullary in ([True, False] if transform2 == 'none' else [False]) for with_mesh in ([True] if transform1 not in ['base', 'jit', 'pjit'] or transform2 != 'none' else [False, True])))\ndef test_cross_platform(self, with_mesh=True, transform1='pjit_in_shardings_P', transform2='pjit_in_shardings_P', nullary=False):\n    if transform2 == 'none' and (transform1 == 'shard_map' or (transform1 in ['pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] and nullary)):\n        raise unittest.SkipTest('Skip because must have pjit at top level')\n    x = np.ones((4, 6), dtype=np.float32)\n    mesh = sharding.Mesh(jax.devices()[:1], ('a',))\n    func = lambda x: lax.cummax(x, axis=0, reverse=False)\n    func_shard_map = lambda x: lax.all_gather(x, 'a', axis=1, tiled=True)\n\n    def apply_transform(func, transform: str):\n        transformed_func = dict(none=func, jit=jax.jit(func), jit_in_shardings_None=jax.jit(func, in_shardings=None), jit_in_shardings_P=jax.jit(func, in_shardings=(P('a'),)), jit_in_shardings_Sharding=jax.jit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),)), pjit=pjit.pjit(func), pjit_in_shardings_None=pjit.pjit(func, in_shardings=None, out_shardings=None), pjit_in_shardings_P=pjit.pjit(func, in_shardings=(P('a'),), out_shardings=P('a')), pjit_in_shardings_Sharding=pjit.pjit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),), out_shardings=sharding.NamedSharding(mesh, P('a'))), shard_map=shard_map(func, mesh, in_specs=(P('a', None),), out_specs=P('a', None)), pmap=jax.pmap(func, in_axes=0, out_axes=0))[transform]\n        return transformed_func\n    transformed1_func = apply_transform(func_shard_map if transform1 == 'shard_map' else func, transform1)\n    assert transform2 not in ['shard_map']\n    transformed2_func = apply_transform(transformed1_func, transform2)\n    if transform1 == 'pmap':\n        x = x.reshape((1, -1))\n    if not nullary:\n        func_to_convert = transformed2_func\n        args = [x]\n    else:\n        func_to_convert = lambda: transformed2_func(jnp.ones(x.shape, dtype=x.dtype))\n        args = []\n    if transform1 == 'pmap':\n        if nullary:\n            raise unittest.SkipTest('Cannot lower nested pmap: jit-of-pmap warning')\n        raise unittest.SkipTest('TODO: figure out how to invoke pmap from TF')\n    f_tf = jax2tf.convert(func_to_convert, native_serialization=True, native_serialization_platforms=('tpu',))\n    f_tf = tf.function(f_tf, jit_compile=True, autograph=False)\n    with contextlib.ExitStack() as stack:\n        if with_mesh:\n            stack.enter_context(mesh)\n        _ = func_to_convert(*args)\n        exported = export.export(jax.jit(func_to_convert) if not hasattr(func_to_convert, 'trace') else func_to_convert, platforms=('tpu',))(*(core.ShapedArray(a.shape, a.dtype) for a in args))\n    if transform1 == 'shard_map':\n        self.assertIn('stablehlo.all_gather', str(exported.mlir_module()))\n    else:\n        self.assertIn('stablehlo.reduce_window', str(exported.mlir_module()))",
    "assertions": [
      "assert transform2 not in ['shard_map']"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_cross_platform_error(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization=True, native_serialization_platforms=('tpu',))\n    x = np.float32(0.5)\n    if jtu.test_device_matches(['tpu']):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    else:\n        f_tf_fun = tf.function(f_tf, jit_compile=True, autograph=False)\n        graph_def = f_tf_fun.get_concrete_function(x).graph.as_graph_def()\n        self.assertIn('XlaCallModule', str(graph_def))\n        with self.assertRaisesRegex(tf.errors.NotFoundError, 'The current platform .* is not among the platforms required by the module'):\n            f_tf(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='using native_serialization_platforms without native_serialization')\ndef test_native_parameters_for_non_native(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_platforms=('cpu',))\n    x = np.float32(0.5)\n    tf_cpus = tf.config.list_logical_devices('CPU')\n    self.assertNotEmpty(tf_cpus)\n    with tf.device(tf_cpus[0]):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_disabled_checks=(jax2tf.DisabledSafetyCheck.platform(),))\n    self.assertAllClose(jnp.sin(x), f_tf(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_native_serialization_grad(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization=True, native_serialization_platforms=('tpu',))\n    x = np.arange(4, dtype=np.float32)\n    x_v = tf.Variable(x)\n\n    @tf.function(autograph=False)\n    def f_grad_tf(x_v):\n        with tf.GradientTape() as tape:\n            tape.watch(x_v)\n            res_tf = f_tf(x_v)\n            return tape.gradient(res_tf, x_v)\n    f_grad_tf_fun = tf.function(f_grad_tf, autograph=False)\n    graph_def = f_grad_tf_fun.get_concrete_function(x).graph.as_graph_def()\n    logging.info('Found graph_def: %s', graph_def)\n    self.assertLen(re.findall('op:\\\\s*\"XlaCallModule\"', str(graph_def)), 2)\n    if not jtu.test_device_matches(['tpu']):\n        with self.assertRaisesRegex(tf.errors.NotFoundError, 'The current platform .* is not among the platforms required by the module: \\\\[TPU\\\\]'):\n            f_grad_tf(x_v)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_effects_error(self):\n\n    def f_jax(x):\n        jax.debug.print('{}', x)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_jax, native_serialization=True)(np.float32(42.0))\n\n    def f_ordered_jax(x):\n        jax.debug.print('{}', x, ordered=True)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_ordered_jax, native_serialization=True)(np.float32(42.0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_multi_platform(self):\n    if config.enable_x64.value:\n        self.skipTest('TODO: enable when we can handle i64 platform_index_argument')\n    _testing_multi_platform_to_add = dict(cpu=2.0, tpu=3.0, cuda=4.0, rocm=5.0)\n\n    def f_jax(x):\n        return x + lax.platform_dependent(tpu=lambda: _testing_multi_platform_to_add['tpu'], cuda=lambda: _testing_multi_platform_to_add['cuda'], rocm=lambda: _testing_multi_platform_to_add['rocm'], default=lambda: _testing_multi_platform_to_add['cpu'])\n    x = np.float32(0.42)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=('cpu', 'cuda', 'tpu'))\n    for tf_device in self.tf_devices:\n        logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n        with tf.device(tf_device):\n            res = f_tf(x)\n        tf_device_jax_platform = dict(CPU='cpu', GPU='cuda', TPU='tpu')[tf_device.device_type]\n        self.assertAllClose(res, x + _testing_multi_platform_to_add[tf_device_jax_platform])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_dot_algorithm(self):\n    if tf.version.VERSION.split('.') <= ['2', '18', '0']:\n        self.skipTest('Because of an XLA bug this test segfaults with TF v2.18.0')\n    if jtu.test_device_matches(['tpu']):\n        algorithm = 'BF16_BF16_F32'\n    else:\n        algorithm = 'F32_F32_F32'\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision=algorithm)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    f_tf(np.ones((128, 128), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_dot_algorithm_non_native_unsupported(self):\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision='F32_F32_F32')\n    x = np.ones((128, 128), dtype=np.float32)\n    with self.assertRaisesRegex(NotImplementedError, 'Unsupported precision in dot_general'):\n        jax2tf.convert(f_jax, native_serialization=False)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), intx), input_output_aliases={0: 0}, grid=())\ndef add(x_ref, o_ref):\n    o_ref[()] = x_ref[()] + 1"
  },
  {
    "test_code": "def test_name_scope_while_loop(self):\n\n    def f(x):\n        with tf.name_scope('outer_scope'):\n\n            def condition(x):\n                return jnp.sum(x, keepdims=False) < 100\n\n            def body(x):\n                return jnp.add(x, 2.0)\n            result = jax.lax.while_loop(condition, body, x)\n            return result\n    tf_f = tf.function(jax2tf.convert(f), jit_compile=True, autograph=False)\n    g = tf_f.get_concrete_function(tf.zeros((1, 3))).graph\n    for func in g._functions.values():\n        for op in func.graph.get_operations():\n            if op.name.count(f'outer_scope/jax2tf_{f.__name__}_/while') > 1:\n                self.fail(f'tf graph has repeated name issue on when converting lax.while to tf.while.See op.name = : {op.name}')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), intx), input_output_aliases={0: 0}, grid=())\ndef add(x_ref, o_ref):\n    o_ref[()] = x_ref[()] + 1"
  },
  {
    "test_code": "def test_sin(self):\n    f_tf = jax2tf.convert(jnp.sin)\n    x = np.float32(0.5)\n    sin_x = np.sin(x)\n    self.assertAllClose(sin_x, f_tf(x))\n    self.assertAllClose(sin_x, tf.function(f_tf, autograph=False, jit_compile=True)(x))\n    tf_preferred_device = (tf.config.list_logical_devices('TPU') + tf.config.list_logical_devices('GPU') + tf.config.list_logical_devices())[0]\n    logging.info('Running TF on %s', tf_preferred_device)\n\n    @tf.function(autograph=False, jit_compile=False)\n    def f_tf_wrapped(x):\n        with tf.device(tf_preferred_device.name):\n            return f_tf(x)\n    with tf.device(tf_preferred_device.name):\n        self.assertAllClose(sin_x, f_tf_wrapped(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_basics(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_jit(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_nested_jit(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jax.jit(jnp.cos)(x)))\n    x = 0.7\n    self.ConvertAndCompare(f_jax, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@jtu.sample_product(dtype=[np.int64, np.float64], with_function=[True, False])\ndef test_converts_64bit(self, dtype=np.int64, with_function=False):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    big_const = np.full((5,), 2 ** 33, dtype=dtype)\n    self.ConvertAndCompare(jnp.sin, big_const)\n    f_conv = jax2tf.convert(jnp.sin)\n    if with_function:\n        f_conv = tf.function(f_conv, autograph=False)\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.Variable(big_const)))\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.constant(big_const)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_64bit_behavior_enable_x64_readme(self):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float64)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_64bit_behavior_not_enable_x64_readme(self):\n    if config.enable_x64.value:\n        self.skipTest('requires not x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float32)\n    self.assertEqual(tf.math.sin(np.float64(3.14)).dtype, tf.float64)\n    self.assertEqual(jnp.sin(np.float64(3.14)).dtype, jnp.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(np.float64(3.14)).dtype, tf.float32)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_function(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_remat(self):\n\n    def f(x1):\n        x2 = jnp.sin(x1)\n        x3 = jnp.sin(x2)\n        x4 = jnp.sin(x3)\n        return x4\n    remat_f = ad_checkpoint.checkpoint(f)\n    arg = np.array(3.0)\n    f_tf = jax2tf.convert(jax.grad(remat_f))\n    f_tf_hlo = self.TfToHlo(f_tf, arg)\n    if config.remat_opt_barrier.value:\n        self.assertRegex(f_tf_hlo, 'opt-barrier')\n    else:\n        self.assertRegex(f_tf_hlo, 'transpose/jax2tf_f_/jvp/checkpoint/cond/branch_1_fun/Sin')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_checkpoint_name(self):\n\n    def f_jax(x):\n        return ad_checkpoint.checkpoint_name(jnp.sin(x), 'sin')\n    jax2tf.convert(f_jax)(1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_name_scope(self):\n\n    def run_tf():\n\n        @jax.named_call\n        def my_test_function_jax(x):\n            return x * x\n\n        def caller_jax(x):\n            return my_test_function_jax(jnp.sin(x))\n        out = jax2tf.convert(caller_jax, with_gradient=False)(2.0)\n        return out\n    if config.jax2tf_default_native_serialization.value:\n        self.assertIn('my_test_function_jax/mul', self.TfToHlo(run_tf))\n    else:\n        graph_def = str(tf.function(run_tf, autograph=False).get_concrete_function().graph.as_graph_def())\n        if 'my_test_function_jax/pjit_multiply_/Mul' not in graph_def:\n            self.assertIn('my_test_function_jax/jit_multiply_/Mul', graph_def)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_op_metadata_simple(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_simple(x):\n        return jnp.sin(x)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_simple, x, [tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 2, op_name='jax2tf(f_simple)/sin', op_type='sin')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_op_metadata_sub_jit(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_callee(x):\n        return jnp.cos(x)\n\n    def f_caller(x):\n        y = jnp.tanh(x)\n        z = jax.jit(f_callee)(y)\n        return jnp.sin(z)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_caller, x, [tf_test_util.OpMetadataGraph(tf_type='Tanh', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_caller)/tanh', op_type='tanh'), tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 2, op_name='jax2tf(f_caller)/jit(f_callee)/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_caller)/sin', op_type='sin')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_op_metadata_named(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_callee(x):\n        return jnp.cos(x)\n\n    def f_caller(x):\n        y = jnp.tanh(x)\n        z = jax.named_call(f_callee, name='callee')(y)\n        return jnp.sin(z)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_caller, x, [tf_test_util.OpMetadataGraph(tf_type='Tanh', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_caller)/tanh', op_type='tanh'), tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 2, op_name='jax2tf(f_caller)/named(callee)/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_caller)/sin', op_type='sin')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_op_metadata_while_and_cond(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_while_cond(x):\n\n        def body_fun(i_acc):\n            i, acc = i_acc\n            return (i + 1, jnp.cos(acc) + lax.cond(jnp.mod(i, 2) == 0, lambda acc: jnp.sin(acc), lambda acc: acc, acc))\n        _, acc = lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))\n        return acc\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_while_cond, x, [tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 5, op_name='jax2tf(f_while_cond)/while/body/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 7, op_name='jax2tf(f_while_cond)/while/body/branch_1_fun/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='FloorMod', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_while_cond)/while/body/rem', op_type='rem')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_op_metadata_batched_while(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    @jax.vmap\n    def f_while(x):\n\n        def body_fun(carry):\n            new_carry = jnp.sin(carry)\n            return new_carry\n        _, carry = lax.while_loop(lambda carry: jnp.all(carry <= x), body_fun, x)\n        return carry\n    shape = (3, 2)\n    x = np.arange(math.prod(shape), dtype=np.float32).reshape(shape)\n    jax_comp = jax.jit(f_while).lower(x).compiler_ir('hlo')\n    backend = xb.get_backend()\n    modules = backend.compile(jax_comp).hlo_modules()\n    jax_opt_hlo = modules[0].to_string()\n    print(f'JAX OPT HLO = {jax_opt_hlo}')\n    self.CheckOpMetadata(f_while, x, [tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 4, op_name='jax2tf(f_while)/while/body/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='LessEqual', source_file=__file__, source_line=user_frame.start_line + 8, op_name='jax2tf(f_while)/while/body_pred/le', op_type='le')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_op_metadata_disabled(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n\n    def f_simple(x):\n        return jnp.sin(x)\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_simple, x, [], include_xla_op_metadata=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_name_scope_polymorphic(self):\n    if config.jax2tf_default_native_serialization.value and (not config.dynamic_shapes.value):\n        self.skipTest('shape polymorphism but --jax_dynamic_shapes is not set.')\n\n    def func_jax(x, y):\n        return jnp.sin(x) + jnp.cos(y)\n    func_tf = jax2tf.convert(func_jax, polymorphic_shapes='(b,...)', with_gradient=True)\n    outer_scope = 'output_a'\n    g = tf.Graph()\n    with g.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = func_tf(x, y)\n    self.assertAllOperationStartWith(g, outer_scope)\n    g2 = tf.Graph()\n    with g2.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = tf.function(func_tf, jit_compile=True, autograph=False)(x, y)\n    self.assertAllOperationStartWith(g2, outer_scope)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_cross_platform_error(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization=True, native_serialization_platforms=('tpu',))\n    x = np.float32(0.5)\n    if jtu.test_device_matches(['tpu']):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    else:\n        f_tf_fun = tf.function(f_tf, jit_compile=True, autograph=False)\n        graph_def = f_tf_fun.get_concrete_function(x).graph.as_graph_def()\n        self.assertIn('XlaCallModule', str(graph_def))\n        with self.assertRaisesRegex(tf.errors.NotFoundError, 'The current platform .* is not among the platforms required by the module'):\n            f_tf(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@jtu.ignore_warning(message='using native_serialization_platforms without native_serialization')\ndef test_native_parameters_for_non_native(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_platforms=('cpu',))\n    x = np.float32(0.5)\n    tf_cpus = tf.config.list_logical_devices('CPU')\n    self.assertNotEmpty(tf_cpus)\n    with tf.device(tf_cpus[0]):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_disabled_checks=(jax2tf.DisabledSafetyCheck.platform(),))\n    self.assertAllClose(jnp.sin(x), f_tf(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_effects_error(self):\n\n    def f_jax(x):\n        jax.debug.print('{}', x)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_jax, native_serialization=True)(np.float32(42.0))\n\n    def f_ordered_jax(x):\n        jax.debug.print('{}', x, ordered=True)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_ordered_jax, native_serialization=True)(np.float32(42.0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_sin(self):\n    f_tf = jax2tf.convert(jnp.sin)\n    x = np.float32(0.5)\n    sin_x = np.sin(x)\n    self.assertAllClose(sin_x, f_tf(x))\n    self.assertAllClose(sin_x, tf.function(f_tf, autograph=False, jit_compile=True)(x))\n    tf_preferred_device = (tf.config.list_logical_devices('TPU') + tf.config.list_logical_devices('GPU') + tf.config.list_logical_devices())[0]\n    logging.info('Running TF on %s', tf_preferred_device)\n\n    @tf.function(autograph=False, jit_compile=False)\n    def f_tf_wrapped(x):\n        with tf.device(tf_preferred_device.name):\n            return f_tf(x)\n    with tf.device(tf_preferred_device.name):\n        self.assertAllClose(sin_x, f_tf_wrapped(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_nested_jit_is_compiled(self):\n\n    def has_xla_must_compile(f_tf, x):\n        f_conc = tf.function(f_tf, autograph=True).get_concrete_function(tf.convert_to_tensor(x))\n        for n in f_conc.graph._nodes_by_id.values():\n            try:\n                n.get_attr('_XlaMustCompile')\n                return True\n            except ValueError:\n                continue\n        return False\n    x = np.array(0.7)\n    f_no_jit = lambda x: x\n    self.assertFalse(has_xla_must_compile(jax2tf.convert(f_no_jit), x))\n    f_jit = lambda x: jax.jit(jnp.sin)(x)\n    self.assertFalse(has_xla_must_compile(jax2tf.convert(f_jit), x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_bfloat16_passed_by_tf(self):\n    f_jax = lambda a, b: a + b\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([512, 512], tf.bfloat16), tf.TensorSpec([512, 512], tf.bfloat16)])\n    self.assertIsNotNone(f_tf.get_concrete_function())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_bfloat16_returned_by_jax(self):\n    f_jax = lambda a, b: (a + b).astype(jnp.bfloat16)\n    f_tf = jax2tf.convert(f_jax)\n    self.assertEqual(f_tf(1.0, 2.0).dtype, tf.bfloat16)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_bfloat16_tf_grad(self):\n    f_jax = lambda a, b: a + b\n\n    def _tf_grad(a, b):\n        with tf.GradientTape() as tape:\n            tape.watch(a)\n            result = jax2tf.convert(f_jax)(a, b)\n        return (result, tape.gradient(result, a))\n    f_tf = tf.function(_tf_grad, autograph=False, input_signature=[tf.TensorSpec([512, 512], tf.bfloat16), tf.TensorSpec([512, 512], tf.bfloat16)])\n    self.assertIsNotNone(f_tf.get_concrete_function())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(dtype=[np.int64, np.float64], with_function=[True, False])\ndef test_converts_64bit(self, dtype=np.int64, with_function=False):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    big_const = np.full((5,), 2 ** 33, dtype=dtype)\n    self.ConvertAndCompare(jnp.sin, big_const)\n    f_conv = jax2tf.convert(jnp.sin)\n    if with_function:\n        f_conv = tf.function(f_conv, autograph=False)\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.Variable(big_const)))\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.constant(big_const)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_64bit_behavior_enable_x64_readme(self):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float64)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_64bit_behavior_not_enable_x64_readme(self):\n    if config.enable_x64.value:\n        self.skipTest('requires not x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float32)\n    self.assertEqual(tf.math.sin(np.float64(3.14)).dtype, tf.float64)\n    self.assertEqual(jnp.sin(np.float64(3.14)).dtype, jnp.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(np.float64(3.14)).dtype, tf.float32)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_disabled(self, with_function=False):\n    if tf.version.VERSION.split('.') <= ['2', '17', '0']:\n        self.skipTest('This test works only with newer versions of TF')\n    f_tf = jax2tf.convert(jnp.tan, with_gradient=False)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    x = tf.ones([])\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            y = f_tf(x)\n            _ = tape.gradient(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients(self, with_function=True):\n\n    def f(x, y):\n        return (x * x, x * y)\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_type = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    y = tf.Variable(5.0, dtype=default_float_type)\n    with tf.GradientTape(persistent=True) as tape:\n        u, v = f_tf(x, y)\n    self.assertAllClose(2.0 * 4.0, tape.gradient(u, x))\n    self.assertAllClose(0.0, tape.gradient(u, y))\n    self.assertAllClose(5.0, tape.gradient(v, x))\n    self.assertAllClose(4.0, tape.gradient(v, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_higher_order_gradients(self):\n    f = lambda x: x ** 3\n    f_tf = jax2tf.convert(f)\n    x = tf.Variable(4.0, dtype=tf.float32)\n    with tf.GradientTape() as t2:\n        with tf.GradientTape() as t1:\n            y = f_tf(x)\n        dy_dx = t1.gradient(y, x)\n    d2y_dx2 = t2.gradient(dy_dx, x)\n    self.assertAllClose(np.float32(48.0), dy_dx.numpy())\n    self.assertAllClose(np.float32(24.0), d2y_dx2.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_pytree(self, with_function=False):\n\n    def f(xy: tuple[float, float]) -> dict[str, float]:\n        x, y = xy\n        return dict(one=x * x, two=x * y)\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_dtype = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable(4.0, dtype=default_float_dtype)\n    y = tf.Variable(5.0, dtype=default_float_dtype)\n    with tf.GradientTape(persistent=True) as tape:\n        uv = f_tf((x, y))\n    self.assertAllClose(2.0 * 4.0, tape.gradient(uv['one'], x))\n    self.assertAllClose(0.0, tape.gradient(uv['one'], y))\n    self.assertAllClose(5.0, tape.gradient(uv['two'], x))\n    self.assertAllClose(4.0, tape.gradient(uv['two'], y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_ordered_dict_input(self, with_function=True):\n\n    def f(inputs):\n        out = 0.0\n        for v in inputs.values():\n            out += jnp.sum(v)\n        return out\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_type = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable([4.0], dtype=default_float_type)\n    y = tf.Variable([4.0, 5.0], dtype=default_float_type)\n    inputs = collections.OrderedDict()\n    inputs['r'] = x\n    inputs['d'] = y\n    with tf.GradientTape(persistent=True) as tape:\n        u = f_tf(inputs)\n    self.assertAllClose(np.array([1.0]), tape.gradient(u, x).numpy())\n    self.assertAllClose(np.array([1.0, 1.0]), tape.gradient(u, y).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_int_argument(self, with_function=False):\n    state = dict(float_used=np.array([0.7, 0.9], dtype=np.float32), float_passthrough=np.float16(1.0), float_unused=np.array([1.1, 2.2, 3.3], dtype=np.float32), int_used=np.int16(5), int_passthrough=np.int8(7), int_unused=np.array([1, 2, 3], dtype=np.uint32), bool_used=np.array([True, False, False, True], dtype=np.bool_), bool_passthrough=np.array([True, False, False, True, False], dtype=np.bool_), bool_unused=np.array([[True, False], [False, True]], dtype=np.bool_))\n\n    def jax_f(state):\n        res = dict(state, float_used=2.0 * state['float_used'], int_used=3 * state['int_used'], bool_used=state['bool_used'] == state['bool_used'])\n        del res['float_unused']\n        del res['int_unused']\n        del res['bool_unused']\n        return res\n    args = (state,)\n    res_jax = jax_f(*args)\n    vjp_jax_fun, args_vjp = tf_test_util.TransformJaxVJP(jax_f, args, res_jax)\n    grad_jax, = vjp_jax_fun(*args_vjp)\n\n    def compare_with_overrides(*, what, expected, **expected_overrides):\n        what_keys = set(what.keys())\n        expected_keys = set(expected.keys())\n        self.assertEqual(what_keys, expected_keys)\n        for k, w in what.items():\n            e = expected[k]\n            if k in expected_overrides:\n                if expected_overrides[k] == 'ZERO':\n                    e = np.zeros_like(w)\n                elif expected_overrides[k] == 'ZERO_BOOL':\n                    e = np.zeros(np.shape(w), dtype=np.bool_)\n                elif expected_overrides[k] == 'ONE':\n                    e = np.ones_like(w)\n                else:\n                    e = expected_overrides[k]\n            if e is None:\n                self.assertIsNone(w, msg=k)\n            else:\n                self.assertIsNotNone(w, msg=k)\n            w = w.numpy() if isinstance(w, tf.Tensor) else e\n            e = e.numpy() if isinstance(e, tf.Tensor) else e\n            try:\n                self.assertAllClose(e, w, err_msg=k)\n            except:\n                print(f'Failed at {k}')\n                raise\n    _, (grad_tf_0,) = tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    compare_with_overrides(what=grad_tf_0, expected=grad_jax, float_unused='ZERO', bool_used='ZERO', bool_passthrough='ONE', bool_unused='ZERO', int_used='ZERO', int_passthrough='ONE', int_unused='ZERO')\n    _, (grad_tf_None,) = tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.NONE)\n    compare_with_overrides(what=grad_tf_None, expected=grad_tf_0, float_unused=None, int_used=None, int_unused=None, bool_used=None, bool_unused=None)\n    f_tf_jax = jax2tf.convert(jax_f)\n    if with_function:\n        f_tf_jax = tf.function(f_tf_jax, autograph=False)\n    _, (grad_tf_jax_0,) = tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args)\n    compare_with_overrides(what=grad_tf_jax_0, expected=grad_tf_0, int_passthrough='ZERO', bool_passthrough='ZERO')\n    _, (grad_tf_jax_None,) = tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args, unconnected_gradients=tf.UnconnectedGradients.NONE)\n    compare_with_overrides(what=grad_tf_jax_None, expected=grad_tf_0, int_used=None, int_passthrough=None, int_unused=None, bool_unused=None, bool_used=None, bool_passthrough=None)\n    tf_vjp_jax_fun = jax2tf.convert(vjp_jax_fun)\n    grad_tf_vjp_jax, = tf_vjp_jax_fun(*args_vjp)\n    compare_with_overrides(what=grad_tf_vjp_jax, expected=grad_tf_0, bool_passthrough='ZERO_BOOL', bool_unused='ZERO_BOOL', bool_used='ZERO_BOOL', int_passthrough='ZERO_BOOL', int_unused='ZERO_BOOL', int_used='ZERO_BOOL')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_readme_gradient_int(self):\n    x = np.array(2, dtype=np.int16)\n\n    def f_jax(x):\n        return x.astype(np.float32) * 2.0\n    print(jax.grad(f_jax, allow_int=True)(x))\n    print(jax2tf.convert(jax.grad(f_jax, allow_int=True))(x))\n\n    def f_tf(x):\n        return tf.cast(x, tf.float32) * 2.0\n    xv = tf.Variable(x)\n    with tf.GradientTape(persistent=True) as tape:\n        print(tape.gradient(f_tf(xv), xv))\n        print(tape.gradient(f_tf(xv), xv, unconnected_gradients=tf.UnconnectedGradients.ZERO))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_convert_argument_non_callable_error(self):\n    with self.assertRaisesRegex(TypeError, 'Expected a callable value'):\n        jax2tf.convert(5.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_convert_argument_non_tensor_error(self):\n    with self.assertRaisesRegex(TypeError, 'Argument.*is not a valid JAX type'):\n        jax2tf.convert(lambda x: x)(lambda y: y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_argument_eager_tensor(self):\n    x = jax2tf.convert(jnp.sin)(1.0)\n    jax2tf.convert(jnp.cos)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@unittest.skip('Test fails at head')\ndef test_issue_10586(self):\n\n    class JaxModule(tf.Module):\n\n        def __init__(self):\n            self._params = {'w': tf.Variable(tf.ones([784, 10]), name='w'), 'b': tf.Variable(tf.ones([10]), name='b')}\n\n        def __call__(self, x):\n            return jax2tf.convert(lambda p, x: x @ p['w'] + p['b'])(self._params, x)\n    net = JaxModule()\n    images = tf.ones([1, 784])\n    with tf.GradientTape() as tape:\n        loss = tf.reduce_sum(net(images))\n    params = tape.watched_variables()\n    grads = tape.gradient(loss, params)\n    for var, grad in zip(params, grads):\n        self.assertEqual(var.shape, grad.shape, msg=var.name)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_remat(self):\n\n    def f(x1):\n        x2 = jnp.sin(x1)\n        x3 = jnp.sin(x2)\n        x4 = jnp.sin(x3)\n        return x4\n    remat_f = ad_checkpoint.checkpoint(f)\n    arg = np.array(3.0)\n    f_tf = jax2tf.convert(jax.grad(remat_f))\n    f_tf_hlo = self.TfToHlo(f_tf, arg)\n    if config.remat_opt_barrier.value:\n        self.assertRegex(f_tf_hlo, 'opt-barrier')\n    else:\n        self.assertRegex(f_tf_hlo, 'transpose/jax2tf_f_/jvp/checkpoint/cond/branch_1_fun/Sin')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_checkpoint_name(self):\n\n    def f_jax(x):\n        return ad_checkpoint.checkpoint_name(jnp.sin(x), 'sin')\n    jax2tf.convert(f_jax)(1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_convert_of_nested_independent_jit(self):\n\n    def func(x):\n\n        def inner1(y):\n            return x + y\n        return jax.jit(inner1)(1.0)\n    jax2tf.convert(func)(2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_convert_of_nested_dependent_jit(self):\n\n    def func(x):\n\n        def inner1(y):\n            return x + y\n        return jax.jit(inner1)(x)\n    jax2tf.convert(func)(2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_jit_unused(self):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))(x, y_unused)\n    self.assertAllClose(f_jax(x, None), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=mode, mode=mode) for mode in ('eager', 'graph', 'compiled')))\ndef test_jit_unused_grad(self, mode='eager'):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_jax = f_jax(x, y_unused)\n    f_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))\n    x_tf, y_unused_tf = (tf.constant(x), tf.constant(y_unused))\n\n    def grad_tf(x, y_unused):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            tape.watch(y_unused)\n            res_tf = f_tf(x, y_unused)\n            grad_tf_x, grad_tf_y = tape.gradient(res_tf, (x, y_unused))\n        return (res_tf, grad_tf_x, grad_tf_y)\n    if mode == 'graph':\n        grad_tf = tf.function(grad_tf, autograph=False)\n    elif mode == 'compiled':\n        grad_tf = tf.function(grad_tf, autograph=False, jit_compile=True)\n    res_tf, grad_tf_x, grad_tf_y = grad_tf(x_tf, y_unused_tf)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(np.float32(2.0), grad_tf_x)\n    self.assertIsNone(grad_tf_y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_nested_convert_error(self):\n\n    def outer(y):\n        return jax2tf.convert(jnp.sin)(y)\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        jax2tf.convert(outer)(np.ones((4,), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_nested_convert_error_non_tracer(self):\n    \"\"\"The inner convert takes non-tracer arguments\"\"\"\n\n    def outer(y):\n        sin_1 = jax2tf.convert(jnp.sin)(1.0)\n        return y + sin_1\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        jax2tf.convert(outer)(2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(transform=['jit', 'jvp', 'grad', 'vmap'])\ndef test_convert_under_transform_error(self, transform='vmap'):\n\n    def outer(y):\n        return jax2tf.convert(jnp.sin)(y)\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        self.TransformConvertAndCompare(outer, np.ones((4,)), transform)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(transform=['jit', 'jvp', 'grad', 'vmap'])\ndef test_convert_under_transform_error_non_tracer(self, transform='vmap'):\n\n    def outer(y):\n        sin_1 = jax2tf.convert(jnp.sin)(1.0)\n        return y + sin_1\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        self.TransformConvertAndCompare(outer, np.ones((4,)), transform)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_name_scope(self):\n\n    def run_tf():\n\n        @jax.named_call\n        def my_test_function_jax(x):\n            return x * x\n\n        def caller_jax(x):\n            return my_test_function_jax(jnp.sin(x))\n        out = jax2tf.convert(caller_jax, with_gradient=False)(2.0)\n        return out\n    if config.jax2tf_default_native_serialization.value:\n        self.assertIn('my_test_function_jax/mul', self.TfToHlo(run_tf))\n    else:\n        graph_def = str(tf.function(run_tf, autograph=False).get_concrete_function().graph.as_graph_def())\n        if 'my_test_function_jax/pjit_multiply_/Mul' not in graph_def:\n            self.assertIn('my_test_function_jax/jit_multiply_/Mul', graph_def)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_bfloat16_constant(self):\n\n    def jax_fn_scalar(x):\n        x = x.astype(jnp.bfloat16)\n        x *= 2.0\n        return x\n\n    def jax_fn_array(x):\n        x = x.astype(jnp.bfloat16)\n        x *= np.array([1.5, 2.5, 3.5], jnp.bfloat16)\n        return x\n    tf_fn_scalar = jax2tf.convert(jax_fn_scalar)\n    self.assertAllClose(tf_fn_scalar(1.375).numpy(), jnp.bfloat16(2.75))\n    tf_fn_array = jax2tf.convert(jax_fn_array)\n    self.assertAllClose(tf_fn_array(np.array([3, 4, 5])), np.array([4.5, 10, 17.5], jnp.bfloat16))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_shared_constants(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const = np.random.uniform(size=256).astype(np.float32)\n\n    def f(x):\n        return x + const + const + const + const\n    f_tf_consts = self.FindLargeTfConstants(jax2tf.convert(f), const)\n    self.assertLen(f_tf_consts, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_shared_constants_under_scan(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    xs = np.ones((8, const_size), dtype=np.float32)\n\n    def f1(xs):\n        res, _ = lax.scan(lambda carry, x: (carry + x + const, None), jnp.zeros((const_size,), dtype=np.float32), xs)\n        return res\n\n    def f2(xs):\n        return f1(xs) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), xs, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), xs, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_shared_constants_under_jit(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const = np.random.uniform(size=(16, 16)).astype(np.float32)\n\n    @jax.jit\n    def g_jit(x):\n        return x * const\n\n    def f(x):\n        return g_jit(x) + const + const\n    f_tf_graph_consts = self.FindLargeTfConstants(jax2tf.convert(f), const)\n    self.assertLen(f_tf_graph_consts, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_weak_types(self):\n    mul = jax.jit(jnp.multiply)\n    tf_fn = jax2tf.convert(lambda x: mul(x, 2.0))\n    self.assertAllClose(tf_fn(tf.constant(1.375, tf.bfloat16)).numpy(), jnp.bfloat16(2.75))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_kwargs(self, with_function=False):\n\n    def f_jax(*, x):\n        return jnp.sum(x)\n    f_tf = jax2tf.convert(f_jax)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(f_tf(x=np.zeros(3, dtype=np.float32)), np.zeros((), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_grad_kwargs(self, with_function=False):\n    x = (np.zeros(3, dtype=np.float32), np.zeros(4, dtype=np.float32))\n\n    def f_jax(*, x=(1.0, 2.0)):\n        return jnp.sum(x[0]) + 2.0 * jnp.sum(x[1])\n    f_tf = jax2tf.convert(f_jax)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    xv = tf.nest.map_structure(tf.Variable, x)\n    with tf.GradientTape() as tape:\n        res = f_tf(x=xv)\n    grad_tf = tape.gradient(res, xv)\n    self.assertAllClose((np.full_like(x[0], fill_value=1.0), np.full_like(x[1], fill_value=2.0)), (grad_tf[0].numpy(), grad_tf[1].numpy()))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_name_scope_polymorphic(self):\n    if config.jax2tf_default_native_serialization.value and (not config.dynamic_shapes.value):\n        self.skipTest('shape polymorphism but --jax_dynamic_shapes is not set.')\n\n    def func_jax(x, y):\n        return jnp.sin(x) + jnp.cos(y)\n    func_tf = jax2tf.convert(func_jax, polymorphic_shapes='(b,...)', with_gradient=True)\n    outer_scope = 'output_a'\n    g = tf.Graph()\n    with g.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = func_tf(x, y)\n    self.assertAllOperationStartWith(g, outer_scope)\n    g2 = tf.Graph()\n    with g2.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = tf.function(func_tf, jit_compile=True, autograph=False)(x, y)\n    self.assertAllOperationStartWith(g2, outer_scope)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_name_scope_cond(self):\n\n    def f(x):\n\n        def f_pos(x):\n            with jax.named_scope('jax_f_pos'):\n                return lax.cond(x < 1.0, jnp.cos, jnp.sin, x)\n        with jax.named_scope('jax_f_outer'):\n            return lax.cond(x > 0.0, f_pos, lambda x: x, x)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_forward():\n        with tf.name_scope('tf_outer_forward'):\n            x = 0.5\n            f_tf = jax2tf.convert(f)\n            _ = f_tf(x)\n    g = outer_forward.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_forward')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_forward/jax2tf_f_/jax_f_outer')\n    x = tf.Variable(0.5, name='tf_outer_back/x')\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_back():\n        with tf.name_scope('tf_outer_back'):\n            f_tf = jax2tf.convert(f)\n            with tf.GradientTape() as tape:\n                res_tf = f_tf(x)\n                _ = tape.gradient(res_tf, x)\n    g = outer_back.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_back')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_back')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_name_scope_while_loop(self):\n\n    def f(x):\n        with tf.name_scope('outer_scope'):\n\n            def condition(x):\n                return jnp.sum(x, keepdims=False) < 100\n\n            def body(x):\n                return jnp.add(x, 2.0)\n            result = jax.lax.while_loop(condition, body, x)\n            return result\n    tf_f = tf.function(jax2tf.convert(f), jit_compile=True, autograph=False)\n    g = tf_f.get_concrete_function(tf.zeros((1, 3))).graph\n    for func in g._functions.values():\n        for op in func.graph.get_operations():\n            if op.name.count(f'outer_scope/jax2tf_{f.__name__}_/while') > 1:\n                self.fail(f'tf graph has repeated name issue on when converting lax.while to tf.while.See op.name = : {op.name}')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'{('with_mesh_' if with_mesh else '')}2={(transform2 if transform2 != 'none' else '')}_1={(transform1 if transform1 != 'none' else '')}{('_nullary' if nullary else '')}', with_mesh=with_mesh, transform1=transform1, transform2=transform2, nullary=nullary) for transform1 in ['none', 'jit', 'pjit', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding', 'shard_map', 'pmap'] for transform2 in ['none', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] for nullary in ([True, False] if transform2 == 'none' else [False]) for with_mesh in ([True] if transform1 not in ['base', 'jit', 'pjit'] or transform2 != 'none' else [False, True])))\ndef test_cross_platform(self, with_mesh=True, transform1='pjit_in_shardings_P', transform2='pjit_in_shardings_P', nullary=False):\n    if transform2 == 'none' and (transform1 == 'shard_map' or (transform1 in ['pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] and nullary)):\n        raise unittest.SkipTest('Skip because must have pjit at top level')\n    x = np.ones((4, 6), dtype=np.float32)\n    mesh = sharding.Mesh(jax.devices()[:1], ('a',))\n    func = lambda x: lax.cummax(x, axis=0, reverse=False)\n    func_shard_map = lambda x: lax.all_gather(x, 'a', axis=1, tiled=True)\n\n    def apply_transform(func, transform: str):\n        transformed_func = dict(none=func, jit=jax.jit(func), jit_in_shardings_None=jax.jit(func, in_shardings=None), jit_in_shardings_P=jax.jit(func, in_shardings=(P('a'),)), jit_in_shardings_Sharding=jax.jit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),)), pjit=pjit.pjit(func), pjit_in_shardings_None=pjit.pjit(func, in_shardings=None, out_shardings=None), pjit_in_shardings_P=pjit.pjit(func, in_shardings=(P('a'),), out_shardings=P('a')), pjit_in_shardings_Sharding=pjit.pjit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),), out_shardings=sharding.NamedSharding(mesh, P('a'))), shard_map=shard_map(func, mesh, in_specs=(P('a', None),), out_specs=P('a', None)), pmap=jax.pmap(func, in_axes=0, out_axes=0))[transform]\n        return transformed_func\n    transformed1_func = apply_transform(func_shard_map if transform1 == 'shard_map' else func, transform1)\n    assert transform2 not in ['shard_map']\n    transformed2_func = apply_transform(transformed1_func, transform2)\n    if transform1 == 'pmap':\n        x = x.reshape((1, -1))\n    if not nullary:\n        func_to_convert = transformed2_func\n        args = [x]\n    else:\n        func_to_convert = lambda: transformed2_func(jnp.ones(x.shape, dtype=x.dtype))\n        args = []\n    if transform1 == 'pmap':\n        if nullary:\n            raise unittest.SkipTest('Cannot lower nested pmap: jit-of-pmap warning')\n        raise unittest.SkipTest('TODO: figure out how to invoke pmap from TF')\n    f_tf = jax2tf.convert(func_to_convert, native_serialization=True, native_serialization_platforms=('tpu',))\n    f_tf = tf.function(f_tf, jit_compile=True, autograph=False)\n    with contextlib.ExitStack() as stack:\n        if with_mesh:\n            stack.enter_context(mesh)\n        _ = func_to_convert(*args)\n        exported = export.export(jax.jit(func_to_convert) if not hasattr(func_to_convert, 'trace') else func_to_convert, platforms=('tpu',))(*(core.ShapedArray(a.shape, a.dtype) for a in args))\n    if transform1 == 'shard_map':\n        self.assertIn('stablehlo.all_gather', str(exported.mlir_module()))\n    else:\n        self.assertIn('stablehlo.reduce_window', str(exported.mlir_module()))",
    "assertions": [
      "assert transform2 not in ['shard_map']"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_cross_platform_error(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization=True, native_serialization_platforms=('tpu',))\n    x = np.float32(0.5)\n    if jtu.test_device_matches(['tpu']):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    else:\n        f_tf_fun = tf.function(f_tf, jit_compile=True, autograph=False)\n        graph_def = f_tf_fun.get_concrete_function(x).graph.as_graph_def()\n        self.assertIn('XlaCallModule', str(graph_def))\n        with self.assertRaisesRegex(tf.errors.NotFoundError, 'The current platform .* is not among the platforms required by the module'):\n            f_tf(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='using native_serialization_platforms without native_serialization')\ndef test_native_parameters_for_non_native(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_platforms=('cpu',))\n    x = np.float32(0.5)\n    tf_cpus = tf.config.list_logical_devices('CPU')\n    self.assertNotEmpty(tf_cpus)\n    with tf.device(tf_cpus[0]):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_disabled_checks=(jax2tf.DisabledSafetyCheck.platform(),))\n    self.assertAllClose(jnp.sin(x), f_tf(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_native_serialization_grad(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization=True, native_serialization_platforms=('tpu',))\n    x = np.arange(4, dtype=np.float32)\n    x_v = tf.Variable(x)\n\n    @tf.function(autograph=False)\n    def f_grad_tf(x_v):\n        with tf.GradientTape() as tape:\n            tape.watch(x_v)\n            res_tf = f_tf(x_v)\n            return tape.gradient(res_tf, x_v)\n    f_grad_tf_fun = tf.function(f_grad_tf, autograph=False)\n    graph_def = f_grad_tf_fun.get_concrete_function(x).graph.as_graph_def()\n    logging.info('Found graph_def: %s', graph_def)\n    self.assertLen(re.findall('op:\\\\s*\"XlaCallModule\"', str(graph_def)), 2)\n    if not jtu.test_device_matches(['tpu']):\n        with self.assertRaisesRegex(tf.errors.NotFoundError, 'The current platform .* is not among the platforms required by the module: \\\\[TPU\\\\]'):\n            f_grad_tf(x_v)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_effects_error(self):\n\n    def f_jax(x):\n        jax.debug.print('{}', x)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_jax, native_serialization=True)(np.float32(42.0))\n\n    def f_ordered_jax(x):\n        jax.debug.print('{}', x, ordered=True)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_ordered_jax, native_serialization=True)(np.float32(42.0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_multi_platform(self):\n    if config.enable_x64.value:\n        self.skipTest('TODO: enable when we can handle i64 platform_index_argument')\n    _testing_multi_platform_to_add = dict(cpu=2.0, tpu=3.0, cuda=4.0, rocm=5.0)\n\n    def f_jax(x):\n        return x + lax.platform_dependent(tpu=lambda: _testing_multi_platform_to_add['tpu'], cuda=lambda: _testing_multi_platform_to_add['cuda'], rocm=lambda: _testing_multi_platform_to_add['rocm'], default=lambda: _testing_multi_platform_to_add['cpu'])\n    x = np.float32(0.42)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=('cpu', 'cuda', 'tpu'))\n    for tf_device in self.tf_devices:\n        logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n        with tf.device(tf_device):\n            res = f_tf(x)\n        tf_device_jax_platform = dict(CPU='cpu', GPU='cuda', TPU='tpu')[tf_device.device_type]\n        self.assertAllClose(res, x + _testing_multi_platform_to_add[tf_device_jax_platform])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_dot_algorithm(self):\n    if tf.version.VERSION.split('.') <= ['2', '18', '0']:\n        self.skipTest('Because of an XLA bug this test segfaults with TF v2.18.0')\n    if jtu.test_device_matches(['tpu']):\n        algorithm = 'BF16_BF16_F32'\n    else:\n        algorithm = 'F32_F32_F32'\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision=algorithm)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    f_tf(np.ones((128, 128), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_dot_algorithm_non_native_unsupported(self):\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision='F32_F32_F32')\n    x = np.ones((128, 128), dtype=np.float32)\n    with self.assertRaisesRegex(NotImplementedError, 'Unsupported precision in dot_general'):\n        jax2tf.convert(f_jax, native_serialization=False)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(state):\n    i, s = state\n    return jnp.logical_and(i < 1024, s < 1024)"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(state):\n    i, s = state\n    return jnp.logical_and(i < 1024, s < 1024)"
  },
  {
    "test_code": "def test_op_metadata_while_and_cond(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_while_cond(x):\n\n        def body_fun(i_acc):\n            i, acc = i_acc\n            return (i + 1, jnp.cos(acc) + lax.cond(jnp.mod(i, 2) == 0, lambda acc: jnp.sin(acc), lambda acc: acc, acc))\n        _, acc = lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))\n        return acc\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_while_cond, x, [tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 5, op_name='jax2tf(f_while_cond)/while/body/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 7, op_name='jax2tf(f_while_cond)/while/body/branch_1_fun/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='FloorMod', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_while_cond)/while/body/rem', op_type='rem')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(state):\n    i, s = state\n    return jnp.logical_and(i < 1024, s < 1024)"
  },
  {
    "test_code": "def test_name_scope_cond(self):\n\n    def f(x):\n\n        def f_pos(x):\n            with jax.named_scope('jax_f_pos'):\n                return lax.cond(x < 1.0, jnp.cos, jnp.sin, x)\n        with jax.named_scope('jax_f_outer'):\n            return lax.cond(x > 0.0, f_pos, lambda x: x, x)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_forward():\n        with tf.name_scope('tf_outer_forward'):\n            x = 0.5\n            f_tf = jax2tf.convert(f)\n            _ = f_tf(x)\n    g = outer_forward.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_forward')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_forward/jax2tf_f_/jax_f_outer')\n    x = tf.Variable(0.5, name='tf_outer_back/x')\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_back():\n        with tf.name_scope('tf_outer_back'):\n            f_tf = jax2tf.convert(f)\n            with tf.GradientTape() as tape:\n                res_tf = f_tf(x)\n                _ = tape.gradient(res_tf, x)\n    g = outer_back.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_back')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_back')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(state):\n    i, s = state\n    return jnp.logical_and(i < 1024, s < 1024)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def fn(x, y, z):\n    return jnp.dot(x, y) / z"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef fn(x):\n    y = x + x\n    return [y for _ in range(200)]"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef g(x, y):\n    return lax.add(x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'{('with_mesh_' if with_mesh else '')}2={(transform2 if transform2 != 'none' else '')}_1={(transform1 if transform1 != 'none' else '')}{('_nullary' if nullary else '')}', with_mesh=with_mesh, transform1=transform1, transform2=transform2, nullary=nullary) for transform1 in ['none', 'jit', 'pjit', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding', 'shard_map', 'pmap'] for transform2 in ['none', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] for nullary in ([True, False] if transform2 == 'none' else [False]) for with_mesh in ([True] if transform1 not in ['base', 'jit', 'pjit'] or transform2 != 'none' else [False, True])))\ndef test_cross_platform(self, with_mesh=True, transform1='pjit_in_shardings_P', transform2='pjit_in_shardings_P', nullary=False):\n    if transform2 == 'none' and (transform1 == 'shard_map' or (transform1 in ['pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] and nullary)):\n        raise unittest.SkipTest('Skip because must have pjit at top level')\n    x = np.ones((4, 6), dtype=np.float32)\n    mesh = sharding.Mesh(jax.devices()[:1], ('a',))\n    func = lambda x: lax.cummax(x, axis=0, reverse=False)\n    func_shard_map = lambda x: lax.all_gather(x, 'a', axis=1, tiled=True)\n\n    def apply_transform(func, transform: str):\n        transformed_func = dict(none=func, jit=jax.jit(func), jit_in_shardings_None=jax.jit(func, in_shardings=None), jit_in_shardings_P=jax.jit(func, in_shardings=(P('a'),)), jit_in_shardings_Sharding=jax.jit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),)), pjit=pjit.pjit(func), pjit_in_shardings_None=pjit.pjit(func, in_shardings=None, out_shardings=None), pjit_in_shardings_P=pjit.pjit(func, in_shardings=(P('a'),), out_shardings=P('a')), pjit_in_shardings_Sharding=pjit.pjit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),), out_shardings=sharding.NamedSharding(mesh, P('a'))), shard_map=shard_map(func, mesh, in_specs=(P('a', None),), out_specs=P('a', None)), pmap=jax.pmap(func, in_axes=0, out_axes=0))[transform]\n        return transformed_func\n    transformed1_func = apply_transform(func_shard_map if transform1 == 'shard_map' else func, transform1)\n    assert transform2 not in ['shard_map']\n    transformed2_func = apply_transform(transformed1_func, transform2)\n    if transform1 == 'pmap':\n        x = x.reshape((1, -1))\n    if not nullary:\n        func_to_convert = transformed2_func\n        args = [x]\n    else:\n        func_to_convert = lambda: transformed2_func(jnp.ones(x.shape, dtype=x.dtype))\n        args = []\n    if transform1 == 'pmap':\n        if nullary:\n            raise unittest.SkipTest('Cannot lower nested pmap: jit-of-pmap warning')\n        raise unittest.SkipTest('TODO: figure out how to invoke pmap from TF')\n    f_tf = jax2tf.convert(func_to_convert, native_serialization=True, native_serialization_platforms=('tpu',))\n    f_tf = tf.function(f_tf, jit_compile=True, autograph=False)\n    with contextlib.ExitStack() as stack:\n        if with_mesh:\n            stack.enter_context(mesh)\n        _ = func_to_convert(*args)\n        exported = export.export(jax.jit(func_to_convert) if not hasattr(func_to_convert, 'trace') else func_to_convert, platforms=('tpu',))(*(core.ShapedArray(a.shape, a.dtype) for a in args))\n    if transform1 == 'shard_map':\n        self.assertIn('stablehlo.all_gather', str(exported.mlir_module()))\n    else:\n        self.assertIn('stablehlo.reduce_window', str(exported.mlir_module()))",
    "assertions": [
      "assert transform2 not in ['shard_map']"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@traceback_util.api_boundary\ndef shard_map(f: Callable, mesh: Mesh | AbstractMesh, in_specs: Specs, out_specs: Specs, check_rep: bool=True, auto: frozenset[AxisName]=frozenset()):\n    \"\"\"Map a function over shards of data.\n\n  Note:\n    ``shard_map`` is an experimental API, and still subject to change. For an\n    introduction to sharded data, refer to :ref:`sharded-computation`. For a more\n    in-depth look at using ``shard_map``, refer to `SPMD multi-device parallelism with shard_map`_.\n\n  Args:\n    f: callable to be mapped. Each application of ``f``, or \"instance\" of ``f``,\n      takes as input a shard of the mapped-over arguments and produces a shard\n      of the output.\n    mesh: a ``jax.sharding.Mesh`` representing the array of devices over which\n      to shard the data and on which to execute instances of ``f``. The names of\n      the ``Mesh`` can be used in collective communication operations in ``f``.\n      This is typically created by a utility function like\n      :func:`jax.experimental.mesh_utils.create_device_mesh`.\n    in_specs: a pytree with :class:`~jax.sharding.PartitionSpec` instances as leaves,\n      with a tree structure that is a tree prefix of the args tuple to be mapped\n      over. Similar to :class:`~jax.sharding.NamedSharding`, each ``PartitionSpec``\n      represents how the corresponding argument (or subtree of arguments) should\n      be sharded along the named axes of ``mesh``. In each ``PartitionSpec``,\n      mentioning a ``mesh`` axis name at a position expresses sharding the\n      corresponding argument array axis along that positional axis; not\n      mentioning an axis name expresses replication. If an argument, or argument\n      subtree, has a corresponding spec of None, that argument is not sharded.\n    out_specs: a pytree with :class:`~jax.sharding.PartitionSpec` instances as leaves,\n      with a tree structure that is a tree prefix of the output of ``f``. Each\n      ``PartitionSpec`` represents how the corresponding output shards should be\n      concatenated. In each ``PartitionSpec``, metioning a ``mesh`` axis name at\n      a position expresses concatenation of that mesh axis's shards along the\n      corresponding positional axis. Not mentioning a ``mesh`` axis name\n      expresses a promise that the output values are equal along that mesh axis,\n      and that rather than concatenating only a single value should be produced.\n    check_rep: If True (default) enable additional validity checks and automatic\n      differentiation optimizations. The validity checks concern whether any mesh\n      axis names not mentioned in ``out_specs`` are consistent with how the outputs\n      of ``f`` are replicated. Must be set False if using a Pallas kernel in ``f``.\n    auto: (experimental) an optional set of axis names from ``mesh`` over which we\n      do not shard the data or map the function, but rather we allow the\n      compiler to control sharding. These names cannot be used in ``in_specs``,\n      ``out_specs``, or in communication collectives in ``f``.\n\n  Returns:\n    A callable that applies the input function ``f`` across data sharded according to\n    the ``mesh`` and ``in_specs``.\n\n  Examples:\n    For examples, refer to :ref:`sharded-computation` or `SPMD multi-device parallelism with shard_map`_.\n\n  .. _SPMD multi-device parallelism with shard_map: https://jax.readthedocs.io/en/latest/notebooks/shard_map.html\n  \"\"\"\n    return _shard_map(f, mesh, in_specs, out_specs, check_rep, auto)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def pmap(f, axis_name=None, *, in_axes=0, out_axes=0, static_broadcasted_argnums=(), devices=None, backend=None, axis_size=None, donate_argnums=(), global_arg_shapes=None):\n    devices = tuple(devices) if devices is not None else devices\n    axis_name, static_broadcasted_tuple, donate_tuple = _shared_code_pmap(f, axis_name, static_broadcasted_argnums, donate_argnums, in_axes, out_axes)\n\n    def infer_params(*args, **kwargs):\n        p = _prepare_pmap(f, in_axes, out_axes, static_broadcasted_tuple, donate_tuple, devices, backend, axis_size, args, kwargs)\n        for arg in p.flat_args:\n            dispatch.check_arg(arg)\n        mesh = Mesh(_get_devices(p, backend), (axis_name,))\n        _pmapped, in_specs, out_specs = _cached_shard_map(p.flat_fun, mesh, p.in_axes_flat, p.out_axes_thunk, axis_name)\n        flat_global_args = host_local_array_to_global_array(p.flat_args, mesh, list(in_specs))\n        jitted_f = jax.jit(_pmapped, donate_argnums=(i for i, val in enumerate(p.donated_invars) if val))\n        return (jitted_f, flat_global_args, p.out_tree, mesh, out_specs)\n\n    def wrapped(*args, **kwargs):\n        jitted_f, flat_global_args, out_tree, mesh, out_specs = infer_params(*args, **kwargs)\n        outs = jitted_f(*flat_global_args)\n        outs = global_array_to_host_local_array(outs, mesh, out_specs())\n        return tree_unflatten(out_tree(), outs)\n\n    def lower(*args, **kwargs):\n        jitted_f, _, _, _, _ = infer_params(*args, **kwargs)\n        return jitted_f.lower(*args, **kwargs)\n    wrapped.lower = lower\n    return wrapped"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'{('with_mesh_' if with_mesh else '')}2={(transform2 if transform2 != 'none' else '')}_1={(transform1 if transform1 != 'none' else '')}{('_nullary' if nullary else '')}', with_mesh=with_mesh, transform1=transform1, transform2=transform2, nullary=nullary) for transform1 in ['none', 'jit', 'pjit', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding', 'shard_map', 'pmap'] for transform2 in ['none', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] for nullary in ([True, False] if transform2 == 'none' else [False]) for with_mesh in ([True] if transform1 not in ['base', 'jit', 'pjit'] or transform2 != 'none' else [False, True])))\ndef test_cross_platform(self, with_mesh=True, transform1='pjit_in_shardings_P', transform2='pjit_in_shardings_P', nullary=False):\n    if transform2 == 'none' and (transform1 == 'shard_map' or (transform1 in ['pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] and nullary)):\n        raise unittest.SkipTest('Skip because must have pjit at top level')\n    x = np.ones((4, 6), dtype=np.float32)\n    mesh = sharding.Mesh(jax.devices()[:1], ('a',))\n    func = lambda x: lax.cummax(x, axis=0, reverse=False)\n    func_shard_map = lambda x: lax.all_gather(x, 'a', axis=1, tiled=True)\n\n    def apply_transform(func, transform: str):\n        transformed_func = dict(none=func, jit=jax.jit(func), jit_in_shardings_None=jax.jit(func, in_shardings=None), jit_in_shardings_P=jax.jit(func, in_shardings=(P('a'),)), jit_in_shardings_Sharding=jax.jit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),)), pjit=pjit.pjit(func), pjit_in_shardings_None=pjit.pjit(func, in_shardings=None, out_shardings=None), pjit_in_shardings_P=pjit.pjit(func, in_shardings=(P('a'),), out_shardings=P('a')), pjit_in_shardings_Sharding=pjit.pjit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),), out_shardings=sharding.NamedSharding(mesh, P('a'))), shard_map=shard_map(func, mesh, in_specs=(P('a', None),), out_specs=P('a', None)), pmap=jax.pmap(func, in_axes=0, out_axes=0))[transform]\n        return transformed_func\n    transformed1_func = apply_transform(func_shard_map if transform1 == 'shard_map' else func, transform1)\n    assert transform2 not in ['shard_map']\n    transformed2_func = apply_transform(transformed1_func, transform2)\n    if transform1 == 'pmap':\n        x = x.reshape((1, -1))\n    if not nullary:\n        func_to_convert = transformed2_func\n        args = [x]\n    else:\n        func_to_convert = lambda: transformed2_func(jnp.ones(x.shape, dtype=x.dtype))\n        args = []\n    if transform1 == 'pmap':\n        if nullary:\n            raise unittest.SkipTest('Cannot lower nested pmap: jit-of-pmap warning')\n        raise unittest.SkipTest('TODO: figure out how to invoke pmap from TF')\n    f_tf = jax2tf.convert(func_to_convert, native_serialization=True, native_serialization_platforms=('tpu',))\n    f_tf = tf.function(f_tf, jit_compile=True, autograph=False)\n    with contextlib.ExitStack() as stack:\n        if with_mesh:\n            stack.enter_context(mesh)\n        _ = func_to_convert(*args)\n        exported = export.export(jax.jit(func_to_convert) if not hasattr(func_to_convert, 'trace') else func_to_convert, platforms=('tpu',))(*(core.ShapedArray(a.shape, a.dtype) for a in args))\n    if transform1 == 'shard_map':\n        self.assertIn('stablehlo.all_gather', str(exported.mlir_module()))\n    else:\n        self.assertIn('stablehlo.reduce_window', str(exported.mlir_module()))",
    "assertions": [
      "assert transform2 not in ['shard_map']"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def pmap(f, axis_name=None, *, in_axes=0, out_axes=0, static_broadcasted_argnums=(), devices=None, backend=None, axis_size=None, donate_argnums=(), global_arg_shapes=None):\n    devices = tuple(devices) if devices is not None else devices\n    axis_name, static_broadcasted_tuple, donate_tuple = _shared_code_pmap(f, axis_name, static_broadcasted_argnums, donate_argnums, in_axes, out_axes)\n\n    def infer_params(*args, **kwargs):\n        p = _prepare_pmap(f, in_axes, out_axes, static_broadcasted_tuple, donate_tuple, devices, backend, axis_size, args, kwargs)\n        for arg in p.flat_args:\n            dispatch.check_arg(arg)\n        mesh = Mesh(_get_devices(p, backend), (axis_name,))\n        _pmapped, in_specs, out_specs = _cached_shard_map(p.flat_fun, mesh, p.in_axes_flat, p.out_axes_thunk, axis_name)\n        flat_global_args = host_local_array_to_global_array(p.flat_args, mesh, list(in_specs))\n        jitted_f = jax.jit(_pmapped, donate_argnums=(i for i, val in enumerate(p.donated_invars) if val))\n        return (jitted_f, flat_global_args, p.out_tree, mesh, out_specs)\n\n    def wrapped(*args, **kwargs):\n        jitted_f, flat_global_args, out_tree, mesh, out_specs = infer_params(*args, **kwargs)\n        outs = jitted_f(*flat_global_args)\n        outs = global_array_to_host_local_array(outs, mesh, out_specs())\n        return tree_unflatten(out_tree(), outs)\n\n    def lower(*args, **kwargs):\n        jitted_f, _, _, _, _ = infer_params(*args, **kwargs)\n        return jitted_f.lower(*args, **kwargs)\n    wrapped.lower = lower\n    return wrapped"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'{('with_mesh_' if with_mesh else '')}2={(transform2 if transform2 != 'none' else '')}_1={(transform1 if transform1 != 'none' else '')}{('_nullary' if nullary else '')}', with_mesh=with_mesh, transform1=transform1, transform2=transform2, nullary=nullary) for transform1 in ['none', 'jit', 'pjit', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding', 'shard_map', 'pmap'] for transform2 in ['none', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] for nullary in ([True, False] if transform2 == 'none' else [False]) for with_mesh in ([True] if transform1 not in ['base', 'jit', 'pjit'] or transform2 != 'none' else [False, True])))\ndef test_cross_platform(self, with_mesh=True, transform1='pjit_in_shardings_P', transform2='pjit_in_shardings_P', nullary=False):\n    if transform2 == 'none' and (transform1 == 'shard_map' or (transform1 in ['pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] and nullary)):\n        raise unittest.SkipTest('Skip because must have pjit at top level')\n    x = np.ones((4, 6), dtype=np.float32)\n    mesh = sharding.Mesh(jax.devices()[:1], ('a',))\n    func = lambda x: lax.cummax(x, axis=0, reverse=False)\n    func_shard_map = lambda x: lax.all_gather(x, 'a', axis=1, tiled=True)\n\n    def apply_transform(func, transform: str):\n        transformed_func = dict(none=func, jit=jax.jit(func), jit_in_shardings_None=jax.jit(func, in_shardings=None), jit_in_shardings_P=jax.jit(func, in_shardings=(P('a'),)), jit_in_shardings_Sharding=jax.jit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),)), pjit=pjit.pjit(func), pjit_in_shardings_None=pjit.pjit(func, in_shardings=None, out_shardings=None), pjit_in_shardings_P=pjit.pjit(func, in_shardings=(P('a'),), out_shardings=P('a')), pjit_in_shardings_Sharding=pjit.pjit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),), out_shardings=sharding.NamedSharding(mesh, P('a'))), shard_map=shard_map(func, mesh, in_specs=(P('a', None),), out_specs=P('a', None)), pmap=jax.pmap(func, in_axes=0, out_axes=0))[transform]\n        return transformed_func\n    transformed1_func = apply_transform(func_shard_map if transform1 == 'shard_map' else func, transform1)\n    assert transform2 not in ['shard_map']\n    transformed2_func = apply_transform(transformed1_func, transform2)\n    if transform1 == 'pmap':\n        x = x.reshape((1, -1))\n    if not nullary:\n        func_to_convert = transformed2_func\n        args = [x]\n    else:\n        func_to_convert = lambda: transformed2_func(jnp.ones(x.shape, dtype=x.dtype))\n        args = []\n    if transform1 == 'pmap':\n        if nullary:\n            raise unittest.SkipTest('Cannot lower nested pmap: jit-of-pmap warning')\n        raise unittest.SkipTest('TODO: figure out how to invoke pmap from TF')\n    f_tf = jax2tf.convert(func_to_convert, native_serialization=True, native_serialization_platforms=('tpu',))\n    f_tf = tf.function(f_tf, jit_compile=True, autograph=False)\n    with contextlib.ExitStack() as stack:\n        if with_mesh:\n            stack.enter_context(mesh)\n        _ = func_to_convert(*args)\n        exported = export.export(jax.jit(func_to_convert) if not hasattr(func_to_convert, 'trace') else func_to_convert, platforms=('tpu',))(*(core.ShapedArray(a.shape, a.dtype) for a in args))\n    if transform1 == 'shard_map':\n        self.assertIn('stablehlo.all_gather', str(exported.mlir_module()))\n    else:\n        self.assertIn('stablehlo.reduce_window', str(exported.mlir_module()))",
    "assertions": [
      "assert transform2 not in ['shard_map']"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['mesh', 'axis_name', 'memory_space'])\ndef all_gather(x, *, mesh: jax.sharding.Mesh, axis_name: str | Sequence[str], memory_space: pltpu.TPUMemorySpace=pltpu.VMEM):\n    if isinstance(axis_name, str):\n        axis_name = (axis_name,)\n    if len(axis_name) > 1:\n        raise NotImplementedError('Only one axis supported.')\n    axis_name, = axis_name\n    if mesh.shape[axis_name] == 1:\n        return x\n\n    def ag_local(x_shard):\n        axis_size = lax.psum(1, axis_name)\n        out_shape = jax.ShapeDtypeStruct((axis_size, *x_shard.shape), x_shard.dtype)\n        out = pl.pallas_call(functools.partial(ag_kernel, axis_name=axis_name, mesh=mesh), out_shape=out_shape, compiler_params=pltpu.TPUCompilerParams(collective_id=0), grid_spec=pltpu.PrefetchScalarGridSpec(num_scalar_prefetch=0, scratch_shapes=((pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA), (pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA)), in_specs=[pl.BlockSpec(memory_space=memory_space)], out_specs=pl.BlockSpec(memory_space=memory_space)))(x_shard)\n        return out.reshape((axis_size * x_shard.shape[0], *x_shard.shape[1:]))\n    return shard_map.shard_map(ag_local, mesh=mesh, in_specs=P(axis_name), out_specs=P(None), check_rep=False)(x)"
  },
  {
    "test_code": "def test_sin(self):\n    f_tf = jax2tf.convert(jnp.sin)\n    x = np.float32(0.5)\n    sin_x = np.sin(x)\n    self.assertAllClose(sin_x, f_tf(x))\n    self.assertAllClose(sin_x, tf.function(f_tf, autograph=False, jit_compile=True)(x))\n    tf_preferred_device = (tf.config.list_logical_devices('TPU') + tf.config.list_logical_devices('GPU') + tf.config.list_logical_devices())[0]\n    logging.info('Running TF on %s', tf_preferred_device)\n\n    @tf.function(autograph=False, jit_compile=False)\n    def f_tf_wrapped(x):\n        with tf.device(tf_preferred_device.name):\n            return f_tf(x)\n    with tf.device(tf_preferred_device.name):\n        self.assertAllClose(sin_x, f_tf_wrapped(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_nested_jit_is_compiled(self):\n\n    def has_xla_must_compile(f_tf, x):\n        f_conc = tf.function(f_tf, autograph=True).get_concrete_function(tf.convert_to_tensor(x))\n        for n in f_conc.graph._nodes_by_id.values():\n            try:\n                n.get_attr('_XlaMustCompile')\n                return True\n            except ValueError:\n                continue\n        return False\n    x = np.array(0.7)\n    f_no_jit = lambda x: x\n    self.assertFalse(has_xla_must_compile(jax2tf.convert(f_no_jit), x))\n    f_jit = lambda x: jax.jit(jnp.sin)(x)\n    self.assertFalse(has_xla_must_compile(jax2tf.convert(f_jit), x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_bfloat16_passed_by_tf(self):\n    f_jax = lambda a, b: a + b\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([512, 512], tf.bfloat16), tf.TensorSpec([512, 512], tf.bfloat16)])\n    self.assertIsNotNone(f_tf.get_concrete_function())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_bfloat16_returned_by_jax(self):\n    f_jax = lambda a, b: (a + b).astype(jnp.bfloat16)\n    f_tf = jax2tf.convert(f_jax)\n    self.assertEqual(f_tf(1.0, 2.0).dtype, tf.bfloat16)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu')\ndef test_bfloat16_tf_grad(self):\n    f_jax = lambda a, b: a + b\n\n    def _tf_grad(a, b):\n        with tf.GradientTape() as tape:\n            tape.watch(a)\n            result = jax2tf.convert(f_jax)(a, b)\n        return (result, tape.gradient(result, a))\n    f_tf = tf.function(_tf_grad, autograph=False, input_signature=[tf.TensorSpec([512, 512], tf.bfloat16), tf.TensorSpec([512, 512], tf.bfloat16)])\n    self.assertIsNotNone(f_tf.get_concrete_function())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(dtype=[np.int64, np.float64], with_function=[True, False])\ndef test_converts_64bit(self, dtype=np.int64, with_function=False):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    big_const = np.full((5,), 2 ** 33, dtype=dtype)\n    self.ConvertAndCompare(jnp.sin, big_const)\n    f_conv = jax2tf.convert(jnp.sin)\n    if with_function:\n        f_conv = tf.function(f_conv, autograph=False)\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.Variable(big_const)))\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.constant(big_const)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_64bit_behavior_enable_x64_readme(self):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float64)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float64)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_64bit_behavior_not_enable_x64_readme(self):\n    if config.enable_x64.value:\n        self.skipTest('requires not x64 mode')\n    self.assertEqual(tf.math.sin(3.14).dtype, tf.float32)\n    self.assertEqual(jnp.sin(3.14).dtype, jnp.float32)\n    self.assertEqual(tf.math.sin(np.float64(3.14)).dtype, tf.float64)\n    self.assertEqual(jnp.sin(np.float64(3.14)).dtype, jnp.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(3.14).dtype, tf.float32)\n    self.assertEqual(jax2tf.convert(jnp.sin)(np.float64(3.14)).dtype, tf.float32)\n    self.assertEqual(tf.function(jax2tf.convert(jnp.sin), autograph=False)(tf.Variable(3.14, dtype=tf.float64)).dtype, tf.float32)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_disabled(self, with_function=False):\n    if tf.version.VERSION.split('.') <= ['2', '17', '0']:\n        self.skipTest('This test works only with newer versions of TF')\n    f_tf = jax2tf.convert(jnp.tan, with_gradient=False)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    x = tf.ones([])\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            y = f_tf(x)\n            _ = tape.gradient(y, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients(self, with_function=True):\n\n    def f(x, y):\n        return (x * x, x * y)\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_type = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    y = tf.Variable(5.0, dtype=default_float_type)\n    with tf.GradientTape(persistent=True) as tape:\n        u, v = f_tf(x, y)\n    self.assertAllClose(2.0 * 4.0, tape.gradient(u, x))\n    self.assertAllClose(0.0, tape.gradient(u, y))\n    self.assertAllClose(5.0, tape.gradient(v, x))\n    self.assertAllClose(4.0, tape.gradient(v, y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_higher_order_gradients(self):\n    f = lambda x: x ** 3\n    f_tf = jax2tf.convert(f)\n    x = tf.Variable(4.0, dtype=tf.float32)\n    with tf.GradientTape() as t2:\n        with tf.GradientTape() as t1:\n            y = f_tf(x)\n        dy_dx = t1.gradient(y, x)\n    d2y_dx2 = t2.gradient(dy_dx, x)\n    self.assertAllClose(np.float32(48.0), dy_dx.numpy())\n    self.assertAllClose(np.float32(24.0), d2y_dx2.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_pytree(self, with_function=False):\n\n    def f(xy: tuple[float, float]) -> dict[str, float]:\n        x, y = xy\n        return dict(one=x * x, two=x * y)\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_dtype = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable(4.0, dtype=default_float_dtype)\n    y = tf.Variable(5.0, dtype=default_float_dtype)\n    with tf.GradientTape(persistent=True) as tape:\n        uv = f_tf((x, y))\n    self.assertAllClose(2.0 * 4.0, tape.gradient(uv['one'], x))\n    self.assertAllClose(0.0, tape.gradient(uv['one'], y))\n    self.assertAllClose(5.0, tape.gradient(uv['two'], x))\n    self.assertAllClose(4.0, tape.gradient(uv['two'], y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_ordered_dict_input(self, with_function=True):\n\n    def f(inputs):\n        out = 0.0\n        for v in inputs.values():\n            out += jnp.sum(v)\n        return out\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    default_float_type = jax2tf.dtype_of_val(4.0)\n    x = tf.Variable([4.0], dtype=default_float_type)\n    y = tf.Variable([4.0, 5.0], dtype=default_float_type)\n    inputs = collections.OrderedDict()\n    inputs['r'] = x\n    inputs['d'] = y\n    with tf.GradientTape(persistent=True) as tape:\n        u = f_tf(inputs)\n    self.assertAllClose(np.array([1.0]), tape.gradient(u, x).numpy())\n    self.assertAllClose(np.array([1.0, 1.0]), tape.gradient(u, y).numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_unused_argument_readme(self, with_function=False):\n\n    def fn(x0, x1, x2, x3):\n        return x0 * 0.0 + x2 * 2.0\n    xs = [tf.Variable(x) for x in [10.0, 11.0, 12.0, 13]]\n    with tf.GradientTape(persistent=True) as tape:\n        res = fn(*xs)\n    g_tf_native = tape.gradient(res, xs)\n    self.assertAllClose(g_tf_native[0].numpy(), np.float32(0.0))\n    self.assertIsNone(g_tf_native[1])\n    self.assertAllClose(g_tf_native[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_tf_native[3])\n    g_tf_native_0 = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_tf_native_0[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_tf_native_0[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_tf_native_0[3].numpy(), np.int32(0))\n    with tf.GradientTape(persistent=True) as tape:\n        conv_fn = jax2tf.convert(fn, with_gradient=True)\n        if with_function:\n            conv_fn = tf.function(conv_fn, autograph=False)\n        res = conv_fn(*xs)\n    g_jax2tf = tape.gradient(res, xs)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertIsNone(g_jax2tf[3])\n    g_jax2tf = tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    self.assertAllClose(g_jax2tf[0].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[1].numpy(), np.float32(0.0))\n    self.assertAllClose(g_jax2tf[2].numpy(), np.float32(2.0))\n    self.assertAllClose(g_jax2tf[3].numpy(), np.int32(0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_int_argument(self, with_function=False):\n    state = dict(float_used=np.array([0.7, 0.9], dtype=np.float32), float_passthrough=np.float16(1.0), float_unused=np.array([1.1, 2.2, 3.3], dtype=np.float32), int_used=np.int16(5), int_passthrough=np.int8(7), int_unused=np.array([1, 2, 3], dtype=np.uint32), bool_used=np.array([True, False, False, True], dtype=np.bool_), bool_passthrough=np.array([True, False, False, True, False], dtype=np.bool_), bool_unused=np.array([[True, False], [False, True]], dtype=np.bool_))\n\n    def jax_f(state):\n        res = dict(state, float_used=2.0 * state['float_used'], int_used=3 * state['int_used'], bool_used=state['bool_used'] == state['bool_used'])\n        del res['float_unused']\n        del res['int_unused']\n        del res['bool_unused']\n        return res\n    args = (state,)\n    res_jax = jax_f(*args)\n    vjp_jax_fun, args_vjp = tf_test_util.TransformJaxVJP(jax_f, args, res_jax)\n    grad_jax, = vjp_jax_fun(*args_vjp)\n\n    def compare_with_overrides(*, what, expected, **expected_overrides):\n        what_keys = set(what.keys())\n        expected_keys = set(expected.keys())\n        self.assertEqual(what_keys, expected_keys)\n        for k, w in what.items():\n            e = expected[k]\n            if k in expected_overrides:\n                if expected_overrides[k] == 'ZERO':\n                    e = np.zeros_like(w)\n                elif expected_overrides[k] == 'ZERO_BOOL':\n                    e = np.zeros(np.shape(w), dtype=np.bool_)\n                elif expected_overrides[k] == 'ONE':\n                    e = np.ones_like(w)\n                else:\n                    e = expected_overrides[k]\n            if e is None:\n                self.assertIsNone(w, msg=k)\n            else:\n                self.assertIsNotNone(w, msg=k)\n            w = w.numpy() if isinstance(w, tf.Tensor) else e\n            e = e.numpy() if isinstance(e, tf.Tensor) else e\n            try:\n                self.assertAllClose(e, w, err_msg=k)\n            except:\n                print(f'Failed at {k}')\n                raise\n    _, (grad_tf_0,) = tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    compare_with_overrides(what=grad_tf_0, expected=grad_jax, float_unused='ZERO', bool_used='ZERO', bool_passthrough='ONE', bool_unused='ZERO', int_used='ZERO', int_passthrough='ONE', int_unused='ZERO')\n    _, (grad_tf_None,) = tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.NONE)\n    compare_with_overrides(what=grad_tf_None, expected=grad_tf_0, float_unused=None, int_used=None, int_unused=None, bool_used=None, bool_unused=None)\n    f_tf_jax = jax2tf.convert(jax_f)\n    if with_function:\n        f_tf_jax = tf.function(f_tf_jax, autograph=False)\n    _, (grad_tf_jax_0,) = tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args)\n    compare_with_overrides(what=grad_tf_jax_0, expected=grad_tf_0, int_passthrough='ZERO', bool_passthrough='ZERO')\n    _, (grad_tf_jax_None,) = tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args, unconnected_gradients=tf.UnconnectedGradients.NONE)\n    compare_with_overrides(what=grad_tf_jax_None, expected=grad_tf_0, int_used=None, int_passthrough=None, int_unused=None, bool_unused=None, bool_used=None, bool_passthrough=None)\n    tf_vjp_jax_fun = jax2tf.convert(vjp_jax_fun)\n    grad_tf_vjp_jax, = tf_vjp_jax_fun(*args_vjp)\n    compare_with_overrides(what=grad_tf_vjp_jax, expected=grad_tf_0, bool_passthrough='ZERO_BOOL', bool_unused='ZERO_BOOL', bool_used='ZERO_BOOL', int_passthrough='ZERO_BOOL', int_unused='ZERO_BOOL', int_used='ZERO_BOOL')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_readme_gradient_int(self):\n    x = np.array(2, dtype=np.int16)\n\n    def f_jax(x):\n        return x.astype(np.float32) * 2.0\n    print(jax.grad(f_jax, allow_int=True)(x))\n    print(jax2tf.convert(jax.grad(f_jax, allow_int=True))(x))\n\n    def f_tf(x):\n        return tf.cast(x, tf.float32) * 2.0\n    xv = tf.Variable(x)\n    with tf.GradientTape(persistent=True) as tape:\n        print(tape.gradient(f_tf(xv), xv))\n        print(tape.gradient(f_tf(xv), xv, unconnected_gradients=tf.UnconnectedGradients.ZERO))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_convert_argument_non_callable_error(self):\n    with self.assertRaisesRegex(TypeError, 'Expected a callable value'):\n        jax2tf.convert(5.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_convert_argument_non_tensor_error(self):\n    with self.assertRaisesRegex(TypeError, 'Argument.*is not a valid JAX type'):\n        jax2tf.convert(lambda x: x)(lambda y: y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_argument_eager_tensor(self):\n    x = jax2tf.convert(jnp.sin)(1.0)\n    jax2tf.convert(jnp.cos)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@unittest.skip('Test fails at head')\ndef test_issue_10586(self):\n\n    class JaxModule(tf.Module):\n\n        def __init__(self):\n            self._params = {'w': tf.Variable(tf.ones([784, 10]), name='w'), 'b': tf.Variable(tf.ones([10]), name='b')}\n\n        def __call__(self, x):\n            return jax2tf.convert(lambda p, x: x @ p['w'] + p['b'])(self._params, x)\n    net = JaxModule()\n    images = tf.ones([1, 784])\n    with tf.GradientTape() as tape:\n        loss = tf.reduce_sum(net(images))\n    params = tape.watched_variables()\n    grads = tape.gradient(loss, params)\n    for var, grad in zip(params, grads):\n        self.assertEqual(var.shape, grad.shape, msg=var.name)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_remat(self):\n\n    def f(x1):\n        x2 = jnp.sin(x1)\n        x3 = jnp.sin(x2)\n        x4 = jnp.sin(x3)\n        return x4\n    remat_f = ad_checkpoint.checkpoint(f)\n    arg = np.array(3.0)\n    f_tf = jax2tf.convert(jax.grad(remat_f))\n    f_tf_hlo = self.TfToHlo(f_tf, arg)\n    if config.remat_opt_barrier.value:\n        self.assertRegex(f_tf_hlo, 'opt-barrier')\n    else:\n        self.assertRegex(f_tf_hlo, 'transpose/jax2tf_f_/jvp/checkpoint/cond/branch_1_fun/Sin')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_checkpoint_name(self):\n\n    def f_jax(x):\n        return ad_checkpoint.checkpoint_name(jnp.sin(x), 'sin')\n    jax2tf.convert(f_jax)(1.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_convert_of_nested_independent_jit(self):\n\n    def func(x):\n\n        def inner1(y):\n            return x + y\n        return jax.jit(inner1)(1.0)\n    jax2tf.convert(func)(2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_convert_of_nested_dependent_jit(self):\n\n    def func(x):\n\n        def inner1(y):\n            return x + y\n        return jax.jit(inner1)(x)\n    jax2tf.convert(func)(2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_jit_unused(self):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))(x, y_unused)\n    self.assertAllClose(f_jax(x, None), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=mode, mode=mode) for mode in ('eager', 'graph', 'compiled')))\ndef test_jit_unused_grad(self, mode='eager'):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_jax = f_jax(x, y_unused)\n    f_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))\n    x_tf, y_unused_tf = (tf.constant(x), tf.constant(y_unused))\n\n    def grad_tf(x, y_unused):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            tape.watch(y_unused)\n            res_tf = f_tf(x, y_unused)\n            grad_tf_x, grad_tf_y = tape.gradient(res_tf, (x, y_unused))\n        return (res_tf, grad_tf_x, grad_tf_y)\n    if mode == 'graph':\n        grad_tf = tf.function(grad_tf, autograph=False)\n    elif mode == 'compiled':\n        grad_tf = tf.function(grad_tf, autograph=False, jit_compile=True)\n    res_tf, grad_tf_x, grad_tf_y = grad_tf(x_tf, y_unused_tf)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(np.float32(2.0), grad_tf_x)\n    self.assertIsNone(grad_tf_y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_nested_convert_error(self):\n\n    def outer(y):\n        return jax2tf.convert(jnp.sin)(y)\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        jax2tf.convert(outer)(np.ones((4,), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_nested_convert_error_non_tracer(self):\n    \"\"\"The inner convert takes non-tracer arguments\"\"\"\n\n    def outer(y):\n        sin_1 = jax2tf.convert(jnp.sin)(1.0)\n        return y + sin_1\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        jax2tf.convert(outer)(2.0)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(transform=['jit', 'jvp', 'grad', 'vmap'])\ndef test_convert_under_transform_error(self, transform='vmap'):\n\n    def outer(y):\n        return jax2tf.convert(jnp.sin)(y)\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        self.TransformConvertAndCompare(outer, np.ones((4,)), transform)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(transform=['jit', 'jvp', 'grad', 'vmap'])\ndef test_convert_under_transform_error_non_tracer(self, transform='vmap'):\n\n    def outer(y):\n        sin_1 = jax2tf.convert(jnp.sin)(1.0)\n        return y + sin_1\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        self.TransformConvertAndCompare(outer, np.ones((4,)), transform)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_name_scope(self):\n\n    def run_tf():\n\n        @jax.named_call\n        def my_test_function_jax(x):\n            return x * x\n\n        def caller_jax(x):\n            return my_test_function_jax(jnp.sin(x))\n        out = jax2tf.convert(caller_jax, with_gradient=False)(2.0)\n        return out\n    if config.jax2tf_default_native_serialization.value:\n        self.assertIn('my_test_function_jax/mul', self.TfToHlo(run_tf))\n    else:\n        graph_def = str(tf.function(run_tf, autograph=False).get_concrete_function().graph.as_graph_def())\n        if 'my_test_function_jax/pjit_multiply_/Mul' not in graph_def:\n            self.assertIn('my_test_function_jax/jit_multiply_/Mul', graph_def)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_bfloat16_constant(self):\n\n    def jax_fn_scalar(x):\n        x = x.astype(jnp.bfloat16)\n        x *= 2.0\n        return x\n\n    def jax_fn_array(x):\n        x = x.astype(jnp.bfloat16)\n        x *= np.array([1.5, 2.5, 3.5], jnp.bfloat16)\n        return x\n    tf_fn_scalar = jax2tf.convert(jax_fn_scalar)\n    self.assertAllClose(tf_fn_scalar(1.375).numpy(), jnp.bfloat16(2.75))\n    tf_fn_array = jax2tf.convert(jax_fn_array)\n    self.assertAllClose(tf_fn_array(np.array([3, 4, 5])), np.array([4.5, 10, 17.5], jnp.bfloat16))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_shared_constants(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const = np.random.uniform(size=256).astype(np.float32)\n\n    def f(x):\n        return x + const + const + const + const\n    f_tf_consts = self.FindLargeTfConstants(jax2tf.convert(f), const)\n    self.assertLen(f_tf_consts, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_shared_constants_under_scan(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    xs = np.ones((8, const_size), dtype=np.float32)\n\n    def f1(xs):\n        res, _ = lax.scan(lambda carry, x: (carry + x + const, None), jnp.zeros((const_size,), dtype=np.float32), xs)\n        return res\n\n    def f2(xs):\n        return f1(xs) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), xs, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), xs, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_shared_constants_under_jit(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const = np.random.uniform(size=(16, 16)).astype(np.float32)\n\n    @jax.jit\n    def g_jit(x):\n        return x * const\n\n    def f(x):\n        return g_jit(x) + const + const\n    f_tf_graph_consts = self.FindLargeTfConstants(jax2tf.convert(f), const)\n    self.assertLen(f_tf_graph_consts, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_weak_types(self):\n    mul = jax.jit(jnp.multiply)\n    tf_fn = jax2tf.convert(lambda x: mul(x, 2.0))\n    self.assertAllClose(tf_fn(tf.constant(1.375, tf.bfloat16)).numpy(), jnp.bfloat16(2.75))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_kwargs(self, with_function=False):\n\n    def f_jax(*, x):\n        return jnp.sum(x)\n    f_tf = jax2tf.convert(f_jax)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(f_tf(x=np.zeros(3, dtype=np.float32)), np.zeros((), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_grad_kwargs(self, with_function=False):\n    x = (np.zeros(3, dtype=np.float32), np.zeros(4, dtype=np.float32))\n\n    def f_jax(*, x=(1.0, 2.0)):\n        return jnp.sum(x[0]) + 2.0 * jnp.sum(x[1])\n    f_tf = jax2tf.convert(f_jax)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    xv = tf.nest.map_structure(tf.Variable, x)\n    with tf.GradientTape() as tape:\n        res = f_tf(x=xv)\n    grad_tf = tape.gradient(res, xv)\n    self.assertAllClose((np.full_like(x[0], fill_value=1.0), np.full_like(x[1], fill_value=2.0)), (grad_tf[0].numpy(), grad_tf[1].numpy()))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_name_scope_polymorphic(self):\n    if config.jax2tf_default_native_serialization.value and (not config.dynamic_shapes.value):\n        self.skipTest('shape polymorphism but --jax_dynamic_shapes is not set.')\n\n    def func_jax(x, y):\n        return jnp.sin(x) + jnp.cos(y)\n    func_tf = jax2tf.convert(func_jax, polymorphic_shapes='(b,...)', with_gradient=True)\n    outer_scope = 'output_a'\n    g = tf.Graph()\n    with g.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = func_tf(x, y)\n    self.assertAllOperationStartWith(g, outer_scope)\n    g2 = tf.Graph()\n    with g2.as_default() as g:\n        with tf.name_scope(outer_scope):\n            x = tf.Variable(tf.zeros(shape=(1, 5), dtype=tf.dtypes.float32), name='x')\n            y = tf.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')\n            _ = tf.function(func_tf, jit_compile=True, autograph=False)(x, y)\n    self.assertAllOperationStartWith(g2, outer_scope)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_name_scope_cond(self):\n\n    def f(x):\n\n        def f_pos(x):\n            with jax.named_scope('jax_f_pos'):\n                return lax.cond(x < 1.0, jnp.cos, jnp.sin, x)\n        with jax.named_scope('jax_f_outer'):\n            return lax.cond(x > 0.0, f_pos, lambda x: x, x)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_forward():\n        with tf.name_scope('tf_outer_forward'):\n            x = 0.5\n            f_tf = jax2tf.convert(f)\n            _ = f_tf(x)\n    g = outer_forward.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_forward')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_forward/jax2tf_f_/jax_f_outer')\n    x = tf.Variable(0.5, name='tf_outer_back/x')\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_back():\n        with tf.name_scope('tf_outer_back'):\n            f_tf = jax2tf.convert(f)\n            with tf.GradientTape() as tape:\n                res_tf = f_tf(x)\n                _ = tape.gradient(res_tf, x)\n    g = outer_back.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_back')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_back')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_name_scope_while_loop(self):\n\n    def f(x):\n        with tf.name_scope('outer_scope'):\n\n            def condition(x):\n                return jnp.sum(x, keepdims=False) < 100\n\n            def body(x):\n                return jnp.add(x, 2.0)\n            result = jax.lax.while_loop(condition, body, x)\n            return result\n    tf_f = tf.function(jax2tf.convert(f), jit_compile=True, autograph=False)\n    g = tf_f.get_concrete_function(tf.zeros((1, 3))).graph\n    for func in g._functions.values():\n        for op in func.graph.get_operations():\n            if op.name.count(f'outer_scope/jax2tf_{f.__name__}_/while') > 1:\n                self.fail(f'tf graph has repeated name issue on when converting lax.while to tf.while.See op.name = : {op.name}')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=f'{('with_mesh_' if with_mesh else '')}2={(transform2 if transform2 != 'none' else '')}_1={(transform1 if transform1 != 'none' else '')}{('_nullary' if nullary else '')}', with_mesh=with_mesh, transform1=transform1, transform2=transform2, nullary=nullary) for transform1 in ['none', 'jit', 'pjit', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding', 'shard_map', 'pmap'] for transform2 in ['none', 'pjit_in_shardings_None', 'pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] for nullary in ([True, False] if transform2 == 'none' else [False]) for with_mesh in ([True] if transform1 not in ['base', 'jit', 'pjit'] or transform2 != 'none' else [False, True])))\ndef test_cross_platform(self, with_mesh=True, transform1='pjit_in_shardings_P', transform2='pjit_in_shardings_P', nullary=False):\n    if transform2 == 'none' and (transform1 == 'shard_map' or (transform1 in ['pjit_in_shardings_P', 'pjit_in_shardings_Sharding'] and nullary)):\n        raise unittest.SkipTest('Skip because must have pjit at top level')\n    x = np.ones((4, 6), dtype=np.float32)\n    mesh = sharding.Mesh(jax.devices()[:1], ('a',))\n    func = lambda x: lax.cummax(x, axis=0, reverse=False)\n    func_shard_map = lambda x: lax.all_gather(x, 'a', axis=1, tiled=True)\n\n    def apply_transform(func, transform: str):\n        transformed_func = dict(none=func, jit=jax.jit(func), jit_in_shardings_None=jax.jit(func, in_shardings=None), jit_in_shardings_P=jax.jit(func, in_shardings=(P('a'),)), jit_in_shardings_Sharding=jax.jit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),)), pjit=pjit.pjit(func), pjit_in_shardings_None=pjit.pjit(func, in_shardings=None, out_shardings=None), pjit_in_shardings_P=pjit.pjit(func, in_shardings=(P('a'),), out_shardings=P('a')), pjit_in_shardings_Sharding=pjit.pjit(func, in_shardings=(sharding.NamedSharding(mesh, P('a')),), out_shardings=sharding.NamedSharding(mesh, P('a'))), shard_map=shard_map(func, mesh, in_specs=(P('a', None),), out_specs=P('a', None)), pmap=jax.pmap(func, in_axes=0, out_axes=0))[transform]\n        return transformed_func\n    transformed1_func = apply_transform(func_shard_map if transform1 == 'shard_map' else func, transform1)\n    assert transform2 not in ['shard_map']\n    transformed2_func = apply_transform(transformed1_func, transform2)\n    if transform1 == 'pmap':\n        x = x.reshape((1, -1))\n    if not nullary:\n        func_to_convert = transformed2_func\n        args = [x]\n    else:\n        func_to_convert = lambda: transformed2_func(jnp.ones(x.shape, dtype=x.dtype))\n        args = []\n    if transform1 == 'pmap':\n        if nullary:\n            raise unittest.SkipTest('Cannot lower nested pmap: jit-of-pmap warning')\n        raise unittest.SkipTest('TODO: figure out how to invoke pmap from TF')\n    f_tf = jax2tf.convert(func_to_convert, native_serialization=True, native_serialization_platforms=('tpu',))\n    f_tf = tf.function(f_tf, jit_compile=True, autograph=False)\n    with contextlib.ExitStack() as stack:\n        if with_mesh:\n            stack.enter_context(mesh)\n        _ = func_to_convert(*args)\n        exported = export.export(jax.jit(func_to_convert) if not hasattr(func_to_convert, 'trace') else func_to_convert, platforms=('tpu',))(*(core.ShapedArray(a.shape, a.dtype) for a in args))\n    if transform1 == 'shard_map':\n        self.assertIn('stablehlo.all_gather', str(exported.mlir_module()))\n    else:\n        self.assertIn('stablehlo.reduce_window', str(exported.mlir_module()))",
    "assertions": [
      "assert transform2 not in ['shard_map']"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_cross_platform_error(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization=True, native_serialization_platforms=('tpu',))\n    x = np.float32(0.5)\n    if jtu.test_device_matches(['tpu']):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    else:\n        f_tf_fun = tf.function(f_tf, jit_compile=True, autograph=False)\n        graph_def = f_tf_fun.get_concrete_function(x).graph.as_graph_def()\n        self.assertIn('XlaCallModule', str(graph_def))\n        with self.assertRaisesRegex(tf.errors.NotFoundError, 'The current platform .* is not among the platforms required by the module'):\n            f_tf(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.ignore_warning(message='using native_serialization_platforms without native_serialization')\ndef test_native_parameters_for_non_native(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_platforms=('cpu',))\n    x = np.float32(0.5)\n    tf_cpus = tf.config.list_logical_devices('CPU')\n    self.assertNotEmpty(tf_cpus)\n    with tf.device(tf_cpus[0]):\n        self.assertAllClose(jnp.sin(x), f_tf(x))\n    f_tf = jax2tf.convert(jnp.sin, native_serialization_disabled_checks=(jax2tf.DisabledSafetyCheck.platform(),))\n    self.assertAllClose(jnp.sin(x), f_tf(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_native_serialization_grad(self):\n    f_tf = jax2tf.convert(jnp.sin, native_serialization=True, native_serialization_platforms=('tpu',))\n    x = np.arange(4, dtype=np.float32)\n    x_v = tf.Variable(x)\n\n    @tf.function(autograph=False)\n    def f_grad_tf(x_v):\n        with tf.GradientTape() as tape:\n            tape.watch(x_v)\n            res_tf = f_tf(x_v)\n            return tape.gradient(res_tf, x_v)\n    f_grad_tf_fun = tf.function(f_grad_tf, autograph=False)\n    graph_def = f_grad_tf_fun.get_concrete_function(x).graph.as_graph_def()\n    logging.info('Found graph_def: %s', graph_def)\n    self.assertLen(re.findall('op:\\\\s*\"XlaCallModule\"', str(graph_def)), 2)\n    if not jtu.test_device_matches(['tpu']):\n        with self.assertRaisesRegex(tf.errors.NotFoundError, 'The current platform .* is not among the platforms required by the module: \\\\[TPU\\\\]'):\n            f_grad_tf(x_v)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_effects_error(self):\n\n    def f_jax(x):\n        jax.debug.print('{}', x)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_jax, native_serialization=True)(np.float32(42.0))\n\n    def f_ordered_jax(x):\n        jax.debug.print('{}', x, ordered=True)\n        return jnp.sin(x)\n    with self.assertRaisesRegex(NotImplementedError, 'serialization of host_callbacks is not yet implemented'):\n        jax2tf.convert(f_ordered_jax, native_serialization=True)(np.float32(42.0))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_multi_platform(self):\n    if config.enable_x64.value:\n        self.skipTest('TODO: enable when we can handle i64 platform_index_argument')\n    _testing_multi_platform_to_add = dict(cpu=2.0, tpu=3.0, cuda=4.0, rocm=5.0)\n\n    def f_jax(x):\n        return x + lax.platform_dependent(tpu=lambda: _testing_multi_platform_to_add['tpu'], cuda=lambda: _testing_multi_platform_to_add['cuda'], rocm=lambda: _testing_multi_platform_to_add['rocm'], default=lambda: _testing_multi_platform_to_add['cpu'])\n    x = np.float32(0.42)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=('cpu', 'cuda', 'tpu'))\n    for tf_device in self.tf_devices:\n        logging.info(f'Running on tf_device = {tf_device} of device_type = {tf_device.device_type}')\n        with tf.device(tf_device):\n            res = f_tf(x)\n        tf_device_jax_platform = dict(CPU='cpu', GPU='cuda', TPU='tpu')[tf_device.device_type]\n        self.assertAllClose(res, x + _testing_multi_platform_to_add[tf_device_jax_platform])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_dot_algorithm(self):\n    if tf.version.VERSION.split('.') <= ['2', '18', '0']:\n        self.skipTest('Because of an XLA bug this test segfaults with TF v2.18.0')\n    if jtu.test_device_matches(['tpu']):\n        algorithm = 'BF16_BF16_F32'\n    else:\n        algorithm = 'F32_F32_F32'\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision=algorithm)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    f_tf(np.ones((128, 128), dtype=np.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_dot_algorithm_non_native_unsupported(self):\n\n    def f_jax(x):\n        return jax.lax.dot(x, x, precision='F32_F32_F32')\n    x = np.ones((128, 128), dtype=np.float32)\n    with self.assertRaisesRegex(NotImplementedError, 'Unsupported precision in dot_general'):\n        jax2tf.convert(f_jax, native_serialization=False)(x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def call_tf(callable_tf: Callable, has_side_effects=True, ordered=False, output_shape_dtype=UnspecifiedOutputShapeDtype(), call_tf_graph=False) -> Callable:\n    \"\"\"Calls a TensorFlow function from JAX, with support for reverse autodiff.\n\n  The ``callable_tf`` will be called with TensorFlow-compatible arguments (\n  numpy.ndarray, ``tf.Tensor`` or ``tf.Variable``) or pytrees thereof. The\n  function must return the same type of results.\n\n  If ``call_tf`` appears in a JAX staging context (:func:`jax.jit`,\n  or :func:`jax.pmap`, or a control-flow primitive) then\n  ``callable_tf`` will be compiled with ``tf.function(callable_tf,\n  jit_compile=True)``\n  and the resulting XLA computation will be embedded in JAX's XLA computation.\n\n  If ``call_tf`` appears outside a JAX staging context, it will be called inline\n  using TensorFlow eager mode.\n\n  The ``call_tf`` supports JAX's reverse-mode autodiff, in which case the\n  ``callable_tf`` will be differentiated using ``tf.GradientTape``. This means\n  that the gradient will be TensorFlow-accurate, e.g., will respect the\n  custom gradients that may be defined for the code in ``callable_tf``.\n\n  For an example and more details see the\n  `README\n  <https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#calling-tensorflow-functions-from-jax>`_.\n\n  Args:\n    callable_tf: a TensorFlow Callable that can take a pytree of TensorFlow\n      arguments.\n    has_side_effects: if True then it ensures that instances of this primitive\n      are not removed or replicated by JAX optimizations such as dead-code\n      elimination.\n    ordered: If true, calls are modeled as having ordered effects.\n    output_shape_dtype: An optional declaration of the expected shape and dtype\n      of the result of the called TensorFlow function. If given it will be used\n      during JAX tracing to form the abstract values of the results of the\n      `call_tf`. If not given then we form a `tf.Graph` for the called\n      TensorFlow function and we use the TensorFlow-inferred shapes and types.\n      Must be a pytree matching the structure of the nested structure returned\n      from the TensorFlow function, containing objects with `.shape` and\n      `.dtype` attributes, e.g., `jax.ShapeDtypeStruct` or `jax.Array`.\n    call_tf_graph: EXPERIMENTAL, DO NOT USE. We may change the name in the\n      future.\n\n  Returns: a JAX callable that can be invoked with JAX pytree arguments, in\n    op-by-op mode or in a staged context. This callable can be used with JAX's\n    reverse-mode autodiff (:func:`jax.grad`).\n  \"\"\"\n\n    @jax.custom_vjp\n    def make_call(*args_jax):\n        \"\"\"We wrap it all in `make_call` so that we can attach custom VJP.\"\"\"\n        args_flat_jax, args_treedef = tree_util.tree_flatten(args_jax)\n\n        def canonical_arg(v):\n            v = v if getattr(v, 'dtype', None) else np.asarray(v)\n            dtype = dtypes.canonicalize_dtype(v.dtype)\n            if dtype != v.dtype:\n                v = v.astype(dtype)\n            return v\n        args_flat_jax = tuple(map(canonical_arg, args_flat_jax))\n\n        def make_tensorspec(a_jax):\n            a_tf_dtype = jax2tf_internal._to_tf_dtype(a_jax.dtype)\n            a_tf_shape = [d if core.is_constant_dim(d) else None for d in a_jax.shape]\n            return tf.TensorSpec(a_tf_shape, a_tf_dtype)\n        args_flat_sig_tf = tuple(map(make_tensorspec, args_flat_jax))\n        if not isinstance(output_shape_dtype, UnspecifiedOutputShapeDtype):\n            output_shape_dtype_flat, output_shape_dtype_tree = tree_util.tree_flatten(output_shape_dtype)\n            output_avals = tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))\n        else:\n            output_avals, output_shape_dtype_tree = (None, None)\n        res_treedef = None\n        res_tf_flat = None\n\n        def callable_flat_tf(*args_tf_flat: TfVal) -> Sequence[TfVal]:\n            args_tf = args_treedef.unflatten(args_tf_flat)\n            res_tf = callable_tf(*args_tf)\n            if isinstance(res_tf, tf.Operation):\n                assert res_tf.type == 'StatefulPartitionedCall' or res_tf.type == 'PartitionedCall'\n                t_out = res_tf.get_attr('Tout')\n                assert not t_out, f'The TF function returned an unexpected result, please check its function body. res_tf = {res_tf}'\n                res_tf = t_out\n            nonlocal res_treedef, res_tf_flat\n            res_tf_flat, res_treedef_now = tree_util.tree_flatten(res_tf)\n            assert res_treedef is None or res_treedef == res_treedef_now, f'Subsequent calls had different results. Previous {res_treedef} and now {res_treedef_now}'\n            res_treedef = res_treedef_now\n            if output_avals is not None:\n                if res_treedef != output_shape_dtype_tree:\n                    raise ValueError(f'The pytree of the TensorFlow function results does not match the pytree of the declared output_shape_dtype:\\nresults pytree: {res_treedef}\\noutput_shape_dtype tree: {output_shape_dtype_tree}')\n                assert len(output_avals) == len(res_tf_flat)\n            checked_res_tf_flat = [check_tf_result(i, r_tf, r_aval) for i, (r_tf, r_aval) in enumerate(zip(res_tf_flat, output_avals if output_avals is not None else (None,) * len(res_tf_flat)))]\n            return checked_res_tf_flat\n        function_flat_tf = tf.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)\n        res_jax_flat = call_tf_p.bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)\n        assert res_treedef is not None\n        return res_treedef.unflatten(res_jax_flat)\n\n    def make_call_vjp_fwd(*args_jax):\n        return (make_call(*args_jax), args_jax)\n\n    def make_call_vjp_bwd(residual_jax, ct_res_jax):\n        args_jax = residual_jax\n\n        def tf_vjp_fun(args_tf, ct_res_tf):\n            \"\"\"Invoke TF gradient.\"\"\"\n\n            def replace_non_float_or_none(arg_tf):\n                if arg_tf is not None and (arg_tf.dtype.is_floating or arg_tf.dtype.is_complex):\n                    return arg_tf\n                else:\n                    return tf.zeros((), dtype=tf.float32)\n            watched_args_tf = tf.nest.map_structure(replace_non_float_or_none, args_tf)\n            with tf.GradientTape(persistent=True) as tape:\n                tape.watch(watched_args_tf)\n                res = callable_tf(*args_tf)\n            tf.nest.assert_same_structure(res, ct_res_tf)\n            dres_darg = tape.gradient(tf.nest.map_structure(replace_non_float_or_none, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n            dres_darg = tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)\n            tf.nest.assert_same_structure(dres_darg, watched_args_tf)\n            return dres_darg\n        ct_args_jax = call_tf(tf_vjp_fun)(args_jax, ct_res_jax)\n\n        def fix_float0(arg_jax, ct_arg_jax):\n            if arg_jax is None:\n                return None\n            arg_dtype = dtypes.result_type(arg_jax)\n            ct_arg_dtype = core.primal_dtype_to_tangent_dtype(arg_dtype)\n            if ct_arg_dtype != ct_arg_jax.dtype:\n                return ad_util.zeros_like_aval(core.ShapedArray(np.shape(arg_jax), ct_arg_dtype))\n            return ct_arg_jax\n        ct_args_jax_fixed = tree_util.tree_map(fix_float0, args_jax, ct_args_jax, is_leaf=lambda x: x is None)\n        return ct_args_jax_fixed\n    make_call.defvjp(make_call_vjp_fwd, make_call_vjp_bwd)\n    return util.wraps(callable_tf)(make_call)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(x):\n\n    def cond(idx_carry):\n        i, c = idx_carry\n        return i < jnp.sum(cond_const)\n\n    def body(idx_carry):\n        i, c = idx_carry\n        return (i + 1, c + body_const1 + body_const2)\n    return lax.while_loop(cond, body, (0, x))"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(x):\n\n    def cond(idx_carry):\n        i, c = idx_carry\n        return i < jnp.sum(cond_const)\n\n    def body(idx_carry):\n        i, c = idx_carry\n        return (i + 1, c + body_const1 + body_const2)\n    return lax.while_loop(cond, body, (0, x))"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(x):\n\n    def cond(idx_carry):\n        i, c = idx_carry\n        return i < jnp.sum(cond_const)\n\n    def body(idx_carry):\n        i, c = idx_carry\n        return (i + 1, c + body_const1 + body_const2)\n    return lax.while_loop(cond, body, (0, x))"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_jit_unused(self):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))(x, y_unused)\n    self.assertAllClose(f_jax(x, None), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=mode, mode=mode) for mode in ('eager', 'graph', 'compiled')))\ndef test_jit_unused_grad(self, mode='eager'):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_jax = f_jax(x, y_unused)\n    f_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))\n    x_tf, y_unused_tf = (tf.constant(x), tf.constant(y_unused))\n\n    def grad_tf(x, y_unused):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            tape.watch(y_unused)\n            res_tf = f_tf(x, y_unused)\n            grad_tf_x, grad_tf_y = tape.gradient(res_tf, (x, y_unused))\n        return (res_tf, grad_tf_x, grad_tf_y)\n    if mode == 'graph':\n        grad_tf = tf.function(grad_tf, autograph=False)\n    elif mode == 'compiled':\n        grad_tf = tf.function(grad_tf, autograph=False, jit_compile=True)\n    res_tf, grad_tf_x, grad_tf_y = grad_tf(x_tf, y_unused_tf)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(np.float32(2.0), grad_tf_x)\n    self.assertIsNone(grad_tf_y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(idx_carry):\n    i, c = idx_carry\n    return i < jnp.sum(cond_const)"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(idx_carry):\n    i, c = idx_carry\n    return i < jnp.sum(cond_const)"
  },
  {
    "test_code": "def test_op_metadata_while_and_cond(self):\n    self.skipTest('include_xla_op_metadata not yet enabled')\n    user_frame = source_info_util.user_frame(source_info_util.current())\n\n    def f_while_cond(x):\n\n        def body_fun(i_acc):\n            i, acc = i_acc\n            return (i + 1, jnp.cos(acc) + lax.cond(jnp.mod(i, 2) == 0, lambda acc: jnp.sin(acc), lambda acc: acc, acc))\n        _, acc = lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))\n        return acc\n    x = np.ones((2, 3), np.float32)\n    self.CheckOpMetadata(f_while_cond, x, [tf_test_util.OpMetadataGraph(tf_type='Cos', source_file=__file__, source_line=user_frame.start_line + 5, op_name='jax2tf(f_while_cond)/while/body/cos', op_type='cos'), tf_test_util.OpMetadataGraph(tf_type='Sin', source_file=__file__, source_line=user_frame.start_line + 7, op_name='jax2tf(f_while_cond)/while/body/branch_1_fun/sin', op_type='sin'), tf_test_util.OpMetadataGraph(tf_type='FloorMod', source_file=__file__, source_line=user_frame.start_line + 6, op_name='jax2tf(f_while_cond)/while/body/rem', op_type='rem')])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(idx_carry):\n    i, c = idx_carry\n    return i < jnp.sum(cond_const)"
  },
  {
    "test_code": "def test_name_scope_cond(self):\n\n    def f(x):\n\n        def f_pos(x):\n            with jax.named_scope('jax_f_pos'):\n                return lax.cond(x < 1.0, jnp.cos, jnp.sin, x)\n        with jax.named_scope('jax_f_outer'):\n            return lax.cond(x > 0.0, f_pos, lambda x: x, x)\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_forward():\n        with tf.name_scope('tf_outer_forward'):\n            x = 0.5\n            f_tf = jax2tf.convert(f)\n            _ = f_tf(x)\n    g = outer_forward.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_forward')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_forward/jax2tf_f_/jax_f_outer')\n    x = tf.Variable(0.5, name='tf_outer_back/x')\n\n    @tf.function(jit_compile=True, autograph=False)\n    def outer_back():\n        with tf.name_scope('tf_outer_back'):\n            f_tf = jax2tf.convert(f)\n            with tf.GradientTape() as tape:\n                res_tf = f_tf(x)\n                _ = tape.gradient(res_tf, x)\n    g = outer_back.get_concrete_function().graph\n    self.assertAllOperationStartWith(g, 'tf_outer_back')\n    for func in g._functions.values():\n        self.assertAllOperationStartWith(func.graph, 'tf_outer_back')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def cond(idx_carry):\n    i, c = idx_carry\n    return i < jnp.sum(cond_const)"
  },
  {
    "test_code": "def test_key_argument(self):\n    func = lambda key: jax.random.uniform(key, ())\n    key = jax.random.PRNGKey(0)\n    key_raw = jax.random.key_data(key)\n    with self.assertWarnsRegex(FutureWarning, 'Raw arrays as random keys.*'):\n        tf_result = jax2tf.convert(func)(key_raw)\n    jax_result = func(key)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(x):\n\n    def func_tf(x):\n        return tf.math.sin(x)\n    return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))"
  },
  {
    "test_code": "def test_key_from_seed(self):\n    func = lambda seed: jax.random.uniform(jax.random.PRNGKey(seed), ())\n    seed = 1701\n    tf_result = jax2tf.convert(func)(seed)\n    jax_result = func(seed)\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(x):\n\n    def func_tf(x):\n        return tf.math.sin(x)\n    return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))"
  },
  {
    "test_code": "def test_key_closure(self):\n\n    def func():\n        key = global_key.reshape(1).squeeze()\n        return jax.random.uniform(key)\n    global_key = jax.random.PRNGKey(0)\n    tf_result = jax2tf.convert(func)()\n    jax_result = func()\n    self.assertEqual(tf_result, jax_result)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def func(x):\n\n    def func_tf(x):\n        return tf.math.sin(x)\n    return jnp.cos(jax2tf.call_tf(func_tf, output_shape_dtype=x, call_tf_graph=True)(x))"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_jit_unused(self):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))(x, y_unused)\n    self.assertAllClose(f_jax(x, None), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=mode, mode=mode) for mode in ('eager', 'graph', 'compiled')))\ndef test_jit_unused_grad(self, mode='eager'):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_jax = f_jax(x, y_unused)\n    f_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))\n    x_tf, y_unused_tf = (tf.constant(x), tf.constant(y_unused))\n\n    def grad_tf(x, y_unused):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            tape.watch(y_unused)\n            res_tf = f_tf(x, y_unused)\n            grad_tf_x, grad_tf_y = tape.gradient(res_tf, (x, y_unused))\n        return (res_tf, grad_tf_x, grad_tf_y)\n    if mode == 'graph':\n        grad_tf = tf.function(grad_tf, autograph=False)\n    elif mode == 'compiled':\n        grad_tf = tf.function(grad_tf, autograph=False, jit_compile=True)\n    res_tf, grad_tf_x, grad_tf_y = grad_tf(x_tf, y_unused_tf)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(np.float32(2.0), grad_tf_x)\n    self.assertIsNone(grad_tf_y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_shared_constants_under_cond(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    x = np.ones((const_size,), dtype=np.float32)\n\n    def f1(x):\n        return lax.cond(x[0] >= 0.0, lambda x: x + const, lambda x: x * const, x) + const\n\n    def f2(x):\n        return f1(x) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), x, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), x, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f1(x, y):\n    return (x, jnp.concatenate([x, y], axis=0))"
  },
  {
    "test_code": "def test_shared_constants_under_scan(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    const_size = 512\n    const = np.random.uniform(size=const_size).astype(np.float32)\n    xs = np.ones((8, const_size), dtype=np.float32)\n\n    def f1(xs):\n        res, _ = lax.scan(lambda carry, x: (carry + x + const, None), jnp.zeros((const_size,), dtype=np.float32), xs)\n        return res\n\n    def f2(xs):\n        return f1(xs) + const\n    f1_consts = self.FindLargeTfConstants(jax2tf.convert(f1), xs, at_least=const_size)\n    f2_consts = self.FindLargeTfConstants(jax2tf.convert(f2), xs, at_least=const_size)\n    self.assertLen(f2_consts, len(f1_consts))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f1(x, y):\n    return (x, jnp.concatenate([x, y], axis=0))"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_int_argument(self, with_function=False):\n    state = dict(float_used=np.array([0.7, 0.9], dtype=np.float32), float_passthrough=np.float16(1.0), float_unused=np.array([1.1, 2.2, 3.3], dtype=np.float32), int_used=np.int16(5), int_passthrough=np.int8(7), int_unused=np.array([1, 2, 3], dtype=np.uint32), bool_used=np.array([True, False, False, True], dtype=np.bool_), bool_passthrough=np.array([True, False, False, True, False], dtype=np.bool_), bool_unused=np.array([[True, False], [False, True]], dtype=np.bool_))\n\n    def jax_f(state):\n        res = dict(state, float_used=2.0 * state['float_used'], int_used=3 * state['int_used'], bool_used=state['bool_used'] == state['bool_used'])\n        del res['float_unused']\n        del res['int_unused']\n        del res['bool_unused']\n        return res\n    args = (state,)\n    res_jax = jax_f(*args)\n    vjp_jax_fun, args_vjp = tf_test_util.TransformJaxVJP(jax_f, args, res_jax)\n    grad_jax, = vjp_jax_fun(*args_vjp)\n\n    def compare_with_overrides(*, what, expected, **expected_overrides):\n        what_keys = set(what.keys())\n        expected_keys = set(expected.keys())\n        self.assertEqual(what_keys, expected_keys)\n        for k, w in what.items():\n            e = expected[k]\n            if k in expected_overrides:\n                if expected_overrides[k] == 'ZERO':\n                    e = np.zeros_like(w)\n                elif expected_overrides[k] == 'ZERO_BOOL':\n                    e = np.zeros(np.shape(w), dtype=np.bool_)\n                elif expected_overrides[k] == 'ONE':\n                    e = np.ones_like(w)\n                else:\n                    e = expected_overrides[k]\n            if e is None:\n                self.assertIsNone(w, msg=k)\n            else:\n                self.assertIsNotNone(w, msg=k)\n            w = w.numpy() if isinstance(w, tf.Tensor) else e\n            e = e.numpy() if isinstance(e, tf.Tensor) else e\n            try:\n                self.assertAllClose(e, w, err_msg=k)\n            except:\n                print(f'Failed at {k}')\n                raise\n    _, (grad_tf_0,) = tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    compare_with_overrides(what=grad_tf_0, expected=grad_jax, float_unused='ZERO', bool_used='ZERO', bool_passthrough='ONE', bool_unused='ZERO', int_used='ZERO', int_passthrough='ONE', int_unused='ZERO')\n    _, (grad_tf_None,) = tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.NONE)\n    compare_with_overrides(what=grad_tf_None, expected=grad_tf_0, float_unused=None, int_used=None, int_unused=None, bool_used=None, bool_unused=None)\n    f_tf_jax = jax2tf.convert(jax_f)\n    if with_function:\n        f_tf_jax = tf.function(f_tf_jax, autograph=False)\n    _, (grad_tf_jax_0,) = tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args)\n    compare_with_overrides(what=grad_tf_jax_0, expected=grad_tf_0, int_passthrough='ZERO', bool_passthrough='ZERO')\n    _, (grad_tf_jax_None,) = tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args, unconnected_gradients=tf.UnconnectedGradients.NONE)\n    compare_with_overrides(what=grad_tf_jax_None, expected=grad_tf_0, int_used=None, int_passthrough=None, int_unused=None, bool_unused=None, bool_used=None, bool_passthrough=None)\n    tf_vjp_jax_fun = jax2tf.convert(vjp_jax_fun)\n    grad_tf_vjp_jax, = tf_vjp_jax_fun(*args_vjp)\n    compare_with_overrides(what=grad_tf_vjp_jax, expected=grad_tf_0, bool_passthrough='ZERO_BOOL', bool_unused='ZERO_BOOL', bool_used='ZERO_BOOL', int_passthrough='ZERO_BOOL', int_unused='ZERO_BOOL', int_used='ZERO_BOOL')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def TransformJaxVJP(f: Callable, args, res_f_of_args):\n\n    def make_ct(res):\n        res_dtype = np.result_type(res)\n        assert res_dtype != dtypes.float0\n        return np.ones(np.shape(res), dtype=res_dtype)\n    cts = tree_util.tree_map(make_ct, res_f_of_args)\n\n    def f_vjp(args, cts):\n        res, pullback = jax.vjp(f, *args)\n        return pullback(cts)\n    return (f_vjp, (args, cts))"
  },
  {
    "test_code": "def test_empty(self):\n    f_jax = lambda x, y: x\n    self.ConvertAndCompare(f_jax, 0.7, 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_basics(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_pytrees(self):\n\n    def f_jax(x: tuple[float, dict[str, float]]) -> tuple[float, dict[str, float]]:\n        x_a, x_dict = x\n        return (x_a * 2.0, {k: v * 3.0 for k, v in x_dict.items()})\n    x = (0.7, {'a': 0.8, 'b': 0.9})\n    self.ConvertAndCompare(f_jax, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_jit(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_nested_jit(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jax.jit(jnp.cos)(x)))\n    x = 0.7\n    self.ConvertAndCompare(f_jax, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_nested_jit_pytree(self):\n\n    @jax.jit\n    def f_jax(xy):\n        x, y = xy\n        return x + y\n    xy = (0.7, 0.8)\n    self.ConvertAndCompare(f_jax, xy)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "@jtu.sample_product(dtype=[np.int64, np.float64], with_function=[True, False])\ndef test_converts_64bit(self, dtype=np.int64, with_function=False):\n    if not config.enable_x64.value:\n        self.skipTest('requires x64 mode')\n    big_const = np.full((5,), 2 ** 33, dtype=dtype)\n    self.ConvertAndCompare(jnp.sin, big_const)\n    f_conv = jax2tf.convert(jnp.sin)\n    if with_function:\n        f_conv = tf.function(f_conv, autograph=False)\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.Variable(big_const)))\n    self.assertAllClose(jnp.sin(big_const), f_conv(tf.constant(big_const)))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_function(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    self.ConvertAndCompare(f_jax, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_device_array_arg(self):\n    self.ConvertAndCompare(jnp.sin, jnp.zeros((2, 3), jnp.float32))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_randint(self):\n\n    def randint():\n        return jax.random.randint(jax.random.PRNGKey(42), shape=(), minval=0, maxval=1)\n    self.ConvertAndCompare(randint)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='jax2tf.convert with native_serialization=False has been deprecated')\ndef test_simple(self):\n    self.ConvertAndCompare(jnp.sin, 0.7)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def ConvertAndCompare(self, func_jax: Callable, *args, enable_xla: bool=True, limitations: Sequence=()):\n    \"\"\"Compares jax_func(*args) with convert(jax_func)(*args).\n\n    It compares the result of JAX, TF (\"eager\" mode),\n    TF with tf.function (\"graph\" mode), and TF with\n    tf.function(jit_compile=True) (\"compiled\" mode). In each mode,\n    either we expect to encounter a known limitation, or the value should\n    match the value from the JAX execution.\n\n    Args:\n      func_jax: the function to invoke (``func_jax(*args)``)\n      args: the arguments.\n      enable_xla: if True, allows the use of XLA ops in jax2tf.convert\n        (default: True).\n      limitations: the set of limitations for this harness (not yet filtered\n        by mode).\n    \"\"\"\n    result_jax = func_jax(*args)\n    result_tf = None\n    func_tf = jax2tf.convert(func_jax, enable_xla=enable_xla)\n    unexpected_successes: list[str] = []\n    for mode in ('compiled', 'eager', 'graph'):\n        if mode == 'graph' and jtu.device_under_test() == 'tpu':\n            continue\n\n        def log_message(extra):\n            return f'[{self._testMethodName}] mode={mode!r}: {extra}'\n        jax2tf_limits = tuple(filter(lambda l: l.filter(mode=mode), limitations))\n        skip_tf_run = [l for l in jax2tf_limits if l.skip_tf_run]\n        if skip_tf_run:\n            logging.info(log_message(f'Skip TF run due to limitations {skip_tf_run}'))\n            continue\n        try:\n            result_tf = _run_tf_function(func_tf, *args, mode=mode)\n            tf_exception = None\n        except Exception as e:\n            tf_exception = e\n        expect_tf_error = [l for l in jax2tf_limits if l.expect_tf_error]\n        if tf_exception:\n            if expect_tf_error:\n                logging.info(log_message(f'Found expected TF error with enabled limitations {expect_tf_error}; TF error is {tf_exception}'))\n                continue\n            else:\n                raise tf_exception\n        elif expect_tf_error:\n            logging.warning(log_message(f'Unexpected execution success with known limitations {expect_tf_error}'))\n            unexpected_successes.append(f'{mode}: {expect_tf_error}')\n        if jtu.device_under_test() == 'gpu' and 'dot_general_preferred' in self._testMethodName:\n            logging.info(log_message(f'Arguments are {args}, JAX result is {result_jax}\\nand TF result is {result_tf}'))\n        skip_comparison = [l for l in jax2tf_limits if l.skip_comparison]\n        if skip_comparison:\n            logging.warning(log_message(f'Skip result comparison due to {skip_comparison}'))\n            continue\n        max_tol = None\n        max_tol_lim = None if not jax2tf_limits else jax2tf_limits[0].get_max_tolerance_limitation(jax2tf_limits)\n        if max_tol_lim is not None:\n            max_tol = max_tol_lim.tol\n            logging.info(log_message(f'Using tol={max_tol} due to {max_tol_lim}'))\n        result_tf = tf.nest.map_structure(lambda t: t.numpy(), result_tf)\n        custom_assert_lim = [l for l in jax2tf_limits if l.custom_assert]\n        assert len(custom_assert_lim) <= 1, f'Expecting at most one applicable limitation with custom_assert, found {custom_assert_lim}'\n        try:\n            err_msg = f'TF mode {mode}.'\n            log_hlo_on_error = mode == 'compiled' or jtu.device_under_test() == 'tpu'\n            if log_hlo_on_error:\n                err_msg += ' See the logs for JAX and TF HLO comparisons.'\n            if custom_assert_lim:\n                logging.info(log_message(f'Running custom_assert with tol={max_tol} due to {custom_assert_lim[0]}'))\n                custom_assert_lim[0].custom_assert(self, result_jax, result_tf, args=args, tol=max_tol, err_msg=err_msg)\n            else:\n                logging.info(log_message(f'Running default assert with tol={max_tol}'))\n                self.assertAllClose(result_jax, result_tf, atol=max_tol, rtol=max_tol, err_msg=err_msg)\n        except AssertionError as e:\n            if not log_hlo_on_error:\n                print(f'[{self._testMethodName}] Not logging HLO because the mode was {mode}')\n                raise\n            logging.info('[%s] Logging HLO for exception in mode %s: %s', self._testMethodName, mode, e)\n            jax_lowered = jax.jit(func_jax).lower(*args)\n            logging.info('[%s] JAX NON_OPT HLO\\n%s', self._testMethodName, jax_lowered.compiler_ir(dialect='hlo').as_hlo_text())\n            tf_args_signature = _make_tf_input_signature(*args)\n            tf_args_no_scalars = tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))\n            tf_func_compiled = tf.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)\n            tf_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')\n            logging.info('[%s] TF NON OPT HLO\\n{%s}', self._testMethodName, tf_hlo)\n            backend = xla_bridge.get_backend()\n            modules = backend.compile(str(jax_lowered.compiler_ir())).hlo_modules()\n            jax_opt_hlo = modules[0].to_string()\n            logging.info('[%s] JAX OPT HLO\\n%s', self._testMethodName, jax_opt_hlo)\n            tf_opt_hlo = tf_func_compiled.experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')\n            logging.info('[%s] TF OPT HLO\\n%s', self._testMethodName, tf_opt_hlo)\n            raise\n    if unexpected_successes:\n        msg = f'[{self._testMethodName}] The following are unexpected successful modes:\\n' + '\\n'.join(unexpected_successes)\n        logging.warning(msg)\n    return (result_jax, result_tf)"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def TransformConvertAndCompare(self, func: Callable, arg, transform: str | None):\n    \"\"\"Like ConvertAndCompare but first applies a transformation.\n\n    `func` must be a function from one argument to one result. `arg` is\n    the argument before the transformation.\n\n    `transform` can be None, \"jit\", \"jvp\", \"grad\", \"vmap\", \"jvp_vmap\",\n    \"grad_vmap\"\n    \"\"\"\n    if transform is None:\n        return self.ConvertAndCompare(func, arg)\n    if transform == 'jit':\n        return self.ConvertAndCompare(jax.jit(func), arg)\n    if transform == 'jvp':\n        t_func = lambda x, xt: jax.jvp(func, (x,), (xt,))\n        return self.ConvertAndCompare(t_func, arg, np.full_like(arg, 0.1))\n    if transform == 'grad':\n        return self.ConvertAndCompare(jax.grad(func), arg)\n    if transform == 'vmap':\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(jax.vmap(func), t_arg)\n    if transform == 'jvp_vmap':\n        jvp_func = lambda x, xt: jax.jvp(jax.vmap(func), (x,), (xt,))\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(jvp_func, t_arg, np.full_like(t_arg, 0.1))\n    if transform == 'grad_vmap':\n        grad_func = jax.grad(lambda x: jnp.sum(jax.vmap(func)(x)))\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(grad_func, t_arg)\n    assert False, transform"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def TransformConvertAndCompare(self, func: Callable, arg, transform: str | None):\n    \"\"\"Like ConvertAndCompare but first applies a transformation.\n\n    `func` must be a function from one argument to one result. `arg` is\n    the argument before the transformation.\n\n    `transform` can be None, \"jit\", \"jvp\", \"grad\", \"vmap\", \"jvp_vmap\",\n    \"grad_vmap\"\n    \"\"\"\n    if transform is None:\n        return self.ConvertAndCompare(func, arg)\n    if transform == 'jit':\n        return self.ConvertAndCompare(jax.jit(func), arg)\n    if transform == 'jvp':\n        t_func = lambda x, xt: jax.jvp(func, (x,), (xt,))\n        return self.ConvertAndCompare(t_func, arg, np.full_like(arg, 0.1))\n    if transform == 'grad':\n        return self.ConvertAndCompare(jax.grad(func), arg)\n    if transform == 'vmap':\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(jax.vmap(func), t_arg)\n    if transform == 'jvp_vmap':\n        jvp_func = lambda x, xt: jax.jvp(jax.vmap(func), (x,), (xt,))\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(jvp_func, t_arg, np.full_like(t_arg, 0.1))\n    if transform == 'grad_vmap':\n        grad_func = jax.grad(lambda x: jnp.sum(jax.vmap(func)(x)))\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(grad_func, t_arg)\n    assert False, transform"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def TransformConvertAndCompare(self, func: Callable, arg, transform: str | None):\n    \"\"\"Like ConvertAndCompare but first applies a transformation.\n\n    `func` must be a function from one argument to one result. `arg` is\n    the argument before the transformation.\n\n    `transform` can be None, \"jit\", \"jvp\", \"grad\", \"vmap\", \"jvp_vmap\",\n    \"grad_vmap\"\n    \"\"\"\n    if transform is None:\n        return self.ConvertAndCompare(func, arg)\n    if transform == 'jit':\n        return self.ConvertAndCompare(jax.jit(func), arg)\n    if transform == 'jvp':\n        t_func = lambda x, xt: jax.jvp(func, (x,), (xt,))\n        return self.ConvertAndCompare(t_func, arg, np.full_like(arg, 0.1))\n    if transform == 'grad':\n        return self.ConvertAndCompare(jax.grad(func), arg)\n    if transform == 'vmap':\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(jax.vmap(func), t_arg)\n    if transform == 'jvp_vmap':\n        jvp_func = lambda x, xt: jax.jvp(jax.vmap(func), (x,), (xt,))\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(jvp_func, t_arg, np.full_like(t_arg, 0.1))\n    if transform == 'grad_vmap':\n        grad_func = jax.grad(lambda x: jnp.sum(jax.vmap(func)(x)))\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(grad_func, t_arg)\n    assert False, transform"
  },
  {
    "test_code": "@jtu.sample_product(transform=['jit', 'jvp', 'grad', 'vmap'])\ndef test_convert_under_transform_error(self, transform='vmap'):\n\n    def outer(y):\n        return jax2tf.convert(jnp.sin)(y)\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        self.TransformConvertAndCompare(outer, np.ones((4,)), transform)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def TransformConvertAndCompare(self, func: Callable, arg, transform: str | None):\n    \"\"\"Like ConvertAndCompare but first applies a transformation.\n\n    `func` must be a function from one argument to one result. `arg` is\n    the argument before the transformation.\n\n    `transform` can be None, \"jit\", \"jvp\", \"grad\", \"vmap\", \"jvp_vmap\",\n    \"grad_vmap\"\n    \"\"\"\n    if transform is None:\n        return self.ConvertAndCompare(func, arg)\n    if transform == 'jit':\n        return self.ConvertAndCompare(jax.jit(func), arg)\n    if transform == 'jvp':\n        t_func = lambda x, xt: jax.jvp(func, (x,), (xt,))\n        return self.ConvertAndCompare(t_func, arg, np.full_like(arg, 0.1))\n    if transform == 'grad':\n        return self.ConvertAndCompare(jax.grad(func), arg)\n    if transform == 'vmap':\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(jax.vmap(func), t_arg)\n    if transform == 'jvp_vmap':\n        jvp_func = lambda x, xt: jax.jvp(jax.vmap(func), (x,), (xt,))\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(jvp_func, t_arg, np.full_like(t_arg, 0.1))\n    if transform == 'grad_vmap':\n        grad_func = jax.grad(lambda x: jnp.sum(jax.vmap(func)(x)))\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(grad_func, t_arg)\n    assert False, transform"
  },
  {
    "test_code": "@jtu.sample_product(transform=['jit', 'jvp', 'grad', 'vmap'])\ndef test_convert_under_transform_error_non_tracer(self, transform='vmap'):\n\n    def outer(y):\n        sin_1 = jax2tf.convert(jnp.sin)(1.0)\n        return y + sin_1\n    with self.assertRaisesRegex(ValueError, 'convert must be used outside all JAX transformations'):\n        self.TransformConvertAndCompare(outer, np.ones((4,)), transform)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def TransformConvertAndCompare(self, func: Callable, arg, transform: str | None):\n    \"\"\"Like ConvertAndCompare but first applies a transformation.\n\n    `func` must be a function from one argument to one result. `arg` is\n    the argument before the transformation.\n\n    `transform` can be None, \"jit\", \"jvp\", \"grad\", \"vmap\", \"jvp_vmap\",\n    \"grad_vmap\"\n    \"\"\"\n    if transform is None:\n        return self.ConvertAndCompare(func, arg)\n    if transform == 'jit':\n        return self.ConvertAndCompare(jax.jit(func), arg)\n    if transform == 'jvp':\n        t_func = lambda x, xt: jax.jvp(func, (x,), (xt,))\n        return self.ConvertAndCompare(t_func, arg, np.full_like(arg, 0.1))\n    if transform == 'grad':\n        return self.ConvertAndCompare(jax.grad(func), arg)\n    if transform == 'vmap':\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(jax.vmap(func), t_arg)\n    if transform == 'jvp_vmap':\n        jvp_func = lambda x, xt: jax.jvp(jax.vmap(func), (x,), (xt,))\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(jvp_func, t_arg, np.full_like(t_arg, 0.1))\n    if transform == 'grad_vmap':\n        grad_func = jax.grad(lambda x: jnp.sum(jax.vmap(func)(x)))\n        t_arg = np.stack([arg] * 4)\n        return self.ConvertAndCompare(grad_func, t_arg)\n    assert False, transform"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_jit_unused(self):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))(x, y_unused)\n    self.assertAllClose(f_jax(x, None), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=mode, mode=mode) for mode in ('eager', 'graph', 'compiled')))\ndef test_jit_unused_grad(self, mode='eager'):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_jax = f_jax(x, y_unused)\n    f_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))\n    x_tf, y_unused_tf = (tf.constant(x), tf.constant(y_unused))\n\n    def grad_tf(x, y_unused):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            tape.watch(y_unused)\n            res_tf = f_tf(x, y_unused)\n            grad_tf_x, grad_tf_y = tape.gradient(res_tf, (x, y_unused))\n        return (res_tf, grad_tf_x, grad_tf_y)\n    if mode == 'graph':\n        grad_tf = tf.function(grad_tf, autograph=False)\n    elif mode == 'compiled':\n        grad_tf = tf.function(grad_tf, autograph=False, jit_compile=True)\n    res_tf, grad_tf_x, grad_tf_y = grad_tf(x_tf, y_unused_tf)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(np.float32(2.0), grad_tf_x)\n    self.assertIsNone(grad_tf_y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_jvp\ndef f_jax(x):\n    return x * x"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  },
  {
    "test_code": "def test_jit_unused(self):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))(x, y_unused)\n    self.assertAllClose(f_jax(x, None), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=mode, mode=mode) for mode in ('eager', 'graph', 'compiled')))\ndef test_jit_unused_grad(self, mode='eager'):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_jax = f_jax(x, y_unused)\n    f_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))\n    x_tf, y_unused_tf = (tf.constant(x), tf.constant(y_unused))\n\n    def grad_tf(x, y_unused):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            tape.watch(y_unused)\n            res_tf = f_tf(x, y_unused)\n            grad_tf_x, grad_tf_y = tape.gradient(res_tf, (x, y_unused))\n        return (res_tf, grad_tf_x, grad_tf_y)\n    if mode == 'graph':\n        grad_tf = tf.function(grad_tf, autograph=False)\n    elif mode == 'compiled':\n        grad_tf = tf.function(grad_tf, autograph=False, jit_compile=True)\n    res_tf, grad_tf_x, grad_tf_y = grad_tf(x_tf, y_unused_tf)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(np.float32(2.0), grad_tf_x)\n    self.assertIsNone(grad_tf_y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.jit\ndef f_jax(x):\n    return jax.lax.fori_loop(0, 4, body, x)"
  },
  {
    "test_code": "def test_input_output_naming(self):\n\n    @jax2tf.convert\n    def f(xs, y):\n        return [jnp.add(x, y) for x in xs]\n\n    @tf.function(autograph=False)\n    def u(xs, y):\n        xs = tf.nest.map_structure(tf.convert_to_tensor, xs)\n        with tf.GradientTape() as tape:\n            tf.nest.map_structure(tape.watch, xs)\n            y = f(xs, y)\n            tape.gradient(y, xs)\n            return y\n    cf = u.get_concrete_function([1.0, 2.0, 3.0], 4.0)\n    g = cf.graph\n    g.get_operation_by_name('jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_out')\n    g.get_operation_by_name('jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_out_2')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_arg_4')\n    with self.assertRaises(KeyError):\n        g.get_operation_by_name('jax2tf_out_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_0')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_arg_3')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_1')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_2')\n    g.get_operation_by_name('jax2tf_vjp/jax2tf_out_3')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_converts_jax_arrays(self):\n    f_tf = tf.function(lambda x: x)\n    self.assertEqual(f_tf(jnp.zeros([])).numpy(), 0.0)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 1.0)\n    f_tf = tf.function(lambda x: x + x)\n    self.assertEqual(f_tf(jnp.ones([])).numpy(), 2.0)\n    n = jax.local_device_count()\n    mk_sharded = lambda f: jax.pmap(lambda x: x)(f([n]))\n    f_tf = tf.function(lambda x: x)\n    self.assertAllClose(f_tf(mk_sharded(jnp.zeros)).numpy(), jnp.zeros([n]))\n    self.assertAllClose(f_tf(mk_sharded(jnp.ones)).numpy(), jnp.ones([n]))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_jvp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom JVP.\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "@jtu.sample_product(with_function=[False, True])\ndef test_gradients_with_custom_vjp(self, with_function=True):\n    \"\"\"Check gradients, for a function with custom VJP.\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    self.assertAllClose(4.0 * 4.0, f(4.0))\n    self.assertAllClose(3.0 * 4.0, jax.grad(f)(4.0))\n    f_tf = jax2tf.convert(f, with_gradient=True)\n    if with_function:\n        f_tf = tf.function(f_tf, autograph=False)\n    self.assertAllClose(4.0 * 4.0, f_tf(4.0))\n    x = tf.Variable(4.0, dtype=jax2tf.dtype_of_val(4.0))\n    with tf.GradientTape() as tape:\n        tape.watch(x)\n        y = f_tf(x)\n    self.assertAllClose(4.0 * 4.0, y)\n    self.assertAllClose(3.0 * 4.0, tape.gradient(y, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_gradient_with_float0_intermediate(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return 2.0 * f(3 * x.astype('int32'), x * 4.0)\n    x = 2.0\n    grad_g = jax.grad(g)\n    self.ConvertAndCompare(grad_g, x)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_gradient_with_float0_result(self):\n\n    def f(x, y):\n        return 2 * x + y\n\n    def g(x):\n        return jnp.sum(2.0 * f(3 * x, 4.0 * jnp.array(x, jnp.dtype('float32'))))\n    grad_g = jax.grad(g, allow_int=True)\n    x = 2\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())\n    shape = (3, 4)\n    x = np.ones(shape, dtype=np.int32)\n    d_dx_jax = grad_g(x)\n    d_dx_tf = jax2tf.convert(grad_g)(x)\n    self.assertEqual(d_dx_jax.dtype, dtypes.float0)\n    self.assertAllClose(jnp.zeros(np.shape(d_dx_jax), np.bool_), d_dx_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_custom_jvp(self):\n    \"\"\"Conversion of function with custom JVP\"\"\"\n\n    @jax.custom_jvp\n    def f(x):\n        return x * x\n\n    @f.defjvp\n    def f_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f(x)\n        tangent_out = 3.0 * x * x_dot\n        return (primal_out, tangent_out)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'jvp')\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'jvp_vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_custom_vjp(self):\n    \"\"\"Conversion of function with custom VJP\"\"\"\n\n    @jax.custom_vjp\n    def f(x):\n        return x * x\n\n    def f_fwd(x):\n        return (f(x), 3.0 * x)\n\n    def f_bwd(residual, ct_b):\n        return (residual * ct_b,)\n    f.defvjp(f_fwd, f_bwd)\n    arg = 0.7\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'vmap')\n    self.TransformConvertAndCompare(f, arg, 'grad')\n    self.TransformConvertAndCompare(f, arg, 'grad_vmap')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_remat_free_var(self):\n\n    def f(x):\n        y = 2 * x\n\n        @ad_checkpoint.checkpoint\n        def g():\n            return y\n        return g()\n    arg = 3.0\n    self.TransformConvertAndCompare(f, arg, None)\n    self.TransformConvertAndCompare(f, arg, 'grad')",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def g(x):\n    return jnp.sum(f(x))"
  },
  {
    "test_code": "def test_variable_input(self):\n    f_jax = lambda x: jnp.sin(jnp.cos(x))\n    f_tf = jax2tf.convert(f_jax)\n    v = tf.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))\n    self.assertIsInstance(f_tf(v), tf.Tensor)\n    self.assertAllClose(f_jax(0.7), f_tf(v))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    return jnp.sum(x * 2.0)"
  },
  {
    "test_code": "def test_custom_pytree_readme(self):\n\n    class CustomPair:\n\n        def __init__(self, a, b):\n            self.a = a\n            self.b = b\n    jax.tree_util.register_pytree_node(CustomPair, lambda x: ((x.a, x.b), None), lambda _, ab: CustomPair(*ab))\n\n    def f_jax(pair: CustomPair):\n        return np.float32(2.0) * pair.a + np.float32(3.0) * pair.b\n    f_tf = jax2tf.convert(f_jax)\n    x = CustomPair(np.float32(4.0), np.float32(5.0))\n    res_jax = f_jax(x)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    res_tf_2 = tf.function(f_tf, autograph=False, jit_compile=True)(x)\n    self.assertAllClose(res_jax, res_tf_2)\n\n    def f_tf_wrapped(a, b):\n        return f_tf(CustomPair(a, b))\n    my_model = tf.Module()\n    my_model.f = tf.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])\n    model_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))\n    tf.saved_model.save(my_model, model_dir, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))\n    restored_model = tf.saved_model.load(model_dir)\n\n    def restored_f(pair: CustomPair):\n        return restored_model.f(pair.a, pair.b)\n    res_tf_3 = restored_f(x)\n    self.assertAllClose(res_jax, res_tf_3)\n    grad_jax = jax.grad(f_jax)(x)\n    x_v = [tf.Variable(x.a), tf.Variable(x.b)]\n    with tf.GradientTape() as tape:\n        res = f_tf_wrapped(*x_v)\n        grad_tf = tape.gradient(res, x_v)\n    self.assertAllClose(grad_jax.a, grad_tf[0])\n    self.assertAllClose(grad_jax.b, grad_tf[1])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    return jnp.sum(x * 2.0)"
  },
  {
    "test_code": "def test_jit_unused(self):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))(x, y_unused)\n    self.assertAllClose(f_jax(x, None), res_tf)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    return jnp.sum(x * 2.0)"
  },
  {
    "test_code": "@parameterized.named_parameters((dict(testcase_name=mode, mode=mode) for mode in ('eager', 'graph', 'compiled')))\ndef test_jit_unused_grad(self, mode='eager'):\n\n    def f_jax(x, y_unused):\n        return x * np.float32(2.0)\n    x, y_unused = (np.float32(5.0), np.arange(7, dtype=np.int32))\n    res_jax = f_jax(x, y_unused)\n    f_tf = jax2tf.convert(jax.jit(f_jax, keep_unused=False))\n    x_tf, y_unused_tf = (tf.constant(x), tf.constant(y_unused))\n\n    def grad_tf(x, y_unused):\n        with tf.GradientTape() as tape:\n            tape.watch(x)\n            tape.watch(y_unused)\n            res_tf = f_tf(x, y_unused)\n            grad_tf_x, grad_tf_y = tape.gradient(res_tf, (x, y_unused))\n        return (res_tf, grad_tf_x, grad_tf_y)\n    if mode == 'graph':\n        grad_tf = tf.function(grad_tf, autograph=False)\n    elif mode == 'compiled':\n        grad_tf = tf.function(grad_tf, autograph=False, jit_compile=True)\n    res_tf, grad_tf_x, grad_tf_y = grad_tf(x_tf, y_unused_tf)\n    self.assertAllClose(res_jax, res_tf)\n    self.assertAllClose(np.float32(2.0), grad_tf_x)\n    self.assertIsNone(grad_tf_y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    return jnp.sum(x * 2.0)"
  },
  {
    "test_code": "def test_shared_constants_randint(self):\n    if config.jax2tf_default_native_serialization.value:\n        raise unittest.SkipTest('shared constants tests not interesting for native serialization')\n    key = jax.random.PRNGKey(42)\n\n    def f_nested_jax(x):\n        return x + jax.random.randint(key, shape=x.shape, minval=0, maxval=100, dtype=np.int32)\n\n    def f_jax(x):\n        res = lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x))\n        res += lax.while_loop(lambda x: f_nested_jax(x)[0] <= 0, f_nested_jax, x)\n        res += jax.vmap(lambda x: lax.cond(x[0] >= 2, lambda: f_nested_jax(x), lambda: f_nested_jax(x)))(jnp.stack([x, x]))\n        res += f_nested_jax(x)\n        return res\n    x = np.array([123, 456, 789], dtype=np.int32)\n    f_tf = tf.function(jax2tf.convert(f_jax), autograph=False)\n    res_tf = f_tf(x)\n    self.assertAllClose(res_tf, f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    return jnp.sum(x * 2.0)"
  },
  {
    "test_code": "def test_tuple_args(self):\n    if not jtu.test_device_matches(['tpu']):\n        raise unittest.SkipTest('Test enabled on TPU only')\n\n    def f_jax(*many_args):\n        acc = 0.0\n        for a in many_args:\n            acc += a\n        return acc\n    many_args = [np.float32(i) for i in range(2001)]\n    lowered = jax.jit(f_jax).lower(*many_args)\n    self.assertTrue(lowered._lowering.compile_args['tuple_args'])\n    res = jax2tf.convert(f_jax, native_serialization=True)(*many_args)\n    self.assertAllClose(f_jax(*many_args), res)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    return jnp.sum(x * 2.0)"
  },
  {
    "test_code": "@jtu.ignore_warning(message='Calling from_dlpack with a DLPack tensor', category=DeprecationWarning)\ndef test_nested_convert(self):\n\n    @jax.jit\n    def f_jax(x):\n        return x + 1\n    inputs = np.ones(10, dtype=np.float32)\n    res = f_jax(inputs)\n    f_tf = jax2tf.convert(f_jax, native_serialization=True)\n    self.assertAllClose(res, f_tf(inputs))\n    f_jax_nested = jax2tf.call_tf(f_tf)\n    self.assertAllClose(res, f_jax_nested(inputs))\n    f_tf_nested = jax2tf.convert(f_jax_nested, native_serialization=True)\n    self.assertAllClose(res, f_tf_nested(inputs))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/jax2tf_test.py",
    "function": "def f_jax(x):\n    return jnp.sum(x * 2.0)"
  }
]