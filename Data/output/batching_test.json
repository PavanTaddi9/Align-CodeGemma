[
  {
    "test_code": "def testConvGeneralDilated(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.02, atol=0.002)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def testConvGeneralDilatedBatchNotMajor(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 4), dtype=np.float32)\n    x = jnp.array(self.rng().randn(3, 5, 7, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('HNWC', 'HWIO', 'HWNC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    per_example = vmap(partial(f, W))(x)\n    per_example = jnp.reshape(jnp.transpose(per_example, (1, 2, 0, 3, 4)), (5, 5, 21, 4))\n    per_example_direct = f(W, jnp.reshape(jnp.transpose(x, (1, 0, 2, 3, 4)), (5, 21, 5, 1)))\n    self.assertAllClose(per_example, per_example_direct)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_op={name}', 'op': op, 'unit': unit} for name, op, unit in [('max', lax.max, -jnp.inf), ('min', lax.min, jnp.inf)]))\ndef testMinMaxPool(self, op, unit):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, unit, op, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.05, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def testSumPool(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, 0.0, lax.add, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.03, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def testNumpyIndexing1(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n    ind = np.array([[0, 1], [2, 0]])\n\n    def f(a, ind):\n        return a[:, ind]\n    expected = np.stack([f(a, ind[i, :]) for i in range(ind.shape[0])])\n    ans = vmap(f, (None, 0))(a, ind)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def testNumpyIndexing2(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n\n    def f(a):\n        inds = jnp.array([0, 2])\n        return a[:, inds]\n    ans = vmap(f)(a)\n    expected = np.stack([f(a[:, i, :]) for i in range(a.shape[1])], axis=1)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_{}_vmap_names={}_collective_names={}'.format(collective.__name__.replace(' ', ''), ''.join(vmap_names), ''.join(collective_names)), 'collective': collective, 'bulk_op': bulk_op, 'vmap_names': vmap_names, 'collective_names': collective_names} for collective, bulk_op in [(lax.psum, jnp.sum), (lax.pmax, jnp.max), (lax.pmin, jnp.min)] for vmap_names in [('i',), ('i', 'j'), ('i', 'j', 'k')] for subset_size in range(1, len(vmap_names) + 1) for collective_subset in it.combinations(vmap_names, subset_size) for collective_names in it.permutations(collective_subset)))\ndef testCommAssocCollective(self, collective, bulk_op, vmap_names, collective_names):\n    shape = (2, 2, 2)\n    x = jnp.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    f = lambda x: x - collective(x, collective_names)\n    for i, axis_name in enumerate(vmap_names):\n        f = vmap(f, axis_name=axis_name, in_axes=i, out_axes=i)\n    pos_axis = [i for i, name in enumerate(vmap_names) if name in collective_names]\n    self.assertAllClose(f(x), x - bulk_op(x, axis=pos_axis, keepdims=True))\n    if collective is lax.psum:\n        jtu.check_grads(f, (x,), 2, eps=1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(3), range(3), range(4))))\ndef testAllToAll(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n    f = vmap(lambda x: lax.all_to_all(x, 'i', split_axis, concat_axis), in_axes=vmap_axis, axis_name='i')\n    y = f(x)\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis + (vmap_axis <= split_axis)), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(2), range(2), range(3))))\ndef testAllToAllSplitAxis(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n\n    @partial(vmap, in_axes=vmap_axis, axis_name='i')\n    @partial(vmap, in_axes=vmap_axis, axis_name='j')\n    def f(x):\n        return lax.all_to_all(x, ('i', 'j'), split_axis, concat_axis)\n    unroll_shape = (2, 2, *shape[1:])\n    unroll_shape = list(shape)\n    unroll_shape[vmap_axis:vmap_axis + 1] = (2, 2)\n    x_unroll = x.reshape(unroll_shape)\n    y_unrolled = f(x_unroll)\n    y = y_unrolled.reshape(shape)\n    if vmap_axis <= split_axis:\n        split_axis += 1\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def testGradOfPsum(self):\n    a = jnp.ones(5)\n    f = vmap(jax.grad(lambda x: -lax.psum(x, 'i')), out_axes=None, axis_name='i')\n    self.assertEqual(f(a), core.jaxpr_as_fun(jax.make_jaxpr(f)(a))(a)[0])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def testIssue6096(self):\n\n    def f(x):\n        return jsp.special.betainc(jnp.ones(3), 1.0, x)\n    self.assertEqual(f(jnp.ones(3)).shape, (3,))\n    self.assertEqual(jax.vmap(f)(jnp.ones((2, 3))).shape, (2, 3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def testBatchingPreservesWeakType(self):\n    x = jnp.ravel(1)\n    self.assertTrue(dtypes.is_weakly_typed(x))\n\n    @vmap\n    def f(x):\n        self.assertTrue(dtypes.is_weakly_typed(x), f'{x} is not weakly-typed')\n        return x\n    y = f(x)\n    self.assertTrue(dtypes.is_weakly_typed(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def testConvGeneralDilated(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.02, atol=0.002)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def testConvGeneralDilatedBatchNotMajor(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 4), dtype=np.float32)\n    x = jnp.array(self.rng().randn(3, 5, 7, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('HNWC', 'HWIO', 'HWNC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    per_example = vmap(partial(f, W))(x)\n    per_example = jnp.reshape(jnp.transpose(per_example, (1, 2, 0, 3, 4)), (5, 5, 21, 4))\n    per_example_direct = f(W, jnp.reshape(jnp.transpose(x, (1, 0, 2, 3, 4)), (5, 21, 5, 1)))\n    self.assertAllClose(per_example, per_example_direct)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_op={name}', 'op': op, 'unit': unit} for name, op, unit in [('max', lax.max, -jnp.inf), ('min', lax.min, jnp.inf)]))\ndef testMinMaxPool(self, op, unit):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, unit, op, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.05, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def testSumPool(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, 0.0, lax.add, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.03, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def testNumpyIndexing1(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n    ind = np.array([[0, 1], [2, 0]])\n\n    def f(a, ind):\n        return a[:, ind]\n    expected = np.stack([f(a, ind[i, :]) for i in range(ind.shape[0])])\n    ans = vmap(f, (None, 0))(a, ind)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def testNumpyIndexing2(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n\n    def f(a):\n        inds = jnp.array([0, 2])\n        return a[:, inds]\n    ans = vmap(f)(a)\n    expected = np.stack([f(a[:, i, :]) for i in range(a.shape[1])], axis=1)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_{}_vmap_names={}_collective_names={}'.format(collective.__name__.replace(' ', ''), ''.join(vmap_names), ''.join(collective_names)), 'collective': collective, 'bulk_op': bulk_op, 'vmap_names': vmap_names, 'collective_names': collective_names} for collective, bulk_op in [(lax.psum, jnp.sum), (lax.pmax, jnp.max), (lax.pmin, jnp.min)] for vmap_names in [('i',), ('i', 'j'), ('i', 'j', 'k')] for subset_size in range(1, len(vmap_names) + 1) for collective_subset in it.combinations(vmap_names, subset_size) for collective_names in it.permutations(collective_subset)))\ndef testCommAssocCollective(self, collective, bulk_op, vmap_names, collective_names):\n    shape = (2, 2, 2)\n    x = jnp.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    f = lambda x: x - collective(x, collective_names)\n    for i, axis_name in enumerate(vmap_names):\n        f = vmap(f, axis_name=axis_name, in_axes=i, out_axes=i)\n    pos_axis = [i for i, name in enumerate(vmap_names) if name in collective_names]\n    self.assertAllClose(f(x), x - bulk_op(x, axis=pos_axis, keepdims=True))\n    if collective is lax.psum:\n        jtu.check_grads(f, (x,), 2, eps=1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(3), range(3), range(4))))\ndef testAllToAll(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n    f = vmap(lambda x: lax.all_to_all(x, 'i', split_axis, concat_axis), in_axes=vmap_axis, axis_name='i')\n    y = f(x)\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis + (vmap_axis <= split_axis)), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(2), range(2), range(3))))\ndef testAllToAllSplitAxis(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n\n    @partial(vmap, in_axes=vmap_axis, axis_name='i')\n    @partial(vmap, in_axes=vmap_axis, axis_name='j')\n    def f(x):\n        return lax.all_to_all(x, ('i', 'j'), split_axis, concat_axis)\n    unroll_shape = (2, 2, *shape[1:])\n    unroll_shape = list(shape)\n    unroll_shape[vmap_axis:vmap_axis + 1] = (2, 2)\n    x_unroll = x.reshape(unroll_shape)\n    y_unrolled = f(x_unroll)\n    y = y_unrolled.reshape(shape)\n    if vmap_axis <= split_axis:\n        split_axis += 1\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def testGradOfPsum(self):\n    a = jnp.ones(5)\n    f = vmap(jax.grad(lambda x: -lax.psum(x, 'i')), out_axes=None, axis_name='i')\n    self.assertEqual(f(a), core.jaxpr_as_fun(jax.make_jaxpr(f)(a))(a)[0])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def testIssue6096(self):\n\n    def f(x):\n        return jsp.special.betainc(jnp.ones(3), 1.0, x)\n    self.assertEqual(f(jnp.ones(3)).shape, (3,))\n    self.assertEqual(jax.vmap(f)(jnp.ones((2, 3))).shape, (2, 3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def testBatchingPreservesWeakType(self):\n    x = jnp.ravel(1)\n    self.assertTrue(dtypes.is_weakly_typed(x))\n\n    @vmap\n    def f(x):\n        self.assertTrue(dtypes.is_weakly_typed(x), f'{x} is not weakly-typed')\n        return x\n    y = f(x)\n    self.assertTrue(dtypes.is_weakly_typed(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def testPerExampleGradients(self):\n\n    def predict(params, inputs):\n        for W, b in params:\n            outputs = jnp.dot(W, inputs) + b\n            inputs = jnp.tanh(outputs)\n        return outputs\n\n    def loss(params, data):\n        inputs, targets = data\n        predictions = predict(params, inputs)\n        return jnp.sum((predictions - targets) ** 2)\n    batch_size = 5\n    layer_sizes = [3, 2, 4]\n    R = self.rng().randn\n    params = [(R(m, n), R(m)) for m, n in zip(layer_sizes[1:], layer_sizes[:-1])]\n    input_batch = R(5, 3)\n    target_batch = R(5, 4)\n    batch = (input_batch, target_batch)\n    ans = vmap(partial(grad(loss), params))(batch)\n    for ans_pair, param_pair in zip(ans, params):\n        dW, db = ans_pair\n        W, b = param_pair\n        self.assertEqual(dW.shape, (batch_size,) + W.shape)\n        self.assertEqual(db.shape, (batch_size,) + b.shape)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def predict(params, inputs):\n    for W, b in params:\n        outputs = jnp.dot(inputs, W) + b\n        inputs = jnp.maximum(0, outputs)\n    return outputs"
  },
  {
    "test_code": "def testDot(self):\n\n    def vecvec(a, b):\n        dot = jnp.dot\n        for ndim in range(1, max(a.ndim, b.ndim)):\n            a_ax = 0 if a.ndim > ndim else None\n            b_ax = 0 if b.ndim > ndim else None\n            dot = vmap(dot, in_axes=(a_ax, b_ax))\n        return dot(a, b)\n    assert vecvec(jnp.zeros((3,)), jnp.zeros((3,))).shape == ()\n    assert vecvec(jnp.zeros((2, 3)), jnp.zeros((3,))).shape == (2,)\n    assert vecvec(jnp.zeros((4, 2, 3)), jnp.zeros((3,))).shape == (4, 2)",
    "assertions": [
      "assert vecvec(jnp.zeros((3,)), jnp.zeros((3,))).shape == ()",
      "assert vecvec(jnp.zeros((2, 3)), jnp.zeros((3,))).shape == (2,)",
      "assert vecvec(jnp.zeros((4, 2, 3)), jnp.zeros((3,))).shape == (4, 2)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def testIndexAddBatchedIndexesOnly(self):\n    f = lambda x, idx, y: jnp.asarray(x).at[idx].add(y)\n    result = vmap(f, (None, 0, None))(np.zeros((10,)), np.arange(10), 1.0)\n    self.assertAllClose(result, np.eye(10), check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jax.default_matmul_precision('float32')\ndef testDotGeneral(self):\n    R = self.rng().randn\n    x = R(10, 3, 4, 5)\n    y = R(10, 3, 5, 6)\n    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])\n    ans = vmap(fun)(x, y)\n    expected = lax.dot_general(x, y, [((3,), (2,)), ((0, 1), (0, 1))])\n    self.assertAllClose(ans, expected)\n    x = R(3, 4, 10, 5)\n    y = R(3, 10, 5, 6)\n    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])\n    ans = vmap(fun, in_axes=(2, 1))(x, y)\n    expected = np.stack([fun(x[..., i, :], y[:, i, ...]) for i in range(10)])\n    self.assertAllClose(ans, expected)\n    x = R(3, 4, 5, 10)\n    y = R(3, 5, 6)\n    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])\n    ans = vmap(fun, in_axes=(3, None))(x, y)\n    expected = np.stack([fun(x[..., i], y) for i in range(10)])\n    self.assertAllClose(ans, expected)\n    x = R(3, 4, 5)\n    y = R(3, 5, 10, 6)\n    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])\n    ans = vmap(fun, in_axes=(None, 2))(x, y)\n    expected = np.stack([fun(x, y[..., i, :]) for i in range(10)])\n    self.assertAllClose(ans, expected)\n    x = R(4)\n    y = R(4, 10)\n    fun = lambda x, y: lax.dot_general(x, y, [((0,), (0,)), ((), ())])\n    ans = vmap(fun, in_axes=(None, 1))(x, y)\n    expected = np.stack([fun(x, y[..., i]) for i in range(10)])\n    self.assertAllClose(ans, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=('n',))\ndef fun(x):\n    return jnp.sum(x)"
  },
  {
    "test_code": "def testHessian(self):\n\n    def fun(x, t):\n        return jnp.sum(jnp.power(jnp.maximum(x, 0.0), 2)) + t\n    x = np.array([-1.0, -0.5, 0.0, 0.5, 1.0])\n    ans = hessian(lambda x: fun(x, 0.0))(x)\n    expected = np.array([[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.5, 0.0, 0.0], [0.0, 0.0, 0.0, 2.0, 0.0], [0.0, 0.0, 0.0, 0.0, 2.0]])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=('n',))\ndef fun(x):\n    return jnp.sum(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), axis, idxs, dnums, slice_sizes), 'axis': axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.int32] for axis, shape, idxs, dnums, slice_sizes in [(0, (3, 5), np.array([[0], [2]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, (10, 3), np.array([[0], [0], [0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (1, (10, 3, 5), np.array([[0], [2], [1]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (2, (10, 5, 3), np.array([[0, 2], [1, 0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherBatchedOperand(self, axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    operand = rng(shape, dtype)\n    ans = vmap(fun, (axis, None))(operand, idxs)\n    expected = np.stack([fun(operand[(slice(None),) * axis + (i,)], idxs) for i in range(operand.shape[axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=('n',))\ndef fun(x):\n    return jnp.sum(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), axis, idxs, dnums, slice_sizes), 'axis': axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.float64] for axis, shape, idxs, dnums, slice_sizes in [(0, (3, 5), np.array([[0], [2]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, (10, 3), np.array([[0], [0], [0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (1, (10, 3, 5), np.array([[0], [2], [1]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (2, (10, 5, 3), np.array([[0, 2], [1, 0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherGradBatchedOperand(self, axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    gfun = grad(lambda x, idx: jnp.sum(jnp.sin(fun(x, idx))))\n    operand = rng(shape, dtype)\n    ans = vmap(gfun, (axis, None))(operand, idxs)\n    expected = np.stack([gfun(operand[(slice(None),) * axis + (i,)], idxs) for i in range(operand.shape[axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=('n',))\ndef fun(x):\n    return jnp.sum(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), axis, idxs, dnums, slice_sizes), 'axis': axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.int32] for axis, shape, idxs, dnums, slice_sizes in [(0, (5,), np.array([[[0], [2]], [[1], [3]]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, (10,), np.array([[0, 0, 0], [0, 2, 1]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (1, (10, 5), np.array([[0, 2, 1], [0, 3, 3]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (0, (10, 5), np.array([[[0, 1], [2, 0]], [[1, 0], [2, 3]]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherBatchedIndices(self, axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    operand = rng(shape, dtype)\n    ans = vmap(fun, (None, axis))(operand, idxs)\n    expected = np.stack([fun(operand, idxs[(slice(None),) * axis + (i,)]) for i in range(idxs.shape[axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=('n',))\ndef fun(x):\n    return jnp.sum(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), axis, idxs, dnums, slice_sizes), 'axis': axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.float64] for axis, shape, idxs, dnums, slice_sizes in [(0, (5,), np.array([[[0], [2]], [[1], [3]]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, (10,), np.array([[0, 0, 0], [0, 2, 1]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (1, (10, 5), np.array([[0, 2, 1], [0, 3, 3]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (0, (10, 5), np.array([[[0, 1], [2, 0]], [[1, 0], [2, 3]]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherGradBatchedIndices(self, axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    gfun = grad(lambda x, idx: jnp.sum(jnp.sin(fun(x, idx))))\n    operand = rng(shape, dtype)\n    ans = vmap(gfun, (None, axis))(operand, idxs)\n    expected = np.stack([gfun(operand, idxs[(slice(None),) * axis + (i,)]) for i in range(idxs.shape[axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=('n',))\ndef fun(x):\n    return jnp.sum(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_op_axis={}_idxs_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), op_axis, idxs_axis, idxs, dnums, slice_sizes), 'op_axis': op_axis, 'idxs_axis': idxs_axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.int32] for op_axis, idxs_axis, shape, idxs, dnums, slice_sizes in [(0, 0, (2, 5), np.array([[[0], [2]], [[1], [3]]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, 1, (10, 2), np.array([[0, 0, 0], [0, 2, 1]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (0, 1, (2, 10, 5), np.array([[[0, 2, 1], [0, 3, 3]]]).T, lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (2, 0, (10, 5, 2), np.array([[[0, 2], [1, 0]], [[1, 0], [2, 0]]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherBatchedBoth(self, op_axis, idxs_axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    operand = rng(shape, dtype)\n    assert operand.shape[op_axis] == idxs.shape[idxs_axis]\n    ans = vmap(fun, (op_axis, idxs_axis))(operand, idxs)\n    expected = np.stack([fun(operand[(slice(None),) * op_axis + (i,)], idxs[(slice(None),) * idxs_axis + (i,)]) for i in range(idxs.shape[idxs_axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [
      "assert operand.shape[op_axis] == idxs.shape[idxs_axis]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=('n',))\ndef fun(x):\n    return jnp.sum(x)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_op_axis={}_idxs_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), op_axis, idxs_axis, idxs, dnums, slice_sizes), 'op_axis': op_axis, 'idxs_axis': idxs_axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32] for op_axis, idxs_axis, shape, idxs, dnums, slice_sizes in [(0, 0, (2, 5), np.array([[[0], [2]], [[1], [3]]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, 1, (10, 2), np.array([[0, 0, 0], [0, 2, 1]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (0, 1, (2, 10, 5), np.array([[[0, 2, 1], [0, 3, 3]]]).T, lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (2, 0, (10, 5, 2), np.array([[[0, 2], [1, 0]], [[1, 0], [2, 0]]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherGradBatchedBoth(self, op_axis, idxs_axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    gfun = grad(lambda x, idx: jnp.sum(jnp.sin(fun(x, idx))))\n    operand = rng(shape, dtype)\n    assert operand.shape[op_axis] == idxs.shape[idxs_axis]\n    ans = vmap(gfun, (op_axis, idxs_axis))(operand, idxs)\n    expected = np.stack([gfun(operand[(slice(None),) * op_axis + (i,)], idxs[(slice(None),) * idxs_axis + (i,)]) for i in range(idxs.shape[idxs_axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [
      "assert operand.shape[op_axis] == idxs.shape[idxs_axis]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@partial(jax.jit, abstracted_axes=('n',))\ndef fun(x):\n    return jnp.sum(x)"
  },
  {
    "test_code": "def testBatchOfCompile(self):\n    side = []\n\n    @jit\n    def f(x):\n        side.append(None)\n        return x + x\n    g = jit(vmap(f))\n    self.assertAllClose(g(np.ones(2)), 2 * np.ones(2), check_dtypes=False)\n    self.assertEqual(len(side), 1)\n    self.assertAllClose(g(2 * np.ones(2)), 4 * np.ones(2), check_dtypes=False)\n    self.assertEqual(len(side), 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef g():\n    return jnp.zeros(n) + x"
  },
  {
    "test_code": "def testIssue1170(self):\n\n    def f(index1, index2):\n        return jnp.arange(36).reshape(6, 6)[index1, index2]\n    g = jax.jit(jax.pmap(f))\n    ans = g(index1=np.asarray([1]), index2=np.asarray([2]))\n    expected = g(np.asarray([1]), np.asarray([2]))\n    self.assertAllClose(ans, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef g():\n    return jnp.zeros(n) + x"
  },
  {
    "test_code": "def test_basic(self):\n    with temporarily_register_named_array_vmappable():\n\n        def f(x):\n            return named_mul(x, x)\n        x = NamedArray(['i', 'j'], jnp.arange(12.0).reshape(3, 4))\n        g = jax.vmap(f, in_axes=NamedMapSpec('i', 0), out_axes=NamedMapSpec('i', 1), axis_size=3)\n        ans = g(x)\n        expected = NamedArray(['j', 'i'], jnp.arange(12.0).reshape(3, 4).T ** 2)\n        self.assertEqual(ans.names, expected.names)\n        self.assertAllClose(ans.data, expected.data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef g():\n    return jnp.zeros(n) + x"
  },
  {
    "test_code": "def testConvGeneralDilated(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.02, atol=0.002)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def testConvGeneralDilatedBatchNotMajor(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 4), dtype=np.float32)\n    x = jnp.array(self.rng().randn(3, 5, 7, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('HNWC', 'HWIO', 'HWNC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    per_example = vmap(partial(f, W))(x)\n    per_example = jnp.reshape(jnp.transpose(per_example, (1, 2, 0, 3, 4)), (5, 5, 21, 4))\n    per_example_direct = f(W, jnp.reshape(jnp.transpose(x, (1, 0, 2, 3, 4)), (5, 21, 5, 1)))\n    self.assertAllClose(per_example, per_example_direct)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_op={name}', 'op': op, 'unit': unit} for name, op, unit in [('max', lax.max, -jnp.inf), ('min', lax.min, jnp.inf)]))\ndef testMinMaxPool(self, op, unit):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, unit, op, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.05, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def testSumPool(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, 0.0, lax.add, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.03, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def testNumpyIndexing1(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n    ind = np.array([[0, 1], [2, 0]])\n\n    def f(a, ind):\n        return a[:, ind]\n    expected = np.stack([f(a, ind[i, :]) for i in range(ind.shape[0])])\n    ans = vmap(f, (None, 0))(a, ind)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def testNumpyIndexing2(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n\n    def f(a):\n        inds = jnp.array([0, 2])\n        return a[:, inds]\n    ans = vmap(f)(a)\n    expected = np.stack([f(a[:, i, :]) for i in range(a.shape[1])], axis=1)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_{}_vmap_names={}_collective_names={}'.format(collective.__name__.replace(' ', ''), ''.join(vmap_names), ''.join(collective_names)), 'collective': collective, 'bulk_op': bulk_op, 'vmap_names': vmap_names, 'collective_names': collective_names} for collective, bulk_op in [(lax.psum, jnp.sum), (lax.pmax, jnp.max), (lax.pmin, jnp.min)] for vmap_names in [('i',), ('i', 'j'), ('i', 'j', 'k')] for subset_size in range(1, len(vmap_names) + 1) for collective_subset in it.combinations(vmap_names, subset_size) for collective_names in it.permutations(collective_subset)))\ndef testCommAssocCollective(self, collective, bulk_op, vmap_names, collective_names):\n    shape = (2, 2, 2)\n    x = jnp.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    f = lambda x: x - collective(x, collective_names)\n    for i, axis_name in enumerate(vmap_names):\n        f = vmap(f, axis_name=axis_name, in_axes=i, out_axes=i)\n    pos_axis = [i for i, name in enumerate(vmap_names) if name in collective_names]\n    self.assertAllClose(f(x), x - bulk_op(x, axis=pos_axis, keepdims=True))\n    if collective is lax.psum:\n        jtu.check_grads(f, (x,), 2, eps=1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(3), range(3), range(4))))\ndef testAllToAll(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n    f = vmap(lambda x: lax.all_to_all(x, 'i', split_axis, concat_axis), in_axes=vmap_axis, axis_name='i')\n    y = f(x)\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis + (vmap_axis <= split_axis)), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(2), range(2), range(3))))\ndef testAllToAllSplitAxis(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n\n    @partial(vmap, in_axes=vmap_axis, axis_name='i')\n    @partial(vmap, in_axes=vmap_axis, axis_name='j')\n    def f(x):\n        return lax.all_to_all(x, ('i', 'j'), split_axis, concat_axis)\n    unroll_shape = (2, 2, *shape[1:])\n    unroll_shape = list(shape)\n    unroll_shape[vmap_axis:vmap_axis + 1] = (2, 2)\n    x_unroll = x.reshape(unroll_shape)\n    y_unrolled = f(x_unroll)\n    y = y_unrolled.reshape(shape)\n    if vmap_axis <= split_axis:\n        split_axis += 1\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def testGradOfPsum(self):\n    a = jnp.ones(5)\n    f = vmap(jax.grad(lambda x: -lax.psum(x, 'i')), out_axes=None, axis_name='i')\n    self.assertEqual(f(a), core.jaxpr_as_fun(jax.make_jaxpr(f)(a))(a)[0])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def testIssue6096(self):\n\n    def f(x):\n        return jsp.special.betainc(jnp.ones(3), 1.0, x)\n    self.assertEqual(f(jnp.ones(3)).shape, (3,))\n    self.assertEqual(jax.vmap(f)(jnp.ones((2, 3))).shape, (2, 3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def testBatchingPreservesWeakType(self):\n    x = jnp.ravel(1)\n    self.assertTrue(dtypes.is_weakly_typed(x))\n\n    @vmap\n    def f(x):\n        self.assertTrue(dtypes.is_weakly_typed(x), f'{x} is not weakly-typed')\n        return x\n    y = f(x)\n    self.assertTrue(dtypes.is_weakly_typed(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def testConvGeneralDilated(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.02, atol=0.002)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def testConvGeneralDilatedBatchNotMajor(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 4), dtype=np.float32)\n    x = jnp.array(self.rng().randn(3, 5, 7, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('HNWC', 'HWIO', 'HWNC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    per_example = vmap(partial(f, W))(x)\n    per_example = jnp.reshape(jnp.transpose(per_example, (1, 2, 0, 3, 4)), (5, 5, 21, 4))\n    per_example_direct = f(W, jnp.reshape(jnp.transpose(x, (1, 0, 2, 3, 4)), (5, 21, 5, 1)))\n    self.assertAllClose(per_example, per_example_direct)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_op={name}', 'op': op, 'unit': unit} for name, op, unit in [('max', lax.max, -jnp.inf), ('min', lax.min, jnp.inf)]))\ndef testMinMaxPool(self, op, unit):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, unit, op, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.05, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def testSumPool(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, 0.0, lax.add, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.03, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def testNumpyIndexing1(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n    ind = np.array([[0, 1], [2, 0]])\n\n    def f(a, ind):\n        return a[:, ind]\n    expected = np.stack([f(a, ind[i, :]) for i in range(ind.shape[0])])\n    ans = vmap(f, (None, 0))(a, ind)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def testNumpyIndexing2(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n\n    def f(a):\n        inds = jnp.array([0, 2])\n        return a[:, inds]\n    ans = vmap(f)(a)\n    expected = np.stack([f(a[:, i, :]) for i in range(a.shape[1])], axis=1)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_{}_vmap_names={}_collective_names={}'.format(collective.__name__.replace(' ', ''), ''.join(vmap_names), ''.join(collective_names)), 'collective': collective, 'bulk_op': bulk_op, 'vmap_names': vmap_names, 'collective_names': collective_names} for collective, bulk_op in [(lax.psum, jnp.sum), (lax.pmax, jnp.max), (lax.pmin, jnp.min)] for vmap_names in [('i',), ('i', 'j'), ('i', 'j', 'k')] for subset_size in range(1, len(vmap_names) + 1) for collective_subset in it.combinations(vmap_names, subset_size) for collective_names in it.permutations(collective_subset)))\ndef testCommAssocCollective(self, collective, bulk_op, vmap_names, collective_names):\n    shape = (2, 2, 2)\n    x = jnp.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    f = lambda x: x - collective(x, collective_names)\n    for i, axis_name in enumerate(vmap_names):\n        f = vmap(f, axis_name=axis_name, in_axes=i, out_axes=i)\n    pos_axis = [i for i, name in enumerate(vmap_names) if name in collective_names]\n    self.assertAllClose(f(x), x - bulk_op(x, axis=pos_axis, keepdims=True))\n    if collective is lax.psum:\n        jtu.check_grads(f, (x,), 2, eps=1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(3), range(3), range(4))))\ndef testAllToAll(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n    f = vmap(lambda x: lax.all_to_all(x, 'i', split_axis, concat_axis), in_axes=vmap_axis, axis_name='i')\n    y = f(x)\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis + (vmap_axis <= split_axis)), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(2), range(2), range(3))))\ndef testAllToAllSplitAxis(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n\n    @partial(vmap, in_axes=vmap_axis, axis_name='i')\n    @partial(vmap, in_axes=vmap_axis, axis_name='j')\n    def f(x):\n        return lax.all_to_all(x, ('i', 'j'), split_axis, concat_axis)\n    unroll_shape = (2, 2, *shape[1:])\n    unroll_shape = list(shape)\n    unroll_shape[vmap_axis:vmap_axis + 1] = (2, 2)\n    x_unroll = x.reshape(unroll_shape)\n    y_unrolled = f(x_unroll)\n    y = y_unrolled.reshape(shape)\n    if vmap_axis <= split_axis:\n        split_axis += 1\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def testGradOfPsum(self):\n    a = jnp.ones(5)\n    f = vmap(jax.grad(lambda x: -lax.psum(x, 'i')), out_axes=None, axis_name='i')\n    self.assertEqual(f(a), core.jaxpr_as_fun(jax.make_jaxpr(f)(a))(a)[0])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def testIssue6096(self):\n\n    def f(x):\n        return jsp.special.betainc(jnp.ones(3), 1.0, x)\n    self.assertEqual(f(jnp.ones(3)).shape, (3,))\n    self.assertEqual(jax.vmap(f)(jnp.ones((2, 3))).shape, (2, 3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def testBatchingPreservesWeakType(self):\n    x = jnp.ravel(1)\n    self.assertTrue(dtypes.is_weakly_typed(x))\n\n    @vmap\n    def f(x):\n        self.assertTrue(dtypes.is_weakly_typed(x), f'{x} is not weakly-typed')\n        return x\n    y = f(x)\n    self.assertTrue(dtypes.is_weakly_typed(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "@jax.default_matmul_precision('float32')\ndef testDotGeneral(self):\n    R = self.rng().randn\n    x = R(10, 3, 4, 5)\n    y = R(10, 3, 5, 6)\n    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])\n    ans = vmap(fun)(x, y)\n    expected = lax.dot_general(x, y, [((3,), (2,)), ((0, 1), (0, 1))])\n    self.assertAllClose(ans, expected)\n    x = R(3, 4, 10, 5)\n    y = R(3, 10, 5, 6)\n    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])\n    ans = vmap(fun, in_axes=(2, 1))(x, y)\n    expected = np.stack([fun(x[..., i, :], y[:, i, ...]) for i in range(10)])\n    self.assertAllClose(ans, expected)\n    x = R(3, 4, 5, 10)\n    y = R(3, 5, 6)\n    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])\n    ans = vmap(fun, in_axes=(3, None))(x, y)\n    expected = np.stack([fun(x[..., i], y) for i in range(10)])\n    self.assertAllClose(ans, expected)\n    x = R(3, 4, 5)\n    y = R(3, 5, 10, 6)\n    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])\n    ans = vmap(fun, in_axes=(None, 2))(x, y)\n    expected = np.stack([fun(x, y[..., i, :]) for i in range(10)])\n    self.assertAllClose(ans, expected)\n    x = R(4)\n    y = R(4, 10)\n    fun = lambda x, y: lax.dot_general(x, y, [((0,), (0,)), ((), ())])\n    ans = vmap(fun, in_axes=(None, 1))(x, y)\n    expected = np.stack([fun(x, y[..., i]) for i in range(10)])\n    self.assertAllClose(ans, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef fun(x, y):\n    return cond(x < 3, None, lambda _: 2.0 * jnp.sin(y), x, lambda x: 2.0 * x)"
  },
  {
    "test_code": "def testHessian(self):\n\n    def fun(x, t):\n        return jnp.sum(jnp.power(jnp.maximum(x, 0.0), 2)) + t\n    x = np.array([-1.0, -0.5, 0.0, 0.5, 1.0])\n    ans = hessian(lambda x: fun(x, 0.0))(x)\n    expected = np.array([[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.5, 0.0, 0.0], [0.0, 0.0, 0.0, 2.0, 0.0], [0.0, 0.0, 0.0, 0.0, 2.0]])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef fun(x, y):\n    return cond(x < 3, None, lambda _: 2.0 * jnp.sin(y), x, lambda x: 2.0 * x)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), axis, idxs, dnums, slice_sizes), 'axis': axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.int32] for axis, shape, idxs, dnums, slice_sizes in [(0, (3, 5), np.array([[0], [2]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, (10, 3), np.array([[0], [0], [0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (1, (10, 3, 5), np.array([[0], [2], [1]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (2, (10, 5, 3), np.array([[0, 2], [1, 0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherBatchedOperand(self, axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    operand = rng(shape, dtype)\n    ans = vmap(fun, (axis, None))(operand, idxs)\n    expected = np.stack([fun(operand[(slice(None),) * axis + (i,)], idxs) for i in range(operand.shape[axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef fun(x, y):\n    return cond(x < 3, None, lambda _: 2.0 * jnp.sin(y), x, lambda x: 2.0 * x)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), axis, idxs, dnums, slice_sizes), 'axis': axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.float64] for axis, shape, idxs, dnums, slice_sizes in [(0, (3, 5), np.array([[0], [2]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, (10, 3), np.array([[0], [0], [0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (1, (10, 3, 5), np.array([[0], [2], [1]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (2, (10, 5, 3), np.array([[0, 2], [1, 0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherGradBatchedOperand(self, axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    gfun = grad(lambda x, idx: jnp.sum(jnp.sin(fun(x, idx))))\n    operand = rng(shape, dtype)\n    ans = vmap(gfun, (axis, None))(operand, idxs)\n    expected = np.stack([gfun(operand[(slice(None),) * axis + (i,)], idxs) for i in range(operand.shape[axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef fun(x, y):\n    return cond(x < 3, None, lambda _: 2.0 * jnp.sin(y), x, lambda x: 2.0 * x)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), axis, idxs, dnums, slice_sizes), 'axis': axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.int32] for axis, shape, idxs, dnums, slice_sizes in [(0, (5,), np.array([[[0], [2]], [[1], [3]]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, (10,), np.array([[0, 0, 0], [0, 2, 1]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (1, (10, 5), np.array([[0, 2, 1], [0, 3, 3]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (0, (10, 5), np.array([[[0, 1], [2, 0]], [[1, 0], [2, 3]]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherBatchedIndices(self, axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    operand = rng(shape, dtype)\n    ans = vmap(fun, (None, axis))(operand, idxs)\n    expected = np.stack([fun(operand, idxs[(slice(None),) * axis + (i,)]) for i in range(idxs.shape[axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef fun(x, y):\n    return cond(x < 3, None, lambda _: 2.0 * jnp.sin(y), x, lambda x: 2.0 * x)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), axis, idxs, dnums, slice_sizes), 'axis': axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.float64] for axis, shape, idxs, dnums, slice_sizes in [(0, (5,), np.array([[[0], [2]], [[1], [3]]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, (10,), np.array([[0, 0, 0], [0, 2, 1]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (1, (10, 5), np.array([[0, 2, 1], [0, 3, 3]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (0, (10, 5), np.array([[[0, 1], [2, 0]], [[1, 0], [2, 3]]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherGradBatchedIndices(self, axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    gfun = grad(lambda x, idx: jnp.sum(jnp.sin(fun(x, idx))))\n    operand = rng(shape, dtype)\n    ans = vmap(gfun, (None, axis))(operand, idxs)\n    expected = np.stack([gfun(operand, idxs[(slice(None),) * axis + (i,)]) for i in range(idxs.shape[axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef fun(x, y):\n    return cond(x < 3, None, lambda _: 2.0 * jnp.sin(y), x, lambda x: 2.0 * x)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_op_axis={}_idxs_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), op_axis, idxs_axis, idxs, dnums, slice_sizes), 'op_axis': op_axis, 'idxs_axis': idxs_axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.int32] for op_axis, idxs_axis, shape, idxs, dnums, slice_sizes in [(0, 0, (2, 5), np.array([[[0], [2]], [[1], [3]]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, 1, (10, 2), np.array([[0, 0, 0], [0, 2, 1]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (0, 1, (2, 10, 5), np.array([[[0, 2, 1], [0, 3, 3]]]).T, lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (2, 0, (10, 5, 2), np.array([[[0, 2], [1, 0]], [[1, 0], [2, 0]]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherBatchedBoth(self, op_axis, idxs_axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    operand = rng(shape, dtype)\n    assert operand.shape[op_axis] == idxs.shape[idxs_axis]\n    ans = vmap(fun, (op_axis, idxs_axis))(operand, idxs)\n    expected = np.stack([fun(operand[(slice(None),) * op_axis + (i,)], idxs[(slice(None),) * idxs_axis + (i,)]) for i in range(idxs.shape[idxs_axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [
      "assert operand.shape[op_axis] == idxs.shape[idxs_axis]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef fun(x, y):\n    return cond(x < 3, None, lambda _: 2.0 * jnp.sin(y), x, lambda x: 2.0 * x)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_op_axis={}_idxs_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), op_axis, idxs_axis, idxs, dnums, slice_sizes), 'op_axis': op_axis, 'idxs_axis': idxs_axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32] for op_axis, idxs_axis, shape, idxs, dnums, slice_sizes in [(0, 0, (2, 5), np.array([[[0], [2]], [[1], [3]]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, 1, (10, 2), np.array([[0, 0, 0], [0, 2, 1]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (0, 1, (2, 10, 5), np.array([[[0, 2, 1], [0, 3, 3]]]).T, lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (2, 0, (10, 5, 2), np.array([[[0, 2], [1, 0]], [[1, 0], [2, 0]]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherGradBatchedBoth(self, op_axis, idxs_axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    gfun = grad(lambda x, idx: jnp.sum(jnp.sin(fun(x, idx))))\n    operand = rng(shape, dtype)\n    assert operand.shape[op_axis] == idxs.shape[idxs_axis]\n    ans = vmap(gfun, (op_axis, idxs_axis))(operand, idxs)\n    expected = np.stack([gfun(operand[(slice(None),) * op_axis + (i,)], idxs[(slice(None),) * idxs_axis + (i,)]) for i in range(idxs.shape[idxs_axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [
      "assert operand.shape[op_axis] == idxs.shape[idxs_axis]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef fun(x, y):\n    return cond(x < 3, None, lambda _: 2.0 * jnp.sin(y), x, lambda x: 2.0 * x)"
  },
  {
    "test_code": "def testBatchOfCompile(self):\n    side = []\n\n    @jit\n    def f(x):\n        side.append(None)\n        return x + x\n    g = jit(vmap(f))\n    self.assertAllClose(g(np.ones(2)), 2 * np.ones(2), check_dtypes=False)\n    self.assertEqual(len(side), 1)\n    self.assertAllClose(g(2 * np.ones(2)), 4 * np.ones(2), check_dtypes=False)\n    self.assertEqual(len(side), 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def g():\n    return jax.lax.cond(True, lambda: data[0], lambda: data[1])"
  },
  {
    "test_code": "def testIssue1170(self):\n\n    def f(index1, index2):\n        return jnp.arange(36).reshape(6, 6)[index1, index2]\n    g = jax.jit(jax.pmap(f))\n    ans = g(index1=np.asarray([1]), index2=np.asarray([2]))\n    expected = g(np.asarray([1]), np.asarray([2]))\n    self.assertAllClose(ans, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def g():\n    return jax.lax.cond(True, lambda: data[0], lambda: data[1])"
  },
  {
    "test_code": "def test_basic(self):\n    with temporarily_register_named_array_vmappable():\n\n        def f(x):\n            return named_mul(x, x)\n        x = NamedArray(['i', 'j'], jnp.arange(12.0).reshape(3, 4))\n        g = jax.vmap(f, in_axes=NamedMapSpec('i', 0), out_axes=NamedMapSpec('i', 1), axis_size=3)\n        ans = g(x)\n        expected = NamedArray(['j', 'i'], jnp.arange(12.0).reshape(3, 4).T ** 2)\n        self.assertEqual(ans.names, expected.names)\n        self.assertAllClose(ans.data, expected.data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def g():\n    return jax.lax.cond(True, lambda: data[0], lambda: data[1])"
  },
  {
    "test_code": "def testCumProd(self):\n    x = jnp.arange(9).reshape(3, 3) + 1\n    y = vmap(lambda x: jnp.cumprod(x, axis=-1))(x)\n    self.assertAllClose(jnp.cumprod(x, axis=1), y)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def cumprod(x):\n    s = jnp.ones((2, 32), jnp.float32)\n    return lax.scan(lambda s, x: (x * s, s), s, x)"
  },
  {
    "test_code": "def testConvGeneralDilated(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.02, atol=0.002)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def testConvGeneralDilatedBatchNotMajor(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 4), dtype=np.float32)\n    x = jnp.array(self.rng().randn(3, 5, 7, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('HNWC', 'HWIO', 'HWNC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    per_example = vmap(partial(f, W))(x)\n    per_example = jnp.reshape(jnp.transpose(per_example, (1, 2, 0, 3, 4)), (5, 5, 21, 4))\n    per_example_direct = f(W, jnp.reshape(jnp.transpose(x, (1, 0, 2, 3, 4)), (5, 21, 5, 1)))\n    self.assertAllClose(per_example, per_example_direct)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_op={name}', 'op': op, 'unit': unit} for name, op, unit in [('max', lax.max, -jnp.inf), ('min', lax.min, jnp.inf)]))\ndef testMinMaxPool(self, op, unit):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, unit, op, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.05, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def testSumPool(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, 0.0, lax.add, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.03, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def testNumpyIndexing1(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n    ind = np.array([[0, 1], [2, 0]])\n\n    def f(a, ind):\n        return a[:, ind]\n    expected = np.stack([f(a, ind[i, :]) for i in range(ind.shape[0])])\n    ans = vmap(f, (None, 0))(a, ind)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def testNumpyIndexing2(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n\n    def f(a):\n        inds = jnp.array([0, 2])\n        return a[:, inds]\n    ans = vmap(f)(a)\n    expected = np.stack([f(a[:, i, :]) for i in range(a.shape[1])], axis=1)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_{}_vmap_names={}_collective_names={}'.format(collective.__name__.replace(' ', ''), ''.join(vmap_names), ''.join(collective_names)), 'collective': collective, 'bulk_op': bulk_op, 'vmap_names': vmap_names, 'collective_names': collective_names} for collective, bulk_op in [(lax.psum, jnp.sum), (lax.pmax, jnp.max), (lax.pmin, jnp.min)] for vmap_names in [('i',), ('i', 'j'), ('i', 'j', 'k')] for subset_size in range(1, len(vmap_names) + 1) for collective_subset in it.combinations(vmap_names, subset_size) for collective_names in it.permutations(collective_subset)))\ndef testCommAssocCollective(self, collective, bulk_op, vmap_names, collective_names):\n    shape = (2, 2, 2)\n    x = jnp.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    f = lambda x: x - collective(x, collective_names)\n    for i, axis_name in enumerate(vmap_names):\n        f = vmap(f, axis_name=axis_name, in_axes=i, out_axes=i)\n    pos_axis = [i for i, name in enumerate(vmap_names) if name in collective_names]\n    self.assertAllClose(f(x), x - bulk_op(x, axis=pos_axis, keepdims=True))\n    if collective is lax.psum:\n        jtu.check_grads(f, (x,), 2, eps=1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(3), range(3), range(4))))\ndef testAllToAll(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n    f = vmap(lambda x: lax.all_to_all(x, 'i', split_axis, concat_axis), in_axes=vmap_axis, axis_name='i')\n    y = f(x)\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis + (vmap_axis <= split_axis)), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(2), range(2), range(3))))\ndef testAllToAllSplitAxis(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n\n    @partial(vmap, in_axes=vmap_axis, axis_name='i')\n    @partial(vmap, in_axes=vmap_axis, axis_name='j')\n    def f(x):\n        return lax.all_to_all(x, ('i', 'j'), split_axis, concat_axis)\n    unroll_shape = (2, 2, *shape[1:])\n    unroll_shape = list(shape)\n    unroll_shape[vmap_axis:vmap_axis + 1] = (2, 2)\n    x_unroll = x.reshape(unroll_shape)\n    y_unrolled = f(x_unroll)\n    y = y_unrolled.reshape(shape)\n    if vmap_axis <= split_axis:\n        split_axis += 1\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def testGradOfPsum(self):\n    a = jnp.ones(5)\n    f = vmap(jax.grad(lambda x: -lax.psum(x, 'i')), out_axes=None, axis_name='i')\n    self.assertEqual(f(a), core.jaxpr_as_fun(jax.make_jaxpr(f)(a))(a)[0])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def testIssue6096(self):\n\n    def f(x):\n        return jsp.special.betainc(jnp.ones(3), 1.0, x)\n    self.assertEqual(f(jnp.ones(3)).shape, (3,))\n    self.assertEqual(jax.vmap(f)(jnp.ones((2, 3))).shape, (2, 3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def testBatchingPreservesWeakType(self):\n    x = jnp.ravel(1)\n    self.assertTrue(dtypes.is_weakly_typed(x))\n\n    @vmap\n    def f(x):\n        self.assertTrue(dtypes.is_weakly_typed(x), f'{x} is not weakly-typed')\n        return x\n    y = f(x)\n    self.assertTrue(dtypes.is_weakly_typed(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def testConvGeneralDilated(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.02, atol=0.002)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def testConvGeneralDilatedBatchNotMajor(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 4), dtype=np.float32)\n    x = jnp.array(self.rng().randn(3, 5, 7, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('HNWC', 'HWIO', 'HWNC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    per_example = vmap(partial(f, W))(x)\n    per_example = jnp.reshape(jnp.transpose(per_example, (1, 2, 0, 3, 4)), (5, 5, 21, 4))\n    per_example_direct = f(W, jnp.reshape(jnp.transpose(x, (1, 0, 2, 3, 4)), (5, 21, 5, 1)))\n    self.assertAllClose(per_example, per_example_direct)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_op={name}', 'op': op, 'unit': unit} for name, op, unit in [('max', lax.max, -jnp.inf), ('min', lax.min, jnp.inf)]))\ndef testMinMaxPool(self, op, unit):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, unit, op, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.05, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def testSumPool(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, 0.0, lax.add, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.03, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def testNumpyIndexing1(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n    ind = np.array([[0, 1], [2, 0]])\n\n    def f(a, ind):\n        return a[:, ind]\n    expected = np.stack([f(a, ind[i, :]) for i in range(ind.shape[0])])\n    ans = vmap(f, (None, 0))(a, ind)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def testNumpyIndexing2(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n\n    def f(a):\n        inds = jnp.array([0, 2])\n        return a[:, inds]\n    ans = vmap(f)(a)\n    expected = np.stack([f(a[:, i, :]) for i in range(a.shape[1])], axis=1)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_{}_vmap_names={}_collective_names={}'.format(collective.__name__.replace(' ', ''), ''.join(vmap_names), ''.join(collective_names)), 'collective': collective, 'bulk_op': bulk_op, 'vmap_names': vmap_names, 'collective_names': collective_names} for collective, bulk_op in [(lax.psum, jnp.sum), (lax.pmax, jnp.max), (lax.pmin, jnp.min)] for vmap_names in [('i',), ('i', 'j'), ('i', 'j', 'k')] for subset_size in range(1, len(vmap_names) + 1) for collective_subset in it.combinations(vmap_names, subset_size) for collective_names in it.permutations(collective_subset)))\ndef testCommAssocCollective(self, collective, bulk_op, vmap_names, collective_names):\n    shape = (2, 2, 2)\n    x = jnp.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    f = lambda x: x - collective(x, collective_names)\n    for i, axis_name in enumerate(vmap_names):\n        f = vmap(f, axis_name=axis_name, in_axes=i, out_axes=i)\n    pos_axis = [i for i, name in enumerate(vmap_names) if name in collective_names]\n    self.assertAllClose(f(x), x - bulk_op(x, axis=pos_axis, keepdims=True))\n    if collective is lax.psum:\n        jtu.check_grads(f, (x,), 2, eps=1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(3), range(3), range(4))))\ndef testAllToAll(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n    f = vmap(lambda x: lax.all_to_all(x, 'i', split_axis, concat_axis), in_axes=vmap_axis, axis_name='i')\n    y = f(x)\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis + (vmap_axis <= split_axis)), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(2), range(2), range(3))))\ndef testAllToAllSplitAxis(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n\n    @partial(vmap, in_axes=vmap_axis, axis_name='i')\n    @partial(vmap, in_axes=vmap_axis, axis_name='j')\n    def f(x):\n        return lax.all_to_all(x, ('i', 'j'), split_axis, concat_axis)\n    unroll_shape = (2, 2, *shape[1:])\n    unroll_shape = list(shape)\n    unroll_shape[vmap_axis:vmap_axis + 1] = (2, 2)\n    x_unroll = x.reshape(unroll_shape)\n    y_unrolled = f(x_unroll)\n    y = y_unrolled.reshape(shape)\n    if vmap_axis <= split_axis:\n        split_axis += 1\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def testGradOfPsum(self):\n    a = jnp.ones(5)\n    f = vmap(jax.grad(lambda x: -lax.psum(x, 'i')), out_axes=None, axis_name='i')\n    self.assertEqual(f(a), core.jaxpr_as_fun(jax.make_jaxpr(f)(a))(a)[0])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def testIssue6096(self):\n\n    def f(x):\n        return jsp.special.betainc(jnp.ones(3), 1.0, x)\n    self.assertEqual(f(jnp.ones(3)).shape, (3,))\n    self.assertEqual(jax.vmap(f)(jnp.ones((2, 3))).shape, (2, 3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def testBatchingPreservesWeakType(self):\n    x = jnp.ravel(1)\n    self.assertTrue(dtypes.is_weakly_typed(x))\n\n    @vmap\n    def f(x):\n        self.assertTrue(dtypes.is_weakly_typed(x), f'{x} is not weakly-typed')\n        return x\n    y = f(x)\n    self.assertTrue(dtypes.is_weakly_typed(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def testBatchOfCompile(self):\n    side = []\n\n    @jit\n    def f(x):\n        side.append(None)\n        return x + x\n    g = jit(vmap(f))\n    self.assertAllClose(g(np.ones(2)), 2 * np.ones(2), check_dtypes=False)\n    self.assertEqual(len(side), 1)\n    self.assertAllClose(g(2 * np.ones(2)), 4 * np.ones(2), check_dtypes=False)\n    self.assertEqual(len(side), 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef g():\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')"
  },
  {
    "test_code": "def testIssue1170(self):\n\n    def f(index1, index2):\n        return jnp.arange(36).reshape(6, 6)[index1, index2]\n    g = jax.jit(jax.pmap(f))\n    ans = g(index1=np.asarray([1]), index2=np.asarray([2]))\n    expected = g(np.asarray([1]), np.asarray([2]))\n    self.assertAllClose(ans, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef g():\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')"
  },
  {
    "test_code": "def test_basic(self):\n    with temporarily_register_named_array_vmappable():\n\n        def f(x):\n            return named_mul(x, x)\n        x = NamedArray(['i', 'j'], jnp.arange(12.0).reshape(3, 4))\n        g = jax.vmap(f, in_axes=NamedMapSpec('i', 0), out_axes=NamedMapSpec('i', 1), axis_size=3)\n        ans = g(x)\n        expected = NamedArray(['j', 'i'], jnp.arange(12.0).reshape(3, 4).T ** 2)\n        self.assertEqual(ans.names, expected.names)\n        self.assertAllClose(ans.data, expected.data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef g():\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')"
  },
  {
    "test_code": "def testConvGeneralDilated(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.02, atol=0.002)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def testConvGeneralDilatedBatchNotMajor(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 4), dtype=np.float32)\n    x = jnp.array(self.rng().randn(3, 5, 7, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('HNWC', 'HWIO', 'HWNC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    per_example = vmap(partial(f, W))(x)\n    per_example = jnp.reshape(jnp.transpose(per_example, (1, 2, 0, 3, 4)), (5, 5, 21, 4))\n    per_example_direct = f(W, jnp.reshape(jnp.transpose(x, (1, 0, 2, 3, 4)), (5, 21, 5, 1)))\n    self.assertAllClose(per_example, per_example_direct)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_op={name}', 'op': op, 'unit': unit} for name, op, unit in [('max', lax.max, -jnp.inf), ('min', lax.min, jnp.inf)]))\ndef testMinMaxPool(self, op, unit):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, unit, op, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.05, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def testSumPool(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, 0.0, lax.add, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.03, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def testNumpyIndexing1(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n    ind = np.array([[0, 1], [2, 0]])\n\n    def f(a, ind):\n        return a[:, ind]\n    expected = np.stack([f(a, ind[i, :]) for i in range(ind.shape[0])])\n    ans = vmap(f, (None, 0))(a, ind)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def testNumpyIndexing2(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n\n    def f(a):\n        inds = jnp.array([0, 2])\n        return a[:, inds]\n    ans = vmap(f)(a)\n    expected = np.stack([f(a[:, i, :]) for i in range(a.shape[1])], axis=1)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_{}_vmap_names={}_collective_names={}'.format(collective.__name__.replace(' ', ''), ''.join(vmap_names), ''.join(collective_names)), 'collective': collective, 'bulk_op': bulk_op, 'vmap_names': vmap_names, 'collective_names': collective_names} for collective, bulk_op in [(lax.psum, jnp.sum), (lax.pmax, jnp.max), (lax.pmin, jnp.min)] for vmap_names in [('i',), ('i', 'j'), ('i', 'j', 'k')] for subset_size in range(1, len(vmap_names) + 1) for collective_subset in it.combinations(vmap_names, subset_size) for collective_names in it.permutations(collective_subset)))\ndef testCommAssocCollective(self, collective, bulk_op, vmap_names, collective_names):\n    shape = (2, 2, 2)\n    x = jnp.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    f = lambda x: x - collective(x, collective_names)\n    for i, axis_name in enumerate(vmap_names):\n        f = vmap(f, axis_name=axis_name, in_axes=i, out_axes=i)\n    pos_axis = [i for i, name in enumerate(vmap_names) if name in collective_names]\n    self.assertAllClose(f(x), x - bulk_op(x, axis=pos_axis, keepdims=True))\n    if collective is lax.psum:\n        jtu.check_grads(f, (x,), 2, eps=1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(3), range(3), range(4))))\ndef testAllToAll(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n    f = vmap(lambda x: lax.all_to_all(x, 'i', split_axis, concat_axis), in_axes=vmap_axis, axis_name='i')\n    y = f(x)\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis + (vmap_axis <= split_axis)), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(2), range(2), range(3))))\ndef testAllToAllSplitAxis(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n\n    @partial(vmap, in_axes=vmap_axis, axis_name='i')\n    @partial(vmap, in_axes=vmap_axis, axis_name='j')\n    def f(x):\n        return lax.all_to_all(x, ('i', 'j'), split_axis, concat_axis)\n    unroll_shape = (2, 2, *shape[1:])\n    unroll_shape = list(shape)\n    unroll_shape[vmap_axis:vmap_axis + 1] = (2, 2)\n    x_unroll = x.reshape(unroll_shape)\n    y_unrolled = f(x_unroll)\n    y = y_unrolled.reshape(shape)\n    if vmap_axis <= split_axis:\n        split_axis += 1\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def testGradOfPsum(self):\n    a = jnp.ones(5)\n    f = vmap(jax.grad(lambda x: -lax.psum(x, 'i')), out_axes=None, axis_name='i')\n    self.assertEqual(f(a), core.jaxpr_as_fun(jax.make_jaxpr(f)(a))(a)[0])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def testIssue6096(self):\n\n    def f(x):\n        return jsp.special.betainc(jnp.ones(3), 1.0, x)\n    self.assertEqual(f(jnp.ones(3)).shape, (3,))\n    self.assertEqual(jax.vmap(f)(jnp.ones((2, 3))).shape, (2, 3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def testBatchingPreservesWeakType(self):\n    x = jnp.ravel(1)\n    self.assertTrue(dtypes.is_weakly_typed(x))\n\n    @vmap\n    def f(x):\n        self.assertTrue(dtypes.is_weakly_typed(x), f'{x} is not weakly-typed')\n        return x\n    y = f(x)\n    self.assertTrue(dtypes.is_weakly_typed(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def testBatchOfCompile(self):\n    side = []\n\n    @jit\n    def f(x):\n        side.append(None)\n        return x + x\n    g = jit(vmap(f))\n    self.assertAllClose(g(np.ones(2)), 2 * np.ones(2), check_dtypes=False)\n    self.assertEqual(len(side), 1)\n    self.assertAllClose(g(2 * np.ones(2)), 4 * np.ones(2), check_dtypes=False)\n    self.assertEqual(len(side), 1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def g(a, b):\n    c = jnp.zeros_like(a)\n    _, b, c, _ = for_impl(5, body2, (a, b, c, 0))\n    return (b, c)"
  },
  {
    "test_code": "def testIssue1170(self):\n\n    def f(index1, index2):\n        return jnp.arange(36).reshape(6, 6)[index1, index2]\n    g = jax.jit(jax.pmap(f))\n    ans = g(index1=np.asarray([1]), index2=np.asarray([2]))\n    expected = g(np.asarray([1]), np.asarray([2]))\n    self.assertAllClose(ans, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def g(a, b):\n    c = jnp.zeros_like(a)\n    _, b, c, _ = for_impl(5, body2, (a, b, c, 0))\n    return (b, c)"
  },
  {
    "test_code": "def test_basic(self):\n    with temporarily_register_named_array_vmappable():\n\n        def f(x):\n            return named_mul(x, x)\n        x = NamedArray(['i', 'j'], jnp.arange(12.0).reshape(3, 4))\n        g = jax.vmap(f, in_axes=NamedMapSpec('i', 0), out_axes=NamedMapSpec('i', 1), axis_size=3)\n        ans = g(x)\n        expected = NamedArray(['j', 'i'], jnp.arange(12.0).reshape(3, 4).T ** 2)\n        self.assertEqual(ans.names, expected.names)\n        self.assertAllClose(ans.data, expected.data)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def g(a, b):\n    c = jnp.zeros_like(a)\n    _, b, c, _ = for_impl(5, body2, (a, b, c, 0))\n    return (b, c)"
  },
  {
    "test_code": "def testConvGeneralDilated(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.02, atol=0.002)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def testConvGeneralDilatedBatchNotMajor(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 4), dtype=np.float32)\n    x = jnp.array(self.rng().randn(3, 5, 7, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('HNWC', 'HWIO', 'HWNC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        return y\n    per_example = vmap(partial(f, W))(x)\n    per_example = jnp.reshape(jnp.transpose(per_example, (1, 2, 0, 3, 4)), (5, 5, 21, 4))\n    per_example_direct = f(W, jnp.reshape(jnp.transpose(x, (1, 0, 2, 3, 4)), (5, 21, 5, 1)))\n    self.assertAllClose(per_example, per_example_direct)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_op={name}', 'op': op, 'unit': unit} for name, op, unit in [('max', lax.max, -jnp.inf), ('min', lax.min, jnp.inf)]))\ndef testMinMaxPool(self, op, unit):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, unit, op, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.05, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def testSumPool(self):\n    W = jnp.array(self.rng().randn(3, 3, 1, 5), dtype=np.float32)\n    X = jnp.array(self.rng().randn(10, 5, 5, 1), dtype=np.float32)\n\n    def f(params, x):\n        one = (1, 1)\n        dimension_numbers = ('NHWC', 'HWIO', 'NHWC')\n        y = lax.conv_general_dilated(x, params, one, 'SAME', one, one, dimension_numbers)\n        y = lax.reduce_window(y, 0.0, lax.add, (1, 2, 2, 1), (1, 1, 1, 1), 'SAME')\n        return y\n    grad_loss = grad(lambda params, x: jnp.mean(f(params, x) ** 2))\n    per_example = vmap(partial(f, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example = jnp.reshape(per_example, (10, 5, 5, 5))\n    per_example_direct = f(W, X)\n    self.assertAllClose(per_example, per_example_direct)\n    per_example = vmap(partial(grad_loss, W))(jnp.reshape(X, (10, 1, 5, 5, 1)))\n    per_example_direct = []\n    for i in range(10):\n        g = grad_loss(W, jnp.reshape(X[i], (1, 5, 5, 1)))\n        per_example_direct += [jnp.reshape(g, (1,) + g.shape)]\n    per_example_direct = jnp.concatenate(per_example_direct, axis=0)\n    self.assertAllClose(per_example, per_example_direct, rtol=0.03, atol=0.001)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def testNumpyIndexing1(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n    ind = np.array([[0, 1], [2, 0]])\n\n    def f(a, ind):\n        return a[:, ind]\n    expected = np.stack([f(a, ind[i, :]) for i in range(ind.shape[0])])\n    ans = vmap(f, (None, 0))(a, ind)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def testNumpyIndexing2(self):\n    a = jnp.arange(2 * 3 * 4).reshape((2, 3, 4))\n\n    def f(a):\n        inds = jnp.array([0, 2])\n        return a[:, inds]\n    ans = vmap(f)(a)\n    expected = np.stack([f(a[:, i, :]) for i in range(a.shape[1])], axis=1)\n    assert np.all(ans == expected)",
    "assertions": [
      "assert np.all(ans == expected)"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_{}_vmap_names={}_collective_names={}'.format(collective.__name__.replace(' ', ''), ''.join(vmap_names), ''.join(collective_names)), 'collective': collective, 'bulk_op': bulk_op, 'vmap_names': vmap_names, 'collective_names': collective_names} for collective, bulk_op in [(lax.psum, jnp.sum), (lax.pmax, jnp.max), (lax.pmin, jnp.min)] for vmap_names in [('i',), ('i', 'j'), ('i', 'j', 'k')] for subset_size in range(1, len(vmap_names) + 1) for collective_subset in it.combinations(vmap_names, subset_size) for collective_names in it.permutations(collective_subset)))\ndef testCommAssocCollective(self, collective, bulk_op, vmap_names, collective_names):\n    shape = (2, 2, 2)\n    x = jnp.arange(np.prod(shape), dtype=jnp.float32).reshape(shape)\n    f = lambda x: x - collective(x, collective_names)\n    for i, axis_name in enumerate(vmap_names):\n        f = vmap(f, axis_name=axis_name, in_axes=i, out_axes=i)\n    pos_axis = [i for i, name in enumerate(vmap_names) if name in collective_names]\n    self.assertAllClose(f(x), x - bulk_op(x, axis=pos_axis, keepdims=True))\n    if collective is lax.psum:\n        jtu.check_grads(f, (x,), 2, eps=1)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(3), range(3), range(4))))\ndef testAllToAll(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n    f = vmap(lambda x: lax.all_to_all(x, 'i', split_axis, concat_axis), in_axes=vmap_axis, axis_name='i')\n    y = f(x)\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis + (vmap_axis <= split_axis)), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': f'_split={split_axis}_concat={concat_axis}_vmap={vmap_axis}', 'split_axis': split_axis, 'concat_axis': concat_axis, 'vmap_axis': vmap_axis} for split_axis, concat_axis, vmap_axis in it.product(range(2), range(2), range(3))))\ndef testAllToAllSplitAxis(self, vmap_axis, split_axis, concat_axis):\n    shape = (4, 4, 4)\n    x = np.arange(np.prod(shape)).reshape(shape)\n\n    @partial(vmap, in_axes=vmap_axis, axis_name='i')\n    @partial(vmap, in_axes=vmap_axis, axis_name='j')\n    def f(x):\n        return lax.all_to_all(x, ('i', 'j'), split_axis, concat_axis)\n    unroll_shape = (2, 2, *shape[1:])\n    unroll_shape = list(shape)\n    unroll_shape[vmap_axis:vmap_axis + 1] = (2, 2)\n    x_unroll = x.reshape(unroll_shape)\n    y_unrolled = f(x_unroll)\n    y = y_unrolled.reshape(shape)\n    if vmap_axis <= split_axis:\n        split_axis += 1\n    ref = jnp.moveaxis(x, (vmap_axis, split_axis), (concat_axis + 1, 0))\n    self.assertAllClose(y, ref)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def testGradOfPsum(self):\n    a = jnp.ones(5)\n    f = vmap(jax.grad(lambda x: -lax.psum(x, 'i')), out_axes=None, axis_name='i')\n    self.assertEqual(f(a), core.jaxpr_as_fun(jax.make_jaxpr(f)(a))(a)[0])",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def testIssue6096(self):\n\n    def f(x):\n        return jsp.special.betainc(jnp.ones(3), 1.0, x)\n    self.assertEqual(f(jnp.ones(3)).shape, (3,))\n    self.assertEqual(jax.vmap(f)(jnp.ones((2, 3))).shape, (2, 3))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def testBatchingPreservesWeakType(self):\n    x = jnp.ravel(1)\n    self.assertTrue(dtypes.is_weakly_typed(x))\n\n    @vmap\n    def f(x):\n        self.assertTrue(dtypes.is_weakly_typed(x), f'{x} is not weakly-typed')\n        return x\n    y = f(x)\n    self.assertTrue(dtypes.is_weakly_typed(y))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "@jax.default_matmul_precision('float32')\ndef testDotGeneral(self):\n    R = self.rng().randn\n    x = R(10, 3, 4, 5)\n    y = R(10, 3, 5, 6)\n    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])\n    ans = vmap(fun)(x, y)\n    expected = lax.dot_general(x, y, [((3,), (2,)), ((0, 1), (0, 1))])\n    self.assertAllClose(ans, expected)\n    x = R(3, 4, 10, 5)\n    y = R(3, 10, 5, 6)\n    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])\n    ans = vmap(fun, in_axes=(2, 1))(x, y)\n    expected = np.stack([fun(x[..., i, :], y[:, i, ...]) for i in range(10)])\n    self.assertAllClose(ans, expected)\n    x = R(3, 4, 5, 10)\n    y = R(3, 5, 6)\n    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])\n    ans = vmap(fun, in_axes=(3, None))(x, y)\n    expected = np.stack([fun(x[..., i], y) for i in range(10)])\n    self.assertAllClose(ans, expected)\n    x = R(3, 4, 5)\n    y = R(3, 5, 10, 6)\n    fun = lambda x, y: lax.dot_general(x, y, [((2,), (1,)), ((0,), (0,))])\n    ans = vmap(fun, in_axes=(None, 2))(x, y)\n    expected = np.stack([fun(x, y[..., i, :]) for i in range(10)])\n    self.assertAllClose(ans, expected)\n    x = R(4)\n    y = R(4, 10)\n    fun = lambda x, y: lax.dot_general(x, y, [((0,), (0,)), ((), ())])\n    ans = vmap(fun, in_axes=(None, 1))(x, y)\n    expected = np.stack([fun(x, y[..., i]) for i in range(10)])\n    self.assertAllClose(ans, expected)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def fun():\n    return jax.ffi.ffi_call('test', jax.ShapeDtypeStruct((), np.int64))()"
  },
  {
    "test_code": "def testHessian(self):\n\n    def fun(x, t):\n        return jnp.sum(jnp.power(jnp.maximum(x, 0.0), 2)) + t\n    x = np.array([-1.0, -0.5, 0.0, 0.5, 1.0])\n    ans = hessian(lambda x: fun(x, 0.0))(x)\n    expected = np.array([[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.5, 0.0, 0.0], [0.0, 0.0, 0.0, 2.0, 0.0], [0.0, 0.0, 0.0, 0.0, 2.0]])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def fun():\n    return jax.ffi.ffi_call('test', jax.ShapeDtypeStruct((), np.int64))()"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), axis, idxs, dnums, slice_sizes), 'axis': axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.int32] for axis, shape, idxs, dnums, slice_sizes in [(0, (3, 5), np.array([[0], [2]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, (10, 3), np.array([[0], [0], [0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (1, (10, 3, 5), np.array([[0], [2], [1]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (2, (10, 5, 3), np.array([[0, 2], [1, 0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherBatchedOperand(self, axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    operand = rng(shape, dtype)\n    ans = vmap(fun, (axis, None))(operand, idxs)\n    expected = np.stack([fun(operand[(slice(None),) * axis + (i,)], idxs) for i in range(operand.shape[axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def fun():\n    return jax.ffi.ffi_call('test', jax.ShapeDtypeStruct((), np.int64))()"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), axis, idxs, dnums, slice_sizes), 'axis': axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.float64] for axis, shape, idxs, dnums, slice_sizes in [(0, (3, 5), np.array([[0], [2]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, (10, 3), np.array([[0], [0], [0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (1, (10, 3, 5), np.array([[0], [2], [1]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (2, (10, 5, 3), np.array([[0, 2], [1, 0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherGradBatchedOperand(self, axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    gfun = grad(lambda x, idx: jnp.sum(jnp.sin(fun(x, idx))))\n    operand = rng(shape, dtype)\n    ans = vmap(gfun, (axis, None))(operand, idxs)\n    expected = np.stack([gfun(operand[(slice(None),) * axis + (i,)], idxs) for i in range(operand.shape[axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def fun():\n    return jax.ffi.ffi_call('test', jax.ShapeDtypeStruct((), np.int64))()"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), axis, idxs, dnums, slice_sizes), 'axis': axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.int32] for axis, shape, idxs, dnums, slice_sizes in [(0, (5,), np.array([[[0], [2]], [[1], [3]]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, (10,), np.array([[0, 0, 0], [0, 2, 1]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (1, (10, 5), np.array([[0, 2, 1], [0, 3, 3]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (0, (10, 5), np.array([[[0, 1], [2, 0]], [[1, 0], [2, 3]]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherBatchedIndices(self, axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    operand = rng(shape, dtype)\n    ans = vmap(fun, (None, axis))(operand, idxs)\n    expected = np.stack([fun(operand, idxs[(slice(None),) * axis + (i,)]) for i in range(idxs.shape[axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def fun():\n    return jax.ffi.ffi_call('test', jax.ShapeDtypeStruct((), np.int64))()"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), axis, idxs, dnums, slice_sizes), 'axis': axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.float64] for axis, shape, idxs, dnums, slice_sizes in [(0, (5,), np.array([[[0], [2]], [[1], [3]]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, (10,), np.array([[0, 0, 0], [0, 2, 1]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (1, (10, 5), np.array([[0, 2, 1], [0, 3, 3]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (0, (10, 5), np.array([[[0, 1], [2, 0]], [[1, 0], [2, 3]]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherGradBatchedIndices(self, axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    gfun = grad(lambda x, idx: jnp.sum(jnp.sin(fun(x, idx))))\n    operand = rng(shape, dtype)\n    ans = vmap(gfun, (None, axis))(operand, idxs)\n    expected = np.stack([gfun(operand, idxs[(slice(None),) * axis + (i,)]) for i in range(idxs.shape[axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def fun():\n    return jax.ffi.ffi_call('test', jax.ShapeDtypeStruct((), np.int64))()"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_op_axis={}_idxs_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), op_axis, idxs_axis, idxs, dnums, slice_sizes), 'op_axis': op_axis, 'idxs_axis': idxs_axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32, np.int32] for op_axis, idxs_axis, shape, idxs, dnums, slice_sizes in [(0, 0, (2, 5), np.array([[[0], [2]], [[1], [3]]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, 1, (10, 2), np.array([[0, 0, 0], [0, 2, 1]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (0, 1, (2, 10, 5), np.array([[[0, 2, 1], [0, 3, 3]]]).T, lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (2, 0, (10, 5, 2), np.array([[[0, 2], [1, 0]], [[1, 0], [2, 0]]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherBatchedBoth(self, op_axis, idxs_axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    operand = rng(shape, dtype)\n    assert operand.shape[op_axis] == idxs.shape[idxs_axis]\n    ans = vmap(fun, (op_axis, idxs_axis))(operand, idxs)\n    expected = np.stack([fun(operand[(slice(None),) * op_axis + (i,)], idxs[(slice(None),) * idxs_axis + (i,)]) for i in range(idxs.shape[idxs_axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [
      "assert operand.shape[op_axis] == idxs.shape[idxs_axis]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def fun():\n    return jax.ffi.ffi_call('test', jax.ShapeDtypeStruct((), np.int64))()"
  },
  {
    "test_code": "@parameterized.named_parameters(({'testcase_name': '_shape={}_op_axis={}_idxs_axis={}_idxs={}_dnums={}_slice_sizes={}'.format(jtu.format_shape_dtype_string(shape, dtype), op_axis, idxs_axis, idxs, dnums, slice_sizes), 'op_axis': op_axis, 'idxs_axis': idxs_axis, 'shape': shape, 'dtype': dtype, 'idxs': idxs, 'dnums': dnums, 'slice_sizes': slice_sizes} for dtype in [np.float32] for op_axis, idxs_axis, shape, idxs, dnums, slice_sizes in [(0, 0, (2, 5), np.array([[[0], [2]], [[1], [3]]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), (1, 1, (10, 2), np.array([[0, 0, 0], [0, 2, 1]]).T[..., None], lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), (0, 1, (2, 10, 5), np.array([[[0, 2, 1], [0, 3, 3]]]).T, lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), (2, 0, (10, 5, 2), np.array([[[0, 2], [1, 0]], [[1, 0], [2, 0]]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]))\ndef testGatherGradBatchedBoth(self, op_axis, idxs_axis, shape, dtype, idxs, dnums, slice_sizes):\n    rng = jtu.rand_default(self.rng())\n    fun = partial(lax.gather, dimension_numbers=dnums, slice_sizes=slice_sizes)\n    gfun = grad(lambda x, idx: jnp.sum(jnp.sin(fun(x, idx))))\n    operand = rng(shape, dtype)\n    assert operand.shape[op_axis] == idxs.shape[idxs_axis]\n    ans = vmap(gfun, (op_axis, idxs_axis))(operand, idxs)\n    expected = np.stack([gfun(operand[(slice(None),) * op_axis + (i,)], idxs[(slice(None),) * idxs_axis + (i,)]) for i in range(idxs.shape[idxs_axis])])\n    self.assertAllClose(ans, expected, check_dtypes=False)",
    "assertions": [
      "assert operand.shape[op_axis] == idxs.shape[idxs_axis]"
    ],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/batching_test.py",
    "function": "def fun():\n    return jax.ffi.ffi_call('test', jax.ShapeDtypeStruct((), np.int64))()"
  }
]