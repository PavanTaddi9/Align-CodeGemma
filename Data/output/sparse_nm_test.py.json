[
  {
    "test_code": "@jtu.run_on_devices('gpu')\ndef test_validate_nm_pack(self):\n    with self.assertRaisesRegex(TypeError, 'Mask should be bool'):\n        nm.nm_pack(jnp.zeros(16, jnp.int8))\n    with self.assertRaisesRegex(TypeError, 'Inner dimension size should be divisible by 16'):\n        nm.nm_pack(jnp.array([False] * 8))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/sparse_nm_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@jtu.run_on_devices('gpu')\ndef test_validate_nm_spmm(self):\n    batch, tile_m, tile_n, tile_k = (2, 64, 32, 128)\n    lhs = jnp.zeros((batch, tile_m, tile_k // 2), dtype=jnp.bfloat16)\n    rhs = jnp.zeros((batch, tile_k, tile_n), dtype=jnp.bfloat16)\n    meta = jnp.zeros((batch, tile_m, tile_k // 16), dtype=jnp.uint16)\n    if config.enable_x64.value:\n        with self.assertRaisesRegex(TypeError, 'Unsupported lhs input type'):\n            nm.nm_spmm(jnp.zeros(lhs.shape, dtype=jnp.int64), rhs, meta)\n        with self.assertRaisesRegex(TypeError, 'Unsupported rhs input type'):\n            nm.nm_spmm(lhs, jnp.zeros(rhs.shape, dtype=jnp.int64), meta)\n        with self.assertRaisesRegex(TypeError, 'Unsupported output type'):\n            nm.nm_spmm(lhs, rhs, meta, output_dtype=jnp.int64)\n    nm_spmm_with_dnums = lambda c, b: nm.nm_spmm(lhs, rhs, meta, dimension_numbers=(c, b))\n    with self.assertRaisesRegex(TypeError, 'Only single contracting dimension is supported'):\n        nm_spmm_with_dnums(((0, 2), (0, 1)), (tuple(), tuple()))\n    with self.assertRaisesRegex(TypeError, 'Incorrect dimension numbers for lhs'):\n        nm_spmm_with_dnums(((2,), (1,)), ((2,), (0,)))\n    with self.assertRaisesRegex(TypeError, 'Incorrect dimension numbers for rhs'):\n        nm_spmm_with_dnums(((2,), (1,)), ((0,), (1,)))\n    with self.assertRaisesRegex(TypeError, 'Only single non-contracting dimension is supported'):\n        nm_spmm_with_dnums(((2,), (1,)), (tuple(), tuple()))\n    with self.assertRaisesRegex(TypeError, 'Batch dimension sizes do not match'):\n        nm.nm_spmm(lhs, rhs.reshape(1, tile_k, tile_n * batch), meta, dimension_numbers=(((2,), (1,)), ((0,), (0,))))\n    nm_spmm_with_meta = lambda m: nm.nm_spmm(lhs, rhs, m, dimension_numbers=(((2,), (1,)), ((0,), (0,))))\n    with self.assertRaisesRegex(TypeError, 'Metadata must be uint16'):\n        nm_spmm_with_meta(jnp.zeros(meta.shape, dtype=jnp.uint8))\n    with self.assertRaisesRegex(TypeError, 'Metadata shape must match the operand shape'):\n        nm_spmm_with_meta(meta.reshape(1, batch * tile_m, tile_k // 16))\n    with self.assertRaisesRegex(TypeError, 'Metadata must be exactly 8 times less than the contracting dimension for 2:4 structured sparsity'):\n        nm_spmm_with_meta(jnp.repeat(meta, 2, axis=-1))\n    with self.assertRaisesRegex(TypeError, 'Contracting dimension must be the minor one'):\n        nm.nm_spmm(lhs, rhs, meta, dimension_numbers=(((1,), (1,)), ((0,), (0,))))\n    with self.assertRaisesRegex(TypeError, 'Contracting dimension sizes should have 2:4 ratio'):\n        nm.nm_spmm(lhs, jnp.repeat(rhs, 2, axis=1), meta, dimension_numbers=(((2,), (1,)), ((0,), (0,))))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/sparse_nm_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "@parameterized.product(lhs_type=[jnp.int8, jnp.int16, jnp.float16, jnp.bfloat16], rhs_type=[jnp.bfloat16], output_type=[jnp.bfloat16, jnp.float32])\n@jtu.run_on_devices('gpu')\ndef test_types(self, lhs_type, rhs_type, output_type):\n    tile_m, tile_n, tile_k = (64, 32, 128)\n    lhs = (np.arange(tile_m * tile_k) % 17).astype(lhs_type).reshape((tile_m, tile_k))\n    rhs = (np.arange(tile_k * tile_n) % 19).astype(rhs_type).reshape((tile_k, tile_n))\n    mask = np.tile([True, False], tile_m * tile_k // 2).reshape(lhs.shape)\n    sparse = lhs[mask].reshape(tile_m, tile_k // 2)\n    meta = nm.nm_pack(mask)\n    dot_sparse = nm.nm_spmm(sparse, rhs, meta, output_dtype=output_type)\n    dot_dense = lhs * mask @ rhs\n    jtu.check_close(dot_sparse, dot_dense.astype(output_type), rtol=0.01)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/tests/sparse_nm_test.py",
    "function": "def check_close(x, y, tol=0.001):\n    assert jnp.shape(x) == jnp.shape(y)\n    assert jnp.allclose(x, y, rtol=tol, atol=tol), f'Value mismatch:\\n{x}\\n  vs\\n{y}\\n'"
  }
]