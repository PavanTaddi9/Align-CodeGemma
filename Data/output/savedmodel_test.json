[
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    jax.random.normal(jax.random.key(0), 1000)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, abstracted_axes=(None, 'n'))\ndef f(x):\n    return x[0]"
  },
  {
    "test_code": "def test_save_grad_integers(self):\n    batch_size = 5\n    state = np.array([1], dtype=np.int32)\n    params = np.ones((3, 3), dtype=np.float32)\n\n    def tf_predict(params, state):\n\n        @tf.custom_gradient\n        def converted_fun_with_custom_gradient(params, state):\n            res_out = tf.zeros((batch_size, 2), dtype=tf.float32)\n            state_out = state\n            return ((res_out, state_out), converted_grad_fn)\n\n        def converted_grad_fn(res_out_ct, state_out_ct, variables=None):\n            return (tf.zeros(params.shape, dtype=params.dtype), state_out_ct)\n        res, state_out = converted_fun_with_custom_gradient(params, state)\n        return (res, state_out)\n    params_v = tf.Variable(params)\n    with tf.GradientTape() as tape:\n        preds = tf_predict(params_v, state)[0]\n        loss = tf.reduce_mean(preds)\n        g = tape.gradient(loss, params_v)\n    self.assertAllClose(g.numpy(), np.zeros(params.shape, dtype=params.dtype))\n    model = tf.Module()\n    model.fn = tf.function(tf_predict, autograph=False)\n    model.fn.get_concrete_function(tf.TensorSpec(params.shape, params.dtype), tf.TensorSpec(state.shape, state.dtype))\n    save_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(model)))\n    options = tf.saved_model.SaveOptions(experimental_custom_gradients=True)\n    _ = tf.saved_model.save(model, save_dir, options=options)\n    restored_module = tf.saved_model.load(save_dir)\n    restored_fn = restored_module.fn\n    with tf.GradientTape() as tape:\n        preds = restored_fn(params_v, state)[0]\n        loss = tf.reduce_mean(preds)\n        g = tape.gradient(loss, params_v)\n    self.assertAllClose(g.numpy(), np.zeros(params.shape, dtype=params.dtype))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def zeros(shape):\n    if not isinstance(shape, (tuple, list)):\n        shape = (shape,)\n    return lax.broadcast(jnp.float32(0.0), shape)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = x + 2\n    return (jnp.nan, y)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    x, y = jax.lax.cond(x < 3, lambda x, y: (x * 2, y), lambda x, y: (x * 3, y), x, y)\n    return (x, y)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return 0.0 / x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    weird_dict = {lambda x: x: 2.0, lambda x: x * 2: 3}\n    weirder_dict = {lambda x: x: weird_dict}\n    x = 2.0\n    debugger.breakpoint(stdin=stdin, stdout=stdout, backend='cli')\n    del weirder_dict\n    return x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    out = jnp.zeros_like(x)\n\n    def body(i, j, k, refs):\n        x_ref, out_ref = refs\n        y = func(x_ref[i, j, k])\n        out_ref[i, j, k] += y\n    return for_loop.for_loop(x.shape, body, (x, out))[1].sum()"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(a, b, c, d, e):\n    return (a, b, c, d, e)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jsp.special.betainc(jnp.ones(3), 1.0, x)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    return jnp.add(3.0, 4.0)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    z = jax_getattr(thing1, 'x')\n    w = jax_getattr(thing2, 'x')\n    out = jnp.sin(x * y * z * w)\n    jax_setattr(thing1, 'x', out)\n    jax_setattr(thing2, 'x', 2 * out)\n    return (3 * out, 4 * out)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.vmap\n@jax.value_and_grad\n@jax.named_scope('foo')\ndef f(x):\n\n    @jax.named_scope('scan_body')\n    def body(carry, x):\n        return (carry * x, carry + x)\n    return lax.scan(body, x, jnp.arange(8.0))[0]"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(c, a):\n    tracer_spy.append(c)\n    d = 0.75\n    b = jnp.sin(c * jnp.sum(jnp.cos(d * a)))\n    c = 0.9 * jnp.cos(d * jnp.sum(jnp.sin(c * a)))\n    return (c, b)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(pred):\n\n    def true_fun():\n        x_ref[()] = 1.0\n\n    def false_fun():\n        x_ref[()] = 2.0\n    jax.lax.cond(pred, true_fun, false_fun)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pjit.pjit, in_shardings=jax.sharding.PartitionSpec(None), out_shardings=jax.sharding.PartitionSpec('x'))\ndef f():\n    return jnp.zeros([32, 10])"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x, r):\n    x = x.at[:, 0].set(x[:, 0] / r)\n    return x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jnp.vectorize, excluded={1})\ndef f(x, y):\n    assert x.ndim == 0\n    assert y == 'foo'\n    return x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.pmap\ndef f(x, y):\n    return x * y"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x, y):\n    return jax.vmap(shard_alike, in_axes=(0, 1))(x, y)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    return random.uniform(self.make_key(3), (308000000, 128), dtype=jnp.bfloat16)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(params, x):\n        return x @ params['w'] + params['b']\n    x = np.ones((2, 3), dtype=np.float32)\n    params = dict(w=np.ones((3, 4), dtype=np.float32), b=np.ones((2, 4), dtype=np.float32))\n    res_jax = f_jax(params, x)\n    f_tf = jax2tf.convert(f_jax)\n    res_tf = f_tf(params, x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    restored_f, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=(params, x), save_gradients=True)\n    self.assertAllClose(restored_f(params, x).numpy(), res_tf.numpy())\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = f_tf(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_tf = tape.gradient(loss, params_v)\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = restored_f(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_restored_f = tape.gradient(loss, params_v)\n    self.assertAllClose(g_tf['w'].numpy(), g_restored_f['w'].numpy())\n    self.assertAllClose(g_tf['b'].numpy(), g_restored_f['b'].numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu', 'tpu')\ndef test_tf_mix_jax_with_uncompilableble(self):\n    \"\"\"Show how to combine TF-uncompilableble code with compiled JAX-converted code.\"\"\"\n\n    def tf_fn(x_str, compute_tf_fn=lambda x: x):\n        numbers_f32 = tf.strings.to_number(x_str, out_type=tf.float32)\n        numbers_f16 = tf.cast(numbers_f32, tf.float16)\n        return compute_tf_fn(numbers_f16)\n    x_str = np.array(['3.14', '2.78'])\n    with self.assertRaisesRegex(Exception, 'Detected unsupported operations when trying to compile graph'):\n        tf.function(tf_fn, jit_compile=True, autograph=False)(x_str)\n    composed_fn = lambda x_str: tf_fn(x_str, compute_tf_fn=tf.function(jax2tf.convert(jnp.sin), autograph=False, jit_compile=True))\n    res_tf = composed_fn(x_str)\n    self.assertAllClose(res_tf.numpy(), jnp.sin(np.array([3.14, 2.78], dtype=np.float16)))\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(composed_fn, input_args=[x_str])\n    res_tf_restored = restored_f(x_str)\n    self.assertAllClose(res_tf_restored.numpy(), res_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef convert(x):\n    return x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jax.lax.convert_element_type(x, FooTy())"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    y = x * jnp.arange(3.0).reshape((1, 3))\n    return jnp.take_along_axis(y, idx, -1).sum()"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())\ndef f(x):\n    return jax.lax.psum(((w * x) ** 2).sum(), 'i')"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f():\n    out = [rng(shape, dtype or jnp.float_) for shape, dtype in zip(shapes, dtypes)]\n    if np_arrays:\n        return out\n    return [jnp.asarray(a) if isinstance(a, (np.ndarray, np.generic)) else a for a in out]"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(seed):\n    key = jax.random.key(seed)\n    return jax.random.uniform(key) + jax.random.normal(key)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(params, x):\n        return x @ params['w'] + params['b']\n    x = np.ones((2, 3), dtype=np.float32)\n    params = dict(w=np.ones((3, 4), dtype=np.float32), b=np.ones((2, 4), dtype=np.float32))\n    res_jax = f_jax(params, x)\n    f_tf = jax2tf.convert(f_jax)\n    res_tf = f_tf(params, x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    restored_f, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=(params, x), save_gradients=True)\n    self.assertAllClose(restored_f(params, x).numpy(), res_tf.numpy())\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = f_tf(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_tf = tape.gradient(loss, params_v)\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = restored_f(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_restored_f = tape.gradient(loss, params_v)\n    self.assertAllClose(g_tf['w'].numpy(), g_restored_f['w'].numpy())\n    self.assertAllClose(g_tf['b'].numpy(), g_restored_f['b'].numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x_ref):\n    ones = jnp.ones_like(x_ref)[slc]\n    ref_primitives.ref_addupdate(x_ref, slc, ones)\n    x1 = ref_primitives.ref_get(x_ref, slc)\n    x2 = x1 + ones\n    ref_primitives.ref_set(x_ref, slc, x2)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(self, ys):\n    return lax.fori_loop(0, 10, loop_body, jnp.ones(4, np.float32))"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu', 'tpu')\ndef test_tf_mix_jax_with_uncompilableble(self):\n    \"\"\"Show how to combine TF-uncompilableble code with compiled JAX-converted code.\"\"\"\n\n    def tf_fn(x_str, compute_tf_fn=lambda x: x):\n        numbers_f32 = tf.strings.to_number(x_str, out_type=tf.float32)\n        numbers_f16 = tf.cast(numbers_f32, tf.float16)\n        return compute_tf_fn(numbers_f16)\n    x_str = np.array(['3.14', '2.78'])\n    with self.assertRaisesRegex(Exception, 'Detected unsupported operations when trying to compile graph'):\n        tf.function(tf_fn, jit_compile=True, autograph=False)(x_str)\n    composed_fn = lambda x_str: tf_fn(x_str, compute_tf_fn=tf.function(jax2tf.convert(jnp.sin), autograph=False, jit_compile=True))\n    res_tf = composed_fn(x_str)\n    self.assertAllClose(res_tf.numpy(), jnp.sin(np.array([3.14, 2.78], dtype=np.float16)))\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(composed_fn, input_args=[x_str])\n    res_tf_restored = restored_f(x_str)\n    self.assertAllClose(res_tf_restored.numpy(), res_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jax.pure_callback(np.sin, x, x)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(hx, _):\n    hx = jax.nn.sigmoid(hx + a)\n    return (hx, None)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    u, s, v = jnp.linalg.svd(a + x * b, full_matrices=full_matrices, compute_uv=compute_uv)\n    vdiag = jnp.vectorize(jnp.diag, signature='(k)->(k,k)')\n    return jnp.matmul(jnp.matmul(u, vdiag(s).astype(u.dtype)), v).real"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.profiler.annotate_function\ndef f(x, *, name):\n    return x + 2 * len(name)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\ndef f(x):\n    return x * 2"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(z):\n    x_re = jnp.concatenate([jnp.real(z), jnp.imag(z)])\n    return f_re(x_re)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@run_state\ndef f(x_ref):\n    x = x_ref[...]\n\n    def _body(ref):\n        ref[...] = jnp.sin(ref[...])\n    x = run_state(_body)(x)\n    x_ref[...] = x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(new_checkpoint, policy=policy)\ndef f(x):\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.einsum('ij,jk->ik', x, x, precision=lax.Precision.HIGHEST)\n    x = jnp.sin(x)\n    x = jnp.sum(x)\n    return x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(jax.jit, static_argnames=['c'])\ndef f(x, *, c):\n    return c * jnp.sin(x)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(params, x):\n        return x @ params['w'] + params['b']\n    x = np.ones((2, 3), dtype=np.float32)\n    params = dict(w=np.ones((3, 4), dtype=np.float32), b=np.ones((2, 4), dtype=np.float32))\n    res_jax = f_jax(params, x)\n    f_tf = jax2tf.convert(f_jax)\n    res_tf = f_tf(params, x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    restored_f, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=(params, x), save_gradients=True)\n    self.assertAllClose(restored_f(params, x).numpy(), res_tf.numpy())\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = f_tf(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_tf = tape.gradient(loss, params_v)\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = restored_f(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_restored_f = tape.gradient(loss, params_v)\n    self.assertAllClose(g_tf['w'].numpy(), g_restored_f['w'].numpy())\n    self.assertAllClose(g_tf['b'].numpy(), g_restored_f['b'].numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n\n    def f_jax_inner(x):\n        return testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingUnorderedEffect1')\n    return 10.0 + jax.jit(f_jax_inner)(x) + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect1') + testing_primitive_with_effect_p.bind(x, effect_class_name='ForTestingOrderedEffect2')"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n\n    @partial(map_fun, in_axes=1, out_axes=2)\n    def h(y):\n        return jnp.sin(x + y)\n    return h(y).sum()"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(carry, x):\n    return (carry + jax.lax.psum(jnp.sum(x), axis_name='x'), None)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(k1, k2, k3, k4):\n    batch_size = 1\n    seq_len = 1\n    input_size = 1\n    hidden_size = 1\n    bidirectional = False\n    num_directions = 2 if bidirectional else 1\n    num_layers = 1\n    x = jax.random.normal(k1, (batch_size, seq_len, input_size), dtype=jnp.float32)\n    h_0 = jax.random.normal(k2, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    c_0 = jax.random.normal(k3, (num_directions * num_layers, batch_size, hidden_size), dtype=jnp.float32)\n    seq_lengths = jnp.ones((batch_size,), dtype=jnp.int32) * seq_len\n    weights = rnn.init_lstm_weight(k4, input_size, hidden_size, num_layers, bidirectional)\n    return rnn.lstm(x, h_0, c_0, weights, seq_lengths=seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=False, bidirectional=bidirectional)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@colocated_python.colocated_python\ndef f(x):\n    out_arrays = []\n    for shard in x.addressable_shards:\n        np_array = jax.device_get(shard.data)\n        input_ints = struct.unpack('<ii', base64.b64decode(np_array[0].encode('ascii')))\n        output_string = base64.b64encode(struct.pack('<ii', input_ints[0] + 1, input_ints[1] + 1)).decode('ascii')\n        out_np_array = np.array([output_string], dtype=np.dtypes.StringDType())\n        out_arrays.append(jax.device_put(out_np_array, device=shard.device))\n    out = jax.make_array_from_single_device_arrays(sharding=x.sharding, shape=x.shape, arrays=out_arrays)\n    return out"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    s = jit(jnp.sin)(x)\n    return jnp.sin(s) + jnp.cos(y)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, PartitionSpec('x')), out_shardings=NamedSharding(mesh, PartitionSpec('x')), compiler_options=compiler_options)\ndef f(x):\n    agg = x\n    for _ in range(its):\n        agg = agg @ x\n    return agg"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(_, y):\n    input_effect(y, index=0)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@checkify.checkify\ndef f():\n    checkify.check(x > 0, 'must be positive!')\n    return jnp.log(x)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu', 'tpu')\ndef test_tf_mix_jax_with_uncompilableble(self):\n    \"\"\"Show how to combine TF-uncompilableble code with compiled JAX-converted code.\"\"\"\n\n    def tf_fn(x_str, compute_tf_fn=lambda x: x):\n        numbers_f32 = tf.strings.to_number(x_str, out_type=tf.float32)\n        numbers_f16 = tf.cast(numbers_f32, tf.float16)\n        return compute_tf_fn(numbers_f16)\n    x_str = np.array(['3.14', '2.78'])\n    with self.assertRaisesRegex(Exception, 'Detected unsupported operations when trying to compile graph'):\n        tf.function(tf_fn, jit_compile=True, autograph=False)(x_str)\n    composed_fn = lambda x_str: tf_fn(x_str, compute_tf_fn=tf.function(jax2tf.convert(jnp.sin), autograph=False, jit_compile=True))\n    res_tf = composed_fn(x_str)\n    self.assertAllClose(res_tf.numpy(), jnp.sin(np.array([3.14, 2.78], dtype=np.float16)))\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(composed_fn, input_args=[x_str])\n    res_tf_restored = restored_f(x_str)\n    self.assertAllClose(res_tf_restored.numpy(), res_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(z):\n    y = odeint(dz_dt, z, jnp.arange(10.0))\n    return jnp.sum(y)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return x.at[1].apply(jax.numpy.sin)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    bits = prng.threefry_random_bits(jnp.array([0, 0], dtype='uint32'), 32, x.shape)\n    return bits + x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, in_shardings=NamedSharding(mesh, P('x')), out_shardings=NamedSharding(mesh, P('x')))\ndef f(x, y):\n    z = x @ y\n    return z @ y"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    debugging.inspect_array_sharding(x, callback=_cb)\n    return jnp.square(x)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(jax.jit, inline=True)\ndef f(x):\n    return lax.add(x, 3)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n    key = jax.random.key(0)\n    return jax.random.bits(key) + jax.random.bits(key)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    nonlocal num_traces\n    num_traces += 1\n    return x + x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(params, x):\n        return x @ params['w'] + params['b']\n    x = np.ones((2, 3), dtype=np.float32)\n    params = dict(w=np.ones((3, 4), dtype=np.float32), b=np.ones((2, 4), dtype=np.float32))\n    res_jax = f_jax(params, x)\n    f_tf = jax2tf.convert(f_jax)\n    res_tf = f_tf(params, x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    restored_f, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=(params, x), save_gradients=True)\n    self.assertAllClose(restored_f(params, x).numpy(), res_tf.numpy())\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = f_tf(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_tf = tape.gradient(loss, params_v)\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = restored_f(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_restored_f = tape.gradient(loss, params_v)\n    self.assertAllClose(g_tf['w'].numpy(), g_restored_f['w'].numpy())\n    self.assertAllClose(g_tf['b'].numpy(), g_restored_f['b'].numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x, y):\n    return jnp.matmul(x, y)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu', 'tpu')\ndef test_tf_mix_jax_with_uncompilableble(self):\n    \"\"\"Show how to combine TF-uncompilableble code with compiled JAX-converted code.\"\"\"\n\n    def tf_fn(x_str, compute_tf_fn=lambda x: x):\n        numbers_f32 = tf.strings.to_number(x_str, out_type=tf.float32)\n        numbers_f16 = tf.cast(numbers_f32, tf.float16)\n        return compute_tf_fn(numbers_f16)\n    x_str = np.array(['3.14', '2.78'])\n    with self.assertRaisesRegex(Exception, 'Detected unsupported operations when trying to compile graph'):\n        tf.function(tf_fn, jit_compile=True, autograph=False)(x_str)\n    composed_fn = lambda x_str: tf_fn(x_str, compute_tf_fn=tf.function(jax2tf.convert(jnp.sin), autograph=False, jit_compile=True))\n    res_tf = composed_fn(x_str)\n    self.assertAllClose(res_tf.numpy(), jnp.sin(np.array([3.14, 2.78], dtype=np.float16)))\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(composed_fn, input_args=[x_str])\n    res_tf_restored = restored_f(x_str)\n    self.assertAllClose(res_tf_restored.numpy(), res_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_jvp\ndef sin(x):\n    return jnp.sin(x)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(a, b):\n    with set_xla_metadata(key2='val2'):\n        return a + b"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(n):\n    token = lax.create_token(n)\n    token = lax.fori_loop(0, n, doubler, token)\n    return n"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.full_like(x, 2.0)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(b):\n    return jax.scipy.sparse.linalg.cg(matvec, (b, b))[0]"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n\n    @jax.jit\n    def g(x):\n        return x * y\n    return g(x) + g(y)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(Xtree, y):\n    if deep:\n        out = Xtree['deep']['X'] @ y\n    else:\n        out = Xtree['X'] @ y\n    if bias:\n        out += Xtree['list'][1][0]\n    out = jnp.sum(out)\n    if has_aux:\n        return (out, {'y': y.shape})\n    else:\n        return out"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    y = jnp.array([2, 5])\n    return lax.rev(x * y, (0,))"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.argmax(x)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return x + c"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.cos(jnp.sum(jnp.exp(-x)) ** 2)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, y):\n    z = jax.numpy.add(x, y)\n    return self.pallas_call(kernel, grid=(3,), in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))], out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)), out_shape=x, compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]))(z)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    return pl.pallas_call(kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), grid=(iters,), in_specs=(pl.BlockSpec(x.shape, lambda i: (0, 0)),), out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)), interpret=mosaic_interpret.TPUInterpretParams())(x)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros_like(x)\n\n    def inner(refs):\n        x_ref, y_ref = refs\n\n        @pl.core_map(mesh)\n        def _():\n            num_cores = jax.lax.psum(1, 'x')\n            slc_size = 16 // num_cores\n\n            def alloc(x_vmem_ref, y_vmem_ref, sem):\n                core_index = jax.lax.axis_index('x')\n                slc = pl.ds(core_index * slc_size, slc_size)\n                pltpu.async_copy(x_ref.at[slc], x_vmem_ref, sem).wait()\n                y = x_vmem_ref[...] + jax.lax.axis_index('x')\n                y_vmem_ref[...] = y\n                pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n            pl.run_scoped(alloc, pltpu.VMEM((slc_size, 128), x_ref.dtype), pltpu.VMEM((slc_size, 128), y_ref.dtype), pltpu.SemaphoreType.DMA)\n    _, y = pl.run_state(inner)((x, y))\n    return y"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jax.nn.relu(x)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    y = jnp.zeros(x.shape[1:], x.dtype)\n\n    def body(refs):\n        copy_start, copy_done = make_stateful_async_slice(2)\n        x_ref, y_ref = refs\n        fut = copy_start(x_ref, y_ref)\n        copy_done(x_ref, y_ref, fut)\n    _, y = state_discharge.run_state(body)((x, y))\n    return y"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x, *, eager=False):\n\n    def copy_one(x_ref, o_ref):\n        o_ref[...] = x_ref[...]\n    grid = (2, 2)\n    block_shape = (x.shape[0] // grid[0], x.shape[1] // grid[1])\n    return pl.pallas_call(copy_one, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype), in_specs=[pl.BlockSpec(block_shape, lambda i, j: (i, j))], out_specs=pl.BlockSpec(block_shape, lambda i, j: (i, j)), grid=grid, interpret=eager and jtu.test_device_matches(['cpu']))(x)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32))\ndef f(x_ref, o_ref):\n    x = x_ref[...]\n    o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)"
  },
  {
    "test_code": "def test_save_grad_integers(self):\n    batch_size = 5\n    state = np.array([1], dtype=np.int32)\n    params = np.ones((3, 3), dtype=np.float32)\n\n    def tf_predict(params, state):\n\n        @tf.custom_gradient\n        def converted_fun_with_custom_gradient(params, state):\n            res_out = tf.zeros((batch_size, 2), dtype=tf.float32)\n            state_out = state\n            return ((res_out, state_out), converted_grad_fn)\n\n        def converted_grad_fn(res_out_ct, state_out_ct, variables=None):\n            return (tf.zeros(params.shape, dtype=params.dtype), state_out_ct)\n        res, state_out = converted_fun_with_custom_gradient(params, state)\n        return (res, state_out)\n    params_v = tf.Variable(params)\n    with tf.GradientTape() as tape:\n        preds = tf_predict(params_v, state)[0]\n        loss = tf.reduce_mean(preds)\n        g = tape.gradient(loss, params_v)\n    self.assertAllClose(g.numpy(), np.zeros(params.shape, dtype=params.dtype))\n    model = tf.Module()\n    model.fn = tf.function(tf_predict, autograph=False)\n    model.fn.get_concrete_function(tf.TensorSpec(params.shape, params.dtype), tf.TensorSpec(state.shape, state.dtype))\n    save_dir = os.path.join(absltest.get_default_test_tmpdir(), str(id(model)))\n    options = tf.saved_model.SaveOptions(experimental_custom_gradients=True)\n    _ = tf.saved_model.save(model, save_dir, options=options)\n    restored_module = tf.saved_model.load(save_dir)\n    restored_fn = restored_module.fn\n    with tf.GradientTape() as tape:\n        preds = restored_fn(params_v, state)[0]\n        loss = tf.reduce_mean(preds)\n        g = tape.gradient(loss, params_v)\n    self.assertAllClose(g.numpy(), np.zeros(params.shape, dtype=params.dtype))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((m, n), floatx))\ndef load(x_ref, o_ref):\n    x = pl.load(x_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]))\n    pl.store(o_ref, (jnp.arange(m)[:, None], jnp.arange(n)[None, :]), x + 1.0)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(params, x):\n        return x @ params['w'] + params['b']\n    x = np.ones((2, 3), dtype=np.float32)\n    params = dict(w=np.ones((3, 4), dtype=np.float32), b=np.ones((2, 4), dtype=np.float32))\n    res_jax = f_jax(params, x)\n    f_tf = jax2tf.convert(f_jax)\n    res_tf = f_tf(params, x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    restored_f, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=(params, x), save_gradients=True)\n    self.assertAllClose(restored_f(params, x).numpy(), res_tf.numpy())\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = f_tf(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_tf = tape.gradient(loss, params_v)\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = restored_f(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_restored_f = tape.gradient(loss, params_v)\n    self.assertAllClose(g_tf['w'].numpy(), g_restored_f['w'].numpy())\n    self.assertAllClose(g_tf['b'].numpy(), g_restored_f['b'].numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu', 'tpu')\ndef test_tf_mix_jax_with_uncompilableble(self):\n    \"\"\"Show how to combine TF-uncompilableble code with compiled JAX-converted code.\"\"\"\n\n    def tf_fn(x_str, compute_tf_fn=lambda x: x):\n        numbers_f32 = tf.strings.to_number(x_str, out_type=tf.float32)\n        numbers_f16 = tf.cast(numbers_f32, tf.float16)\n        return compute_tf_fn(numbers_f16)\n    x_str = np.array(['3.14', '2.78'])\n    with self.assertRaisesRegex(Exception, 'Detected unsupported operations when trying to compile graph'):\n        tf.function(tf_fn, jit_compile=True, autograph=False)(x_str)\n    composed_fn = lambda x_str: tf_fn(x_str, compute_tf_fn=tf.function(jax2tf.convert(jnp.sin), autograph=False, jit_compile=True))\n    res_tf = composed_fn(x_str)\n    self.assertAllClose(res_tf.numpy(), jnp.sin(np.array([3.14, 2.78], dtype=np.float16)))\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(composed_fn, input_args=[x_str])\n    res_tf_restored = restored_f(x_str)\n    self.assertAllClose(res_tf_restored.numpy(), res_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=out_shape, grid=grid)\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_dtype)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu', 'tpu')\ndef test_tf_mix_jax_with_uncompilableble(self):\n    \"\"\"Show how to combine TF-uncompilableble code with compiled JAX-converted code.\"\"\"\n\n    def tf_fn(x_str, compute_tf_fn=lambda x: x):\n        numbers_f32 = tf.strings.to_number(x_str, out_type=tf.float32)\n        numbers_f16 = tf.cast(numbers_f32, tf.float16)\n        return compute_tf_fn(numbers_f16)\n    x_str = np.array(['3.14', '2.78'])\n    with self.assertRaisesRegex(Exception, 'Detected unsupported operations when trying to compile graph'):\n        tf.function(tf_fn, jit_compile=True, autograph=False)(x_str)\n    composed_fn = lambda x_str: tf_fn(x_str, compute_tf_fn=tf.function(jax2tf.convert(jnp.sin), autograph=False, jit_compile=True))\n    res_tf = composed_fn(x_str)\n    self.assertAllClose(res_tf.numpy(), jnp.sin(np.array([3.14, 2.78], dtype=np.float16)))\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(composed_fn, input_args=[x_str])\n    res_tf_restored = restored_f(x_str)\n    self.assertAllClose(res_tf_restored.numpy(), res_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), floatx), grid=(4,))\ndef sin(x_ref, o_ref):\n    i = pl.program_id(0)\n    o_ref[i] = jnp.sin(x_ref[i])"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(params, x):\n        return x @ params['w'] + params['b']\n    x = np.ones((2, 3), dtype=np.float32)\n    params = dict(w=np.ones((3, 4), dtype=np.float32), b=np.ones((2, 4), dtype=np.float32))\n    res_jax = f_jax(params, x)\n    f_tf = jax2tf.convert(f_jax)\n    res_tf = f_tf(params, x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    restored_f, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=(params, x), save_gradients=True)\n    self.assertAllClose(restored_f(params, x).numpy(), res_tf.numpy())\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = f_tf(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_tf = tape.gradient(loss, params_v)\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = restored_f(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_restored_f = tape.gradient(loss, params_v)\n    self.assertAllClose(g_tf['w'].numpy(), g_restored_f['w'].numpy())\n    self.assertAllClose(g_tf['b'].numpy(), g_restored_f['b'].numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu', 'tpu')\ndef test_tf_mix_jax_with_uncompilableble(self):\n    \"\"\"Show how to combine TF-uncompilableble code with compiled JAX-converted code.\"\"\"\n\n    def tf_fn(x_str, compute_tf_fn=lambda x: x):\n        numbers_f32 = tf.strings.to_number(x_str, out_type=tf.float32)\n        numbers_f16 = tf.cast(numbers_f32, tf.float16)\n        return compute_tf_fn(numbers_f16)\n    x_str = np.array(['3.14', '2.78'])\n    with self.assertRaisesRegex(Exception, 'Detected unsupported operations when trying to compile graph'):\n        tf.function(tf_fn, jit_compile=True, autograph=False)(x_str)\n    composed_fn = lambda x_str: tf_fn(x_str, compute_tf_fn=tf.function(jax2tf.convert(jnp.sin), autograph=False, jit_compile=True))\n    res_tf = composed_fn(x_str)\n    self.assertAllClose(res_tf.numpy(), jnp.sin(np.array([3.14, 2.78], dtype=np.float16)))\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(composed_fn, input_args=[x_str])\n    res_tf_restored = restored_f(x_str)\n    self.assertAllClose(res_tf_restored.numpy(), res_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(pl.pallas_call, out_shape=out_shape, compiler_params=plgpu.GPUCompilerParams(thread_semantics=thread_semantics))\ndef convert(x_ref, y_ref):\n    y_ref[...] = jax.lax.bitcast_convert_type(x_ref[...], out_shape)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f():\n\n    @pl.run_state\n    def inner(y_ref):\n\n        @pl.core_map(mesh)\n        def kernel():\n\n            def scoped(barrier):\n                plgpu.barrier_arrive(barrier)\n                plgpu.barrier_wait(barrier)\n                wg_idx = jax.lax.axis_index('wg')\n                y_ref[wg_idx] = jnp.broadcast_to(wg_idx, (128,))\n            pl.run_scoped(scoped, plgpu.Barrier(num_arrivals=2))\n    y_init = jnp.zeros((2, 128), np.int32)\n    return inner(y_init)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@functools.partial(self.pallas_call, out_shape=jax.ShapeDtypeStruct((4,), jnp.float64))\ndef f(x_ref, y_ref):\n\n    def body(i, acc):\n        return acc + x_ref[...] + i * 0\n    y_ref[...] = lax.fori_loop(0, 3, body, jnp.zeros((4,), jnp.float64))"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.jit\ndef f(x):\n    for i in range(SIZE):\n        x = g(x, x)\n    return x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(a, b):\n    c = jnp.dot(a, b)\n    return jnp.tanh(c)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(params, x):\n        return x @ params['w'] + params['b']\n    x = np.ones((2, 3), dtype=np.float32)\n    params = dict(w=np.ones((3, 4), dtype=np.float32), b=np.ones((2, 4), dtype=np.float32))\n    res_jax = f_jax(params, x)\n    f_tf = jax2tf.convert(f_jax)\n    res_tf = f_tf(params, x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    restored_f, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=(params, x), save_gradients=True)\n    self.assertAllClose(restored_f(params, x).numpy(), res_tf.numpy())\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = f_tf(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_tf = tape.gradient(loss, params_v)\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = restored_f(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_restored_f = tape.gradient(loss, params_v)\n    self.assertAllClose(g_tf['w'].numpy(), g_restored_f['w'].numpy())\n    self.assertAllClose(g_tf['b'].numpy(), g_restored_f['b'].numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "@jtu.skip_on_devices('gpu', 'tpu')\ndef test_tf_mix_jax_with_uncompilableble(self):\n    \"\"\"Show how to combine TF-uncompilableble code with compiled JAX-converted code.\"\"\"\n\n    def tf_fn(x_str, compute_tf_fn=lambda x: x):\n        numbers_f32 = tf.strings.to_number(x_str, out_type=tf.float32)\n        numbers_f16 = tf.cast(numbers_f32, tf.float16)\n        return compute_tf_fn(numbers_f16)\n    x_str = np.array(['3.14', '2.78'])\n    with self.assertRaisesRegex(Exception, 'Detected unsupported operations when trying to compile graph'):\n        tf.function(tf_fn, jit_compile=True, autograph=False)(x_str)\n    composed_fn = lambda x_str: tf_fn(x_str, compute_tf_fn=tf.function(jax2tf.convert(jnp.sin), autograph=False, jit_compile=True))\n    res_tf = composed_fn(x_str)\n    self.assertAllClose(res_tf.numpy(), jnp.sin(np.array([3.14, 2.78], dtype=np.float16)))\n    restored_f, _ = tf_test_util.SaveAndLoadFunction(composed_fn, input_args=[x_str])\n    res_tf_restored = restored_f(x_str)\n    self.assertAllClose(res_tf_restored.numpy(), res_tf.numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@partial(api_util.api_hook, tag='jax2tf_convert')\ndef convert(fun_jax: Callable, *, polymorphic_shapes: str | None=None, polymorphic_constraints: Sequence[str]=(), with_gradient: bool=True, enable_xla: bool=True, native_serialization: bool | _DefaultNativeSerialization=DEFAULT_NATIVE_SERIALIZATION, native_serialization_platforms: Sequence[str] | None=None, native_serialization_disabled_checks: Sequence[DisabledSafetyCheck]=()) -> Callable:\n    \"\"\"Allows calling a JAX function from a TensorFlow program.\n\n  See\n  [README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md)\n  for more details about usage and common problems.\n\n  Args:\n    fun_jax: target JAX function to be called. Its arguments and return value\n      should be JAX arrays, or nested standard Python containers\n      (tuple/list/dict) thereof (pytrees).\n    polymorphic_shapes: Specifies input shapes to be treated polymorphically\n      during lowering.\n\n      .. warning:: The shape-polymorphic lowering is an experimental feature.\n        It is meant to be sound, but it is known to reject some JAX programs\n        that are shape polymorphic. The details of this feature can change.\n\n      It should be `None` (all arguments are monomorphic), a single PolyShape\n      or string (applies to all arguments), or a tuple/list of the same length\n      as the function arguments. For each argument the shape specification\n      should be `None` (monomorphic argument), or a Python object with the\n      same pytree structure as the argument.\n      See [how optional parameters are matched to\n      arguments](https://jax.readthedocs.io/en/latest/pytrees.html#applying-optional-parameters-to-pytrees).\n\n      A shape specification for an array argument should be an object\n      `PolyShape(dim0, dim1, ..., dimn)`\n      where each `dim` is a dimension specification: a positive integer denoting\n      a monomorphic dimension of the given size, or a string denoting a\n      dimension variable assumed to range over non-zero dimension sizes, or\n      the special placeholder string \"_\" denoting a monomorphic dimension\n      whose size is given by the actual argument. As a shortcut, an Ellipsis\n      suffix in the list of dimension specifications stands for a list of \"_\"\n      placeholders.\n\n      For convenience, a shape specification can also be given as a string\n      representation, e.g.: \"batch, ...\", \"batch, height, width, _\", possibly\n      with surrounding parentheses: \"(batch, ...)\".\n\n      The lowering fails if it cannot ensure that the it would produce the same\n      sequence of TF ops for any non-zero values of the dimension variables.\n\n      polymorphic_shapes are only supported for positional arguments; shape\n      polymorphism is not supported for keyword arguments.\n\n      See [the README](https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#shape-polymorphic-conversion)\n      for more details.\n\n    polymorphic_constraints: a sequence of constraints on symbolic dimension\n      expressions, of the form `e1 >= e2` or `e1 <= e2`.\n      See more details at https://github.com/jax-ml/jax/blob/main/jax/experimental/jax2tf/README.md#user-specified-symbolic-constraints.\n    with_gradient: if set (default), add a tf.custom_gradient to the lowered\n      function, by converting the ``jax.vjp(fun)``. This means that reverse-mode\n      TensorFlow AD is supported for the output TensorFlow function, and the\n      value of the gradient will be JAX-accurate.\n    enable_xla: if set (default), use the simplest conversion\n      and use XLA TF ops when necessary. These ops are known to create issues\n      for the TFLite and TFjs converters. For those cases, unset this parameter\n      so the lowering tries harder to use non-XLA TF ops to lower the\n      function and aborts if this is not possible. Cannot be set to `False`\n      when using `native_serialization`.\n      Starting with JAX 0.4.31 support for `enable_xla=False` is deprecated.\n    native_serialization: serialize the JAX function natively to\n      StableHLO with compatibility guarantees. This makes it easier to have\n      confidence that the code executed when calling this function from\n      TensorFlow is exactly the same as JAX would run natively.\n      The DEFAULT_NATIVE_SERIALIZATION value defers to `False` if `enable_xla`\n      is set to `False` or to the configuration flag\n      `--jax2tf_default_native_serialization` otherwise.\n      Native serialization cannot be used with `enable_xla=False`.\n      Starting with JAX 0.4.31 support for non-native serialization is deprecated.\n    native_serialization_platforms: In conjunction with\n      `native_serialization`, specify the platform(s)\n      for which to lower the code. Must be a tuple of\n      strings, including a subset of: 'cpu', 'cuda', 'rocm', 'tpu'.\n      The default (`None``), specifies the JAX default\n      backend on the machine where the lowering is done.\n    native_serialization_disabled_checks: In conjunction with\n      `native_serialization`, disable the specified safety checks.\n      See docstring of `DisabledSafetyCheck`.\n\n  Returns:\n    A version of `fun_jax` that expects TfVals as arguments (or\n    tuple/lists/dicts thereof), and returns TfVals as outputs, and uses\n    only TensorFlow ops and thus can be called from a TensorFlow program.\n  \"\"\"\n    if native_serialization is DEFAULT_NATIVE_SERIALIZATION:\n        if not enable_xla:\n            native_serialization = False\n        else:\n            native_serialization = config.jax2tf_default_native_serialization.value\n    if not enable_xla:\n        if allow_enable_xla_false():\n            warnings.warn('jax2tf.convert with enable_xla=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n            if native_serialization:\n                raise ValueError('native_serialization is not supported with enable_xla=False')\n        else:\n            raise ValueError('jax2tf.convert with enable_xla=False has been deprecated since July 2024 and it is not supported anymore.')\n    elif not native_serialization:\n        if allow_native_serialization_false():\n            warnings.warn('jax2tf.convert with native_serialization=False has been deprecated since July 2024.', DeprecationWarning, stacklevel=2)\n        else:\n            raise ValueError('jax2tf.convert with native_serialization=False has been deprecated since July 2024 and it is not supported anymore.')\n    if not native_serialization and polymorphic_constraints:\n        raise ValueError('polymorphic_constraints are supported only with native serialization')\n    if native_serialization_platforms:\n        if not native_serialization:\n            warnings.warn('using native_serialization_platforms without native_serialization. The parameter will have no effect, since the same code is serialized for all platforms without native_serialization.')\n        if not isinstance(native_serialization_platforms, (list, tuple)) or not all((p in ['cpu', 'cuda', 'rocm', 'tpu'] for p in native_serialization_platforms)):\n            raise ValueError(f\"native_serialization_platforms must be a sequence containing a subset of {{'cpu', 'cuda', 'rocm', 'tpu'}}. Got: {native_serialization_platforms}\")\n        native_serialization_platforms = tuple(native_serialization_platforms)\n    api.check_callable(fun_jax)\n\n    def converted_fun_tf(*args_tf: TfVal, **kwargs_tf: TfVal) -> TfVal:\n        if not core.trace_state_clean() and (not _thread_local_state.inside_call_tf):\n            raise ValueError('convert must be used outside all JAX transformations.' + f'Trace state: {core.trace_ctx}')\n        global _has_registered_tf_source_path\n        if not _has_registered_tf_source_path:\n            source_info_util.register_exclusion(os.path.dirname(tf.__file__))\n            _has_registered_tf_source_path = True\n\n        def jax_arg_spec_from_tf(a: TfVal) -> jax.ShapeDtypeStruct:\n            tf_arg_shape = np.shape(a)\n            tf_arg_shape = tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))\n            _, a_jax_dtype = _tfval_to_tensor_jax_dtype(a)\n            return jax.ShapeDtypeStruct(tf_arg_shape, a_jax_dtype)\n        args_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, args_tf)\n        args_specs = export.symbolic_args_specs(args_jax_specs, polymorphic_shapes, constraints=polymorphic_constraints)\n        kwargs_jax_specs = tree_util.tree_map(jax_arg_spec_from_tf, kwargs_tf)\n        kwargs_specs = export.symbolic_args_specs(kwargs_jax_specs, None)\n        combined_args_tf = (args_tf, kwargs_tf)\n        args_flat_tf: Sequence[TfVal]\n        args_flat_tf, args_kwargs_tree = tree_util.tree_flatten(combined_args_tf)\n        args_flat_tf = tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf))\n        impl: SerializationImpl\n        if native_serialization:\n            impl = NativeSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, native_serialization_platforms=native_serialization_platforms, native_serialization_disabled_checks=native_serialization_disabled_checks)\n        else:\n            impl = GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)\n        try:\n            impl.before_conversion()\n            outs_tree: tree_util.PyTreeDef = None\n            if with_gradient:\n\n                @tf.custom_gradient\n                def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) -> TfVal:\n                    nonlocal outs_tree\n                    outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)\n                    return (tuple(outs_tf), _make_custom_gradient_fn_tf(fun_jax, impl=impl, with_gradient=with_gradient, args_specs=args_specs, kwargs_specs=kwargs_specs, args_tf=args_flat_tf, outs_avals=outs_avals, outs_tf=outs_tf))\n                outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)\n            else:\n                outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf)\n                message = 'The jax2tf-converted function does not support gradients. Use `with_gradient` parameter to enable gradients'\n                outs_flat_tf = [tf.raw_ops.PreventGradient(input=o, message=message) for o in outs_tf]\n        finally:\n            impl.after_conversion()\n        outs_flat_tf = [tf.identity(x, 'jax2tf_out') for x in outs_flat_tf]\n        out_tf = tree_util.tree_unflatten(outs_tree, outs_flat_tf)\n        return out_tf\n    return converted_fun_tf"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "@jax.custom_vjp\ndef f(x):\n    return x * x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(params, x):\n        return x @ params['w'] + params['b']\n    x = np.ones((2, 3), dtype=np.float32)\n    params = dict(w=np.ones((3, 4), dtype=np.float32), b=np.ones((2, 4), dtype=np.float32))\n    res_jax = f_jax(params, x)\n    f_tf = jax2tf.convert(f_jax)\n    res_tf = f_tf(params, x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    restored_f, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=(params, x), save_gradients=True)\n    self.assertAllClose(restored_f(params, x).numpy(), res_tf.numpy())\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = f_tf(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_tf = tape.gradient(loss, params_v)\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = restored_f(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_restored_f = tape.gradient(loss, params_v)\n    self.assertAllClose(g_tf['w'].numpy(), g_restored_f['w'].numpy())\n    self.assertAllClose(g_tf['b'].numpy(), g_restored_f['b'].numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(xs):\n\n    @jax.remat\n    def body_fun(carry, x):\n        return (carry * x, xs)\n    res1, res2 = lax.scan(body_fun, 0.0, xs + 1.0)\n    return jnp.sum(res1) + jnp.sum(res2)"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_pytree(self):\n\n    def f_jax(params, x):\n        return x @ params['w'] + params['b']\n    x = np.ones((2, 3), dtype=np.float32)\n    params = dict(w=np.ones((3, 4), dtype=np.float32), b=np.ones((2, 4), dtype=np.float32))\n    res_jax = f_jax(params, x)\n    f_tf = jax2tf.convert(f_jax)\n    res_tf = f_tf(params, x)\n    self.assertAllClose(res_jax, res_tf.numpy())\n    restored_f, restored_model = tf_test_util.SaveAndLoadFunction(f_tf, input_args=(params, x), save_gradients=True)\n    self.assertAllClose(restored_f(params, x).numpy(), res_tf.numpy())\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = f_tf(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_tf = tape.gradient(loss, params_v)\n    params_v = tf.nest.map_structure(tf.Variable, params)\n    with tf.GradientTape() as tape:\n        res = restored_f(params_v, x)\n        loss = tf.reduce_sum(res)\n        g_restored_f = tape.gradient(loss, params_v)\n    self.assertAllClose(g_tf['w'].numpy(), g_restored_f['w'].numpy())\n    self.assertAllClose(g_tf['b'].numpy(), g_restored_f['b'].numpy())",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f_jax(x):\n    key = random.PRNGKey(123)\n    broadcast_keys = lax.broadcast_in_dim(key, x.shape, ())\n    gather_keys = lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))\n    slice_keys1 = lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))\n    slice_keys2 = lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))\n    upd1 = lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))\n    _ = lax.dynamic_update_slice(upd1, gather_keys, start_indices=(0, 0))\n    xs = broadcast_keys\n    counts = jnp.arange(broadcast_keys.shape[0], dtype=np.int32)\n\n    def f_vmap_jax(counts, xs):\n\n        def inner(count, x):\n            return lax.fori_loop(0, count, lambda _, acc: acc, x)\n        return jax.vmap(inner)(counts, xs)\n    _ = f_vmap_jax(counts, xs)\n    return x"
  },
  {
    "test_code": "def test_eval(self):\n    f_jax = jax.jit(lambda x: jnp.sin(jnp.cos(x)))\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    self.assertAllClose(restored_model.f(x), f_jax(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_gradient(self):\n    \"\"\"Save and restore the custom gradient.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_gradient_nested(self):\n    \"\"\"Save and restore the custom gradient, when combined with other TF code.\"\"\"\n\n    @jax.custom_jvp\n    def f_jax(x):\n        return x * x\n\n    @f_jax.defjvp\n    def f_jax_jvp(primals, tangents):\n        x, = primals\n        x_dot, = tangents\n        primal_out = f_jax(x)\n        tangent_out = x * x_dot * 3.0\n        return (primal_out, tangent_out)\n    model = tf.Module()\n    model.f = tf.function(lambda x: tf.math.sin(jax2tf.convert(f_jax, with_gradient=True)(x)), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    f_jax_equiv = lambda x: jnp.sin(f_jax(x))\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax_equiv(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(x)\n    self.assertAllClose(restored_model.f(x), f_jax_equiv(x))\n    with tf.GradientTape() as tape:\n        y = restored_model.f(xv)\n    self.assertAllClose(tape.gradient(y, xv).numpy(), jax.grad(f_jax_equiv)(x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_gradient_disabled(self):\n    f_jax = lambda x: x * x\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=False), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model)\n    xv = tf.Variable(0.7, dtype=jnp.float32)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    with self.assertRaisesRegex(LookupError, 'Gradient explicitly disabled.*The jax2tf-converted function does not support gradients'):\n        with tf.GradientTape():\n            _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_save_without_gradients(self):\n    f_jax = lambda x: x * x\n    x = np.array(0.7, dtype=jnp.float32)\n    model = tf.Module()\n    model.f = tf.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])\n    self.assertAllClose(model.f(x), f_jax(x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), f_jax(x))\n    xv = tf.Variable(x)\n    with tf.GradientTape():\n        _ = restored_model.f(xv)",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  },
  {
    "test_code": "def test_save_without_embedding_params(self):\n\n    def model_jax(params, inputs):\n        return params[0] + params[1] * inputs\n    params = (np.array(1.0, dtype=jnp.float32), np.array(2.0, dtype=jnp.float32))\n    params_vars = tf.nest.map_structure(tf.Variable, params)\n    prediction_tf = lambda x: jax2tf.convert(model_jax)(params_vars, x)\n    model = tf.Module()\n    model._variables = tf.nest.flatten(params_vars)\n    model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)\n    x = np.array(0.7, dtype=jnp.float32)\n    self.assertAllClose(model.f(x), model_jax(params, x))\n    restored_model = tf_test_util.SaveAndLoadModel(model, save_gradients=False)\n    self.assertAllClose(restored_model.f(x), model_jax(params, x))",
    "assertions": [],
    "test_file": "/var/folders/q5/p2sqhr0d6nqb_h8x_fxyxpz80000gn/T/tmp_1jzy6em/jax/jax/experimental/jax2tf/tests/savedmodel_test.py",
    "function": "def f(x):\n    return jnp.sum(x, axis=0) * x.shape[0]"
  }
]